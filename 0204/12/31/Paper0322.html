<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Self-supervised visual representation learning for histopathological images | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Self-supervised visual representation learning for histopathological images" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Pengshuai Yang, Zhiwei Hong, Xiaoxu Yin, Chengzhan Zhu, Rui Jiang Abstract Self-supervised learning provides a possible solution to extract effective visual representations from unlabeled histopathological images. However, existing methods either fail to make good use of domain-specific knowledge, or rely on side information like spatial proximity and magnification. In this paper, we propose CS-CO, a hybrid self-supervised visual representation learning method tailored for histopathological images, which integrates advantages of both generative and discriminative models. The proposed method consists of two self-supervised learning stages: cross-stain prediction (CS) and contrastive learning (CO), both of which are designed based on domain-specific knowledge and do not require side information. A novel data augmentation approach, stain vector perturbation, is specifically proposed to serve contrastive learning. Experimental results on the public dataset NCT-CRC-HE-100K demonstrate the superiority of the proposed method for histopathological image visual representation. Under the common linear evaluation protocol, our method achieves 0.915 eight-class classification accuracy with only 1,000 labeled data, which is about 1.3% higher than the fully-supervised ResNet18 classifier trained with the whole 89,434 labeled training data. Our code is available at https://github.com/easonyang1996/CS-CO. Link to paper https://doi.org/10.1007/978-3-030-87196-3_5 Link to the code repository https://github.com/easonyang1996/CS-CO Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper Firstly, the authors design a new pretext task, i.e. cross-stain prediction, for self-supervised learning, aiming to make good use of the domain specic knowledge of histopathological images. Secondly, they propose a new data augmentation approach, i.e. stain vector perturbation, to serve histopathological image contrastive learning. Finally, they integrate the advantages of generative and discriminative approaches and build a hybrid self-supervised visual representation learning framework for histopathological images. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper proposes the CS-CO method for histopathological images, which is divided into two self-supervised learning stages. In the first stage, a pretext task is proposed for histopathological images named cross-stain prediction, and the second stage uses the stain vector perturbation for data augmentation and contrastive learning. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. (1) The figure and text description are inconsistent, and the corresponding description of Part C and D in Fig. 1 is insufficient. (2) The visualization of stain vector perturbation in Fig. 2 is not obvious, which is of little significance in this paper. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No source code provided, but the dataset and implementation details are described clearly, so the results would be reproduced. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html (1) The implementation details of the second stage, i.e., contrastive learning are not clear. (2) The motivation of the approach of stain vector perturbation is insufficient. Why is the new data augmentation approach more suitable for histopathological images? Its novelty should be highlighted clearer. (3) The content of H and E channel images is different. Why can cross prediction be carried out? What is the basis for this? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novel self-supervised learning method and good experimental results What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper presents a novel framework for self-supervised visual representation learning for histipathological images. The novelty is related to architecture based on two self-supervised learning stages: the first that learns visual representation in the cross-stain prediction task, and the second that is based on contrastive learning and further trains encoders for visual representation learning. Furthermore, a new data augmentation approach named stain vector perturbation is also introduced to serve contrastive learning. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strength of this paper is fully self-supervised computational pipeline for visual representation learning for histological images. In addition to contrastive learning stage the important contribution is the cross-stain prediction task that learns hameleon-to-eosin and eosin-to-hameleon encoders from stain-separated images. Learned encoders are used to initialize related ones in the contrastive learning stage. Further strength is novel data augmentation method coined stain vector perturbation that is based on statin separation error from the first stage. It is shown in an ablation study that proposed data augmentation method plays a crucial role in achieving state-of-the-art performance on public colorectal carcinoma dataset. That brings us to the third strength of this paper: achieved eight-class prediction accuracy is higher than the one obtained by fully supervised ResNet18 model. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. I did not see the catastrophic weakness this paper. However, there is inconsistency between description of the contrastive learning process in section 2.4 and corresponding stage in Figure 1. To be specific, there is no predictor q-prime in Figure 1. Also, in part (d) of Figure 1 both learned encoder hameleon-to-eosin and learned encoder eosin-to-hameleon are used for the final visual representation. However, eosin-to-hameleon encoder is learned in cross-stain prediction (pretext) stage only? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Based on reported information all reproducibility requirements are met. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html There is inconsistency between description of the contrastive learning process in section 2.4 and corresponding stage in Figure 1. To be specific, there is no predictor q-prime in Figure 1. Also, in part (d) of Figure 1 both learned encoder hameleon-to-eosin and learned encoder eosin-to-hameleon are used for the final visual representation. However, eosin-to-hameleon encoder is learned in cross-stain prediction (pretext) stage only? In the forthcoming work the authors should perform statistical significance analysis of the results obtained by their method. It appears to me that in Eq.(7) one regularization constant is enough to take care about relative importance of the two terms in the total loss. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The main strength of this paper is fully self-supervised computational pipeline for visual representation learning for histological images. In addition to contrastive learning stage the important contribution is the cross-stain prediction task that learns hameleon-to-eosin and eosin-to-hameleon encoders from stain-separated images. Learned encoders are used to initialize related ones in the contrastive learning stage. Further strength is novel data augmentation method coined stain vector perturbation that is based on stain separation error from the first stage. It is shown in an ablation study that proposed data augmentation method plays a crucial role in achieving state-of-the-art performance on public colorectal carcinoma dataset. That brings us to the third strength of this paper: achieved eight-class prediction accuracy is higher than the one obtained by fully supervised ResNet18 model. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The color deconvolution is used to generate H and E images for auto-encoder. A stain vector perturbation is proposed as data augmentation for contrastive learning. A hybrid system is built for contrastive learning in histopathology. The performance of using 1000 cases for a linear classifier achieves better performance than the fully supervised learning method with fully supervised learning. Ablation study shows the impacts of different components. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The framework (c) contrastive learning in Fig 1 is the most innovative part in this paper. The frozen HE is used to avoid the model collapse. Two encoders are used to achieve representations from H and E respectively to utilize the domain knowledge of pathological images. The decoder is freezer to avoid model collapse. Ablation study is a strong evaluation in this paper to show the impacts of different pieces of the method as the design contains many different elements. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. It is a little bit weird that ResNet18 is used as the backbone for Table 1, without good explanations. In the BYOL or SimSiam paper, the ResNet50 is typically used for real imaging dataset (rather than ResNet18 for CIFAR10). The rationale of using normalized L2 distance as the loss function of the contrastive learning is not clear. It is not clear what data augmentations are used for training the resnet18 supervised learning approach. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance good Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Maybe the ResNet50 is a more reasonable backbone. More training details such as number of epochs, GPU resources would be helpful for readers to reproduce the work. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Good performance, inspiring idea What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper presents a novel framework for self-supervised learning for histopathological images, based on two stages: i) a pretext cross-stain prediction task, and ii) contrastive learning using a new data augmentation approach named stain vector perturbation. All reviewers acknowledge the novelty of the paper and suggest accepting the paper. Yet authors could further improve their manuscript by incorporating reviews comments. Additional to these comments, I have a concern that whether the relatively slow speed of Sparse NMF stain decomposition will affect contrastive training and maybe the authors can discuss it a bit. Also, it is also worth to carry an ablation study to show which component (cross-stain prediction or contrastive learning) contribute more to the boosted performance. Finally, authors can also consider release their code to promote reusability. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback Response to Reviewer #1 We thank the reviewer for the valuable feedback. We will revise Fig. 1 and the corresponding description according to the comments. As for Fig. 2, we put it in the manuscript to make readers better understand the effect of stain vector perturbation (SVP). The proposed SVP disturbs the estimated stain vector matrix and affects the results of stain separation. The degree of difference between results of original stain separation and results with SVP is related to the strength of sigma (standard deviation of normal disturbance). As for the basis of cross-stain prediction, we suppose circular blank areas in E channel image are more likely to be nuclei, and blank areas between nuclei in H channel image are more likely to be extracellular matrix or cytoplasm. Therefore, though the content of H and E channel images is different, the cross-stain prediction still can be carried out. As for SVP, it is specially designed for histopathological images based on the domain-specific knowledge. It can introduce variances into the stain-separated single channel images, which is suitable for contrastive learning. In addition, SVP can also make the encoder robust to the error of stain vector estimation in the process of visual representation extraction. Because of the anonymity, we didn’t provide the source code in the manuscript. We will release the source code soon. Response to Reviewer #3 We thank the reviewer for the valuable feedback. We will revise Fig. 1 and the corresponding description according to the comments. For Eq. (7), we will use only one lambda in the upcoming camera-ready version. In our manuscript, q-prime is not a predictor. We denote the projector as ‘f’ and the predictor as ‘g’. z and z-prime are the outputs of ‘f’, and q and q-prime are the outputs of ‘g’. We train both H2E encoder and E2H encoder in the cross-stain prediction stage. For the sake of simplicity, we denote the combination of H2E encoder and E2H encoder as HE encoder ϕ in our manuscript. ϕ is also trained in the contrastive learning stage. After two-stage training, the visual representations are extracted using ϕ. Response to Reviewer #4 We thank the reviewer for the valuable feedback. We choose ResNet18 as the backbone because of its small computation cost. ResNet50 also works under the proposed self-supervised learning scheme. The loss of contrastive learning is the same as BYOL, and it actually evaluates the cosine distance. Maybe the ICML2020 paper “Understanding contrastive representation learning through alignment and uniformity on the hypersphere” could give further explanation. As for the fully-supervised ResNet18, we didn’t do data augmentation during training. The total number of epochs is set to 100, but early-stopping is used to avoid overfitting. The GPU we used to train the model is RTX 3090. We will release the source code to facilitate reproducing. Response to Meta-Reviews We thank the reviewer for the valuable feedback, and we will revise the manuscript according to the reviewers’ comments. The sparse NMF is indeed a little slow but bearable. Maybe in practice, the user could do stain vector perturbation once and save the results locally to accelerate contrastive learning. We have carried ablation study in our manuscript. If only contrastive learning is performed, the proposed method is equivalent to Simsiam, so we didn’t report the performance in the section of ablation study. Because of the anonymity, we didn’t provide the source code in the manuscript. We will release the source code soon. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Pengshuai Yang, Zhiwei Hong, Xiaoxu Yin, Chengzhan Zhu, Rui Jiang Abstract Self-supervised learning provides a possible solution to extract effective visual representations from unlabeled histopathological images. However, existing methods either fail to make good use of domain-specific knowledge, or rely on side information like spatial proximity and magnification. In this paper, we propose CS-CO, a hybrid self-supervised visual representation learning method tailored for histopathological images, which integrates advantages of both generative and discriminative models. The proposed method consists of two self-supervised learning stages: cross-stain prediction (CS) and contrastive learning (CO), both of which are designed based on domain-specific knowledge and do not require side information. A novel data augmentation approach, stain vector perturbation, is specifically proposed to serve contrastive learning. Experimental results on the public dataset NCT-CRC-HE-100K demonstrate the superiority of the proposed method for histopathological image visual representation. Under the common linear evaluation protocol, our method achieves 0.915 eight-class classification accuracy with only 1,000 labeled data, which is about 1.3% higher than the fully-supervised ResNet18 classifier trained with the whole 89,434 labeled training data. Our code is available at https://github.com/easonyang1996/CS-CO. Link to paper https://doi.org/10.1007/978-3-030-87196-3_5 Link to the code repository https://github.com/easonyang1996/CS-CO Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper Firstly, the authors design a new pretext task, i.e. cross-stain prediction, for self-supervised learning, aiming to make good use of the domain specic knowledge of histopathological images. Secondly, they propose a new data augmentation approach, i.e. stain vector perturbation, to serve histopathological image contrastive learning. Finally, they integrate the advantages of generative and discriminative approaches and build a hybrid self-supervised visual representation learning framework for histopathological images. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper proposes the CS-CO method for histopathological images, which is divided into two self-supervised learning stages. In the first stage, a pretext task is proposed for histopathological images named cross-stain prediction, and the second stage uses the stain vector perturbation for data augmentation and contrastive learning. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. (1) The figure and text description are inconsistent, and the corresponding description of Part C and D in Fig. 1 is insufficient. (2) The visualization of stain vector perturbation in Fig. 2 is not obvious, which is of little significance in this paper. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No source code provided, but the dataset and implementation details are described clearly, so the results would be reproduced. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html (1) The implementation details of the second stage, i.e., contrastive learning are not clear. (2) The motivation of the approach of stain vector perturbation is insufficient. Why is the new data augmentation approach more suitable for histopathological images? Its novelty should be highlighted clearer. (3) The content of H and E channel images is different. Why can cross prediction be carried out? What is the basis for this? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novel self-supervised learning method and good experimental results What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper presents a novel framework for self-supervised visual representation learning for histipathological images. The novelty is related to architecture based on two self-supervised learning stages: the first that learns visual representation in the cross-stain prediction task, and the second that is based on contrastive learning and further trains encoders for visual representation learning. Furthermore, a new data augmentation approach named stain vector perturbation is also introduced to serve contrastive learning. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strength of this paper is fully self-supervised computational pipeline for visual representation learning for histological images. In addition to contrastive learning stage the important contribution is the cross-stain prediction task that learns hameleon-to-eosin and eosin-to-hameleon encoders from stain-separated images. Learned encoders are used to initialize related ones in the contrastive learning stage. Further strength is novel data augmentation method coined stain vector perturbation that is based on statin separation error from the first stage. It is shown in an ablation study that proposed data augmentation method plays a crucial role in achieving state-of-the-art performance on public colorectal carcinoma dataset. That brings us to the third strength of this paper: achieved eight-class prediction accuracy is higher than the one obtained by fully supervised ResNet18 model. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. I did not see the catastrophic weakness this paper. However, there is inconsistency between description of the contrastive learning process in section 2.4 and corresponding stage in Figure 1. To be specific, there is no predictor q-prime in Figure 1. Also, in part (d) of Figure 1 both learned encoder hameleon-to-eosin and learned encoder eosin-to-hameleon are used for the final visual representation. However, eosin-to-hameleon encoder is learned in cross-stain prediction (pretext) stage only? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Based on reported information all reproducibility requirements are met. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html There is inconsistency between description of the contrastive learning process in section 2.4 and corresponding stage in Figure 1. To be specific, there is no predictor q-prime in Figure 1. Also, in part (d) of Figure 1 both learned encoder hameleon-to-eosin and learned encoder eosin-to-hameleon are used for the final visual representation. However, eosin-to-hameleon encoder is learned in cross-stain prediction (pretext) stage only? In the forthcoming work the authors should perform statistical significance analysis of the results obtained by their method. It appears to me that in Eq.(7) one regularization constant is enough to take care about relative importance of the two terms in the total loss. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The main strength of this paper is fully self-supervised computational pipeline for visual representation learning for histological images. In addition to contrastive learning stage the important contribution is the cross-stain prediction task that learns hameleon-to-eosin and eosin-to-hameleon encoders from stain-separated images. Learned encoders are used to initialize related ones in the contrastive learning stage. Further strength is novel data augmentation method coined stain vector perturbation that is based on stain separation error from the first stage. It is shown in an ablation study that proposed data augmentation method plays a crucial role in achieving state-of-the-art performance on public colorectal carcinoma dataset. That brings us to the third strength of this paper: achieved eight-class prediction accuracy is higher than the one obtained by fully supervised ResNet18 model. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The color deconvolution is used to generate H and E images for auto-encoder. A stain vector perturbation is proposed as data augmentation for contrastive learning. A hybrid system is built for contrastive learning in histopathology. The performance of using 1000 cases for a linear classifier achieves better performance than the fully supervised learning method with fully supervised learning. Ablation study shows the impacts of different components. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The framework (c) contrastive learning in Fig 1 is the most innovative part in this paper. The frozen HE is used to avoid the model collapse. Two encoders are used to achieve representations from H and E respectively to utilize the domain knowledge of pathological images. The decoder is freezer to avoid model collapse. Ablation study is a strong evaluation in this paper to show the impacts of different pieces of the method as the design contains many different elements. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. It is a little bit weird that ResNet18 is used as the backbone for Table 1, without good explanations. In the BYOL or SimSiam paper, the ResNet50 is typically used for real imaging dataset (rather than ResNet18 for CIFAR10). The rationale of using normalized L2 distance as the loss function of the contrastive learning is not clear. It is not clear what data augmentations are used for training the resnet18 supervised learning approach. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance good Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Maybe the ResNet50 is a more reasonable backbone. More training details such as number of epochs, GPU resources would be helpful for readers to reproduce the work. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Good performance, inspiring idea What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper presents a novel framework for self-supervised learning for histopathological images, based on two stages: i) a pretext cross-stain prediction task, and ii) contrastive learning using a new data augmentation approach named stain vector perturbation. All reviewers acknowledge the novelty of the paper and suggest accepting the paper. Yet authors could further improve their manuscript by incorporating reviews comments. Additional to these comments, I have a concern that whether the relatively slow speed of Sparse NMF stain decomposition will affect contrastive training and maybe the authors can discuss it a bit. Also, it is also worth to carry an ablation study to show which component (cross-stain prediction or contrastive learning) contribute more to the boosted performance. Finally, authors can also consider release their code to promote reusability. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback Response to Reviewer #1 We thank the reviewer for the valuable feedback. We will revise Fig. 1 and the corresponding description according to the comments. As for Fig. 2, we put it in the manuscript to make readers better understand the effect of stain vector perturbation (SVP). The proposed SVP disturbs the estimated stain vector matrix and affects the results of stain separation. The degree of difference between results of original stain separation and results with SVP is related to the strength of sigma (standard deviation of normal disturbance). As for the basis of cross-stain prediction, we suppose circular blank areas in E channel image are more likely to be nuclei, and blank areas between nuclei in H channel image are more likely to be extracellular matrix or cytoplasm. Therefore, though the content of H and E channel images is different, the cross-stain prediction still can be carried out. As for SVP, it is specially designed for histopathological images based on the domain-specific knowledge. It can introduce variances into the stain-separated single channel images, which is suitable for contrastive learning. In addition, SVP can also make the encoder robust to the error of stain vector estimation in the process of visual representation extraction. Because of the anonymity, we didn’t provide the source code in the manuscript. We will release the source code soon. Response to Reviewer #3 We thank the reviewer for the valuable feedback. We will revise Fig. 1 and the corresponding description according to the comments. For Eq. (7), we will use only one lambda in the upcoming camera-ready version. In our manuscript, q-prime is not a predictor. We denote the projector as ‘f’ and the predictor as ‘g’. z and z-prime are the outputs of ‘f’, and q and q-prime are the outputs of ‘g’. We train both H2E encoder and E2H encoder in the cross-stain prediction stage. For the sake of simplicity, we denote the combination of H2E encoder and E2H encoder as HE encoder ϕ in our manuscript. ϕ is also trained in the contrastive learning stage. After two-stage training, the visual representations are extracted using ϕ. Response to Reviewer #4 We thank the reviewer for the valuable feedback. We choose ResNet18 as the backbone because of its small computation cost. ResNet50 also works under the proposed self-supervised learning scheme. The loss of contrastive learning is the same as BYOL, and it actually evaluates the cosine distance. Maybe the ICML2020 paper “Understanding contrastive representation learning through alignment and uniformity on the hypersphere” could give further explanation. As for the fully-supervised ResNet18, we didn’t do data augmentation during training. The total number of epochs is set to 100, but early-stopping is used to avoid overfitting. The GPU we used to train the model is RTX 3090. We will release the source code to facilitate reproducing. Response to Meta-Reviews We thank the reviewer for the valuable feedback, and we will revise the manuscript according to the reviewers’ comments. The sparse NMF is indeed a little slow but bearable. Maybe in practice, the user could do stain vector perturbation once and save the results locally to accelerate contrastive learning. We have carried ablation study in our manuscript. If only contrastive learning is performed, the proposed method is equivalent to Simsiam, so we didn’t report the performance in the section of ablation study. Because of the anonymity, we didn’t provide the source code in the manuscript. We will release the source code soon. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0204/12/31/Paper0322" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0204/12/31/Paper0322" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0204-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Self-supervised visual representation learning for histopathological images" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0204/12/31/Paper0322"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0204/12/31/Paper0322","headline":"Self-supervised visual representation learning for histopathological images","dateModified":"0205-01-01T00:00:00-05:17","datePublished":"0204-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Pengshuai Yang, Zhiwei Hong, Xiaoxu Yin, Chengzhan Zhu, Rui Jiang Abstract Self-supervised learning provides a possible solution to extract effective visual representations from unlabeled histopathological images. However, existing methods either fail to make good use of domain-specific knowledge, or rely on side information like spatial proximity and magnification. In this paper, we propose CS-CO, a hybrid self-supervised visual representation learning method tailored for histopathological images, which integrates advantages of both generative and discriminative models. The proposed method consists of two self-supervised learning stages: cross-stain prediction (CS) and contrastive learning (CO), both of which are designed based on domain-specific knowledge and do not require side information. A novel data augmentation approach, stain vector perturbation, is specifically proposed to serve contrastive learning. Experimental results on the public dataset NCT-CRC-HE-100K demonstrate the superiority of the proposed method for histopathological image visual representation. Under the common linear evaluation protocol, our method achieves 0.915 eight-class classification accuracy with only 1,000 labeled data, which is about 1.3% higher than the fully-supervised ResNet18 classifier trained with the whole 89,434 labeled training data. Our code is available at https://github.com/easonyang1996/CS-CO. Link to paper https://doi.org/10.1007/978-3-030-87196-3_5 Link to the code repository https://github.com/easonyang1996/CS-CO Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper Firstly, the authors design a new pretext task, i.e. cross-stain prediction, for self-supervised learning, aiming to make good use of the domain specic knowledge of histopathological images. Secondly, they propose a new data augmentation approach, i.e. stain vector perturbation, to serve histopathological image contrastive learning. Finally, they integrate the advantages of generative and discriminative approaches and build a hybrid self-supervised visual representation learning framework for histopathological images. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper proposes the CS-CO method for histopathological images, which is divided into two self-supervised learning stages. In the first stage, a pretext task is proposed for histopathological images named cross-stain prediction, and the second stage uses the stain vector perturbation for data augmentation and contrastive learning. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. (1) The figure and text description are inconsistent, and the corresponding description of Part C and D in Fig. 1 is insufficient. (2) The visualization of stain vector perturbation in Fig. 2 is not obvious, which is of little significance in this paper. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No source code provided, but the dataset and implementation details are described clearly, so the results would be reproduced. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html (1) The implementation details of the second stage, i.e., contrastive learning are not clear. (2) The motivation of the approach of stain vector perturbation is insufficient. Why is the new data augmentation approach more suitable for histopathological images? Its novelty should be highlighted clearer. (3) The content of H and E channel images is different. Why can cross prediction be carried out? What is the basis for this? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novel self-supervised learning method and good experimental results What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper presents a novel framework for self-supervised visual representation learning for histipathological images. The novelty is related to architecture based on two self-supervised learning stages: the first that learns visual representation in the cross-stain prediction task, and the second that is based on contrastive learning and further trains encoders for visual representation learning. Furthermore, a new data augmentation approach named stain vector perturbation is also introduced to serve contrastive learning. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strength of this paper is fully self-supervised computational pipeline for visual representation learning for histological images. In addition to contrastive learning stage the important contribution is the cross-stain prediction task that learns hameleon-to-eosin and eosin-to-hameleon encoders from stain-separated images. Learned encoders are used to initialize related ones in the contrastive learning stage. Further strength is novel data augmentation method coined stain vector perturbation that is based on statin separation error from the first stage. It is shown in an ablation study that proposed data augmentation method plays a crucial role in achieving state-of-the-art performance on public colorectal carcinoma dataset. That brings us to the third strength of this paper: achieved eight-class prediction accuracy is higher than the one obtained by fully supervised ResNet18 model. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. I did not see the catastrophic weakness this paper. However, there is inconsistency between description of the contrastive learning process in section 2.4 and corresponding stage in Figure 1. To be specific, there is no predictor q-prime in Figure 1. Also, in part (d) of Figure 1 both learned encoder hameleon-to-eosin and learned encoder eosin-to-hameleon are used for the final visual representation. However, eosin-to-hameleon encoder is learned in cross-stain prediction (pretext) stage only? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Based on reported information all reproducibility requirements are met. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html There is inconsistency between description of the contrastive learning process in section 2.4 and corresponding stage in Figure 1. To be specific, there is no predictor q-prime in Figure 1. Also, in part (d) of Figure 1 both learned encoder hameleon-to-eosin and learned encoder eosin-to-hameleon are used for the final visual representation. However, eosin-to-hameleon encoder is learned in cross-stain prediction (pretext) stage only? In the forthcoming work the authors should perform statistical significance analysis of the results obtained by their method. It appears to me that in Eq.(7) one regularization constant is enough to take care about relative importance of the two terms in the total loss. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The main strength of this paper is fully self-supervised computational pipeline for visual representation learning for histological images. In addition to contrastive learning stage the important contribution is the cross-stain prediction task that learns hameleon-to-eosin and eosin-to-hameleon encoders from stain-separated images. Learned encoders are used to initialize related ones in the contrastive learning stage. Further strength is novel data augmentation method coined stain vector perturbation that is based on stain separation error from the first stage. It is shown in an ablation study that proposed data augmentation method plays a crucial role in achieving state-of-the-art performance on public colorectal carcinoma dataset. That brings us to the third strength of this paper: achieved eight-class prediction accuracy is higher than the one obtained by fully supervised ResNet18 model. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The color deconvolution is used to generate H and E images for auto-encoder. A stain vector perturbation is proposed as data augmentation for contrastive learning. A hybrid system is built for contrastive learning in histopathology. The performance of using 1000 cases for a linear classifier achieves better performance than the fully supervised learning method with fully supervised learning. Ablation study shows the impacts of different components. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The framework (c) contrastive learning in Fig 1 is the most innovative part in this paper. The frozen HE is used to avoid the model collapse. Two encoders are used to achieve representations from H and E respectively to utilize the domain knowledge of pathological images. The decoder is freezer to avoid model collapse. Ablation study is a strong evaluation in this paper to show the impacts of different pieces of the method as the design contains many different elements. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. It is a little bit weird that ResNet18 is used as the backbone for Table 1, without good explanations. In the BYOL or SimSiam paper, the ResNet50 is typically used for real imaging dataset (rather than ResNet18 for CIFAR10). The rationale of using normalized L2 distance as the loss function of the contrastive learning is not clear. It is not clear what data augmentations are used for training the resnet18 supervised learning approach. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance good Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Maybe the ResNet50 is a more reasonable backbone. More training details such as number of epochs, GPU resources would be helpful for readers to reproduce the work. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Good performance, inspiring idea What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper presents a novel framework for self-supervised learning for histopathological images, based on two stages: i) a pretext cross-stain prediction task, and ii) contrastive learning using a new data augmentation approach named stain vector perturbation. All reviewers acknowledge the novelty of the paper and suggest accepting the paper. Yet authors could further improve their manuscript by incorporating reviews comments. Additional to these comments, I have a concern that whether the relatively slow speed of Sparse NMF stain decomposition will affect contrastive training and maybe the authors can discuss it a bit. Also, it is also worth to carry an ablation study to show which component (cross-stain prediction or contrastive learning) contribute more to the boosted performance. Finally, authors can also consider release their code to promote reusability. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback Response to Reviewer #1 We thank the reviewer for the valuable feedback. We will revise Fig. 1 and the corresponding description according to the comments. As for Fig. 2, we put it in the manuscript to make readers better understand the effect of stain vector perturbation (SVP). The proposed SVP disturbs the estimated stain vector matrix and affects the results of stain separation. The degree of difference between results of original stain separation and results with SVP is related to the strength of sigma (standard deviation of normal disturbance). As for the basis of cross-stain prediction, we suppose circular blank areas in E channel image are more likely to be nuclei, and blank areas between nuclei in H channel image are more likely to be extracellular matrix or cytoplasm. Therefore, though the content of H and E channel images is different, the cross-stain prediction still can be carried out. As for SVP, it is specially designed for histopathological images based on the domain-specific knowledge. It can introduce variances into the stain-separated single channel images, which is suitable for contrastive learning. In addition, SVP can also make the encoder robust to the error of stain vector estimation in the process of visual representation extraction. Because of the anonymity, we didn’t provide the source code in the manuscript. We will release the source code soon. Response to Reviewer #3 We thank the reviewer for the valuable feedback. We will revise Fig. 1 and the corresponding description according to the comments. For Eq. (7), we will use only one lambda in the upcoming camera-ready version. In our manuscript, q-prime is not a predictor. We denote the projector as ‘f’ and the predictor as ‘g’. z and z-prime are the outputs of ‘f’, and q and q-prime are the outputs of ‘g’. We train both H2E encoder and E2H encoder in the cross-stain prediction stage. For the sake of simplicity, we denote the combination of H2E encoder and E2H encoder as HE encoder ϕ in our manuscript. ϕ is also trained in the contrastive learning stage. After two-stage training, the visual representations are extracted using ϕ. Response to Reviewer #4 We thank the reviewer for the valuable feedback. We choose ResNet18 as the backbone because of its small computation cost. ResNet50 also works under the proposed self-supervised learning scheme. The loss of contrastive learning is the same as BYOL, and it actually evaluates the cosine distance. Maybe the ICML2020 paper “Understanding contrastive representation learning through alignment and uniformity on the hypersphere” could give further explanation. As for the fully-supervised ResNet18, we didn’t do data augmentation during training. The total number of epochs is set to 100, but early-stopping is used to avoid overfitting. The GPU we used to train the model is RTX 3090. We will release the source code to facilitate reproducing. Response to Meta-Reviews We thank the reviewer for the valuable feedback, and we will revise the manuscript according to the reviewers’ comments. The sparse NMF is indeed a little slow but bearable. Maybe in practice, the user could do stain vector perturbation once and save the results locally to accelerate contrastive learning. We have carried ablation study in our manuscript. If only contrastive learning is performed, the proposed method is equivalent to Simsiam, so we didn’t report the performance in the section of ablation study. Because of the anonymity, we didn’t provide the source code in the manuscript. We will release the source code soon. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Yang, Pengshuai,Hong, Zhiwei,Yin, Xiaoxu,Zhu, Chengzhan,Jiang, Rui" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Self-supervised visual representation learning for histopathological images</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a>
      
      <a 
        href="kittywong/categories#Computational (Integrative) Pathology"
        class="post-category">
        Computational (Integrative) Pathology
      </a>
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a>
      
      <a 
        href="kittywong/categories#Image Reconstruction"
        class="post-category">
        Image Reconstruction
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Interpretability / Explainability"
        class="post-category">
        Machine Learning - Interpretability / Explainability
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Semi-supervised learning"
        class="post-category">
        Machine Learning - Semi-supervised learning
      </a>
      
      <a 
        href="kittywong/categories#Modalities - Histopathology"
        class="post-category">
        Modalities - Histopathology
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Yang, Pengshuai"
        class="post-tags">
        Yang, Pengshuai
      </a> |  
      
      <a href="kittywong/tags#Hong, Zhiwei"
        class="post-tags">
        Hong, Zhiwei
      </a> |  
      
      <a href="kittywong/tags#Yin, Xiaoxu"
        class="post-tags">
        Yin, Xiaoxu
      </a> |  
      
      <a href="kittywong/tags#Zhu, Chengzhan"
        class="post-tags">
        Zhu, Chengzhan
      </a> |  
      
      <a href="kittywong/tags#Jiang, Rui"
        class="post-tags">
        Jiang, Rui
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Pengshuai Yang, Zhiwei Hong, Xiaoxu Yin, Chengzhan Zhu, Rui Jiang
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Self-supervised learning provides a possible solution to extract effective visual representations from unlabeled histopathological images. However, existing methods either fail to make good use of domain-specific knowledge, or rely on side information like spatial proximity and magnification. In this paper, we propose CS-CO, a hybrid self-supervised visual representation learning method tailored for histopathological images, which integrates advantages of both generative and discriminative models. The proposed method consists of two self-supervised learning stages: cross-stain prediction (CS) and contrastive learning (CO), both of which are designed based on domain-specific knowledge and do not require side information. A novel data augmentation approach, stain vector perturbation, is specifically proposed to serve contrastive learning. Experimental results on the public dataset NCT-CRC-HE-100K demonstrate the superiority of the proposed method for histopathological image visual representation. Under the common linear evaluation protocol, our method achieves 0.915 eight-class classification accuracy with only 1,000 labeled data, which is about 1.3% higher than the fully-supervised ResNet18 classifier trained with the whole 89,434 labeled training data. Our code is available at https://github.com/easonyang1996/CS-CO. 
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87196-3_5">https://doi.org/10.1007/978-3-030-87196-3_5</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/easonyang1996/CS-CO
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>Firstly, the authors design a new pretext task, i.e. cross-stain prediction, for self-supervised learning, aiming to make good use of the domain specic knowledge of histopathological images.
Secondly, they propose a new data augmentation approach, i.e. stain vector perturbation, to serve histopathological image contrastive learning.
Finally, they integrate the advantages of generative and discriminative approaches and build a hybrid self-supervised visual representation learning framework for histopathological images.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>This paper proposes the CS-CO method for histopathological images, which is divided into two self-supervised learning stages. In the first stage, a pretext task is proposed for histopathological images named cross-stain prediction, and the second stage uses the stain vector perturbation for data augmentation and contrastive learning.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>(1) The figure and text description are inconsistent, and the corresponding description of Part C and D in Fig. 1 is insufficient.
(2) The visualization of stain vector perturbation in Fig. 2 is not obvious, which is of little significance in this paper.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>No source code provided, but the dataset and implementation details are described clearly, so the results would be reproduced.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>(1) The implementation details of the second stage, i.e., contrastive learning are not clear. 
(2) The motivation of the approach of stain vector perturbation is insufficient. Why is the new data augmentation approach more suitable for histopathological images? Its novelty should be highlighted clearer.
(3) The content of H and E channel images is different. Why can cross prediction be carried out? What is the basis for this?</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Novel self-supervised learning method and good experimental results</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper presents a novel framework for self-supervised visual representation learning for histipathological images. The novelty is related to architecture based on two self-supervised learning stages: the first that learns visual representation in the cross-stain prediction task, and the second that is based on contrastive learning and further trains encoders for visual representation learning. Furthermore, a new data augmentation approach named stain vector perturbation is also introduced to serve contrastive learning.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The main strength of this paper is fully self-supervised computational pipeline for visual representation learning for histological images. In addition to contrastive learning stage the important contribution is the cross-stain prediction task that learns hameleon-to-eosin and eosin-to-hameleon encoders from stain-separated images. Learned encoders are used to initialize related ones in the contrastive learning stage. Further strength is novel data augmentation method coined stain vector perturbation that is based on statin separation error from the first stage. It is shown in an ablation study that proposed data augmentation method plays a crucial role in achieving state-of-the-art performance on public colorectal carcinoma dataset. That brings us to the third strength of this paper: achieved eight-class prediction accuracy is higher than the one obtained by fully supervised ResNet18 model.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>I did not see the catastrophic weakness this paper. However, there is inconsistency between description of the contrastive learning process in section 2.4 and corresponding stage in Figure 1. To be specific, there is no predictor q-prime in Figure 1. Also, in part (d) of Figure 1 both learned encoder hameleon-to-eosin and learned encoder eosin-to-hameleon are used for the final visual representation. However, eosin-to-hameleon encoder is learned in cross-stain prediction (pretext) stage only?</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Based on reported information all reproducibility requirements are met.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>There is inconsistency between description of the contrastive learning process in section 2.4 and corresponding stage in Figure 1. To be specific, there is no predictor q-prime in Figure 1. Also, in part (d) of Figure 1 both learned encoder hameleon-to-eosin and learned encoder eosin-to-hameleon are used for the final visual representation. However, eosin-to-hameleon encoder is learned in cross-stain prediction (pretext) stage only?</p>

      <p>In the forthcoming work the authors should perform statistical significance analysis of the results obtained by their method. It appears to me that in Eq.(7) one regularization constant is enough to take care about relative importance of the two terms in the total loss.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The main strength of this paper is fully self-supervised computational pipeline for visual representation learning for histological images. In addition to contrastive learning stage the important contribution is the cross-stain prediction task that learns hameleon-to-eosin and eosin-to-hameleon encoders from stain-separated images. Learned encoders are used to initialize related ones in the contrastive learning stage. Further strength is novel data augmentation method coined stain vector perturbation that is based on stain separation error from the first stage. It is shown in an ablation study that proposed data augmentation method plays a crucial role in achieving state-of-the-art performance on public colorectal carcinoma dataset. That brings us to the third strength of this paper: achieved eight-class prediction accuracy is higher than the one obtained by fully supervised ResNet18 model.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The color deconvolution is used to generate H and E images for auto-encoder.
A stain vector perturbation is proposed as data augmentation for contrastive learning.
A hybrid system is built for contrastive learning in histopathology.
The performance of using 1000 cases for a linear classifier achieves better performance than the fully supervised learning method with fully supervised learning.
Ablation study shows the impacts of different components.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The framework (c) contrastive learning in Fig 1 is the most innovative part in this paper. The frozen HE is used to avoid the model collapse.
Two encoders are used to achieve representations from H and E respectively to utilize the domain knowledge of pathological images.
The decoder is freezer to avoid model collapse.
Ablation study is a strong evaluation in this paper to show the impacts of different pieces of the method as the design contains many different elements.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>It is a little bit weird that ResNet18 is used as the backbone for Table 1, without good explanations. In the BYOL or SimSiam paper, the ResNet50 is typically used for real imaging dataset (rather than ResNet18 for CIFAR10).
The rationale of using normalized L2 distance as the loss function of the contrastive learning is not clear.
It is not clear what data augmentations are used for training the resnet18 supervised learning approach.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>good</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Maybe the ResNet50 is a more reasonable backbone.
More training details such as number of epochs, GPU resources would be helpful for readers to reproduce the work.</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Good performance, inspiring idea</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>This paper presents a novel framework for self-supervised learning for histopathological images, based on two stages: i) a pretext cross-stain prediction task, and ii) contrastive learning using a new data augmentation approach named stain vector perturbation. All reviewers acknowledge the novelty of the
paper and suggest accepting the paper. Yet authors could further improve their manuscript by incorporating reviews comments. Additional to these comments, I have a concern that whether the relatively slow speed of Sparse NMF stain decomposition will affect contrastive training and maybe the authors can discuss it a bit. Also, it is also worth to carry an ablation study to show which component (cross-stain prediction or contrastive learning) contribute more to the boosted performance. Finally, authors can also consider release their code to promote reusability.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>Response to Reviewer #1
We thank the reviewer for the valuable feedback.  We will revise Fig. 1 and the corresponding description according to the comments. 
As for Fig. 2, we put it in the manuscript to make readers better understand the effect of stain vector perturbation (SVP). The proposed SVP disturbs the estimated stain vector matrix and affects the results of stain separation. The degree of difference between results of original stain separation and results with SVP is related to the strength of sigma (standard deviation of normal disturbance).
As for the basis of cross-stain prediction, we suppose circular blank areas in E channel image are more likely to be nuclei, and blank areas between nuclei in H channel image are more likely to be extracellular matrix or cytoplasm. Therefore, though the content of H and E channel images is different, the cross-stain prediction still can be carried out.
As for SVP, it is specially designed for histopathological images based on the domain-specific knowledge. It can introduce variances into the stain-separated single channel images, which is suitable for contrastive learning. In addition, SVP can also make the encoder robust to the error of stain vector estimation in the process of visual representation extraction.
Because of the anonymity, we didn’t provide the source code in the manuscript. We will release the source code soon.</p>

  <p>Response to Reviewer #3
We thank the reviewer for the valuable feedback.  We will revise Fig. 1 and the corresponding description according to the comments. For Eq. (7), we will use only one lambda in the upcoming camera-ready version.
In our manuscript, q-prime is not a predictor. We denote the projector as ‘f’ and the predictor as ‘g’. z and z-prime are the outputs of ‘f’, and q and q-prime are the outputs of ‘g’. 
We train both H2E encoder and E2H encoder in the cross-stain prediction stage. For the sake of simplicity, we denote the combination of H2E encoder and E2H encoder as HE encoder ϕ in our manuscript. ϕ is also trained in the contrastive learning stage. After two-stage training, the visual representations are extracted using ϕ.</p>

  <p>Response to Reviewer #4
We thank the reviewer for the valuable feedback.<br />
We choose ResNet18 as the backbone because of its small computation cost. ResNet50 also works under the proposed self-supervised learning scheme.
The loss of contrastive learning is the same as BYOL, and it actually evaluates the cosine distance. Maybe the ICML2020 paper “Understanding contrastive representation learning through alignment and uniformity on the hypersphere” could give further explanation.
As for the fully-supervised ResNet18, we didn’t do data augmentation during training. 
The total number of epochs is set to 100, but early-stopping is used to avoid overfitting. The GPU we used to train the model is RTX 3090. We will release the source code to facilitate reproducing.</p>

  <p>Response to Meta-Reviews
We thank the reviewer for the valuable feedback, and we will revise the manuscript according to the reviewers’ comments. 
The sparse NMF is indeed a little slow but bearable. Maybe in practice, the user could do stain vector perturbation once and save the results locally to accelerate contrastive learning.
We have carried ablation study in our manuscript. If only contrastive learning is performed, the proposed method is equivalent to Simsiam, so we didn’t report the performance in the section of ablation study.
Because of the anonymity, we didn’t provide the source code in the manuscript. We will release the source code soon.</p>

</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0204-12-31
      -->
      <!--
      
        ,
        updated at 
        0205-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a> |
      
      <a 
        href="kittywong/categories#Computational (Integrative) Pathology"
        class="post-category">
        Computational (Integrative) Pathology
      </a> |
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a> |
      
      <a 
        href="kittywong/categories#Image Reconstruction"
        class="post-category">
        Image Reconstruction
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Interpretability / Explainability"
        class="post-category">
        Machine Learning - Interpretability / Explainability
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Semi-supervised learning"
        class="post-category">
        Machine Learning - Semi-supervised learning
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - Histopathology"
        class="post-category">
        Modalities - Histopathology
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Yang, Pengshuai"
        class="post-category">
        Yang, Pengshuai
      </a> |  
      
      <a href="kittywong/tags#Hong, Zhiwei"
        class="post-category">
        Hong, Zhiwei
      </a> |  
      
      <a href="kittywong/tags#Yin, Xiaoxu"
        class="post-category">
        Yin, Xiaoxu
      </a> |  
      
      <a href="kittywong/tags#Zhu, Chengzhan"
        class="post-category">
        Zhu, Chengzhan
      </a> |  
      
      <a href="kittywong/tags#Jiang, Rui"
        class="post-category">
        Jiang, Rui
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0205/12/31/Paper0382">
          Contrastive Learning with Continuous Proxy Meta-Data For 3D MRI Classification
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0203/12/31/Paper0316">
          Imbalance-Aware Self-Supervised Learning for 3D Radiomic Representations
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
