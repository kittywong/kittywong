<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Targeted Gradient Descent: A Novel Method for Convolutional Neural Networks Fine-tuning and Online-learning | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Targeted Gradient Descent: A Novel Method for Convolutional Neural Networks Fine-tuning and Online-learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Junyu Chen, Evren Asma, Chung Chan Abstract A convolutional neural network (ConvNet) is usually trained and then tested using images drawn from the same distribution. To generalize a ConvNet to various tasks often requires a complete training dataset that consists of images drawn from different tasks. In most scenarios, it is nearly impossible to collect every possible representative dataset as a priori. The new data may only become available after the ConvNet is deployed in clinical practice. ConvNet, however, may generate artifacts on out-of-distribution testing samples. In this study, we present Targeted Gradient Descent (TGD), a novel fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. To a further extent, the proposed method also enables online learning of patient-specific data. The method is built on the idea of reusing a pre-trained ConvNet’s redundant kernels to learn new knowledge. We compare the performance of TGD to several commonly used training approaches on the task of Positron emission tomography (PET) image denoising. Results from clinical images show that TGD generated results on par with training-from-scratch while significantly reducing data preparation and network training time. More importantly, it enables online learning on the testing study to enhance the network’s generalization capability in real-world applications. Link to paper https://doi.org/10.1007/978-3-030-87199-4_3 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors present Targeted Gradient Descent (TGD), a novel fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. It enables online learning that adapts a pre-trained network to each testing dataset to avoid generating artifacts on unseen features. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Instead of blindly fine-tuning all the kernels in the specific layers or retraining the entire network with a mixture of new and old labels, it might be more sensible to precisely retrain the “meaningless” kernels to make them adapt to the new tasks while the “useful” kernels are preserved so they can retain the knowledge acquired from the prior training with a larger training dataset (a wider coverage of data distribution). This is an interesting idea, and makes sense. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. This paper didn’t mention nor compare with many methods of similar taste in lifelong learning, such as elastic weight consolidation. Recycling redundant weights for new knowledge is kinda well-known there. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Good Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html See above Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? See above What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper In this paper, the authors proposed a novel fine-tuning method, Targeted Gradient Descent (TGD), that reuses a pre-trained ConvNet’s redundant kernels to learn new knowledge. The proposed method can extend a pre-trained network to a new task without revising data from the previous task while preserving the knowledge learned from previous training. The proposed method showed effectiveness in PET image denoising task, where the proposed TGD can be used to fine-tune an existing denoising ConvNet to make it adapt to a new reconstruction protocol using substantially fewer training studies. Using TGD for online-learning also showed improved artifact control on unseen features during testing. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of identifying and using redundant kernels in a pre-trained network for incremental learning while retaining the learned knowledge is innovative, and can potentially have great clinical impact in image denoising as well as other medical imaging tasks. The writing is also good. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The dataset for evaluation is too small, with only 2 patient studies. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The details in the paper are sufficient. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Expanding the testing dataset would make this paper more convincing. If not possible, please address this limitation in the paper. In section 2, what is the difference between X and Y? I don’t think Y is defined. Section 4.1, second paragraph, “where the bladder’s shape is early the same…”, the “early” should be “nearly” or removed. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novel method. Great potential clinical impact. Good writing. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper proposed a finetuning method based on the Targeted Gradient Descent which can reuse the redundant kernels in a pre-trained network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper proposed a novel method to score the meaningfulness of feature maps and make the meaningless feature maps trainable by adding the Targeted Gradient Descent Layer into the ConvNet. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The KSE threshold is defined by the author based on the results in the fig4. This threshold is very important. Is it suitable for all the datasets? More evaluations are needed. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper provides sufficient details about the models/algorithms, datasets, and evaluation. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please make the image as large as possible in order to see it clearly. In the sentence “where the bladder’s shape is early the same as that of the input image”, is there a typo for “early”? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? the novelty of the method What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This work proposed a new fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. Overall, reviewers recognized the novelty and contribution of this work. Potential improvements include comparison with alternative methods, and evaluation on other tasks / datasets. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback Reviewer #1 Comments: This paper didn’t mention nor compare with many methods of similar taste in lifelong learning, such as elastic weight consolidation. Recycling redundant weights for new knowledge is kinda well-known there. Response: We thank the reviewer for the suggestion. We have included the following related articles on lifelong learning in the revised manuscript. [1] Karani, N., Chaitanya, K., Baumgartner, C., &amp; Konukoglu, E. (2018, September). A lifelong learning approach to brain MR segmentation across scanners and protocols. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 476-484). Springer, Cham. [2] McClure, P., Zheng, C. Y., Kaczmarzyk, J. R., Lee, J. A., Ghosh, S. S., Nielson, D., … &amp; Pereira, F. (2018, December). Distributed weight consolidation: a brain segmentation case study. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (pp. 4097-4107). [3] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, page 201611835, 2017. Reviewer #2 Comments: The dataset for evaluation is too small, with only 2 patient studies. Expanding the testing dataset would make this paper more convincing. If not possible, please address this limitation in the paper. Response: We thank the reviewer for the suggestion. We have included more patient studies in the camera-ready version up to the page limit. It is also important to emphasize that the low-BMI studies consist of 10 i.i.d. noise realizations. The quantitative metrics (i.e., ensemble bias and ensemble CoV) were calculated based on the 10 noise realizations. In section 2, what is the difference between X and Y? I don’t think Y is defined. Response: We are sorry for the confusion caused. Here, X and Y represent, respectively, the input and output feature maps of a convolutional layer. Their relationship can be written as: Y_n= \sum_{c=1}^C W_{n,c}*X_c , where * represents the convolution operation, and W_(n,c) represents the n^th kernel of the c^th channel. Note that the bias and activation are omitted for simplicity. We have clarified this in our revision. Section 4.1, second paragraph, “where the bladder’s shape is early the same…”, the “early” should be “nearly” or removed. Response: We have corrected this typo in our revision. Reviewer #3 Comments: Please make the image as large as possible in order to see it clearly. In the sentence “where the bladder’s shape is early the same as that of the input image,” is there a typo for “early”? Response: We thank the reviewer for the suggestion. We have corrected the typo and made the figures larger in our revision. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Junyu Chen, Evren Asma, Chung Chan Abstract A convolutional neural network (ConvNet) is usually trained and then tested using images drawn from the same distribution. To generalize a ConvNet to various tasks often requires a complete training dataset that consists of images drawn from different tasks. In most scenarios, it is nearly impossible to collect every possible representative dataset as a priori. The new data may only become available after the ConvNet is deployed in clinical practice. ConvNet, however, may generate artifacts on out-of-distribution testing samples. In this study, we present Targeted Gradient Descent (TGD), a novel fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. To a further extent, the proposed method also enables online learning of patient-specific data. The method is built on the idea of reusing a pre-trained ConvNet’s redundant kernels to learn new knowledge. We compare the performance of TGD to several commonly used training approaches on the task of Positron emission tomography (PET) image denoising. Results from clinical images show that TGD generated results on par with training-from-scratch while significantly reducing data preparation and network training time. More importantly, it enables online learning on the testing study to enhance the network’s generalization capability in real-world applications. Link to paper https://doi.org/10.1007/978-3-030-87199-4_3 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors present Targeted Gradient Descent (TGD), a novel fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. It enables online learning that adapts a pre-trained network to each testing dataset to avoid generating artifacts on unseen features. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Instead of blindly fine-tuning all the kernels in the specific layers or retraining the entire network with a mixture of new and old labels, it might be more sensible to precisely retrain the “meaningless” kernels to make them adapt to the new tasks while the “useful” kernels are preserved so they can retain the knowledge acquired from the prior training with a larger training dataset (a wider coverage of data distribution). This is an interesting idea, and makes sense. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. This paper didn’t mention nor compare with many methods of similar taste in lifelong learning, such as elastic weight consolidation. Recycling redundant weights for new knowledge is kinda well-known there. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Good Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html See above Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? See above What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper In this paper, the authors proposed a novel fine-tuning method, Targeted Gradient Descent (TGD), that reuses a pre-trained ConvNet’s redundant kernels to learn new knowledge. The proposed method can extend a pre-trained network to a new task without revising data from the previous task while preserving the knowledge learned from previous training. The proposed method showed effectiveness in PET image denoising task, where the proposed TGD can be used to fine-tune an existing denoising ConvNet to make it adapt to a new reconstruction protocol using substantially fewer training studies. Using TGD for online-learning also showed improved artifact control on unseen features during testing. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of identifying and using redundant kernels in a pre-trained network for incremental learning while retaining the learned knowledge is innovative, and can potentially have great clinical impact in image denoising as well as other medical imaging tasks. The writing is also good. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The dataset for evaluation is too small, with only 2 patient studies. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The details in the paper are sufficient. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Expanding the testing dataset would make this paper more convincing. If not possible, please address this limitation in the paper. In section 2, what is the difference between X and Y? I don’t think Y is defined. Section 4.1, second paragraph, “where the bladder’s shape is early the same…”, the “early” should be “nearly” or removed. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novel method. Great potential clinical impact. Good writing. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper proposed a finetuning method based on the Targeted Gradient Descent which can reuse the redundant kernels in a pre-trained network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper proposed a novel method to score the meaningfulness of feature maps and make the meaningless feature maps trainable by adding the Targeted Gradient Descent Layer into the ConvNet. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The KSE threshold is defined by the author based on the results in the fig4. This threshold is very important. Is it suitable for all the datasets? More evaluations are needed. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper provides sufficient details about the models/algorithms, datasets, and evaluation. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please make the image as large as possible in order to see it clearly. In the sentence “where the bladder’s shape is early the same as that of the input image”, is there a typo for “early”? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? the novelty of the method What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This work proposed a new fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. Overall, reviewers recognized the novelty and contribution of this work. Potential improvements include comparison with alternative methods, and evaluation on other tasks / datasets. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback Reviewer #1 Comments: This paper didn’t mention nor compare with many methods of similar taste in lifelong learning, such as elastic weight consolidation. Recycling redundant weights for new knowledge is kinda well-known there. Response: We thank the reviewer for the suggestion. We have included the following related articles on lifelong learning in the revised manuscript. [1] Karani, N., Chaitanya, K., Baumgartner, C., &amp; Konukoglu, E. (2018, September). A lifelong learning approach to brain MR segmentation across scanners and protocols. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 476-484). Springer, Cham. [2] McClure, P., Zheng, C. Y., Kaczmarzyk, J. R., Lee, J. A., Ghosh, S. S., Nielson, D., … &amp; Pereira, F. (2018, December). Distributed weight consolidation: a brain segmentation case study. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (pp. 4097-4107). [3] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, page 201611835, 2017. Reviewer #2 Comments: The dataset for evaluation is too small, with only 2 patient studies. Expanding the testing dataset would make this paper more convincing. If not possible, please address this limitation in the paper. Response: We thank the reviewer for the suggestion. We have included more patient studies in the camera-ready version up to the page limit. It is also important to emphasize that the low-BMI studies consist of 10 i.i.d. noise realizations. The quantitative metrics (i.e., ensemble bias and ensemble CoV) were calculated based on the 10 noise realizations. In section 2, what is the difference between X and Y? I don’t think Y is defined. Response: We are sorry for the confusion caused. Here, X and Y represent, respectively, the input and output feature maps of a convolutional layer. Their relationship can be written as: Y_n= \sum_{c=1}^C W_{n,c}*X_c , where * represents the convolution operation, and W_(n,c) represents the n^th kernel of the c^th channel. Note that the bias and activation are omitted for simplicity. We have clarified this in our revision. Section 4.1, second paragraph, “where the bladder’s shape is early the same…”, the “early” should be “nearly” or removed. Response: We have corrected this typo in our revision. Reviewer #3 Comments: Please make the image as large as possible in order to see it clearly. In the sentence “where the bladder’s shape is early the same as that of the input image,” is there a typo for “early”? Response: We thank the reviewer for the suggestion. We have corrected the typo and made the figures larger in our revision. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0302/12/31/Paper1012" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0302/12/31/Paper1012" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0302-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Targeted Gradient Descent: A Novel Method for Convolutional Neural Networks Fine-tuning and Online-learning" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0302/12/31/Paper1012"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0302/12/31/Paper1012","headline":"Targeted Gradient Descent: A Novel Method for Convolutional Neural Networks Fine-tuning and Online-learning","dateModified":"0303-01-02T00:00:00-05:17","datePublished":"0302-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Junyu Chen, Evren Asma, Chung Chan Abstract A convolutional neural network (ConvNet) is usually trained and then tested using images drawn from the same distribution. To generalize a ConvNet to various tasks often requires a complete training dataset that consists of images drawn from different tasks. In most scenarios, it is nearly impossible to collect every possible representative dataset as a priori. The new data may only become available after the ConvNet is deployed in clinical practice. ConvNet, however, may generate artifacts on out-of-distribution testing samples. In this study, we present Targeted Gradient Descent (TGD), a novel fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. To a further extent, the proposed method also enables online learning of patient-specific data. The method is built on the idea of reusing a pre-trained ConvNet’s redundant kernels to learn new knowledge. We compare the performance of TGD to several commonly used training approaches on the task of Positron emission tomography (PET) image denoising. Results from clinical images show that TGD generated results on par with training-from-scratch while significantly reducing data preparation and network training time. More importantly, it enables online learning on the testing study to enhance the network’s generalization capability in real-world applications. Link to paper https://doi.org/10.1007/978-3-030-87199-4_3 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors present Targeted Gradient Descent (TGD), a novel fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. It enables online learning that adapts a pre-trained network to each testing dataset to avoid generating artifacts on unseen features. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Instead of blindly fine-tuning all the kernels in the specific layers or retraining the entire network with a mixture of new and old labels, it might be more sensible to precisely retrain the “meaningless” kernels to make them adapt to the new tasks while the “useful” kernels are preserved so they can retain the knowledge acquired from the prior training with a larger training dataset (a wider coverage of data distribution). This is an interesting idea, and makes sense. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. This paper didn’t mention nor compare with many methods of similar taste in lifelong learning, such as elastic weight consolidation. Recycling redundant weights for new knowledge is kinda well-known there. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Good Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html See above Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? See above What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper In this paper, the authors proposed a novel fine-tuning method, Targeted Gradient Descent (TGD), that reuses a pre-trained ConvNet’s redundant kernels to learn new knowledge. The proposed method can extend a pre-trained network to a new task without revising data from the previous task while preserving the knowledge learned from previous training. The proposed method showed effectiveness in PET image denoising task, where the proposed TGD can be used to fine-tune an existing denoising ConvNet to make it adapt to a new reconstruction protocol using substantially fewer training studies. Using TGD for online-learning also showed improved artifact control on unseen features during testing. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of identifying and using redundant kernels in a pre-trained network for incremental learning while retaining the learned knowledge is innovative, and can potentially have great clinical impact in image denoising as well as other medical imaging tasks. The writing is also good. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The dataset for evaluation is too small, with only 2 patient studies. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The details in the paper are sufficient. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Expanding the testing dataset would make this paper more convincing. If not possible, please address this limitation in the paper. In section 2, what is the difference between X and Y? I don’t think Y is defined. Section 4.1, second paragraph, “where the bladder’s shape is early the same…”, the “early” should be “nearly” or removed. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novel method. Great potential clinical impact. Good writing. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper proposed a finetuning method based on the Targeted Gradient Descent which can reuse the redundant kernels in a pre-trained network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper proposed a novel method to score the meaningfulness of feature maps and make the meaningless feature maps trainable by adding the Targeted Gradient Descent Layer into the ConvNet. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The KSE threshold is defined by the author based on the results in the fig4. This threshold is very important. Is it suitable for all the datasets? More evaluations are needed. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper provides sufficient details about the models/algorithms, datasets, and evaluation. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please make the image as large as possible in order to see it clearly. In the sentence “where the bladder’s shape is early the same as that of the input image”, is there a typo for “early”? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? the novelty of the method What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This work proposed a new fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. Overall, reviewers recognized the novelty and contribution of this work. Potential improvements include comparison with alternative methods, and evaluation on other tasks / datasets. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback Reviewer #1 Comments: This paper didn’t mention nor compare with many methods of similar taste in lifelong learning, such as elastic weight consolidation. Recycling redundant weights for new knowledge is kinda well-known there. Response: We thank the reviewer for the suggestion. We have included the following related articles on lifelong learning in the revised manuscript. [1] Karani, N., Chaitanya, K., Baumgartner, C., &amp; Konukoglu, E. (2018, September). A lifelong learning approach to brain MR segmentation across scanners and protocols. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 476-484). Springer, Cham. [2] McClure, P., Zheng, C. Y., Kaczmarzyk, J. R., Lee, J. A., Ghosh, S. S., Nielson, D., … &amp; Pereira, F. (2018, December). Distributed weight consolidation: a brain segmentation case study. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (pp. 4097-4107). [3] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, page 201611835, 2017. Reviewer #2 Comments: The dataset for evaluation is too small, with only 2 patient studies. Expanding the testing dataset would make this paper more convincing. If not possible, please address this limitation in the paper. Response: We thank the reviewer for the suggestion. We have included more patient studies in the camera-ready version up to the page limit. It is also important to emphasize that the low-BMI studies consist of 10 i.i.d. noise realizations. The quantitative metrics (i.e., ensemble bias and ensemble CoV) were calculated based on the 10 noise realizations. In section 2, what is the difference between X and Y? I don’t think Y is defined. Response: We are sorry for the confusion caused. Here, X and Y represent, respectively, the input and output feature maps of a convolutional layer. Their relationship can be written as: Y_n= \\sum_{c=1}^C W_{n,c}*X_c , where * represents the convolution operation, and W_(n,c) represents the n^th kernel of the c^th channel. Note that the bias and activation are omitted for simplicity. We have clarified this in our revision. Section 4.1, second paragraph, “where the bladder’s shape is early the same…”, the “early” should be “nearly” or removed. Response: We have corrected this typo in our revision. Reviewer #3 Comments: Please make the image as large as possible in order to see it clearly. In the sentence “where the bladder’s shape is early the same as that of the input image,” is there a typo for “early”? Response: We thank the reviewer for the suggestion. We have corrected the typo and made the figures larger in our revision. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Chen, Junyu,Asma, Evren,Chan, Chung" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Targeted Gradient Descent: A Novel Method for Convolutional Neural Networks Fine-tuning and Online-learning</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Modalities - PET/SPECT"
        class="post-category">
        Modalities - PET/SPECT
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Active Learning"
        class="post-category">
        Machine Learning - Active Learning
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Domain adaptation"
        class="post-category">
        Machine Learning - Domain adaptation
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Chen, Junyu"
        class="post-tags">
        Chen, Junyu
      </a> |  
      
      <a href="kittywong/tags#Asma, Evren"
        class="post-tags">
        Asma, Evren
      </a> |  
      
      <a href="kittywong/tags#Chan, Chung"
        class="post-tags">
        Chan, Chung
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Junyu Chen, Evren Asma, Chung Chan
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>A convolutional neural network (ConvNet) is usually trained and then tested using images drawn from the same distribution. To generalize a ConvNet to various tasks often requires a complete training dataset that consists of images drawn from different tasks. In most scenarios, it is nearly impossible to collect every possible representative dataset as a priori. The new data may only become available after the ConvNet is deployed in clinical practice. ConvNet, however, may generate artifacts on out-of-distribution testing samples. In this study, we present Targeted Gradient Descent (TGD), a novel fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. To a further extent, the proposed method also enables online learning of patient-specific data. The method is built on the idea of reusing a pre-trained ConvNet’s redundant kernels to learn new knowledge. We compare the performance of TGD to several commonly used training approaches on the task of Positron emission tomography (PET) image denoising. Results from clinical images show that TGD generated results on par with training-from-scratch while significantly reducing data preparation and network training time. More importantly, it enables online learning on the testing study to enhance the network’s generalization capability in real-world applications.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87199-4_3">https://doi.org/10.1007/978-3-030-87199-4_3</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors present Targeted Gradient Descent (TGD), a novel fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. It enables online learning that adapts a pre-trained network to each testing dataset to avoid generating artifacts on unseen features.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>Instead of blindly fine-tuning all the kernels in the specific layers or retraining the
entire network with a mixture of new and old labels, it might be more sensible to precisely retrain the “meaningless” kernels to make them adapt to the new tasks while the “useful” kernels are preserved so they can retain the knowledge acquired from the prior training with a larger training dataset (a wider coverage of data distribution). This is an interesting idea, and makes sense.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>This paper didn’t mention nor compare with many methods of similar taste in lifelong learning, such as elastic weight consolidation. Recycling redundant weights for new knowledge is kinda well-known there.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>See above</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>See above</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>In this paper, the authors proposed a novel fine-tuning method, Targeted Gradient Descent (TGD), that reuses a pre-trained ConvNet’s redundant kernels to learn new knowledge. The proposed method can extend a pre-trained network to a new task without revising data from the previous task while preserving the knowledge learned from previous training. The proposed method showed effectiveness in PET image denoising task, where the proposed TGD can be used to fine-tune an existing denoising ConvNet to make it adapt to a new reconstruction protocol using substantially fewer training studies. Using TGD for online-learning also showed improved artifact control on unseen features during testing.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The idea of identifying and using redundant kernels in a pre-trained network for incremental learning while retaining the learned knowledge is innovative, and can potentially have great clinical impact in image denoising as well as other medical imaging tasks. The writing is also good.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The dataset for evaluation is too small, with only 2 patient studies.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The details in the paper are sufficient.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ol>
        <li>Expanding the testing dataset would make this paper more convincing. If not possible, please address this limitation in the paper.</li>
        <li>In section 2, what is the difference between X and Y? I don’t think Y is defined.</li>
        <li>Section 4.1, second paragraph, “where the bladder’s shape is early the same…”, the “early” should be “nearly” or removed.</li>
      </ol>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Novel method.
Great potential clinical impact.
Good writing.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper proposed a finetuning method based on the Targeted Gradient Descent which can reuse the redundant kernels in a pre-trained network.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The paper proposed a novel method to score the meaningfulness of feature maps and make the meaningless feature maps trainable by adding the Targeted Gradient Descent Layer into the ConvNet.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The KSE threshold is defined by the author based on the results in the fig4. This threshold is very important. Is it suitable for all the datasets? More evaluations are needed.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The paper provides sufficient details about the models/algorithms, datasets, and evaluation.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Please make the image as large as possible in order to see it clearly.
In the sentence “where the bladder’s shape is early the same as that of the input image”, is there a typo for “early”?</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>the novelty of the method</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>This work proposed a new fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. Overall, reviewers recognized the novelty and contribution of this work. Potential improvements include comparison with alternative methods, and evaluation on other tasks / datasets.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>Reviewer #1
Comments:</p>
  <ol>
    <li>This paper didn’t mention nor compare with many methods of similar taste in lifelong learning, such as elastic weight consolidation. Recycling redundant weights for new knowledge is kinda well-known there.
Response:
We thank the reviewer for the suggestion. We have included the following related articles on lifelong learning in the revised manuscript.</li>
  </ol>

  <p>[1]	Karani, N., Chaitanya, K., Baumgartner, C., &amp; Konukoglu, E. (2018, September). A lifelong learning approach to brain MR segmentation across scanners and protocols. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 476-484). Springer, Cham.
[2]	McClure, P., Zheng, C. Y., Kaczmarzyk, J. R., Lee, J. A., Ghosh, S. S., Nielson, D., … &amp; Pereira, F. (2018, December). Distributed weight consolidation: a brain segmentation case study. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (pp. 4097-4107).
[3] 	James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, page 201611835, 2017.</p>

  <p>Reviewer #2
Comments:</p>
  <ol>
    <li>The dataset for evaluation is too small, with only 2 patient studies.</li>
    <li>
      <p>Expanding the testing dataset would make this paper more convincing. If not possible, please address this limitation in the paper.
Response:
We thank the reviewer for the suggestion. We have included more patient studies in the camera-ready version up to the page limit. It is also important to emphasize that the low-BMI studies consist of 10 i.i.d. noise realizations. The quantitative metrics (i.e., ensemble bias and ensemble CoV) were calculated based on the 10 noise realizations.</p>
    </li>
    <li>
      <p>In section 2, what is the difference between X and Y? I don’t think Y is defined.
Response:
We are sorry for the confusion caused. Here, X and Y represent, respectively, the input and output feature maps of a convolutional layer. Their relationship can be written as:
Y_n= \sum_{c=1}^C W_{n,c}*X_c ,
where * represents the convolution operation, and W_(n,c) represents the n^th kernel of the c^th channel. Note that the bias and activation are omitted for simplicity.
We have clarified this in our revision.</p>
    </li>
    <li>Section 4.1, second paragraph, “where the bladder’s shape is early the same…”, the “early” should be “nearly” or removed.
Response:
We have corrected this typo in our revision.</li>
  </ol>

  <p>Reviewer #3
Comments:</p>
  <ol>
    <li>Please make the image as large as possible in order to see it clearly.</li>
    <li>In the sentence “where the bladder’s shape is early the same as that of the input image,” is there a typo for “early”?
Response: 
We thank the reviewer for the suggestion. We have corrected the typo and made the figures larger in our revision.</li>
  </ol>

</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0302-12-31
      -->
      <!--
      
        ,
        updated at 
        0303-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Modalities - PET/SPECT"
        class="post-category">
        Modalities - PET/SPECT
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Active Learning"
        class="post-category">
        Machine Learning - Active Learning
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Domain adaptation"
        class="post-category">
        Machine Learning - Domain adaptation
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Chen, Junyu"
        class="post-category">
        Chen, Junyu
      </a> |  
      
      <a href="kittywong/tags#Asma, Evren"
        class="post-category">
        Asma, Evren
      </a> |  
      
      <a href="kittywong/tags#Chan, Chung"
        class="post-category">
        Chan, Chung
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0303/12/31/Paper1101">
          A Hierarchical Feature Constraint to CamouflageMedical Adversarial Attacks
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0301/12/31/Paper0821">
          Joint Motion Correction and Super Resolution for Cardiac Segmentation via Latent Optimisation
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
