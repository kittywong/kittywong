<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Observational Supervision for Medical Image Classification using Gaze Data | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Observational Supervision for Medical Image Classification using Gaze Data" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Khaled Saab, Sarah M. Hooper, Nimit S. Sohoni, Jupinder Parmar, Brian Pogatchnik, Sen Wu, Jared A. Dunnmon, Hongyang R. Zhang, Daniel Rubin, Christopher Ré Abstract Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinician’s workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines. Link to paper https://doi.org/10.1007/978-3-030-87196-3_56 Link to the code repository https://github.com/HazyResearch/observational Link to the dataset(s) https://github.com/HazyResearch/observational Reviews Review #1 Please describe the contribution of the paper The work reports using eye gaze data for observational supervision. They use eye gaze in supervised and unsupervised settings. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This is in my opinion good bit of work focusing on a modality that should be taken seriously (eye gaze data). The methodology described is fresh, although it could be a bit simplistic specially with statistical features. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. My main concern is that the actual coordinates information in eye gaze data is somewhat underused. As the authors in [13] claim, one can use eye gaze data in UNet like architectures. The authors here use helper tasks. It seems at least a discussion of other possibilities is warranted. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Two datasets are claimed to be slated for public release. A third party dataset is already in public domain. I am optimistic about the reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please define the helper tasks a bit more clearly. Currently this is vague in the paper. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I think despite the limited computational contributions, this is a fresh kind of work that promises the possibility of training interpretable classifiers without need for localized labeling. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The paper “Observational Supervision for Medical Image Classification using Gaze Data” is about the usage of visual attention as annotation source for X-Ray images. They claim that the fixation duration is a strong indicator for a valid label. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Positive: Interesting approach Interesting findings Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Negative: Used three experts for one dataset and one expert for another. Use other public datasets with gaze information for evaluation. Claims in “Gaze data statistics” are not empirically shown. Evaluated only one model. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Not possible due to the hold back data. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Required improvements: Use more experts for the recording Evaluate your claims in “Gaze data statistics” empirically Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall to few experts for recording and the claims are not evaluated. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper focuses on the use of gaze data from medical imaging review as support for imaging classification. The authors propose to extract several gaze features (namely “Time on maximum patch”, “Diffusivity”, “Unique visits”, “Time spent”) from the gaze data and train a neural network to predict these features that are later converted to the probability of binary classification. Moreover, the authors propose to train the neural network to predict both, binary class as the main task and gaze features as secondary task aiming for improved performance on the main task. The authors propose an evaluation on several medical imaging classification tasks to prove their intuition, in particular showing that training a network on multiple tasks allows for higher performances than a baseline binary classifier. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper provides a respectful amount of experiments to verify and prove the intuition behind the proposed method. Several datasets have been used with different protocols for collecting the gaze data. Several ablation studies are described, in particular, evaluating the contribution of the proposed gaze features and the results are given in supplementary material. Overall the paper is well written and the descriptions of the method are clear. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Unfortunately, the clinical feasibility of the proposed method remains the main concern. That is, the collection of the gaze data in the clinical environment may be burdensome and the obtained data may be too operator-specific. That is, the clinical review of the radiological imaging substantially varies, with the hanging protocols usually also operator-specific (e.g., one or several screens being used, displaying one or several images on the screens at a time, various order of the displayed images, etc.). This makes the implementation of the proposed method doubtful and eventually losing compared to NLP methods based on clinical reports. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors describe well the datasets being used The authors give details of the neural network being used in the experiments The authors state the training process settings and eventual pre-training (i.e., 15 epochs on ImageNet) The results are given over several runs with the respective standard deviations The codes are provided in supplementary material Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Table 1 the authors list the gaze features used in the method. It would help the reader if the authors could give the types, the ranges of the features, preventing the reader from looking in the supplementary material. In 3.3 the authors give the equation of the loss function being minimized, with l_j, j=1..m being the losses for the secondary tasks. Could the authors clarify in section 3.3 or later in section 4, which losses have been used for each task (e.g., MAE, MSE, etc.)? Similarly, in the equation in section 3.3, there are no weights on any of the losses. It would be helpful if the authors clearly state the lack of weights, or their presence, and the argument behind the adopted approach. In 4.2 the authors discuss the usefulness of the features, further detailed in the supplementary material. I wonder whether the authors could discuss or add numerical results on experiments having a different number of secondary tasks (i.e., using a different number of features)? That is, from table S.2, one can see that time plays little role in the performances. What would be the effect of removing the time from the scope of secondary tasks? Both models, namely Gaze-WS and Gaze-MTL are built to predict Gaze features. Could the authors discuss or give numerical results on the performances on these secondary tasks? In Related work, the authors talk about NLP-based methods that could be used conjointly with the proposed method. Such methods are generally serving the same purpose, eventually allowing for improved performances. It would be useful for a better understanding of the results of the proposed method if the authors could discuss (e.g., list the result reported in other works) the gains in performances in similar tasks of the NLP-based methods, to illustrate, whether the gains are comparable. Minor observation: The Equation in 3.3 is not referenced. Could the authors put a reference on it (it should be (2))? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is generally well organized, clear, and pleasant to read. The authors describe well the proposed method and do a decent job of evaluating on several datasets. However, the uncertain clinical feasibility remains from my point of view the main disadvantage of the method: that is, the collection of the Gaze data amongst the clinicians (e.g., radiologists) may be a considerably challenging task, more burdensome than, for example, the collection of clinical reports mentioned in the paper. This prevents me from firmly accepting the paper. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #4 Please describe the contribution of the paper The main challenge in deep learning methods for medical imaging is the lack of high quality labelled data for a variety of tasks. This paper hypothesis that gaze data contains latent information regarding the actual task at hand and proposes to use gaze data to weakly supervise deep learning models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. 1)The idea of incorporating gaze tracking data to weakly supervise trained models is novel and interesting 2) Strong evaluation with clearly defined baselines and experiments. The authors compare a CNN model trained with gaze tracking data and compare it against a CNN model trained using high quality labels. Additionally, the authors present clear comparison of the Gaze-MTL model with several gaze data incorporation methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Gaze tracking requires specialized hardware and software, which may be expensive to scale and sometimes be impractical in clinical settings. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have provided the modeling code with clear documentation. They also plan to release their gaze tracking datasets after the review process. Based on this, I believe this work will be reproducible Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It’s a very interesting idea to use gaze tracking data as weakly supervised labels. From a research standpoint, it would be interesting to see how much of the gaze tracking hotspots actually overlap with abnormalities in the image (whenever labeled data i available). This can inform the noisy-ness / quality of the gaze tracking labels. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The idea of using observation information such as gaze to generate weakly supervise medical imaging is an interesting and novel idea. The paper is well written and the authors present a clear evaluation of their methodology. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 1 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The reviewers have favorably assessed the work in question. We ask the authors to go through the comments made by all the reviewers to improve the overall quality of the paper. In particular, some of the claims made could be re-evaluated due to the fact that data was only collected by a single expert. Similarly, the authors should make not of the practicial implications their solution has and how it compares to the NLP based approaches. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback We thank reviewers for their helpful feedback. We are pleased they found our approach novel, our paper well written, and our experimental results compelling, and that they appreciate our contribution of two novel datasets that will be publicly available. Below, we address the main points brought up by the reviewers. Practicality of gaze collection (R3,5, MR): As medical AI tools continue to advance and wearable technology (e.g. AR with eye tracking capability) improves, the required hardware needed to collect gaze data is expected to become more ubiquitous, affordable, and standardized [1*] (R5). In this context, straightforward strategies could be developed to mitigate the impact of clinical protocol variations on the utility of gaze data – for instance, a wearable’s frontal camera data could be used to keep track of which gaze data belongs to which image (R3). Indeed, we hope that by demonstrating the usefulness of gaze data for medical ML supervision, our work will also inspire further development of efficient approaches to large-scale gaze data collection. NLP Annotators (R1,3, MR): Since our datasets do not contain medical reports, we could not do a direct comparison with NLP annotators (R3, MR). However, we would like to emphasize that we view observational supervision as a complement to NLP annotators. Firstly, in situations where text reports or trained NLP models are not available, we show that gaze data is a viable alternative source of supervision. Second, if NLP-based labels are available, combining information from the gaze signals may improve label accuracy (since gaze may contain information beyond the reports, e.g. expert confidence), or the gaze data may be used alongside the NLP-derived labels to further improve performance, e.g. via the Gaze-MTL method we propose (R3, MR). While we mentioned these points on Page 3 of our original manuscript, we will emphasize them in the revised manuscript. As suggested by R1, we will expand our related work section to include recently published techniques by Karargyris et al. [13] (R1). Helper tasks in Gaze-MTL (R1,3): A helper task is defined to be the task of predicting the value of a gaze feature for each training image (R1). The loss function used for all helper tasks is the standard soft cross-entropy loss [23]. The equation in Section 3.3 should include different weights on the helper task losses, as these hyperparameters may impact positive task transfer in MTL (R3). In our experiments, we chose each weight from the set {0, 0.5, 1, 2} that achieves the highest validation accuracy (R3). For experimenting on a different number of helper tasks, we found that in three of our medical tasks, tuned weights corresponded to choosing a single helper task (i.e. weight of zero was given to all but one helper task). The chosen helper tasks are those that are bolded in Table S.2 (we will include our hyperparameters in the code and revised manuscript) (R3). We will include the helper task performances in a new column of Table S.2 (R3). We will add a column in Table 1 summarizing the range of the features shown in Figure S.1 (R3). We will integrate these clarifications in the revised manuscript. Reproducibility, datasets, and claims (R2, MR): We emphasize that both our code and datasets will be publicly released, making it possible to reproduce our results (as pointed out by R1,3,5) (R2). We also emphasize that we collected gaze data on three highly-trained radiologists for CXR-P, an expensive effort (MR). While having one expert for METS is a current limitation, we hope our work inspires the community to contribute more gaze datasets from multiple experts (R2). Our claims in the “gaze data statistics” section are empirically supported by Figure S.1 (R2). Lastly, we used a standard CNN architecture (ResNet50) commonly used for our tasks in the literature [4, 31] (R2). [1*] M.R. Desselle et al. “Augmented and virtual reality in surgery.” Computing in Science &amp; Engineering. 2020 back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Khaled Saab, Sarah M. Hooper, Nimit S. Sohoni, Jupinder Parmar, Brian Pogatchnik, Sen Wu, Jared A. Dunnmon, Hongyang R. Zhang, Daniel Rubin, Christopher Ré Abstract Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinician’s workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines. Link to paper https://doi.org/10.1007/978-3-030-87196-3_56 Link to the code repository https://github.com/HazyResearch/observational Link to the dataset(s) https://github.com/HazyResearch/observational Reviews Review #1 Please describe the contribution of the paper The work reports using eye gaze data for observational supervision. They use eye gaze in supervised and unsupervised settings. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This is in my opinion good bit of work focusing on a modality that should be taken seriously (eye gaze data). The methodology described is fresh, although it could be a bit simplistic specially with statistical features. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. My main concern is that the actual coordinates information in eye gaze data is somewhat underused. As the authors in [13] claim, one can use eye gaze data in UNet like architectures. The authors here use helper tasks. It seems at least a discussion of other possibilities is warranted. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Two datasets are claimed to be slated for public release. A third party dataset is already in public domain. I am optimistic about the reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please define the helper tasks a bit more clearly. Currently this is vague in the paper. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I think despite the limited computational contributions, this is a fresh kind of work that promises the possibility of training interpretable classifiers without need for localized labeling. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The paper “Observational Supervision for Medical Image Classification using Gaze Data” is about the usage of visual attention as annotation source for X-Ray images. They claim that the fixation duration is a strong indicator for a valid label. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Positive: Interesting approach Interesting findings Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Negative: Used three experts for one dataset and one expert for another. Use other public datasets with gaze information for evaluation. Claims in “Gaze data statistics” are not empirically shown. Evaluated only one model. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Not possible due to the hold back data. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Required improvements: Use more experts for the recording Evaluate your claims in “Gaze data statistics” empirically Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall to few experts for recording and the claims are not evaluated. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper focuses on the use of gaze data from medical imaging review as support for imaging classification. The authors propose to extract several gaze features (namely “Time on maximum patch”, “Diffusivity”, “Unique visits”, “Time spent”) from the gaze data and train a neural network to predict these features that are later converted to the probability of binary classification. Moreover, the authors propose to train the neural network to predict both, binary class as the main task and gaze features as secondary task aiming for improved performance on the main task. The authors propose an evaluation on several medical imaging classification tasks to prove their intuition, in particular showing that training a network on multiple tasks allows for higher performances than a baseline binary classifier. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper provides a respectful amount of experiments to verify and prove the intuition behind the proposed method. Several datasets have been used with different protocols for collecting the gaze data. Several ablation studies are described, in particular, evaluating the contribution of the proposed gaze features and the results are given in supplementary material. Overall the paper is well written and the descriptions of the method are clear. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Unfortunately, the clinical feasibility of the proposed method remains the main concern. That is, the collection of the gaze data in the clinical environment may be burdensome and the obtained data may be too operator-specific. That is, the clinical review of the radiological imaging substantially varies, with the hanging protocols usually also operator-specific (e.g., one or several screens being used, displaying one or several images on the screens at a time, various order of the displayed images, etc.). This makes the implementation of the proposed method doubtful and eventually losing compared to NLP methods based on clinical reports. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors describe well the datasets being used The authors give details of the neural network being used in the experiments The authors state the training process settings and eventual pre-training (i.e., 15 epochs on ImageNet) The results are given over several runs with the respective standard deviations The codes are provided in supplementary material Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Table 1 the authors list the gaze features used in the method. It would help the reader if the authors could give the types, the ranges of the features, preventing the reader from looking in the supplementary material. In 3.3 the authors give the equation of the loss function being minimized, with l_j, j=1..m being the losses for the secondary tasks. Could the authors clarify in section 3.3 or later in section 4, which losses have been used for each task (e.g., MAE, MSE, etc.)? Similarly, in the equation in section 3.3, there are no weights on any of the losses. It would be helpful if the authors clearly state the lack of weights, or their presence, and the argument behind the adopted approach. In 4.2 the authors discuss the usefulness of the features, further detailed in the supplementary material. I wonder whether the authors could discuss or add numerical results on experiments having a different number of secondary tasks (i.e., using a different number of features)? That is, from table S.2, one can see that time plays little role in the performances. What would be the effect of removing the time from the scope of secondary tasks? Both models, namely Gaze-WS and Gaze-MTL are built to predict Gaze features. Could the authors discuss or give numerical results on the performances on these secondary tasks? In Related work, the authors talk about NLP-based methods that could be used conjointly with the proposed method. Such methods are generally serving the same purpose, eventually allowing for improved performances. It would be useful for a better understanding of the results of the proposed method if the authors could discuss (e.g., list the result reported in other works) the gains in performances in similar tasks of the NLP-based methods, to illustrate, whether the gains are comparable. Minor observation: The Equation in 3.3 is not referenced. Could the authors put a reference on it (it should be (2))? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is generally well organized, clear, and pleasant to read. The authors describe well the proposed method and do a decent job of evaluating on several datasets. However, the uncertain clinical feasibility remains from my point of view the main disadvantage of the method: that is, the collection of the Gaze data amongst the clinicians (e.g., radiologists) may be a considerably challenging task, more burdensome than, for example, the collection of clinical reports mentioned in the paper. This prevents me from firmly accepting the paper. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #4 Please describe the contribution of the paper The main challenge in deep learning methods for medical imaging is the lack of high quality labelled data for a variety of tasks. This paper hypothesis that gaze data contains latent information regarding the actual task at hand and proposes to use gaze data to weakly supervise deep learning models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. 1)The idea of incorporating gaze tracking data to weakly supervise trained models is novel and interesting 2) Strong evaluation with clearly defined baselines and experiments. The authors compare a CNN model trained with gaze tracking data and compare it against a CNN model trained using high quality labels. Additionally, the authors present clear comparison of the Gaze-MTL model with several gaze data incorporation methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Gaze tracking requires specialized hardware and software, which may be expensive to scale and sometimes be impractical in clinical settings. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have provided the modeling code with clear documentation. They also plan to release their gaze tracking datasets after the review process. Based on this, I believe this work will be reproducible Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It’s a very interesting idea to use gaze tracking data as weakly supervised labels. From a research standpoint, it would be interesting to see how much of the gaze tracking hotspots actually overlap with abnormalities in the image (whenever labeled data i available). This can inform the noisy-ness / quality of the gaze tracking labels. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The idea of using observation information such as gaze to generate weakly supervise medical imaging is an interesting and novel idea. The paper is well written and the authors present a clear evaluation of their methodology. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 1 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The reviewers have favorably assessed the work in question. We ask the authors to go through the comments made by all the reviewers to improve the overall quality of the paper. In particular, some of the claims made could be re-evaluated due to the fact that data was only collected by a single expert. Similarly, the authors should make not of the practicial implications their solution has and how it compares to the NLP based approaches. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback We thank reviewers for their helpful feedback. We are pleased they found our approach novel, our paper well written, and our experimental results compelling, and that they appreciate our contribution of two novel datasets that will be publicly available. Below, we address the main points brought up by the reviewers. Practicality of gaze collection (R3,5, MR): As medical AI tools continue to advance and wearable technology (e.g. AR with eye tracking capability) improves, the required hardware needed to collect gaze data is expected to become more ubiquitous, affordable, and standardized [1*] (R5). In this context, straightforward strategies could be developed to mitigate the impact of clinical protocol variations on the utility of gaze data – for instance, a wearable’s frontal camera data could be used to keep track of which gaze data belongs to which image (R3). Indeed, we hope that by demonstrating the usefulness of gaze data for medical ML supervision, our work will also inspire further development of efficient approaches to large-scale gaze data collection. NLP Annotators (R1,3, MR): Since our datasets do not contain medical reports, we could not do a direct comparison with NLP annotators (R3, MR). However, we would like to emphasize that we view observational supervision as a complement to NLP annotators. Firstly, in situations where text reports or trained NLP models are not available, we show that gaze data is a viable alternative source of supervision. Second, if NLP-based labels are available, combining information from the gaze signals may improve label accuracy (since gaze may contain information beyond the reports, e.g. expert confidence), or the gaze data may be used alongside the NLP-derived labels to further improve performance, e.g. via the Gaze-MTL method we propose (R3, MR). While we mentioned these points on Page 3 of our original manuscript, we will emphasize them in the revised manuscript. As suggested by R1, we will expand our related work section to include recently published techniques by Karargyris et al. [13] (R1). Helper tasks in Gaze-MTL (R1,3): A helper task is defined to be the task of predicting the value of a gaze feature for each training image (R1). The loss function used for all helper tasks is the standard soft cross-entropy loss [23]. The equation in Section 3.3 should include different weights on the helper task losses, as these hyperparameters may impact positive task transfer in MTL (R3). In our experiments, we chose each weight from the set {0, 0.5, 1, 2} that achieves the highest validation accuracy (R3). For experimenting on a different number of helper tasks, we found that in three of our medical tasks, tuned weights corresponded to choosing a single helper task (i.e. weight of zero was given to all but one helper task). The chosen helper tasks are those that are bolded in Table S.2 (we will include our hyperparameters in the code and revised manuscript) (R3). We will include the helper task performances in a new column of Table S.2 (R3). We will add a column in Table 1 summarizing the range of the features shown in Figure S.1 (R3). We will integrate these clarifications in the revised manuscript. Reproducibility, datasets, and claims (R2, MR): We emphasize that both our code and datasets will be publicly released, making it possible to reproduce our results (as pointed out by R1,3,5) (R2). We also emphasize that we collected gaze data on three highly-trained radiologists for CXR-P, an expensive effort (MR). While having one expert for METS is a current limitation, we hope our work inspires the community to contribute more gaze datasets from multiple experts (R2). Our claims in the “gaze data statistics” section are empirically supported by Figure S.1 (R2). Lastly, we used a standard CNN architecture (ResNet50) commonly used for our tasks in the literature [4, 31] (R2). [1*] M.R. Desselle et al. “Augmented and virtual reality in surgery.” Computing in Science &amp; Engineering. 2020 back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0255/12/31/Paper1584" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0255/12/31/Paper1584" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0255-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Observational Supervision for Medical Image Classification using Gaze Data" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0255/12/31/Paper1584"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0255/12/31/Paper1584","headline":"Observational Supervision for Medical Image Classification using Gaze Data","dateModified":"0256-01-01T00:00:00-05:17","datePublished":"0255-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Khaled Saab, Sarah M. Hooper, Nimit S. Sohoni, Jupinder Parmar, Brian Pogatchnik, Sen Wu, Jared A. Dunnmon, Hongyang R. Zhang, Daniel Rubin, Christopher Ré Abstract Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinician’s workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines. Link to paper https://doi.org/10.1007/978-3-030-87196-3_56 Link to the code repository https://github.com/HazyResearch/observational Link to the dataset(s) https://github.com/HazyResearch/observational Reviews Review #1 Please describe the contribution of the paper The work reports using eye gaze data for observational supervision. They use eye gaze in supervised and unsupervised settings. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This is in my opinion good bit of work focusing on a modality that should be taken seriously (eye gaze data). The methodology described is fresh, although it could be a bit simplistic specially with statistical features. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. My main concern is that the actual coordinates information in eye gaze data is somewhat underused. As the authors in [13] claim, one can use eye gaze data in UNet like architectures. The authors here use helper tasks. It seems at least a discussion of other possibilities is warranted. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Two datasets are claimed to be slated for public release. A third party dataset is already in public domain. I am optimistic about the reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please define the helper tasks a bit more clearly. Currently this is vague in the paper. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I think despite the limited computational contributions, this is a fresh kind of work that promises the possibility of training interpretable classifiers without need for localized labeling. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The paper “Observational Supervision for Medical Image Classification using Gaze Data” is about the usage of visual attention as annotation source for X-Ray images. They claim that the fixation duration is a strong indicator for a valid label. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Positive: Interesting approach Interesting findings Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Negative: Used three experts for one dataset and one expert for another. Use other public datasets with gaze information for evaluation. Claims in “Gaze data statistics” are not empirically shown. Evaluated only one model. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Not possible due to the hold back data. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Required improvements: Use more experts for the recording Evaluate your claims in “Gaze data statistics” empirically Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall to few experts for recording and the claims are not evaluated. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper focuses on the use of gaze data from medical imaging review as support for imaging classification. The authors propose to extract several gaze features (namely “Time on maximum patch”, “Diffusivity”, “Unique visits”, “Time spent”) from the gaze data and train a neural network to predict these features that are later converted to the probability of binary classification. Moreover, the authors propose to train the neural network to predict both, binary class as the main task and gaze features as secondary task aiming for improved performance on the main task. The authors propose an evaluation on several medical imaging classification tasks to prove their intuition, in particular showing that training a network on multiple tasks allows for higher performances than a baseline binary classifier. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper provides a respectful amount of experiments to verify and prove the intuition behind the proposed method. Several datasets have been used with different protocols for collecting the gaze data. Several ablation studies are described, in particular, evaluating the contribution of the proposed gaze features and the results are given in supplementary material. Overall the paper is well written and the descriptions of the method are clear. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Unfortunately, the clinical feasibility of the proposed method remains the main concern. That is, the collection of the gaze data in the clinical environment may be burdensome and the obtained data may be too operator-specific. That is, the clinical review of the radiological imaging substantially varies, with the hanging protocols usually also operator-specific (e.g., one or several screens being used, displaying one or several images on the screens at a time, various order of the displayed images, etc.). This makes the implementation of the proposed method doubtful and eventually losing compared to NLP methods based on clinical reports. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors describe well the datasets being used The authors give details of the neural network being used in the experiments The authors state the training process settings and eventual pre-training (i.e., 15 epochs on ImageNet) The results are given over several runs with the respective standard deviations The codes are provided in supplementary material Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Table 1 the authors list the gaze features used in the method. It would help the reader if the authors could give the types, the ranges of the features, preventing the reader from looking in the supplementary material. In 3.3 the authors give the equation of the loss function being minimized, with l_j, j=1..m being the losses for the secondary tasks. Could the authors clarify in section 3.3 or later in section 4, which losses have been used for each task (e.g., MAE, MSE, etc.)? Similarly, in the equation in section 3.3, there are no weights on any of the losses. It would be helpful if the authors clearly state the lack of weights, or their presence, and the argument behind the adopted approach. In 4.2 the authors discuss the usefulness of the features, further detailed in the supplementary material. I wonder whether the authors could discuss or add numerical results on experiments having a different number of secondary tasks (i.e., using a different number of features)? That is, from table S.2, one can see that time plays little role in the performances. What would be the effect of removing the time from the scope of secondary tasks? Both models, namely Gaze-WS and Gaze-MTL are built to predict Gaze features. Could the authors discuss or give numerical results on the performances on these secondary tasks? In Related work, the authors talk about NLP-based methods that could be used conjointly with the proposed method. Such methods are generally serving the same purpose, eventually allowing for improved performances. It would be useful for a better understanding of the results of the proposed method if the authors could discuss (e.g., list the result reported in other works) the gains in performances in similar tasks of the NLP-based methods, to illustrate, whether the gains are comparable. Minor observation: The Equation in 3.3 is not referenced. Could the authors put a reference on it (it should be (2))? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is generally well organized, clear, and pleasant to read. The authors describe well the proposed method and do a decent job of evaluating on several datasets. However, the uncertain clinical feasibility remains from my point of view the main disadvantage of the method: that is, the collection of the Gaze data amongst the clinicians (e.g., radiologists) may be a considerably challenging task, more burdensome than, for example, the collection of clinical reports mentioned in the paper. This prevents me from firmly accepting the paper. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #4 Please describe the contribution of the paper The main challenge in deep learning methods for medical imaging is the lack of high quality labelled data for a variety of tasks. This paper hypothesis that gaze data contains latent information regarding the actual task at hand and proposes to use gaze data to weakly supervise deep learning models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. 1)The idea of incorporating gaze tracking data to weakly supervise trained models is novel and interesting 2) Strong evaluation with clearly defined baselines and experiments. The authors compare a CNN model trained with gaze tracking data and compare it against a CNN model trained using high quality labels. Additionally, the authors present clear comparison of the Gaze-MTL model with several gaze data incorporation methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Gaze tracking requires specialized hardware and software, which may be expensive to scale and sometimes be impractical in clinical settings. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have provided the modeling code with clear documentation. They also plan to release their gaze tracking datasets after the review process. Based on this, I believe this work will be reproducible Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It’s a very interesting idea to use gaze tracking data as weakly supervised labels. From a research standpoint, it would be interesting to see how much of the gaze tracking hotspots actually overlap with abnormalities in the image (whenever labeled data i available). This can inform the noisy-ness / quality of the gaze tracking labels. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The idea of using observation information such as gaze to generate weakly supervise medical imaging is an interesting and novel idea. The paper is well written and the authors present a clear evaluation of their methodology. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 1 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The reviewers have favorably assessed the work in question. We ask the authors to go through the comments made by all the reviewers to improve the overall quality of the paper. In particular, some of the claims made could be re-evaluated due to the fact that data was only collected by a single expert. Similarly, the authors should make not of the practicial implications their solution has and how it compares to the NLP based approaches. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback We thank reviewers for their helpful feedback. We are pleased they found our approach novel, our paper well written, and our experimental results compelling, and that they appreciate our contribution of two novel datasets that will be publicly available. Below, we address the main points brought up by the reviewers. Practicality of gaze collection (R3,5, MR): As medical AI tools continue to advance and wearable technology (e.g. AR with eye tracking capability) improves, the required hardware needed to collect gaze data is expected to become more ubiquitous, affordable, and standardized [1*] (R5). In this context, straightforward strategies could be developed to mitigate the impact of clinical protocol variations on the utility of gaze data – for instance, a wearable’s frontal camera data could be used to keep track of which gaze data belongs to which image (R3). Indeed, we hope that by demonstrating the usefulness of gaze data for medical ML supervision, our work will also inspire further development of efficient approaches to large-scale gaze data collection. NLP Annotators (R1,3, MR): Since our datasets do not contain medical reports, we could not do a direct comparison with NLP annotators (R3, MR). However, we would like to emphasize that we view observational supervision as a complement to NLP annotators. Firstly, in situations where text reports or trained NLP models are not available, we show that gaze data is a viable alternative source of supervision. Second, if NLP-based labels are available, combining information from the gaze signals may improve label accuracy (since gaze may contain information beyond the reports, e.g. expert confidence), or the gaze data may be used alongside the NLP-derived labels to further improve performance, e.g. via the Gaze-MTL method we propose (R3, MR). While we mentioned these points on Page 3 of our original manuscript, we will emphasize them in the revised manuscript. As suggested by R1, we will expand our related work section to include recently published techniques by Karargyris et al. [13] (R1). Helper tasks in Gaze-MTL (R1,3): A helper task is defined to be the task of predicting the value of a gaze feature for each training image (R1). The loss function used for all helper tasks is the standard soft cross-entropy loss [23]. The equation in Section 3.3 should include different weights on the helper task losses, as these hyperparameters may impact positive task transfer in MTL (R3). In our experiments, we chose each weight from the set {0, 0.5, 1, 2} that achieves the highest validation accuracy (R3). For experimenting on a different number of helper tasks, we found that in three of our medical tasks, tuned weights corresponded to choosing a single helper task (i.e. weight of zero was given to all but one helper task). The chosen helper tasks are those that are bolded in Table S.2 (we will include our hyperparameters in the code and revised manuscript) (R3). We will include the helper task performances in a new column of Table S.2 (R3). We will add a column in Table 1 summarizing the range of the features shown in Figure S.1 (R3). We will integrate these clarifications in the revised manuscript. Reproducibility, datasets, and claims (R2, MR): We emphasize that both our code and datasets will be publicly released, making it possible to reproduce our results (as pointed out by R1,3,5) (R2). We also emphasize that we collected gaze data on three highly-trained radiologists for CXR-P, an expensive effort (MR). While having one expert for METS is a current limitation, we hope our work inspires the community to contribute more gaze datasets from multiple experts (R2). Our claims in the “gaze data statistics” section are empirically supported by Figure S.1 (R2). Lastly, we used a standard CNN architecture (ResNet50) commonly used for our tasks in the literature [4, 31] (R2). [1*] M.R. Desselle et al. “Augmented and virtual reality in surgery.” Computing in Science &amp; Engineering. 2020 back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Saab, Khaled,Hooper, Sarah M.,Sohoni, Nimit S.,Parmar, Jupinder,Pogatchnik, Brian,Wu, Sen,Dunnmon, Jared A.,Zhang, Hongyang R.,Rubin, Daniel,Ré, Christopher" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Observational Supervision for Medical Image Classification using Gaze Data</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Machine Learning - Weakly supervised learning"
        class="post-category">
        Machine Learning - Weakly supervised learning
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Saab, Khaled"
        class="post-tags">
        Saab, Khaled
      </a> |  
      
      <a href="kittywong/tags#Hooper, Sarah M."
        class="post-tags">
        Hooper, Sarah M.
      </a> |  
      
      <a href="kittywong/tags#Sohoni, Nimit S."
        class="post-tags">
        Sohoni, Nimit S.
      </a> |  
      
      <a href="kittywong/tags#Parmar, Jupinder"
        class="post-tags">
        Parmar, Jupinder
      </a> |  
      
      <a href="kittywong/tags#Pogatchnik, Brian"
        class="post-tags">
        Pogatchnik, Brian
      </a> |  
      
      <a href="kittywong/tags#Wu, Sen"
        class="post-tags">
        Wu, Sen
      </a> |  
      
      <a href="kittywong/tags#Dunnmon, Jared A."
        class="post-tags">
        Dunnmon, Jared A.
      </a> |  
      
      <a href="kittywong/tags#Zhang, Hongyang R."
        class="post-tags">
        Zhang, Hongyang R.
      </a> |  
      
      <a href="kittywong/tags#Rubin, Daniel"
        class="post-tags">
        Rubin, Daniel
      </a> |  
      
      <a href="kittywong/tags#Ré, Christopher"
        class="post-tags">
        Ré, Christopher
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Khaled Saab, Sarah M. Hooper, Nimit S. Sohoni, Jupinder Parmar, Brian Pogatchnik, Sen Wu, Jared A. Dunnmon, Hongyang R. Zhang, Daniel Rubin, Christopher Ré
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinician’s workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87196-3_56">https://doi.org/10.1007/978-3-030-87196-3_56</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/HazyResearch/observational
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>https://github.com/HazyResearch/observational
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The work reports using eye gaze data for observational supervision. They use eye gaze in supervised and unsupervised settings.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>This is in my opinion good bit of work focusing on a modality that should be taken seriously (eye gaze data). The methodology described is fresh, although it could be a bit simplistic specially with statistical features.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>My main concern is that the actual coordinates information in eye gaze data is somewhat underused. As the authors in [13] claim, one can use eye gaze data in UNet like architectures. The authors here use helper tasks. It seems at least a discussion of other possibilities is warranted.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Two datasets are claimed to be slated for public release. A third party dataset is already in public domain. I am optimistic about the reproducibility.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Please define the helper tasks a bit more clearly. Currently this is vague in the paper.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>I think despite the limited computational contributions, this is a fresh kind of work that promises the possibility of training interpretable classifiers without need for localized labeling.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper “Observational Supervision for Medical Image Classification using Gaze Data” is about the usage of visual attention as annotation source for X-Ray images. They claim that the fixation duration is a strong indicator for a valid label.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>Positive:</p>
      <ul>
        <li>Interesting approach</li>
        <li>Interesting findings</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>Negative:</p>
      <ul>
        <li>Used three experts for one dataset and one expert for another.</li>
        <li>Use other public datasets with gaze information for evaluation.</li>
        <li>Claims in “Gaze data statistics” are not empirically shown.</li>
        <li>Evaluated only one model.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Not possible due to the hold back data.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Required improvements:</p>
      <ul>
        <li>Use more experts for the recording</li>
        <li>Evaluate your claims in “Gaze data statistics” empirically</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>reject (3)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Overall to few experts for recording and the claims are not evaluated.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper focuses on the use of gaze data from medical imaging review as support for imaging classification. The authors propose to extract several gaze features (namely “Time on maximum patch”, “Diffusivity”, “Unique visits”, “Time spent”) from the gaze data and train a neural network to predict these features that are later converted to the probability of binary classification. Moreover, the authors propose to train the neural network to predict both, binary class as the main task and gaze features as secondary task aiming for improved performance on the main task. The authors propose an evaluation on several medical imaging classification tasks to prove their intuition, in particular showing that training a network on multiple tasks allows for higher performances than a baseline binary classifier.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The paper provides a respectful amount of experiments to verify and prove the intuition behind the proposed method. Several datasets have been used with different protocols for collecting the gaze data. Several ablation studies are described, in particular, evaluating the contribution of the proposed gaze features and the results are given in supplementary material.</p>

      <p>Overall the paper is well written and the descriptions of the method are clear.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>Unfortunately, the clinical feasibility of the proposed method remains the main concern. That is, the collection of the gaze data in the clinical environment may be burdensome and the obtained data may be too operator-specific. That is, the clinical review of the radiological imaging substantially varies, with the hanging protocols usually also operator-specific (e.g., one or several screens being used, displaying one or several images on the screens at a time, various order of the displayed images, etc.). This makes the implementation of the proposed method doubtful and eventually losing compared to NLP methods based on clinical reports.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The authors describe well the datasets being used</p>

      <p>The authors give details of the neural network being used in the experiments</p>

      <p>The authors state the training process settings and eventual pre-training (i.e., 15 epochs on ImageNet)</p>

      <p>The results are given over several runs with the respective standard deviations</p>

      <p>The codes are provided in supplementary material</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ol>
        <li>
          <p>In Table 1 the authors list the gaze features used in the method. It would help the reader if the authors could give the types, the ranges of the features, preventing the reader from looking in the supplementary material.</p>
        </li>
        <li>
          <p>In 3.3 the authors give the equation of the loss function being minimized, with l_j, j=1..m being the losses for the secondary tasks. Could the authors clarify in section 3.3 or later in section 4, which losses have been used for each task (e.g., MAE, MSE, etc.)?</p>
        </li>
        <li>
          <p>Similarly, in the equation in section 3.3, there are no weights on any of the losses. It would be helpful if the authors clearly state the lack of weights, or their presence, and the argument behind the adopted approach.</p>
        </li>
        <li>
          <p>In 4.2 the authors discuss the usefulness of the features, further detailed in the supplementary material. I wonder whether the authors could discuss or add numerical results on experiments having a different number of secondary tasks (i.e., using a different number of features)? That is, from table S.2, one can see that time plays little role in the performances. What would be the effect of removing the time from the scope of secondary tasks?</p>
        </li>
        <li>
          <p>Both models, namely Gaze-WS and Gaze-MTL are built to predict Gaze features. Could the authors discuss or give numerical results on the performances on these secondary tasks?</p>
        </li>
        <li>
          <p>In Related work, the authors talk about NLP-based methods that could be used conjointly with the proposed method. Such methods are generally serving the same purpose, eventually allowing for improved performances. It would be useful for a better understanding of the results of the proposed method if the authors could discuss (e.g., list the result reported in other works) the gains in performances in similar tasks of the NLP-based methods, to illustrate, whether the gains are comparable.</p>
        </li>
      </ol>

      <p>Minor observation:</p>

      <ol>
        <li>The Equation in 3.3 is not referenced. Could the authors put a reference on it (it should be (2))?</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper is generally well organized, clear, and pleasant to read. The authors describe well the proposed method and do a decent job of evaluating on several datasets. However, the uncertain clinical feasibility remains from my point of view the main disadvantage of the method: that is, the collection of the Gaze data amongst the clinicians (e.g., radiologists) may be a considerably challenging task, more burdensome than, for example, the collection of clinical reports mentioned in the paper. This prevents me from firmly accepting the paper.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-4">Review #4</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The main challenge in deep learning methods for  medical imaging is the lack of high quality labelled data for a variety of tasks. This paper hypothesis that gaze data contains latent information regarding the actual task at hand and proposes to use gaze data to weakly supervise deep learning models.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>1)The idea of incorporating gaze tracking data to weakly supervise trained models  is novel and interesting</p>

      <p>2)  Strong evaluation with clearly defined baselines and experiments. The authors compare a CNN model  trained with gaze tracking data and compare it against a CNN model trained using high quality labels. Additionally, the authors present clear comparison of the Gaze-MTL model with several gaze data incorporation methods.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>Gaze tracking requires specialized hardware and software, which may be expensive to scale and sometimes be impractical in clinical settings.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The authors have  provided the modeling code with clear documentation. They also plan to release their gaze tracking datasets after the review process.  Based on this, I believe this work will be reproducible</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>It’s a very interesting idea to use gaze tracking data as weakly supervised labels.
From a research standpoint, it would be interesting to see how much of the gaze tracking hotspots actually overlap with abnormalities in the image (whenever labeled data i available). This can inform the noisy-ness / quality of the gaze tracking labels.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The idea of using observation information such as gaze to generate weakly supervise medical imaging is an interesting and novel idea. The paper is well written and the authors present a clear evaluation of their methodology.</p>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>The reviewers have favorably assessed the work in question. We ask the authors to go through the comments made by all the reviewers to improve the overall quality of the paper.</p>

      <p>In particular, some of the claims made could be re-evaluated due to the fact that data was only collected by a single expert. Similarly, the authors should make not of the practicial implications their solution has and how it compares to the NLP based approaches.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We thank reviewers for their helpful feedback. We are pleased they found our approach novel, our paper well written, and our experimental results compelling, and that they appreciate our contribution of two novel datasets that will be publicly available. Below, we address the main points brought up by the reviewers.</p>

  <p>Practicality of gaze collection (R3,5, MR): As medical AI tools continue to advance and wearable technology (e.g. AR with eye tracking capability) improves, the required hardware needed to collect gaze data is expected to become more ubiquitous, affordable, and standardized [1*] (R5). In this context, straightforward strategies could be developed to mitigate the impact of clinical protocol variations on the utility of gaze data – for instance, a wearable’s frontal camera data could be used to keep track of which gaze data belongs to which image (R3). Indeed, we hope that by demonstrating the usefulness of gaze data for medical ML supervision, our work will also inspire further development of efficient approaches to large-scale gaze data collection.</p>

  <p>NLP Annotators (R1,3, MR): Since our datasets do not contain medical reports, we could not do a direct comparison with NLP annotators (R3, MR). However, we would like to emphasize that we view observational supervision as a complement to NLP annotators. Firstly, in situations where text reports or trained NLP models are not available, we show that gaze data is a viable alternative source of supervision. Second, if NLP-based labels are available, combining information from the gaze signals may improve label accuracy (since gaze may contain information beyond the reports, e.g. expert confidence), or the gaze data may be used alongside the NLP-derived labels to further improve performance, e.g. via the Gaze-MTL method we propose (R3, MR). While we mentioned these points on Page 3 of our original manuscript, we will emphasize them in the revised manuscript. As suggested by R1, we will expand our related work section to include recently published techniques by Karargyris et al. [13] (R1).</p>

  <p>Helper tasks in Gaze-MTL (R1,3): A helper task is defined to be the task of predicting the value of a gaze feature for each training image (R1). The loss function used for all helper tasks is the standard soft cross-entropy loss [23]. The equation in Section 3.3 should include different weights on the helper task losses, as these hyperparameters may impact positive task transfer in MTL (R3). In our experiments, we chose each weight from the set {0, 0.5, 1, 2} that achieves the highest validation accuracy (R3). For experimenting on a different number of helper tasks, we found that in three of our medical tasks, tuned weights corresponded to choosing a single helper task (i.e. weight of zero was given to all but one helper task). The chosen helper tasks are those that are bolded in Table S.2 (we will include our hyperparameters in the code and revised manuscript) (R3). We will include the helper task performances in a new column of Table S.2 (R3). We will add a column in Table 1 summarizing the range of the features shown in Figure S.1 (R3). We will integrate these clarifications in the revised manuscript.</p>

  <p>Reproducibility, datasets, and claims (R2, MR): We emphasize that both our code and datasets will be publicly released, making it possible to reproduce our results (as pointed out by R1,3,5) (R2). We also emphasize that we collected gaze data on three highly-trained radiologists for CXR-P, an expensive effort (MR). While having one expert for METS is a current limitation, we hope our work inspires the community to contribute more gaze datasets from multiple experts (R2). Our claims in the “gaze data statistics” section are empirically supported by Figure S.1 (R2). Lastly, we used a standard CNN architecture (ResNet50) commonly used for our tasks in the literature [4, 31] (R2).</p>

  <p>[1*] M.R. Desselle et al. “Augmented and virtual reality in surgery.” Computing in Science &amp; Engineering. 2020</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0255-12-31
      -->
      <!--
      
        ,
        updated at 
        0256-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Machine Learning - Weakly supervised learning"
        class="post-category">
        Machine Learning - Weakly supervised learning
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Saab, Khaled"
        class="post-category">
        Saab, Khaled
      </a> |  
      
      <a href="kittywong/tags#Hooper, Sarah M."
        class="post-category">
        Hooper, Sarah M.
      </a> |  
      
      <a href="kittywong/tags#Sohoni, Nimit S."
        class="post-category">
        Sohoni, Nimit S.
      </a> |  
      
      <a href="kittywong/tags#Parmar, Jupinder"
        class="post-category">
        Parmar, Jupinder
      </a> |  
      
      <a href="kittywong/tags#Pogatchnik, Brian"
        class="post-category">
        Pogatchnik, Brian
      </a> |  
      
      <a href="kittywong/tags#Wu, Sen"
        class="post-category">
        Wu, Sen
      </a> |  
      
      <a href="kittywong/tags#Dunnmon, Jared A."
        class="post-category">
        Dunnmon, Jared A.
      </a> |  
      
      <a href="kittywong/tags#Zhang, Hongyang R."
        class="post-category">
        Zhang, Hongyang R.
      </a> |  
      
      <a href="kittywong/tags#Rubin, Daniel"
        class="post-category">
        Rubin, Daniel
      </a> |  
      
      <a href="kittywong/tags#Ré, Christopher"
        class="post-category">
        Ré, Christopher
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0256/12/31/Paper1754">
          Inter Extreme Points Geodesics for End-to-End Weakly Supervised Image Segmentation
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0254/12/31/Paper1362">
          CPNet: Cycle Prototype Network for Weakly-supervised 3D Renal Chamber Segmentation
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
