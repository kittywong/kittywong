<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Semi-Supervised Unpaired Multi-Modal Learning for Label-Efficient Medical Image Segmentation | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Semi-Supervised Unpaired Multi-Modal Learning for Label-Efficient Medical Image Segmentation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Lei Zhu, Kaiyuan Yang, Meihui Zhang, Ling Ling Chan, Teck Khim Ng, Beng Chin Ooi Abstract Multi-modal learning using unpaired labeled data from multiple modalities to boost the performance of deep learning models on each individual modality has attracted a lot of interest in medical image segmentation recently. However, existing unpaired multi-modal learning methods require a considerable amount of labeled data from both modalities to obtain satisfying segmentation results which are not easy to obtain in reality. In this paper, we investigate the use of unlabeled data for label-efficient unpaired multi-modal learning, with a focus on the scenario when labeled data is scarce and unlabeled data is abundant. We term this new problem as Semi-Supervised Unpaired Multi-Modal Learning and thereupon, propose a novel deep co-training framework. Specifically, our framework consists of two segmentation networks, where we train one of them for each modality. Unlabeled data is effectively applied to learn two image translation networks for translating images across modalities. Thus, labeled data from one modality is employed for the training of the segmentation network in the other modality after image translation. To prevent overfitting under the label scarce scenario, we introduce a new semantic consistency loss to regularize the predictions of an image and its translation from the two segmentation networks to be semantically consistent. We further design a novel class-balanced deep co-training scheme to effectively leverage the valuable complementary information from both modalities to boost the segmentation performance. We verify the effectiveness of our framework with two medical image segmentation tasks and our framework outperforms existing methods significantly. Link to paper https://doi.org/10.1007/978-3-030-87196-3_37 Link to the code repository https://github.com/nusdbsystem/SSUMML Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper presents a semi-supervised learning setting for unpaired multi-modality medical image segmentation to reduce the annotation effort. The proposed method utilizes GAN to translate images between two modalities and adopt the deep co-training strategy to utilize the unlabeled data. The proposed method was evaluated on public cardiac image segmentation and abdominal multi-organ segmentation tasks and its performance outperforms other baseline methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The whole pipeline is reasonable and the whole paper is well-written. The authors conduct an extensive evaluation on two public datasets and the proposed method achieves better performances. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. In genral, the whole pipeline is clearly demonstrated, while some important technical details are unclear. For example, how to train the image translation module and the image translation module is pre-trained or trained with the segmentation network in an end-to-end manner. It seems that the dataset split (train vs. test and labeled vs. unlabeled) is conducted on image slice level. However, this setting maybe unfair and leaks the testing data information. And for the annotation issue, it is very common to only annotate some scenes/volumes while it is uncommon to annotation some slices in each volume. It seems that the proposed co-training scheme is redundant with the semantic consistency scheme. The proposed problem setting and method is very similar to semi-supervised domain adaption (or semi-supervised multi-modality learning). The authors should compare their method with these methods, like Li, Kang, et al. “Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation.” International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2020. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance There is no public code for this project. And the whole pipeline includes many hyperparameters and the training of GAN. it may be difficult to reproduce the results only from this paper. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html From Fig. 3, we can see that the performance difference for different \lambda is about 2-3%. Therefore, it is not very confusing to argue that “our method is generally robust to the change of λsc “. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Some issues about method design/experiment setting/comparison should be solved. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper proposes an approach for semi-supervised unpaired multi-modal learning. The method first applies CycleGAN for image translation between CT and MRI data, and regards an image and its cross-modality translation as two different views of the same object. Two segmentation models for each modality are separately trained and a consistency regularization and co-training algorithm are proposed to optimize the segmentation models with the unlabeled data. Validation is performed on public datasets with two segmentation tasks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Although co-training has already been a well-established semi-supervised method, this paper leverages good practices from prior works and applies to unpaired multi-modal learning. Effectiveness is shown with two different segmentation tasks. This paper is easy to follow. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness of this work is that the method is not compared with strong and sufficient baselines, thus the claimed effectiveness is not very convincing. Authors compare their methods mainly with different multi-modal approaches, which are not specifically designed for semi-supervised setting, thus it is as expected that the proposed method outperforms those approaches. Only one simple semi-supervised method proposed in 2013 is considered in the comparison, which is insufficient to demonstrate the superiority of the proposed method. Authors are suggested to compare with other state-of-the-art semi-supervised approaches. Also, authors should also consider previous multi-modal learning method presented in [1], which uses similar design of image translation for obtaining different views and may serve as stronger and more relevant baselines than [3][9]. From the results, it is difficult to know how helpful the semi-supervised unpaired multi-modal learning is. The models trained with 100% labels should be included to know the performance gap when only 0.5% and 2.5% labels are used. A curve of the performance with the increase in the percentage of labeled data should be shown. Another interesting baseline would be training “ST-single” with double percentage of labels. For example, training “ST-single” with 1% labels of single modality and comparing to the proposed method trained with 0.5% labels of two modalities, which can demonstrate the benefits of unpaired multi-modal learning. Authors propose a class-balanced deep co-training scheme, but whether the selection of \alpha% for each class instead of the all pixels contributes to the performance is not experimentally shown. Also, the “class-balanced” claim is misleading. If the classes are imbalanced in an image, for example, spleen is much smaller than liver, with the presented pseudo-label selection strategy, the classes would still remain imbalanced. [1] Li, Kang, Lequan Yu, Shujun Wang, and Pheng-Ann Heng. “Towards cross-modality medical image segmentation with online mutual knowledge distillation.” In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 01, pp. 775-783. 2020. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Reproducibility is satisfactory. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html One problem of the experimental setting is that the labeled/unlabeled data are split in the image level instead of the volume level. Although the 3D MRI/CT are converted into slices to train 2D networks, the more practical split of the labeled/unlabeled data should still be in the volume level of different subjects. Is the evaluation also performed slice by slice? The evaluation metrics should be calculated in the volume level. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The method seems work well for the semi-supervised unpaired multi-modal learning, but the comparison baselines are not strong/sufficient enough. The technical contribution of this paper is tangible. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper presents a semi-supervised learning method for unpaired multi-modal medical segmentation tasks. Built upon the image translation models, the authors propose a class-balanced deep co-training method, which achieves superior results than previous state-of-the-arts on fully-supervised multi-modal learning methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The contributions are well illustrated and quite clear. The paper is well organized and easy to follow The superiority of the proposed approach is demonstrated on different unpaired multi-modal segmentation tasks. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Missing references: for deep co-training [a], there have been multiple applications [b,c] which use multiple learners to learn representation from different views and make the final prediction by mining the consensus information among these different learners. These literatures should also be included in the introduction or experiment part. For the comparison, image translation seems to be not included in all of the compared methods (e.g., X/Y-shape, ST-joint), which makes the comparison not completely fair. For a fairer comparison, please include translated images as augmentation for all the compared methods, to guarantee that the input data are the same. For the SC loss, I am curious about whether other loss terms (e.g., contrastive loss) could outperform KL divergence loss. It would be interesting to see some discussion related to this part. [a] Qiao, Siyuan, et al. “Deep co-training for semi-supervised image recognition.” Proceedings of the european conference on computer vision (eccv). 2018. [b] Zhou, Yuyin, et al. “Semi-supervised 3D abdominal multi-organ segmentation via deep multi-planar co-training.” 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019. [c] Peng, Jizong, et al. “Deep co-training for semi-supervised image segmentation.” Pattern Recognition 107 (2020): 107269. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The code is not included in the submission. I strongly recommend the authors to release the code in the next version. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html please see above. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall the paper presents an important problem and also designs a viable solution by using generative models and deep co-training. Some minor issues need to be addressed, such as adding references &amp; experimental comparison. Other than that, I feel this could be one important study along the semi-supervised learning direction. Therefore I recommend acceptance. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation to utilize the unlabeled dataset. They evaluated their methods on two public datasets, i.e., cardiac images and abdominal multi-organ images, and claimed better performance than other baseline methods. However, there are three main points raised by reviewers needed to considers: 1) the image format (2D slice instead of 3D volume) used in the training/test stage. 2) the comparison experiments are not convincing enough as almost no semi-supervised methods were employed for comparison. 3) the parameter experiments of the deep co-training scheme seem to be confusing. Therefore, my recommendation for this paper is “Invite for Rebuttal”. Note that the purpose of the rebuttal is to provide clarification or to point out misunderstandings instead of promising additional experiments. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This work proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation. Although the idea of co-training is not novel, all reviewers evaluated the writing and experiments highly enough. The authors also answered the concerns well. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. From the first meta-review: The authors proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation to utilize the unlabeled dataset. They evaluated their methods on two public datasets, i.e., cardiac images and abdominal multi-organ images, and claimed better performance than other baseline methods. However, there are three main points raised by reviewers needed to considers: 1) the image format (2D slice instead of 3D volume) used in the training/test stage. 2) the comparison experiments are not convincing enough as almost no semi-supervised methods were employed for comparison. 3) the parameter experiments of the deep co-training scheme seem to be confusing. Therefore, my recommendation for this paper is “Invite for Rebuttal”. Note that the purpose of the rebuttal is to provide clarification or to point out misunderstandings instead of promising additional experiments. In the rebuttal, the authors concurred that additional experiments are required to address reviews. These additional experiments cannot be fully peer reviewed, so the paper cannot be accepted. Here is the relevant section of the rebuttal instructions: “An effective rebuttal addresses reviewers’ criticisms by explaining where in the paper you had provided the requisite information, perhaps further clarifying it. Do not promise to expand your paper to address all the questions raised by the reviewers, as you will not be able to change your article substantially, and in all likelihood you don’t have sufficient room to add to the paper.” After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 17 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper studies the unpaired multi-modality medical image segmentation using a co-training neural network under the semi-supervised setting. In the neural network, GAN is used to translate images acroos modalities and adopts the deep co-training strategy to utilize the unlabeled data. The idea appears reasonable, which is supported by the experimental results reported in the paper. The authors’ rebuttal have largely addressed the questions and concerns raised by the reviewers. Thus I recommend an acceptance to this paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Author Feedback We thank all the reviewers for their valuable comments. The main concerns are addressed as follows: *1. Image format in training/test stage (Weakness 2 by Reviewer#1 and Question from Reviewer#2): We would like to clarify that our training and test data have been split at volume level, not on image slice level, so there is no test data information leakage to training data. Similarly, for labeled and unlabeled data in training stage, the split has also been at volume level. We also want to highlight that there is no information leakage from labeled set to unlabeled set because when we select certain slices in a volume to form labeled set, the remaining slices from the same volume will not be used as unlabeled data. Lastly, the evaluation metrics were calculated at volume level. Thanks for pointing out the confusion, we will clarify the above information in the revision. *2. Volume vs slice level annotation (Weakness 2 by Reviewer#1): We want to clarify that our method does not impose any annotation restriction, and doctors are free to annotate either by volume or by ad hoc slices, which are both practised in clinical settings. In fact, one of our contributions is that when doctors do label by ad hoc slices, our method would still be performant when only a small amount of slices are labeled, effectively reducing the annotation burden. *3. Insufficient comparison experiments with semi-supervised methods (Weakness 4 by Reviewer#1 and Weakness 1 by Reviewer#2): First, we clarify the selection of ST-single and ST-joint as our semi-supervised baseline methods. Although the semi-supervised learning method proposed in the ST-single/ST-joint paper is generic, we should have mentioned that we intentionally implemented ST-single/ST-joint with several improvements. For example, ST-joint employs modality specific batch normalization layers to reduce modality difference, so that it can effectively leverage shared cross-modality information for semi-supervised learning. Thus we felt the implemented ST-single and ST-joint to be reasonable semi-supervised baselines, where they represent semi-supervised learning with different numbers of modalities. Second, we have performed the following new experiments on the two works from Li Kang et al., as requested, for more complete and stronger comparison. Here are the test dice scores on cardiac segmentation: Li Kang (MICCAI 2020): 0.5% data: MRI 70.8, CT 75.7; 2.5% data: MRI 81.9, CT 84.9; Li Kang (AAAI 2020): 0.5% data: MRI 70.9, CT 79.6; 2.5% data: MRI 82.3, CT 85.6; Our method outperforms Li Kang’s methods by average 5.3% and 4.0% respectively. We reason this is because they do not fully utilize all available datasets, as unlabeled source data is not used for training segmentation networks by them. In addition, while our method optimizes performance for both modalities, Li Kang’s two methods only optimize performance for target modality, thus they may not be able to fully leverage on the multi-modality information. Interestingly, ST-single and ST-joint also give comparable or stronger results compared with Li Kang’s two methods. Thanks for the suggestions, we will add both Li Kang’s methods in the revision. *4. Redundant co-training (Weakness 3 by Reviewer#1): Semantic Consistency (SC) and co-training are complementary instead of redundant as justified in our ablation study in Table 2. Co-training learns discriminative features for unlabeled data while SC is for regularization with scarce labeled data. *5. Robustness of lambda_sc (Question from Reviewer#1): Our sensitivity analysis in Fig3 is meant to convey that even when we scale lambda_sc by 100 times, the performance only fluctuates by 2-3%, thus we felt our method is relatively robust to hyper-parameter change. *6. Training details and Code release (Reviewer#1 and Reviewer#3): Our entire framework, including image translation module, is trained in an end-to-end manner. We will release our code on GitHub in the revision. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Lei Zhu, Kaiyuan Yang, Meihui Zhang, Ling Ling Chan, Teck Khim Ng, Beng Chin Ooi Abstract Multi-modal learning using unpaired labeled data from multiple modalities to boost the performance of deep learning models on each individual modality has attracted a lot of interest in medical image segmentation recently. However, existing unpaired multi-modal learning methods require a considerable amount of labeled data from both modalities to obtain satisfying segmentation results which are not easy to obtain in reality. In this paper, we investigate the use of unlabeled data for label-efficient unpaired multi-modal learning, with a focus on the scenario when labeled data is scarce and unlabeled data is abundant. We term this new problem as Semi-Supervised Unpaired Multi-Modal Learning and thereupon, propose a novel deep co-training framework. Specifically, our framework consists of two segmentation networks, where we train one of them for each modality. Unlabeled data is effectively applied to learn two image translation networks for translating images across modalities. Thus, labeled data from one modality is employed for the training of the segmentation network in the other modality after image translation. To prevent overfitting under the label scarce scenario, we introduce a new semantic consistency loss to regularize the predictions of an image and its translation from the two segmentation networks to be semantically consistent. We further design a novel class-balanced deep co-training scheme to effectively leverage the valuable complementary information from both modalities to boost the segmentation performance. We verify the effectiveness of our framework with two medical image segmentation tasks and our framework outperforms existing methods significantly. Link to paper https://doi.org/10.1007/978-3-030-87196-3_37 Link to the code repository https://github.com/nusdbsystem/SSUMML Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper presents a semi-supervised learning setting for unpaired multi-modality medical image segmentation to reduce the annotation effort. The proposed method utilizes GAN to translate images between two modalities and adopt the deep co-training strategy to utilize the unlabeled data. The proposed method was evaluated on public cardiac image segmentation and abdominal multi-organ segmentation tasks and its performance outperforms other baseline methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The whole pipeline is reasonable and the whole paper is well-written. The authors conduct an extensive evaluation on two public datasets and the proposed method achieves better performances. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. In genral, the whole pipeline is clearly demonstrated, while some important technical details are unclear. For example, how to train the image translation module and the image translation module is pre-trained or trained with the segmentation network in an end-to-end manner. It seems that the dataset split (train vs. test and labeled vs. unlabeled) is conducted on image slice level. However, this setting maybe unfair and leaks the testing data information. And for the annotation issue, it is very common to only annotate some scenes/volumes while it is uncommon to annotation some slices in each volume. It seems that the proposed co-training scheme is redundant with the semantic consistency scheme. The proposed problem setting and method is very similar to semi-supervised domain adaption (or semi-supervised multi-modality learning). The authors should compare their method with these methods, like Li, Kang, et al. “Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation.” International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2020. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance There is no public code for this project. And the whole pipeline includes many hyperparameters and the training of GAN. it may be difficult to reproduce the results only from this paper. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html From Fig. 3, we can see that the performance difference for different \lambda is about 2-3%. Therefore, it is not very confusing to argue that “our method is generally robust to the change of λsc “. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Some issues about method design/experiment setting/comparison should be solved. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper proposes an approach for semi-supervised unpaired multi-modal learning. The method first applies CycleGAN for image translation between CT and MRI data, and regards an image and its cross-modality translation as two different views of the same object. Two segmentation models for each modality are separately trained and a consistency regularization and co-training algorithm are proposed to optimize the segmentation models with the unlabeled data. Validation is performed on public datasets with two segmentation tasks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Although co-training has already been a well-established semi-supervised method, this paper leverages good practices from prior works and applies to unpaired multi-modal learning. Effectiveness is shown with two different segmentation tasks. This paper is easy to follow. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness of this work is that the method is not compared with strong and sufficient baselines, thus the claimed effectiveness is not very convincing. Authors compare their methods mainly with different multi-modal approaches, which are not specifically designed for semi-supervised setting, thus it is as expected that the proposed method outperforms those approaches. Only one simple semi-supervised method proposed in 2013 is considered in the comparison, which is insufficient to demonstrate the superiority of the proposed method. Authors are suggested to compare with other state-of-the-art semi-supervised approaches. Also, authors should also consider previous multi-modal learning method presented in [1], which uses similar design of image translation for obtaining different views and may serve as stronger and more relevant baselines than [3][9]. From the results, it is difficult to know how helpful the semi-supervised unpaired multi-modal learning is. The models trained with 100% labels should be included to know the performance gap when only 0.5% and 2.5% labels are used. A curve of the performance with the increase in the percentage of labeled data should be shown. Another interesting baseline would be training “ST-single” with double percentage of labels. For example, training “ST-single” with 1% labels of single modality and comparing to the proposed method trained with 0.5% labels of two modalities, which can demonstrate the benefits of unpaired multi-modal learning. Authors propose a class-balanced deep co-training scheme, but whether the selection of \alpha% for each class instead of the all pixels contributes to the performance is not experimentally shown. Also, the “class-balanced” claim is misleading. If the classes are imbalanced in an image, for example, spleen is much smaller than liver, with the presented pseudo-label selection strategy, the classes would still remain imbalanced. [1] Li, Kang, Lequan Yu, Shujun Wang, and Pheng-Ann Heng. “Towards cross-modality medical image segmentation with online mutual knowledge distillation.” In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 01, pp. 775-783. 2020. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Reproducibility is satisfactory. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html One problem of the experimental setting is that the labeled/unlabeled data are split in the image level instead of the volume level. Although the 3D MRI/CT are converted into slices to train 2D networks, the more practical split of the labeled/unlabeled data should still be in the volume level of different subjects. Is the evaluation also performed slice by slice? The evaluation metrics should be calculated in the volume level. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The method seems work well for the semi-supervised unpaired multi-modal learning, but the comparison baselines are not strong/sufficient enough. The technical contribution of this paper is tangible. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper presents a semi-supervised learning method for unpaired multi-modal medical segmentation tasks. Built upon the image translation models, the authors propose a class-balanced deep co-training method, which achieves superior results than previous state-of-the-arts on fully-supervised multi-modal learning methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The contributions are well illustrated and quite clear. The paper is well organized and easy to follow The superiority of the proposed approach is demonstrated on different unpaired multi-modal segmentation tasks. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Missing references: for deep co-training [a], there have been multiple applications [b,c] which use multiple learners to learn representation from different views and make the final prediction by mining the consensus information among these different learners. These literatures should also be included in the introduction or experiment part. For the comparison, image translation seems to be not included in all of the compared methods (e.g., X/Y-shape, ST-joint), which makes the comparison not completely fair. For a fairer comparison, please include translated images as augmentation for all the compared methods, to guarantee that the input data are the same. For the SC loss, I am curious about whether other loss terms (e.g., contrastive loss) could outperform KL divergence loss. It would be interesting to see some discussion related to this part. [a] Qiao, Siyuan, et al. “Deep co-training for semi-supervised image recognition.” Proceedings of the european conference on computer vision (eccv). 2018. [b] Zhou, Yuyin, et al. “Semi-supervised 3D abdominal multi-organ segmentation via deep multi-planar co-training.” 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019. [c] Peng, Jizong, et al. “Deep co-training for semi-supervised image segmentation.” Pattern Recognition 107 (2020): 107269. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The code is not included in the submission. I strongly recommend the authors to release the code in the next version. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html please see above. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall the paper presents an important problem and also designs a viable solution by using generative models and deep co-training. Some minor issues need to be addressed, such as adding references &amp; experimental comparison. Other than that, I feel this could be one important study along the semi-supervised learning direction. Therefore I recommend acceptance. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation to utilize the unlabeled dataset. They evaluated their methods on two public datasets, i.e., cardiac images and abdominal multi-organ images, and claimed better performance than other baseline methods. However, there are three main points raised by reviewers needed to considers: 1) the image format (2D slice instead of 3D volume) used in the training/test stage. 2) the comparison experiments are not convincing enough as almost no semi-supervised methods were employed for comparison. 3) the parameter experiments of the deep co-training scheme seem to be confusing. Therefore, my recommendation for this paper is “Invite for Rebuttal”. Note that the purpose of the rebuttal is to provide clarification or to point out misunderstandings instead of promising additional experiments. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This work proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation. Although the idea of co-training is not novel, all reviewers evaluated the writing and experiments highly enough. The authors also answered the concerns well. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. From the first meta-review: The authors proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation to utilize the unlabeled dataset. They evaluated their methods on two public datasets, i.e., cardiac images and abdominal multi-organ images, and claimed better performance than other baseline methods. However, there are three main points raised by reviewers needed to considers: 1) the image format (2D slice instead of 3D volume) used in the training/test stage. 2) the comparison experiments are not convincing enough as almost no semi-supervised methods were employed for comparison. 3) the parameter experiments of the deep co-training scheme seem to be confusing. Therefore, my recommendation for this paper is “Invite for Rebuttal”. Note that the purpose of the rebuttal is to provide clarification or to point out misunderstandings instead of promising additional experiments. In the rebuttal, the authors concurred that additional experiments are required to address reviews. These additional experiments cannot be fully peer reviewed, so the paper cannot be accepted. Here is the relevant section of the rebuttal instructions: “An effective rebuttal addresses reviewers’ criticisms by explaining where in the paper you had provided the requisite information, perhaps further clarifying it. Do not promise to expand your paper to address all the questions raised by the reviewers, as you will not be able to change your article substantially, and in all likelihood you don’t have sufficient room to add to the paper.” After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 17 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper studies the unpaired multi-modality medical image segmentation using a co-training neural network under the semi-supervised setting. In the neural network, GAN is used to translate images acroos modalities and adopts the deep co-training strategy to utilize the unlabeled data. The idea appears reasonable, which is supported by the experimental results reported in the paper. The authors’ rebuttal have largely addressed the questions and concerns raised by the reviewers. Thus I recommend an acceptance to this paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Author Feedback We thank all the reviewers for their valuable comments. The main concerns are addressed as follows: *1. Image format in training/test stage (Weakness 2 by Reviewer#1 and Question from Reviewer#2): We would like to clarify that our training and test data have been split at volume level, not on image slice level, so there is no test data information leakage to training data. Similarly, for labeled and unlabeled data in training stage, the split has also been at volume level. We also want to highlight that there is no information leakage from labeled set to unlabeled set because when we select certain slices in a volume to form labeled set, the remaining slices from the same volume will not be used as unlabeled data. Lastly, the evaluation metrics were calculated at volume level. Thanks for pointing out the confusion, we will clarify the above information in the revision. *2. Volume vs slice level annotation (Weakness 2 by Reviewer#1): We want to clarify that our method does not impose any annotation restriction, and doctors are free to annotate either by volume or by ad hoc slices, which are both practised in clinical settings. In fact, one of our contributions is that when doctors do label by ad hoc slices, our method would still be performant when only a small amount of slices are labeled, effectively reducing the annotation burden. *3. Insufficient comparison experiments with semi-supervised methods (Weakness 4 by Reviewer#1 and Weakness 1 by Reviewer#2): First, we clarify the selection of ST-single and ST-joint as our semi-supervised baseline methods. Although the semi-supervised learning method proposed in the ST-single/ST-joint paper is generic, we should have mentioned that we intentionally implemented ST-single/ST-joint with several improvements. For example, ST-joint employs modality specific batch normalization layers to reduce modality difference, so that it can effectively leverage shared cross-modality information for semi-supervised learning. Thus we felt the implemented ST-single and ST-joint to be reasonable semi-supervised baselines, where they represent semi-supervised learning with different numbers of modalities. Second, we have performed the following new experiments on the two works from Li Kang et al., as requested, for more complete and stronger comparison. Here are the test dice scores on cardiac segmentation: Li Kang (MICCAI 2020): 0.5% data: MRI 70.8, CT 75.7; 2.5% data: MRI 81.9, CT 84.9; Li Kang (AAAI 2020): 0.5% data: MRI 70.9, CT 79.6; 2.5% data: MRI 82.3, CT 85.6; Our method outperforms Li Kang’s methods by average 5.3% and 4.0% respectively. We reason this is because they do not fully utilize all available datasets, as unlabeled source data is not used for training segmentation networks by them. In addition, while our method optimizes performance for both modalities, Li Kang’s two methods only optimize performance for target modality, thus they may not be able to fully leverage on the multi-modality information. Interestingly, ST-single and ST-joint also give comparable or stronger results compared with Li Kang’s two methods. Thanks for the suggestions, we will add both Li Kang’s methods in the revision. *4. Redundant co-training (Weakness 3 by Reviewer#1): Semantic Consistency (SC) and co-training are complementary instead of redundant as justified in our ablation study in Table 2. Co-training learns discriminative features for unlabeled data while SC is for regularization with scarce labeled data. *5. Robustness of lambda_sc (Question from Reviewer#1): Our sensitivity analysis in Fig3 is meant to convey that even when we scale lambda_sc by 100 times, the performance only fluctuates by 2-3%, thus we felt our method is relatively robust to hyper-parameter change. *6. Training details and Code release (Reviewer#1 and Reviewer#3): Our entire framework, including image translation module, is trained in an end-to-end manner. We will release our code on GitHub in the revision. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0236/12/31/Paper0915" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0236/12/31/Paper0915" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0236-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Semi-Supervised Unpaired Multi-Modal Learning for Label-Efficient Medical Image Segmentation" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0236/12/31/Paper0915"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0236/12/31/Paper0915","headline":"Semi-Supervised Unpaired Multi-Modal Learning for Label-Efficient Medical Image Segmentation","dateModified":"0237-01-01T00:00:00-05:17","datePublished":"0236-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Lei Zhu, Kaiyuan Yang, Meihui Zhang, Ling Ling Chan, Teck Khim Ng, Beng Chin Ooi Abstract Multi-modal learning using unpaired labeled data from multiple modalities to boost the performance of deep learning models on each individual modality has attracted a lot of interest in medical image segmentation recently. However, existing unpaired multi-modal learning methods require a considerable amount of labeled data from both modalities to obtain satisfying segmentation results which are not easy to obtain in reality. In this paper, we investigate the use of unlabeled data for label-efficient unpaired multi-modal learning, with a focus on the scenario when labeled data is scarce and unlabeled data is abundant. We term this new problem as Semi-Supervised Unpaired Multi-Modal Learning and thereupon, propose a novel deep co-training framework. Specifically, our framework consists of two segmentation networks, where we train one of them for each modality. Unlabeled data is effectively applied to learn two image translation networks for translating images across modalities. Thus, labeled data from one modality is employed for the training of the segmentation network in the other modality after image translation. To prevent overfitting under the label scarce scenario, we introduce a new semantic consistency loss to regularize the predictions of an image and its translation from the two segmentation networks to be semantically consistent. We further design a novel class-balanced deep co-training scheme to effectively leverage the valuable complementary information from both modalities to boost the segmentation performance. We verify the effectiveness of our framework with two medical image segmentation tasks and our framework outperforms existing methods significantly. Link to paper https://doi.org/10.1007/978-3-030-87196-3_37 Link to the code repository https://github.com/nusdbsystem/SSUMML Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper presents a semi-supervised learning setting for unpaired multi-modality medical image segmentation to reduce the annotation effort. The proposed method utilizes GAN to translate images between two modalities and adopt the deep co-training strategy to utilize the unlabeled data. The proposed method was evaluated on public cardiac image segmentation and abdominal multi-organ segmentation tasks and its performance outperforms other baseline methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The whole pipeline is reasonable and the whole paper is well-written. The authors conduct an extensive evaluation on two public datasets and the proposed method achieves better performances. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. In genral, the whole pipeline is clearly demonstrated, while some important technical details are unclear. For example, how to train the image translation module and the image translation module is pre-trained or trained with the segmentation network in an end-to-end manner. It seems that the dataset split (train vs. test and labeled vs. unlabeled) is conducted on image slice level. However, this setting maybe unfair and leaks the testing data information. And for the annotation issue, it is very common to only annotate some scenes/volumes while it is uncommon to annotation some slices in each volume. It seems that the proposed co-training scheme is redundant with the semantic consistency scheme. The proposed problem setting and method is very similar to semi-supervised domain adaption (or semi-supervised multi-modality learning). The authors should compare their method with these methods, like Li, Kang, et al. “Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation.” International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2020. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance There is no public code for this project. And the whole pipeline includes many hyperparameters and the training of GAN. it may be difficult to reproduce the results only from this paper. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html From Fig. 3, we can see that the performance difference for different \\lambda is about 2-3%. Therefore, it is not very confusing to argue that “our method is generally robust to the change of λsc “. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Some issues about method design/experiment setting/comparison should be solved. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper proposes an approach for semi-supervised unpaired multi-modal learning. The method first applies CycleGAN for image translation between CT and MRI data, and regards an image and its cross-modality translation as two different views of the same object. Two segmentation models for each modality are separately trained and a consistency regularization and co-training algorithm are proposed to optimize the segmentation models with the unlabeled data. Validation is performed on public datasets with two segmentation tasks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Although co-training has already been a well-established semi-supervised method, this paper leverages good practices from prior works and applies to unpaired multi-modal learning. Effectiveness is shown with two different segmentation tasks. This paper is easy to follow. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness of this work is that the method is not compared with strong and sufficient baselines, thus the claimed effectiveness is not very convincing. Authors compare their methods mainly with different multi-modal approaches, which are not specifically designed for semi-supervised setting, thus it is as expected that the proposed method outperforms those approaches. Only one simple semi-supervised method proposed in 2013 is considered in the comparison, which is insufficient to demonstrate the superiority of the proposed method. Authors are suggested to compare with other state-of-the-art semi-supervised approaches. Also, authors should also consider previous multi-modal learning method presented in [1], which uses similar design of image translation for obtaining different views and may serve as stronger and more relevant baselines than [3][9]. From the results, it is difficult to know how helpful the semi-supervised unpaired multi-modal learning is. The models trained with 100% labels should be included to know the performance gap when only 0.5% and 2.5% labels are used. A curve of the performance with the increase in the percentage of labeled data should be shown. Another interesting baseline would be training “ST-single” with double percentage of labels. For example, training “ST-single” with 1% labels of single modality and comparing to the proposed method trained with 0.5% labels of two modalities, which can demonstrate the benefits of unpaired multi-modal learning. Authors propose a class-balanced deep co-training scheme, but whether the selection of \\alpha% for each class instead of the all pixels contributes to the performance is not experimentally shown. Also, the “class-balanced” claim is misleading. If the classes are imbalanced in an image, for example, spleen is much smaller than liver, with the presented pseudo-label selection strategy, the classes would still remain imbalanced. [1] Li, Kang, Lequan Yu, Shujun Wang, and Pheng-Ann Heng. “Towards cross-modality medical image segmentation with online mutual knowledge distillation.” In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 01, pp. 775-783. 2020. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Reproducibility is satisfactory. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html One problem of the experimental setting is that the labeled/unlabeled data are split in the image level instead of the volume level. Although the 3D MRI/CT are converted into slices to train 2D networks, the more practical split of the labeled/unlabeled data should still be in the volume level of different subjects. Is the evaluation also performed slice by slice? The evaluation metrics should be calculated in the volume level. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The method seems work well for the semi-supervised unpaired multi-modal learning, but the comparison baselines are not strong/sufficient enough. The technical contribution of this paper is tangible. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper presents a semi-supervised learning method for unpaired multi-modal medical segmentation tasks. Built upon the image translation models, the authors propose a class-balanced deep co-training method, which achieves superior results than previous state-of-the-arts on fully-supervised multi-modal learning methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The contributions are well illustrated and quite clear. The paper is well organized and easy to follow The superiority of the proposed approach is demonstrated on different unpaired multi-modal segmentation tasks. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Missing references: for deep co-training [a], there have been multiple applications [b,c] which use multiple learners to learn representation from different views and make the final prediction by mining the consensus information among these different learners. These literatures should also be included in the introduction or experiment part. For the comparison, image translation seems to be not included in all of the compared methods (e.g., X/Y-shape, ST-joint), which makes the comparison not completely fair. For a fairer comparison, please include translated images as augmentation for all the compared methods, to guarantee that the input data are the same. For the SC loss, I am curious about whether other loss terms (e.g., contrastive loss) could outperform KL divergence loss. It would be interesting to see some discussion related to this part. [a] Qiao, Siyuan, et al. “Deep co-training for semi-supervised image recognition.” Proceedings of the european conference on computer vision (eccv). 2018. [b] Zhou, Yuyin, et al. “Semi-supervised 3D abdominal multi-organ segmentation via deep multi-planar co-training.” 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019. [c] Peng, Jizong, et al. “Deep co-training for semi-supervised image segmentation.” Pattern Recognition 107 (2020): 107269. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The code is not included in the submission. I strongly recommend the authors to release the code in the next version. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html please see above. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall the paper presents an important problem and also designs a viable solution by using generative models and deep co-training. Some minor issues need to be addressed, such as adding references &amp; experimental comparison. Other than that, I feel this could be one important study along the semi-supervised learning direction. Therefore I recommend acceptance. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation to utilize the unlabeled dataset. They evaluated their methods on two public datasets, i.e., cardiac images and abdominal multi-organ images, and claimed better performance than other baseline methods. However, there are three main points raised by reviewers needed to considers: 1) the image format (2D slice instead of 3D volume) used in the training/test stage. 2) the comparison experiments are not convincing enough as almost no semi-supervised methods were employed for comparison. 3) the parameter experiments of the deep co-training scheme seem to be confusing. Therefore, my recommendation for this paper is “Invite for Rebuttal”. Note that the purpose of the rebuttal is to provide clarification or to point out misunderstandings instead of promising additional experiments. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This work proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation. Although the idea of co-training is not novel, all reviewers evaluated the writing and experiments highly enough. The authors also answered the concerns well. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. From the first meta-review: The authors proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation to utilize the unlabeled dataset. They evaluated their methods on two public datasets, i.e., cardiac images and abdominal multi-organ images, and claimed better performance than other baseline methods. However, there are three main points raised by reviewers needed to considers: 1) the image format (2D slice instead of 3D volume) used in the training/test stage. 2) the comparison experiments are not convincing enough as almost no semi-supervised methods were employed for comparison. 3) the parameter experiments of the deep co-training scheme seem to be confusing. Therefore, my recommendation for this paper is “Invite for Rebuttal”. Note that the purpose of the rebuttal is to provide clarification or to point out misunderstandings instead of promising additional experiments. In the rebuttal, the authors concurred that additional experiments are required to address reviews. These additional experiments cannot be fully peer reviewed, so the paper cannot be accepted. Here is the relevant section of the rebuttal instructions: “An effective rebuttal addresses reviewers’ criticisms by explaining where in the paper you had provided the requisite information, perhaps further clarifying it. Do not promise to expand your paper to address all the questions raised by the reviewers, as you will not be able to change your article substantially, and in all likelihood you don’t have sufficient room to add to the paper.” After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 17 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper studies the unpaired multi-modality medical image segmentation using a co-training neural network under the semi-supervised setting. In the neural network, GAN is used to translate images acroos modalities and adopts the deep co-training strategy to utilize the unlabeled data. The idea appears reasonable, which is supported by the experimental results reported in the paper. The authors’ rebuttal have largely addressed the questions and concerns raised by the reviewers. Thus I recommend an acceptance to this paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Author Feedback We thank all the reviewers for their valuable comments. The main concerns are addressed as follows: *1. Image format in training/test stage (Weakness 2 by Reviewer#1 and Question from Reviewer#2): We would like to clarify that our training and test data have been split at volume level, not on image slice level, so there is no test data information leakage to training data. Similarly, for labeled and unlabeled data in training stage, the split has also been at volume level. We also want to highlight that there is no information leakage from labeled set to unlabeled set because when we select certain slices in a volume to form labeled set, the remaining slices from the same volume will not be used as unlabeled data. Lastly, the evaluation metrics were calculated at volume level. Thanks for pointing out the confusion, we will clarify the above information in the revision. *2. Volume vs slice level annotation (Weakness 2 by Reviewer#1): We want to clarify that our method does not impose any annotation restriction, and doctors are free to annotate either by volume or by ad hoc slices, which are both practised in clinical settings. In fact, one of our contributions is that when doctors do label by ad hoc slices, our method would still be performant when only a small amount of slices are labeled, effectively reducing the annotation burden. *3. Insufficient comparison experiments with semi-supervised methods (Weakness 4 by Reviewer#1 and Weakness 1 by Reviewer#2): First, we clarify the selection of ST-single and ST-joint as our semi-supervised baseline methods. Although the semi-supervised learning method proposed in the ST-single/ST-joint paper is generic, we should have mentioned that we intentionally implemented ST-single/ST-joint with several improvements. For example, ST-joint employs modality specific batch normalization layers to reduce modality difference, so that it can effectively leverage shared cross-modality information for semi-supervised learning. Thus we felt the implemented ST-single and ST-joint to be reasonable semi-supervised baselines, where they represent semi-supervised learning with different numbers of modalities. Second, we have performed the following new experiments on the two works from Li Kang et al., as requested, for more complete and stronger comparison. Here are the test dice scores on cardiac segmentation: Li Kang (MICCAI 2020): 0.5% data: MRI 70.8, CT 75.7; 2.5% data: MRI 81.9, CT 84.9; Li Kang (AAAI 2020): 0.5% data: MRI 70.9, CT 79.6; 2.5% data: MRI 82.3, CT 85.6; Our method outperforms Li Kang’s methods by average 5.3% and 4.0% respectively. We reason this is because they do not fully utilize all available datasets, as unlabeled source data is not used for training segmentation networks by them. In addition, while our method optimizes performance for both modalities, Li Kang’s two methods only optimize performance for target modality, thus they may not be able to fully leverage on the multi-modality information. Interestingly, ST-single and ST-joint also give comparable or stronger results compared with Li Kang’s two methods. Thanks for the suggestions, we will add both Li Kang’s methods in the revision. *4. Redundant co-training (Weakness 3 by Reviewer#1): Semantic Consistency (SC) and co-training are complementary instead of redundant as justified in our ablation study in Table 2. Co-training learns discriminative features for unlabeled data while SC is for regularization with scarce labeled data. *5. Robustness of lambda_sc (Question from Reviewer#1): Our sensitivity analysis in Fig3 is meant to convey that even when we scale lambda_sc by 100 times, the performance only fluctuates by 2-3%, thus we felt our method is relatively robust to hyper-parameter change. *6. Training details and Code release (Reviewer#1 and Reviewer#3): Our entire framework, including image translation module, is trained in an end-to-end manner. We will release our code on GitHub in the revision. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Zhu, Lei,Yang, Kaiyuan,Zhang, Meihui,Chan, Ling Ling,Ng, Teck Khim,Ooi, Beng Chin" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Semi-Supervised Unpaired Multi-Modal Learning for Label-Efficient Medical Image Segmentation</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Machine Learning - Semi-supervised learning"
        class="post-category">
        Machine Learning - Semi-supervised learning
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Abdomen"
        class="post-category">
        Clinical applications - Abdomen
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Cardiac"
        class="post-category">
        Clinical applications - Cardiac
      </a>
      
      <a 
        href="kittywong/categories#Image Segmentation"
        class="post-category">
        Image Segmentation
      </a>
      
      <a 
        href="kittywong/categories#Modalities - CT"
        class="post-category">
        Modalities - CT
      </a>
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Zhu, Lei"
        class="post-tags">
        Zhu, Lei
      </a> |  
      
      <a href="kittywong/tags#Yang, Kaiyuan"
        class="post-tags">
        Yang, Kaiyuan
      </a> |  
      
      <a href="kittywong/tags#Zhang, Meihui"
        class="post-tags">
        Zhang, Meihui
      </a> |  
      
      <a href="kittywong/tags#Chan, Ling Ling"
        class="post-tags">
        Chan, Ling Ling
      </a> |  
      
      <a href="kittywong/tags#Ng, Teck Khim"
        class="post-tags">
        Ng, Teck Khim
      </a> |  
      
      <a href="kittywong/tags#Ooi, Beng Chin"
        class="post-tags">
        Ooi, Beng Chin
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Lei Zhu, Kaiyuan Yang, Meihui Zhang, Ling Ling Chan, Teck Khim Ng, Beng Chin Ooi
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Multi-modal learning using unpaired labeled data from multiple modalities to boost the performance of deep learning models on each individual modality has attracted a lot of interest in medical image segmentation recently. However, existing unpaired multi-modal learning methods require a considerable amount of labeled data from both modalities to obtain satisfying segmentation results which are not easy to obtain in reality. In this paper, we investigate the use of unlabeled data for label-efficient unpaired multi-modal learning, with a focus on the scenario when labeled data is scarce and unlabeled data is abundant. We term this new problem as Semi-Supervised Unpaired Multi-Modal Learning and thereupon, propose a novel deep co-training framework. Specifically, our framework consists of two segmentation networks, where we train one of them for each modality. Unlabeled data is effectively applied to learn two image translation networks for translating images across modalities. Thus, labeled data from one modality is employed for the training of the segmentation network in the other modality after image translation. To prevent overfitting under the label scarce scenario, we introduce a new semantic consistency loss to regularize the predictions of an image and its translation from the two segmentation networks to be semantically consistent. We further design a novel class-balanced deep co-training scheme to effectively leverage the valuable complementary information from both modalities to boost the segmentation performance. We verify the effectiveness of our framework with two medical image segmentation tasks and our framework outperforms existing methods significantly.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87196-3_37">https://doi.org/10.1007/978-3-030-87196-3_37</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/nusdbsystem/SSUMML
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper presents a semi-supervised learning setting for unpaired multi-modality medical image segmentation to reduce the annotation effort. The proposed method utilizes GAN to translate images between two modalities and adopt the deep co-training strategy to utilize the unlabeled data.</p>

      <p>The proposed method was evaluated on public cardiac image segmentation and abdominal multi-organ segmentation tasks and its performance outperforms other baseline methods.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>The whole pipeline is reasonable and the whole paper is well-written.</li>
        <li>The authors conduct an extensive evaluation on two public datasets and the proposed method achieves better performances.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>
          <p>In genral, the whole pipeline is clearly demonstrated, while some important technical details are unclear. For example, how to train the image translation module and the image translation module is pre-trained or trained with the segmentation network in an end-to-end manner.</p>
        </li>
        <li>
          <p>It seems that the dataset split (train vs. test and labeled vs. unlabeled) is conducted on image slice level. However, this setting maybe unfair and leaks the testing data information. And for the annotation issue, it is very common to only annotate some scenes/volumes while it is uncommon to annotation some slices in each volume.</p>
        </li>
        <li>
          <p>It seems that the proposed co-training scheme is redundant with the semantic consistency scheme.</p>
        </li>
        <li>
          <p>The proposed problem setting and method is very similar to semi-supervised domain adaption (or semi-supervised multi-modality learning). The authors should compare their method with these methods, like</p>
        </li>
      </ol>

      <p>Li, Kang, et al. “Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation.” International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2020.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>There is no public code for this project. And the whole pipeline includes many hyperparameters and the training of GAN. it may be difficult to reproduce the results only from this paper.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ol>
        <li>From Fig. 3, we can see that the performance difference for different \lambda is about 2-3%. Therefore, it is not very confusing to argue that “our method is generally robust to the change of λsc “.</li>
      </ol>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline reject (5)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Some issues about method design/experiment setting/comparison should be solved.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper proposes an approach for semi-supervised unpaired multi-modal learning. The method first applies CycleGAN for image translation between CT and MRI data, and regards an image and its cross-modality translation as two different views of the same object. Two segmentation models for each modality are separately trained and a consistency regularization and co-training algorithm are proposed to optimize the segmentation models with the unlabeled data. Validation is performed on public datasets with two segmentation tasks.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>Although co-training has already been a well-established semi-supervised method, this paper leverages good practices from prior works and applies to unpaired multi-modal learning.</li>
        <li>Effectiveness is shown with two different segmentation tasks.</li>
        <li>This paper is easy to follow.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The main weakness of this work is that the method is not compared with strong and sufficient baselines, thus the claimed effectiveness is not very convincing.</p>

      <ul>
        <li>
          <p>Authors compare their methods mainly with different multi-modal approaches, which are not specifically designed for semi-supervised setting, thus it is as expected that the proposed method outperforms those approaches. Only one simple semi-supervised method proposed in 2013 is considered in the comparison, which is insufficient to demonstrate the superiority of the proposed method.  Authors are suggested to compare with other state-of-the-art semi-supervised approaches. Also, authors should also consider previous multi-modal learning method presented in [1], which uses similar design of image translation for obtaining different views and may serve as stronger and more relevant baselines than [3][9].</p>
        </li>
        <li>
          <p>From the results, it is difficult to know how helpful the semi-supervised unpaired multi-modal learning is. The models trained with 100% labels should be included to know the performance gap when only 0.5% and 2.5% labels are used. A curve of the performance with the increase in the percentage of labeled data should be shown. Another interesting baseline would be training “ST-single” with double percentage of labels. For example, training “ST-single” with 1% labels of single modality and comparing to the proposed method trained with 0.5% labels of two modalities, which can demonstrate the benefits of unpaired multi-modal learning.</p>
        </li>
        <li>
          <p>Authors propose a class-balanced deep co-training scheme, but whether the selection of \alpha% for each class instead of the all pixels contributes to the performance is not experimentally shown. Also, the “class-balanced” claim is misleading. If the classes are imbalanced in an image, for example, spleen is much smaller than liver, with the presented pseudo-label selection strategy, the classes would still remain imbalanced.</p>
        </li>
      </ul>

      <p>[1] Li, Kang, Lequan Yu, Shujun Wang, and Pheng-Ann Heng. “Towards cross-modality medical image segmentation with online mutual knowledge distillation.” In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 01, pp. 775-783. 2020.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Reproducibility is satisfactory.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>One problem of the experimental setting is that the labeled/unlabeled data are split in the image level instead of the volume level. Although the 3D MRI/CT are converted into slices to train 2D networks, the more practical split of the labeled/unlabeled data should still be in the volume level of different subjects. Is the evaluation also performed slice by slice? The evaluation metrics should be calculated in the volume level.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The method seems work well for the semi-supervised unpaired multi-modal learning, but the comparison baselines are not strong/sufficient enough. The technical contribution of this paper is tangible.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper presents a semi-supervised learning method for unpaired multi-modal medical segmentation tasks. Built upon the image translation models, the authors propose a class-balanced deep co-training method, which achieves superior results than previous state-of-the-arts on fully-supervised multi-modal learning methods.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>
          <p>The contributions are well illustrated and quite clear. The paper is well organized and easy to follow</p>
        </li>
        <li>
          <p>The superiority of the proposed approach is demonstrated on different unpaired multi-modal segmentation tasks.</p>
        </li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>
          <p>Missing references: for deep co-training [a], there have been multiple applications [b,c] which use multiple learners to learn representation from different views and make the final prediction by mining the consensus information among these different learners. These literatures should also be included in the introduction or experiment part.</p>
        </li>
        <li>
          <p>For the comparison, image translation seems to be not included in all of the compared methods (e.g., X/Y-shape, ST-joint), which makes the comparison not completely fair. For a fairer comparison, please include translated images as augmentation for all the compared methods, to guarantee that the input data are the same.</p>
        </li>
        <li>
          <p>For the SC loss, I am curious about whether other loss terms (e.g., contrastive loss) could outperform KL divergence loss. It would be interesting to see some discussion related to this part.</p>
        </li>
      </ul>

      <p>[a] Qiao, Siyuan, et al. “Deep co-training for semi-supervised image recognition.” Proceedings of the european conference on computer vision (eccv). 2018.
[b] Zhou, Yuyin, et al. “Semi-supervised 3D abdominal multi-organ segmentation via deep multi-planar co-training.” 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019.
[c] Peng, Jizong, et al. “Deep co-training for semi-supervised image segmentation.” Pattern Recognition 107 (2020): 107269.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The code is not included in the submission. I strongly recommend the authors to release the code in the next version.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>please see above.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Overall the paper presents an important problem and also designs a viable solution by using generative models and deep co-training. Some minor issues need to be addressed, such as adding references &amp; experimental comparison. Other than that, I feel this could be one important study along the semi-supervised learning direction. Therefore I recommend acceptance.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>The authors proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation to utilize the unlabeled dataset. They evaluated their methods on two public datasets, i.e., cardiac images and abdominal multi-organ images, and claimed better performance than other baseline methods. However, there are three main points raised by reviewers needed to considers: 1) the image format (2D slice instead of 3D volume) used in the training/test stage. 2) the comparison experiments are not convincing enough as almost no semi-supervised methods were employed for comparison. 3) the parameter experiments of the deep co-training scheme seem to be confusing. Therefore, my recommendation for this paper is “Invite for Rebuttal”. Note that the purpose of the rebuttal is to provide clarification or to point out misunderstandings instead of promising additional experiments.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>This work proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation. Although the idea of co-training is not novel, all reviewers evaluated the writing and experiments highly enough. The authors also answered the concerns well.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="meta-review-2">Meta-Review #2</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>From the first meta-review: 
The authors proposed a semi-supervised learning co-training network for unpaired multi-modality medical image segmentation to utilize the unlabeled dataset. They evaluated their methods on two public datasets, i.e., cardiac images and abdominal multi-organ images, and claimed better performance than other baseline methods. However, there are three main points raised by reviewers needed to considers: 1) the image format (2D slice instead of 3D volume) used in the training/test stage. 2) the comparison experiments are not convincing enough as almost no semi-supervised methods were employed for comparison. 3) the parameter experiments of the deep co-training scheme seem to be confusing. Therefore, my recommendation for this paper is “Invite for Rebuttal”. Note that the purpose of the rebuttal is to provide clarification or to point out misunderstandings instead of promising additional experiments.</p>

      <p>In the rebuttal, the authors concurred that additional experiments are required to address reviews. These additional experiments cannot be fully peer reviewed, so the paper cannot be accepted.</p>

      <p>Here is the relevant section of the rebuttal instructions: “An effective rebuttal addresses reviewers’ criticisms by explaining where in the paper you had provided the requisite information, perhaps further clarifying it. Do not promise to expand your paper to address all the questions raised by the reviewers, as you will not be able to change your article substantially, and in all likelihood you don’t have sufficient room to add to the paper.”</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Reject</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>17</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>
<h2 id="meta-review-3">Meta-Review #3</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>This paper studies the unpaired multi-modality medical image segmentation using 
a co-training neural network under the semi-supervised setting. In the neural network, GAN is used to translate images acroos modalities and adopts the deep co-training strategy to utilize the unlabeled data. The idea appears reasonable, which is supported by the experimental results reported in the paper. The authors’ rebuttal have largely addressed the questions and concerns raised by the reviewers. Thus I recommend an acceptance to this paper.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
</ul>

<h2><br /><br /></h2>
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We thank all the reviewers for their valuable comments. The main concerns are addressed as follows:</p>

  <p>*1. Image format in training/test stage (Weakness 2 by Reviewer#1 and Question from Reviewer#2):</p>

  <p>We would like to clarify that our training and test data have been split at volume level, not on image slice level, so there is no test data information leakage to training data.</p>

  <p>Similarly, for labeled and unlabeled data in training stage, the split has also been at volume level.</p>

  <p>We also want to highlight that there is no information leakage from labeled set to unlabeled set because when we select certain slices in a volume to form labeled set, the remaining slices from the same volume will not be used as unlabeled data.</p>

  <p>Lastly, the evaluation metrics were calculated at volume level.</p>

  <p>Thanks for pointing out the confusion, we will clarify the above information in the revision.</p>

  <p>*2. Volume vs slice level annotation (Weakness 2 by Reviewer#1):</p>

  <p>We want to clarify that our method does not impose any annotation restriction, and doctors are free to annotate either by volume or by ad hoc slices, which are both practised in clinical settings. In fact, one of our contributions is that when doctors do label by ad hoc slices, our method would still be performant when only a small amount of slices are labeled, effectively reducing the annotation burden.</p>

  <p>*3. Insufficient comparison experiments with semi-supervised methods (Weakness 4 by Reviewer#1 and Weakness 1 by Reviewer#2):</p>

  <p>First, we clarify the selection of ST-single and ST-joint as our semi-supervised baseline methods.</p>

  <p>Although the semi-supervised learning method proposed in the ST-single/ST-joint paper is generic, we should have mentioned that we intentionally implemented ST-single/ST-joint with several improvements. For example, ST-joint employs modality specific batch normalization layers to reduce modality difference, so that it can effectively leverage shared cross-modality information for semi-supervised learning. Thus we felt the implemented ST-single and ST-joint to be reasonable semi-supervised baselines, where they represent semi-supervised learning with different numbers of modalities.</p>

  <p>Second, we have performed the following new experiments on the two works from Li Kang et al., as requested, for more complete and stronger comparison. Here are the test dice scores on cardiac segmentation:</p>

  <p>Li Kang (MICCAI 2020):
0.5% data: MRI 70.8, CT 75.7;
2.5% data: MRI 81.9, CT 84.9;</p>

  <p>Li Kang (AAAI 2020):
0.5% data: MRI 70.9, CT 79.6;
2.5% data: MRI 82.3, CT 85.6;</p>

  <p>Our method outperforms Li Kang’s methods by average 5.3% and 4.0% respectively. We reason this is because they do not fully utilize all available datasets, as unlabeled source data is not used for training segmentation networks by them. In addition, while our method optimizes performance for both modalities, Li Kang’s two methods only optimize performance for target modality, thus they may not be able to fully leverage on the multi-modality information.</p>

  <p>Interestingly, ST-single and ST-joint also give comparable or stronger results compared with Li Kang’s two methods.</p>

  <p>Thanks for the suggestions, we will add both Li Kang’s methods in the revision.</p>

  <p>*4. Redundant co-training (Weakness 3 by Reviewer#1):</p>

  <p>Semantic Consistency (SC) and co-training are complementary instead of redundant as justified in our ablation study in Table 2. Co-training learns discriminative features for unlabeled data while SC is for regularization with scarce labeled data.</p>

  <p>*5. Robustness of lambda_sc (Question from Reviewer#1):</p>

  <p>Our sensitivity analysis in Fig3 is meant to convey that even when we scale lambda_sc by 100 times, the performance only fluctuates by 2-3%, thus we felt our method is relatively robust to hyper-parameter change.</p>

  <p>*6. Training details and Code release (Reviewer#1 and Reviewer#3):</p>

  <p>Our entire framework, including image translation module, is trained in an end-to-end manner. We will release our code on GitHub in the revision.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0236-12-31
      -->
      <!--
      
        ,
        updated at 
        0237-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Machine Learning - Semi-supervised learning"
        class="post-category">
        Machine Learning - Semi-supervised learning
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Abdomen"
        class="post-category">
        Clinical applications - Abdomen
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Cardiac"
        class="post-category">
        Clinical applications - Cardiac
      </a> |
      
      <a 
        href="kittywong/categories#Image Segmentation"
        class="post-category">
        Image Segmentation
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - CT"
        class="post-category">
        Modalities - CT
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Zhu, Lei"
        class="post-category">
        Zhu, Lei
      </a> |  
      
      <a href="kittywong/tags#Yang, Kaiyuan"
        class="post-category">
        Yang, Kaiyuan
      </a> |  
      
      <a href="kittywong/tags#Zhang, Meihui"
        class="post-category">
        Zhang, Meihui
      </a> |  
      
      <a href="kittywong/tags#Chan, Ling Ling"
        class="post-category">
        Chan, Ling Ling
      </a> |  
      
      <a href="kittywong/tags#Ng, Teck Khim"
        class="post-category">
        Ng, Teck Khim
      </a> |  
      
      <a href="kittywong/tags#Ooi, Beng Chin"
        class="post-category">
        Ooi, Beng Chin
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0237/12/31/Paper0939">
          Implicit Neural Distance Representation for Unsupervised and Supervised Classification of Complex Anatomies
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0235/12/31/Paper0364">
          3D Semantic Mapping from Arthroscopy using Out-of-distribution Pose and Depth and In-distribution Segmentation Training
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
