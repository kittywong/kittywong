<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Hadrien Reynaud, Athanasios Vlontzos, Benjamin Hou, Arian Beqiri, Paul Leeson, Bernhard Kainz Abstract Cardiac ultrasound imaging is used to diagnose various heart diseases. Common analysis pipelines involve manual processing of the video frames by expert clinicians. This suffers from intra- and inter-observer variability. We propose a novel approach to ultrasound video analysis using a transformer architecture based on a Residual Auto-Encoder Network and a BERT model adapted for token classification. This enables videos of any length to be processed. We apply our model to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection and the automated computation of the left ventricular ejection fraction. We achieve an average frame distance of 3.36 frames for the ES and 7.17 frames for the ED on videos of arbitrary length. Our end-to-end learnable approach can estimate the ejection fraction with a MAE of 5.95 and R^2 of 0.52 in 0.15s per video, showing that segmentation is not the only way to predict ejection fraction. Code and models are available at https://github.com/HReynaud/UVT. Link to paper https://doi.org/10.1007/978-3-030-87231-1_48 Link to the code repository https://github.com/HReynaud/UVT Link to the dataset(s) https://echonet.github.io/dynamic/index.html#access Reviews Review #1 Please describe the contribution of the paper The paper show a methodological improvement on phase detection (ED/ES) in cardiac echo image sequences and ejection fraction estimation by using a transformer network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It was a true pleasure for me to read this paper (!) and I congratulate the authors on this nice piece of work. The manuscript is extremely well written and has a high quality w.r.t. the evaluation. The proposed method follows recent trends in time-series processing (usage of transformer networks). The authors show the benefit of their novel approach in comparison to other existing work (Table 1). Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There is no major weakness that comes into my mind. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I could not find a statement in the paper that the source code will be released (which is encouraged). However, an open data set was used and the clarity of the description is high, therefore, I would rate the reproducibility as high. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Just minor/out of curiosity: From a reader’s perspective I would like to hear more about the performance of the ED/ES frame detection on the portion of the sequences which were not labeled. For patients with a regular heart beat (non-arrhythmia), is it possible to show that there is a regular pattern in the phase occurance (e.g. distance between the target frames)? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? methodological contribution very good evaluation clarity of the paper What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The paper presents a method for the automatic interpretation of echocardiography videos, both the ED/ES frame selection and LVEF direct estimation. This method is based on a novel deep learning architecture incorporating both encoding for dimensionality reduction (ResNetAE) and a recurrent network used in NLP (BERT). Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strengths of the paper are the novel neural network construction, extensive apparent validation, and ablation study. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weaknesses of the paper are in the ignorance to relevant prior work and unfair comparison to cited prior work. -To the best of our knowledge…paradigm of discrete frame processing with limited temporal support – see Qin MICCAI 18 in MR (https://doi.org/10.1007/978-3-030-00934-2_53), Li MICCAI 19 in echo (https://arxiv.org/abs/1907.11292 ) and Wei MICCAI 2020 (https://link.springer.com/chapter/10.1007/978-3-030-59713-9_60 ) also on echo. Even the reference [15] itself uses r2plus1d_18 for direct estimation and which does convolve across time. -We have discussed probably the first transformer architecture that is able to analyse US videos of arbitrary length – other than [14/15] that you reference. It is unclear to this reader whether the comparisons to this prior work are fair. -It’s unclear how your Video sampling process works in the context of test-time and full video. Maybe supplemental table 1 answers the issue. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance This work may be difficult to reproduce. The hyperparameters for the ResNetAE are not specified. The Video sampling process is unclear for test time. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Small issues: -LVEF is the ratio between End-Systolic (ES) and End-Diastolic (ED) blood volumes in the left ventricle – technically the complement of that ratio, no? -[14] and [15] are the same reference. -In Table 1 caption, use `` and ‘’ for open and close quotes. -For Table 1 LVEF prediction you report [14/15]’s MAE as 7.35. I guess this comes from Extended Data Table 2, which is their r2plus1d_18 result with the whole video inferred. But your methods R and M seem to be clips centered around the labelled ED/ES frames, at least as explained in your video sampling section. The more appropriate comparison would be their “32 frame sample,” which is 4.22 and doesn’t require the labelled ED/ES frames, at least as I understand it. -Aren’t the clips in EchoNet usually ED-ES? Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The major factors are the lack of reference to highly relevant prior work, and a seemingly unfair comparison to cited work. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The authors propose a combined CNN-transformer architecture for ED/ES frame detection and LVEF estimation in apical four-chamber echo videos. The per-frame encodings of the echo cine by a ResNet are fed into a BERT module to predict the targeted measurements. The public dataset of Echonet-Dynamic is used for the experiments. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper has a good organization and the method is easy to comprehend. A public dataset is used for the experiments and the method has high reproducibility. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Technical novelty: – Application of video transformers for video classification/regression may not be quite novel and has been explored in the community [1,2,3,4]. Technical soundness: – There is a concern about the way LVEF is calculated. In the proposed architecture, the LVEF is obtained by averaging the LVEF predictions across the frames of the video (see Ejection Fraction Regressor block in Fig 1 and see the first 4 lines of page 5). This seems not to be clinically and technically correct. The LVEF correlates to the variations of the LV across the video. Calculating the video LVEF by averaging the LVEF predictions over the frames may not be justified. Results: – The proposed method noticeably underperforms the current literature (see table 1- LVEF estimation’s R2 score on the left side). The method has about 27% less R2 score (0.81 vs 0.64) compared to an existing method. The above-mentioned technical problem might be a reason for the low R2 score. Experiments: – The results reported in table 1 for LVEF (right side) may not be an accurate comparison. The method R14 (reference [14] in the paper) uses the LV segmentation results throughout the video to find the beat-to-beat cardiac cycles. R14 runs the video LVEF calculator across multiple detected cycles and reports the average LVEF among automatically detected cycles. This way the method R14 is already compatible with variable length videos, as the cycles are automatically detected based on LV segmentation. In this setup, it seems the reported results in the paper (table 1 - LVEF prediction - right side) for full video processing are different from the full video results reported in R14. – The method R14, uses 32 frames in each segment of the beat-to-beat LVEF estimation network. Another valid comparison is running method R14 (Resnet 2D+1) with 128 input frames, the same input frame length used in this paper. References: [1] Video Action Transformer Network, https://openaccess.thecvf.com/content_CVPR_2019/papers/Girdhar_Video_Action_Transformer_Network_CVPR_2019_paper.pdf [2] Video Transformer Network, https://arxiv.org/pdf/2102.00719.pdf [3] Late Temporal Modeling in 3D CNNArchitectures with BERT for Action Recognition, https://arxiv.org/pdf/2008.01232v3.pdf [4] EchoBERT: A Transformer-Based Approach for Behavior Detection in Echograms, https://ieeexplore.ieee.org/abstract/document/9281296 Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The provided details seem to be adequate and the method has good reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The adaptation of transformers for echo video processing seems to be an interesting path to explore; however, the paper at its current format seems to need technical improvement to properly address the tackled problem. Detailed comments: The method for trimming the video (into 128 frames) could be improved. Mirroring or random sampling may be a naive approach for video trimming in echo and may not be meaningful when noticing the underlying cardiac cycle patterns in echo videos. The paper claims the method is compatible with different video lengths; however, in this method, the input needs to be capped to 128 frames. The method zero-pads the videos from 112112 to 128128. It is not clear why the frames are not up-sampled to the higher resolution. Zero paddings may not use the full capacity of the network input. Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The major factors for my recommendation, as detailed in the “weaknesses” section, are the poor technical soundness, low performance, and inaccurate comparison with the prior art. What is the ranking of this paper in your review stack? 5 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors propose a combined CNN-transformer architecture for ED/ES frame detection and LVEF estimation from four-chamber echo videos. The public dataset of Echonet-Dynamic was used. Strengths: technical novelty of using a video transformer to address a classical problem of EF estimation, clarity of presentation, and well-conducted evaluation and ablation studies. Weaknesses: incomplete literature overview (reviewer 3 and 4), and technical ground of the LVEF prediction method used in this work (reviewer 4). The authors are expected present a more comprehensive literature review over related work and address the issues of LVEF estimation in response to the question raised by reviewer 4 and the issue of unfair comparison as raised by reviewer 3. The authors are encouraged to release the code for the research community (reviewer 1). What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 7 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The study is well conducted, clearly written, and the authors have addressed in the rebuttal on (1) more comprehensive literature review over related work and (2) the issue of unfair comparison as raised by reviewer 3. (3) code release (reviewer 1). After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. As indicated by the primary AC, the manuscript introduces a new hybrid approach for ED/ES detection. The proposed method is novel and there are sufficient experiments to explain the advantages for the proposed method. However, there are still some critical concerns after the rebuttal: 1) the literature review is incomplete. Authors response that the focus of the work is on LVEF / ESED applications and will integrate additional literatures referred by the reviewers. However, the responses didn’t explain the key technical difference to these works; and 2) quite limited information provided about the calculation of LVEF from the rebuttal. Therefore, it’s still not clear that the averaging of all the predications is the correct way to estimate the LVEF. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 19 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The reviews have highlighted the fact that this paper proposes a valuable contribution in terms of methodology design. There were some concerns regarding the way the comparison to other SotA method was done, and the citation of prior works. In their rebuttal, the authors have provided explanations regarding both these issues, therefore I recommend acceptance for this paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback We thank the reviewers and the AC for their feedback! Results comparison We compare our LVEF prediction results with Ouyang et al. [2] as we use their dataset. They present multiple modularized approaches with different preprocessing and LVEF accuracy. Our approach takes videos “as is” without preprocessing and predicts both LVEF and ES/ED frames indices in a single forward pass. Thus, we compare our results with the closest “All frames” evaluation from [2]. Indeed, we do not state clearly enough that their “beat-by-beat” method can be applied successively on all the heartbeats of a single US video. However, our method does not rely on segmentation performance and presents a novel approach to address LVEF prediction, which requires much less labelling work and shows excellent performance on the detection of the ES/ED frames. We also avoid all manual processing steps (e.g. resampling) which induce biases in the pipeline and prevent the generalization of the method to other domains. Our method is also much faster with an average processing time of 0.15s per entire video compared to 1.6s per heartbeat for EchoNet (0.05s x 32 frames). The best results from [2] will be added to our results (Table 1) in the LVEF prediction section for “Full video”, and we will clarify the abstract to emphasize the specificity of our approach. LVEF computation Our method computes LVEF by averaging. As the outputs from the transformer are sent into fully connected layers, the outputs of the “EF Regressor, Dense 2” layer are not bound to their frame indices (because of the averaging step). Each neuron predicts a LVEF and the averaging takes all of them into account. Literature Review: In the current version of our related works section we focus on LVEF and ES/ED applications. Thus, we do not elaborate on areas related in other ways to our work. We will integrate the references brought up by the reviewers and discuss their relation to our work. Of course, we will correct the duplication between references [14/15]. Code release Our code and trained models are ready to be released as a Github repository and the link will be added to the final version of the paper. We did not provide the link in the submitted version to respect the double-blind reviewing process. Test time inference The network is trained on 128 frames videos to enable batch training. After training, the network is ready to handle videos of any length: shorter or longer. At test time, the videos are sent into the network with no modification other than scaling the pixel intensity. The number and the order of the frames is not modified. We show examples of arbitrary length videos in Figure 2 and in the supplementary figures. R1 Regarding the assessment of the ES/ED detection performance on unlabelled portions, we can see from Figure 2 and the supplement that the distance between ES/ED frames is reasonably constant, indicating that the network is detecting the sensible frames. R3 Our description of the LVEF may be misleading. To make our statement more accurate, we will change it to “The LVEF is the ratio of the stroke volume and the end-diastolic volume of the left ventricle”. To clarify the structure of the dataset, the videos have variable lengths and framerates. Most of them contain more than one full heartbeat and all of them have a single labelled ES and ED, which can come in any order (ES then ED or ED then ES). R4 Our training-time sampling methods were shown to produce convincing results at test time. “Guided random sampling” is an improved version of the usual sampling method for ES/ED prediction. We came up with temporal mirroring to improve the network capabilities on longer videos. We added padding to the frames to keep the optimal ResNetAE network configuration. Using any interpolation on such a small scale would cause the pixels to smear and would degrade the visual quality of the ultrasound images. [1] Li 2019, https://bit.ly/3hLwPQZ [2] Ouyang 2020, https: back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Hadrien Reynaud, Athanasios Vlontzos, Benjamin Hou, Arian Beqiri, Paul Leeson, Bernhard Kainz Abstract Cardiac ultrasound imaging is used to diagnose various heart diseases. Common analysis pipelines involve manual processing of the video frames by expert clinicians. This suffers from intra- and inter-observer variability. We propose a novel approach to ultrasound video analysis using a transformer architecture based on a Residual Auto-Encoder Network and a BERT model adapted for token classification. This enables videos of any length to be processed. We apply our model to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection and the automated computation of the left ventricular ejection fraction. We achieve an average frame distance of 3.36 frames for the ES and 7.17 frames for the ED on videos of arbitrary length. Our end-to-end learnable approach can estimate the ejection fraction with a MAE of 5.95 and R^2 of 0.52 in 0.15s per video, showing that segmentation is not the only way to predict ejection fraction. Code and models are available at https://github.com/HReynaud/UVT. Link to paper https://doi.org/10.1007/978-3-030-87231-1_48 Link to the code repository https://github.com/HReynaud/UVT Link to the dataset(s) https://echonet.github.io/dynamic/index.html#access Reviews Review #1 Please describe the contribution of the paper The paper show a methodological improvement on phase detection (ED/ES) in cardiac echo image sequences and ejection fraction estimation by using a transformer network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It was a true pleasure for me to read this paper (!) and I congratulate the authors on this nice piece of work. The manuscript is extremely well written and has a high quality w.r.t. the evaluation. The proposed method follows recent trends in time-series processing (usage of transformer networks). The authors show the benefit of their novel approach in comparison to other existing work (Table 1). Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There is no major weakness that comes into my mind. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I could not find a statement in the paper that the source code will be released (which is encouraged). However, an open data set was used and the clarity of the description is high, therefore, I would rate the reproducibility as high. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Just minor/out of curiosity: From a reader’s perspective I would like to hear more about the performance of the ED/ES frame detection on the portion of the sequences which were not labeled. For patients with a regular heart beat (non-arrhythmia), is it possible to show that there is a regular pattern in the phase occurance (e.g. distance between the target frames)? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? methodological contribution very good evaluation clarity of the paper What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The paper presents a method for the automatic interpretation of echocardiography videos, both the ED/ES frame selection and LVEF direct estimation. This method is based on a novel deep learning architecture incorporating both encoding for dimensionality reduction (ResNetAE) and a recurrent network used in NLP (BERT). Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strengths of the paper are the novel neural network construction, extensive apparent validation, and ablation study. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weaknesses of the paper are in the ignorance to relevant prior work and unfair comparison to cited prior work. -To the best of our knowledge…paradigm of discrete frame processing with limited temporal support – see Qin MICCAI 18 in MR (https://doi.org/10.1007/978-3-030-00934-2_53), Li MICCAI 19 in echo (https://arxiv.org/abs/1907.11292 ) and Wei MICCAI 2020 (https://link.springer.com/chapter/10.1007/978-3-030-59713-9_60 ) also on echo. Even the reference [15] itself uses r2plus1d_18 for direct estimation and which does convolve across time. -We have discussed probably the first transformer architecture that is able to analyse US videos of arbitrary length – other than [14/15] that you reference. It is unclear to this reader whether the comparisons to this prior work are fair. -It’s unclear how your Video sampling process works in the context of test-time and full video. Maybe supplemental table 1 answers the issue. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance This work may be difficult to reproduce. The hyperparameters for the ResNetAE are not specified. The Video sampling process is unclear for test time. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Small issues: -LVEF is the ratio between End-Systolic (ES) and End-Diastolic (ED) blood volumes in the left ventricle – technically the complement of that ratio, no? -[14] and [15] are the same reference. -In Table 1 caption, use `` and ‘’ for open and close quotes. -For Table 1 LVEF prediction you report [14/15]’s MAE as 7.35. I guess this comes from Extended Data Table 2, which is their r2plus1d_18 result with the whole video inferred. But your methods R and M seem to be clips centered around the labelled ED/ES frames, at least as explained in your video sampling section. The more appropriate comparison would be their “32 frame sample,” which is 4.22 and doesn’t require the labelled ED/ES frames, at least as I understand it. -Aren’t the clips in EchoNet usually ED-ES? Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The major factors are the lack of reference to highly relevant prior work, and a seemingly unfair comparison to cited work. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The authors propose a combined CNN-transformer architecture for ED/ES frame detection and LVEF estimation in apical four-chamber echo videos. The per-frame encodings of the echo cine by a ResNet are fed into a BERT module to predict the targeted measurements. The public dataset of Echonet-Dynamic is used for the experiments. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper has a good organization and the method is easy to comprehend. A public dataset is used for the experiments and the method has high reproducibility. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Technical novelty: – Application of video transformers for video classification/regression may not be quite novel and has been explored in the community [1,2,3,4]. Technical soundness: – There is a concern about the way LVEF is calculated. In the proposed architecture, the LVEF is obtained by averaging the LVEF predictions across the frames of the video (see Ejection Fraction Regressor block in Fig 1 and see the first 4 lines of page 5). This seems not to be clinically and technically correct. The LVEF correlates to the variations of the LV across the video. Calculating the video LVEF by averaging the LVEF predictions over the frames may not be justified. Results: – The proposed method noticeably underperforms the current literature (see table 1- LVEF estimation’s R2 score on the left side). The method has about 27% less R2 score (0.81 vs 0.64) compared to an existing method. The above-mentioned technical problem might be a reason for the low R2 score. Experiments: – The results reported in table 1 for LVEF (right side) may not be an accurate comparison. The method R14 (reference [14] in the paper) uses the LV segmentation results throughout the video to find the beat-to-beat cardiac cycles. R14 runs the video LVEF calculator across multiple detected cycles and reports the average LVEF among automatically detected cycles. This way the method R14 is already compatible with variable length videos, as the cycles are automatically detected based on LV segmentation. In this setup, it seems the reported results in the paper (table 1 - LVEF prediction - right side) for full video processing are different from the full video results reported in R14. – The method R14, uses 32 frames in each segment of the beat-to-beat LVEF estimation network. Another valid comparison is running method R14 (Resnet 2D+1) with 128 input frames, the same input frame length used in this paper. References: [1] Video Action Transformer Network, https://openaccess.thecvf.com/content_CVPR_2019/papers/Girdhar_Video_Action_Transformer_Network_CVPR_2019_paper.pdf [2] Video Transformer Network, https://arxiv.org/pdf/2102.00719.pdf [3] Late Temporal Modeling in 3D CNNArchitectures with BERT for Action Recognition, https://arxiv.org/pdf/2008.01232v3.pdf [4] EchoBERT: A Transformer-Based Approach for Behavior Detection in Echograms, https://ieeexplore.ieee.org/abstract/document/9281296 Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The provided details seem to be adequate and the method has good reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The adaptation of transformers for echo video processing seems to be an interesting path to explore; however, the paper at its current format seems to need technical improvement to properly address the tackled problem. Detailed comments: The method for trimming the video (into 128 frames) could be improved. Mirroring or random sampling may be a naive approach for video trimming in echo and may not be meaningful when noticing the underlying cardiac cycle patterns in echo videos. The paper claims the method is compatible with different video lengths; however, in this method, the input needs to be capped to 128 frames. The method zero-pads the videos from 112112 to 128128. It is not clear why the frames are not up-sampled to the higher resolution. Zero paddings may not use the full capacity of the network input. Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The major factors for my recommendation, as detailed in the “weaknesses” section, are the poor technical soundness, low performance, and inaccurate comparison with the prior art. What is the ranking of this paper in your review stack? 5 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors propose a combined CNN-transformer architecture for ED/ES frame detection and LVEF estimation from four-chamber echo videos. The public dataset of Echonet-Dynamic was used. Strengths: technical novelty of using a video transformer to address a classical problem of EF estimation, clarity of presentation, and well-conducted evaluation and ablation studies. Weaknesses: incomplete literature overview (reviewer 3 and 4), and technical ground of the LVEF prediction method used in this work (reviewer 4). The authors are expected present a more comprehensive literature review over related work and address the issues of LVEF estimation in response to the question raised by reviewer 4 and the issue of unfair comparison as raised by reviewer 3. The authors are encouraged to release the code for the research community (reviewer 1). What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 7 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The study is well conducted, clearly written, and the authors have addressed in the rebuttal on (1) more comprehensive literature review over related work and (2) the issue of unfair comparison as raised by reviewer 3. (3) code release (reviewer 1). After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. As indicated by the primary AC, the manuscript introduces a new hybrid approach for ED/ES detection. The proposed method is novel and there are sufficient experiments to explain the advantages for the proposed method. However, there are still some critical concerns after the rebuttal: 1) the literature review is incomplete. Authors response that the focus of the work is on LVEF / ESED applications and will integrate additional literatures referred by the reviewers. However, the responses didn’t explain the key technical difference to these works; and 2) quite limited information provided about the calculation of LVEF from the rebuttal. Therefore, it’s still not clear that the averaging of all the predications is the correct way to estimate the LVEF. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 19 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The reviews have highlighted the fact that this paper proposes a valuable contribution in terms of methodology design. There were some concerns regarding the way the comparison to other SotA method was done, and the citation of prior works. In their rebuttal, the authors have provided explanations regarding both these issues, therefore I recommend acceptance for this paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback We thank the reviewers and the AC for their feedback! Results comparison We compare our LVEF prediction results with Ouyang et al. [2] as we use their dataset. They present multiple modularized approaches with different preprocessing and LVEF accuracy. Our approach takes videos “as is” without preprocessing and predicts both LVEF and ES/ED frames indices in a single forward pass. Thus, we compare our results with the closest “All frames” evaluation from [2]. Indeed, we do not state clearly enough that their “beat-by-beat” method can be applied successively on all the heartbeats of a single US video. However, our method does not rely on segmentation performance and presents a novel approach to address LVEF prediction, which requires much less labelling work and shows excellent performance on the detection of the ES/ED frames. We also avoid all manual processing steps (e.g. resampling) which induce biases in the pipeline and prevent the generalization of the method to other domains. Our method is also much faster with an average processing time of 0.15s per entire video compared to 1.6s per heartbeat for EchoNet (0.05s x 32 frames). The best results from [2] will be added to our results (Table 1) in the LVEF prediction section for “Full video”, and we will clarify the abstract to emphasize the specificity of our approach. LVEF computation Our method computes LVEF by averaging. As the outputs from the transformer are sent into fully connected layers, the outputs of the “EF Regressor, Dense 2” layer are not bound to their frame indices (because of the averaging step). Each neuron predicts a LVEF and the averaging takes all of them into account. Literature Review: In the current version of our related works section we focus on LVEF and ES/ED applications. Thus, we do not elaborate on areas related in other ways to our work. We will integrate the references brought up by the reviewers and discuss their relation to our work. Of course, we will correct the duplication between references [14/15]. Code release Our code and trained models are ready to be released as a Github repository and the link will be added to the final version of the paper. We did not provide the link in the submitted version to respect the double-blind reviewing process. Test time inference The network is trained on 128 frames videos to enable batch training. After training, the network is ready to handle videos of any length: shorter or longer. At test time, the videos are sent into the network with no modification other than scaling the pixel intensity. The number and the order of the frames is not modified. We show examples of arbitrary length videos in Figure 2 and in the supplementary figures. R1 Regarding the assessment of the ES/ED detection performance on unlabelled portions, we can see from Figure 2 and the supplement that the distance between ES/ED frames is reasonably constant, indicating that the network is detecting the sensible frames. R3 Our description of the LVEF may be misleading. To make our statement more accurate, we will change it to “The LVEF is the ratio of the stroke volume and the end-diastolic volume of the left ventricle”. To clarify the structure of the dataset, the videos have variable lengths and framerates. Most of them contain more than one full heartbeat and all of them have a single labelled ES and ED, which can come in any order (ES then ED or ED then ES). R4 Our training-time sampling methods were shown to produce convincing results at test time. “Guided random sampling” is an improved version of the usual sampling method for ES/ED prediction. We came up with temporal mirroring to improve the network capabilities on longer videos. We added padding to the frames to keep the optimal ResNetAE network configuration. Using any interpolation on such a small scale would cause the pixels to smear and would degrade the visual quality of the ultrasound images. [1] Li 2019, https://bit.ly/3hLwPQZ [2] Ouyang 2020, https: back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0647/12/31/Paper0702" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0647/12/31/Paper0702" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0647-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0647/12/31/Paper0702"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0647/12/31/Paper0702","headline":"Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation","dateModified":"0648-01-04T00:00:00-05:17","datePublished":"0647-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Hadrien Reynaud, Athanasios Vlontzos, Benjamin Hou, Arian Beqiri, Paul Leeson, Bernhard Kainz Abstract Cardiac ultrasound imaging is used to diagnose various heart diseases. Common analysis pipelines involve manual processing of the video frames by expert clinicians. This suffers from intra- and inter-observer variability. We propose a novel approach to ultrasound video analysis using a transformer architecture based on a Residual Auto-Encoder Network and a BERT model adapted for token classification. This enables videos of any length to be processed. We apply our model to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection and the automated computation of the left ventricular ejection fraction. We achieve an average frame distance of 3.36 frames for the ES and 7.17 frames for the ED on videos of arbitrary length. Our end-to-end learnable approach can estimate the ejection fraction with a MAE of 5.95 and R^2 of 0.52 in 0.15s per video, showing that segmentation is not the only way to predict ejection fraction. Code and models are available at https://github.com/HReynaud/UVT. Link to paper https://doi.org/10.1007/978-3-030-87231-1_48 Link to the code repository https://github.com/HReynaud/UVT Link to the dataset(s) https://echonet.github.io/dynamic/index.html#access Reviews Review #1 Please describe the contribution of the paper The paper show a methodological improvement on phase detection (ED/ES) in cardiac echo image sequences and ejection fraction estimation by using a transformer network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It was a true pleasure for me to read this paper (!) and I congratulate the authors on this nice piece of work. The manuscript is extremely well written and has a high quality w.r.t. the evaluation. The proposed method follows recent trends in time-series processing (usage of transformer networks). The authors show the benefit of their novel approach in comparison to other existing work (Table 1). Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There is no major weakness that comes into my mind. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I could not find a statement in the paper that the source code will be released (which is encouraged). However, an open data set was used and the clarity of the description is high, therefore, I would rate the reproducibility as high. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Just minor/out of curiosity: From a reader’s perspective I would like to hear more about the performance of the ED/ES frame detection on the portion of the sequences which were not labeled. For patients with a regular heart beat (non-arrhythmia), is it possible to show that there is a regular pattern in the phase occurance (e.g. distance between the target frames)? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? methodological contribution very good evaluation clarity of the paper What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The paper presents a method for the automatic interpretation of echocardiography videos, both the ED/ES frame selection and LVEF direct estimation. This method is based on a novel deep learning architecture incorporating both encoding for dimensionality reduction (ResNetAE) and a recurrent network used in NLP (BERT). Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strengths of the paper are the novel neural network construction, extensive apparent validation, and ablation study. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weaknesses of the paper are in the ignorance to relevant prior work and unfair comparison to cited prior work. -To the best of our knowledge…paradigm of discrete frame processing with limited temporal support – see Qin MICCAI 18 in MR (https://doi.org/10.1007/978-3-030-00934-2_53), Li MICCAI 19 in echo (https://arxiv.org/abs/1907.11292 ) and Wei MICCAI 2020 (https://link.springer.com/chapter/10.1007/978-3-030-59713-9_60 ) also on echo. Even the reference [15] itself uses r2plus1d_18 for direct estimation and which does convolve across time. -We have discussed probably the first transformer architecture that is able to analyse US videos of arbitrary length – other than [14/15] that you reference. It is unclear to this reader whether the comparisons to this prior work are fair. -It’s unclear how your Video sampling process works in the context of test-time and full video. Maybe supplemental table 1 answers the issue. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance This work may be difficult to reproduce. The hyperparameters for the ResNetAE are not specified. The Video sampling process is unclear for test time. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Small issues: -LVEF is the ratio between End-Systolic (ES) and End-Diastolic (ED) blood volumes in the left ventricle – technically the complement of that ratio, no? -[14] and [15] are the same reference. -In Table 1 caption, use `` and ‘’ for open and close quotes. -For Table 1 LVEF prediction you report [14/15]’s MAE as 7.35. I guess this comes from Extended Data Table 2, which is their r2plus1d_18 result with the whole video inferred. But your methods R and M seem to be clips centered around the labelled ED/ES frames, at least as explained in your video sampling section. The more appropriate comparison would be their “32 frame sample,” which is 4.22 and doesn’t require the labelled ED/ES frames, at least as I understand it. -Aren’t the clips in EchoNet usually ED-ES? Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The major factors are the lack of reference to highly relevant prior work, and a seemingly unfair comparison to cited work. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The authors propose a combined CNN-transformer architecture for ED/ES frame detection and LVEF estimation in apical four-chamber echo videos. The per-frame encodings of the echo cine by a ResNet are fed into a BERT module to predict the targeted measurements. The public dataset of Echonet-Dynamic is used for the experiments. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper has a good organization and the method is easy to comprehend. A public dataset is used for the experiments and the method has high reproducibility. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Technical novelty: – Application of video transformers for video classification/regression may not be quite novel and has been explored in the community [1,2,3,4]. Technical soundness: – There is a concern about the way LVEF is calculated. In the proposed architecture, the LVEF is obtained by averaging the LVEF predictions across the frames of the video (see Ejection Fraction Regressor block in Fig 1 and see the first 4 lines of page 5). This seems not to be clinically and technically correct. The LVEF correlates to the variations of the LV across the video. Calculating the video LVEF by averaging the LVEF predictions over the frames may not be justified. Results: – The proposed method noticeably underperforms the current literature (see table 1- LVEF estimation’s R2 score on the left side). The method has about 27% less R2 score (0.81 vs 0.64) compared to an existing method. The above-mentioned technical problem might be a reason for the low R2 score. Experiments: – The results reported in table 1 for LVEF (right side) may not be an accurate comparison. The method R14 (reference [14] in the paper) uses the LV segmentation results throughout the video to find the beat-to-beat cardiac cycles. R14 runs the video LVEF calculator across multiple detected cycles and reports the average LVEF among automatically detected cycles. This way the method R14 is already compatible with variable length videos, as the cycles are automatically detected based on LV segmentation. In this setup, it seems the reported results in the paper (table 1 - LVEF prediction - right side) for full video processing are different from the full video results reported in R14. – The method R14, uses 32 frames in each segment of the beat-to-beat LVEF estimation network. Another valid comparison is running method R14 (Resnet 2D+1) with 128 input frames, the same input frame length used in this paper. References: [1] Video Action Transformer Network, https://openaccess.thecvf.com/content_CVPR_2019/papers/Girdhar_Video_Action_Transformer_Network_CVPR_2019_paper.pdf [2] Video Transformer Network, https://arxiv.org/pdf/2102.00719.pdf [3] Late Temporal Modeling in 3D CNNArchitectures with BERT for Action Recognition, https://arxiv.org/pdf/2008.01232v3.pdf [4] EchoBERT: A Transformer-Based Approach for Behavior Detection in Echograms, https://ieeexplore.ieee.org/abstract/document/9281296 Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The provided details seem to be adequate and the method has good reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The adaptation of transformers for echo video processing seems to be an interesting path to explore; however, the paper at its current format seems to need technical improvement to properly address the tackled problem. Detailed comments: The method for trimming the video (into 128 frames) could be improved. Mirroring or random sampling may be a naive approach for video trimming in echo and may not be meaningful when noticing the underlying cardiac cycle patterns in echo videos. The paper claims the method is compatible with different video lengths; however, in this method, the input needs to be capped to 128 frames. The method zero-pads the videos from 112112 to 128128. It is not clear why the frames are not up-sampled to the higher resolution. Zero paddings may not use the full capacity of the network input. Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The major factors for my recommendation, as detailed in the “weaknesses” section, are the poor technical soundness, low performance, and inaccurate comparison with the prior art. What is the ranking of this paper in your review stack? 5 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors propose a combined CNN-transformer architecture for ED/ES frame detection and LVEF estimation from four-chamber echo videos. The public dataset of Echonet-Dynamic was used. Strengths: technical novelty of using a video transformer to address a classical problem of EF estimation, clarity of presentation, and well-conducted evaluation and ablation studies. Weaknesses: incomplete literature overview (reviewer 3 and 4), and technical ground of the LVEF prediction method used in this work (reviewer 4). The authors are expected present a more comprehensive literature review over related work and address the issues of LVEF estimation in response to the question raised by reviewer 4 and the issue of unfair comparison as raised by reviewer 3. The authors are encouraged to release the code for the research community (reviewer 1). What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 7 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The study is well conducted, clearly written, and the authors have addressed in the rebuttal on (1) more comprehensive literature review over related work and (2) the issue of unfair comparison as raised by reviewer 3. (3) code release (reviewer 1). After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. As indicated by the primary AC, the manuscript introduces a new hybrid approach for ED/ES detection. The proposed method is novel and there are sufficient experiments to explain the advantages for the proposed method. However, there are still some critical concerns after the rebuttal: 1) the literature review is incomplete. Authors response that the focus of the work is on LVEF / ESED applications and will integrate additional literatures referred by the reviewers. However, the responses didn’t explain the key technical difference to these works; and 2) quite limited information provided about the calculation of LVEF from the rebuttal. Therefore, it’s still not clear that the averaging of all the predications is the correct way to estimate the LVEF. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 19 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The reviews have highlighted the fact that this paper proposes a valuable contribution in terms of methodology design. There were some concerns regarding the way the comparison to other SotA method was done, and the citation of prior works. In their rebuttal, the authors have provided explanations regarding both these issues, therefore I recommend acceptance for this paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback We thank the reviewers and the AC for their feedback! Results comparison We compare our LVEF prediction results with Ouyang et al. [2] as we use their dataset. They present multiple modularized approaches with different preprocessing and LVEF accuracy. Our approach takes videos “as is” without preprocessing and predicts both LVEF and ES/ED frames indices in a single forward pass. Thus, we compare our results with the closest “All frames” evaluation from [2]. Indeed, we do not state clearly enough that their “beat-by-beat” method can be applied successively on all the heartbeats of a single US video. However, our method does not rely on segmentation performance and presents a novel approach to address LVEF prediction, which requires much less labelling work and shows excellent performance on the detection of the ES/ED frames. We also avoid all manual processing steps (e.g. resampling) which induce biases in the pipeline and prevent the generalization of the method to other domains. Our method is also much faster with an average processing time of 0.15s per entire video compared to 1.6s per heartbeat for EchoNet (0.05s x 32 frames). The best results from [2] will be added to our results (Table 1) in the LVEF prediction section for “Full video”, and we will clarify the abstract to emphasize the specificity of our approach. LVEF computation Our method computes LVEF by averaging. As the outputs from the transformer are sent into fully connected layers, the outputs of the “EF Regressor, Dense 2” layer are not bound to their frame indices (because of the averaging step). Each neuron predicts a LVEF and the averaging takes all of them into account. Literature Review: In the current version of our related works section we focus on LVEF and ES/ED applications. Thus, we do not elaborate on areas related in other ways to our work. We will integrate the references brought up by the reviewers and discuss their relation to our work. Of course, we will correct the duplication between references [14/15]. Code release Our code and trained models are ready to be released as a Github repository and the link will be added to the final version of the paper. We did not provide the link in the submitted version to respect the double-blind reviewing process. Test time inference The network is trained on 128 frames videos to enable batch training. After training, the network is ready to handle videos of any length: shorter or longer. At test time, the videos are sent into the network with no modification other than scaling the pixel intensity. The number and the order of the frames is not modified. We show examples of arbitrary length videos in Figure 2 and in the supplementary figures. R1 Regarding the assessment of the ES/ED detection performance on unlabelled portions, we can see from Figure 2 and the supplement that the distance between ES/ED frames is reasonably constant, indicating that the network is detecting the sensible frames. R3 Our description of the LVEF may be misleading. To make our statement more accurate, we will change it to “The LVEF is the ratio of the stroke volume and the end-diastolic volume of the left ventricle”. To clarify the structure of the dataset, the videos have variable lengths and framerates. Most of them contain more than one full heartbeat and all of them have a single labelled ES and ED, which can come in any order (ES then ED or ED then ES). R4 Our training-time sampling methods were shown to produce convincing results at test time. “Guided random sampling” is an improved version of the usual sampling method for ES/ED prediction. We came up with temporal mirroring to improve the network capabilities on longer videos. We added padding to the frames to keep the optimal ResNetAE network configuration. Using any interpolation on such a small scale would cause the pixels to smear and would degrade the visual quality of the ultrasound images. [1] Li 2019, https://bit.ly/3hLwPQZ [2] Ouyang 2020, https: back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Reynaud, Hadrien,Vlontzos, Athanasios,Hou, Benjamin,Beqiri, Arian,Leeson, Paul,Kainz, Bernhard" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Clinical applications - Cardiac"
        class="post-category">
        Clinical applications - Cardiac
      </a>
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a>
      
      <a 
        href="kittywong/categories#Modalities - Ultrasound"
        class="post-category">
        Modalities - Ultrasound
      </a>
      
      <a 
        href="kittywong/categories#Modalities - Video"
        class="post-category">
        Modalities - Video
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Reynaud, Hadrien"
        class="post-tags">
        Reynaud, Hadrien
      </a> |  
      
      <a href="kittywong/tags#Vlontzos, Athanasios"
        class="post-tags">
        Vlontzos, Athanasios
      </a> |  
      
      <a href="kittywong/tags#Hou, Benjamin"
        class="post-tags">
        Hou, Benjamin
      </a> |  
      
      <a href="kittywong/tags#Beqiri, Arian"
        class="post-tags">
        Beqiri, Arian
      </a> |  
      
      <a href="kittywong/tags#Leeson, Paul"
        class="post-tags">
        Leeson, Paul
      </a> |  
      
      <a href="kittywong/tags#Kainz, Bernhard"
        class="post-tags">
        Kainz, Bernhard
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Hadrien Reynaud, Athanasios Vlontzos, Benjamin Hou, Arian Beqiri, Paul Leeson, Bernhard Kainz
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Cardiac ultrasound imaging is used to diagnose various heart diseases. Common analysis pipelines involve manual processing of the video frames by expert clinicians. This suffers from intra- and inter-observer variability. We propose a novel approach to ultrasound video analysis using a transformer architecture based on a Residual Auto-Encoder Network and a BERT model adapted for token classification. This enables videos of  any length to be processed. We apply our model to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection and the automated computation of the left ventricular ejection fraction. We achieve an average frame distance of 3.36 frames for the ES and 7.17 frames for the ED on videos of arbitrary length. Our end-to-end learnable approach can estimate the ejection fraction with a MAE of 5.95 and R^2 of 0.52 in 0.15s per video, showing that segmentation is not the only way to predict ejection fraction. Code and models are available at https://github.com/HReynaud/UVT.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87231-1_48">https://doi.org/10.1007/978-3-030-87231-1_48</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/HReynaud/UVT
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>https://echonet.github.io/dynamic/index.html#access
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper show a methodological improvement on phase detection (ED/ES) in cardiac echo image sequences and ejection fraction estimation by using a transformer network.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>It was a true pleasure for me to read this paper (!) and I congratulate the authors on this nice piece of work. The manuscript is extremely well written and has a high quality w.r.t. the evaluation. The proposed method follows recent trends in time-series processing (usage of transformer networks). The authors show the benefit of their novel approach in comparison to other existing work (Table 1).</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>There is no major weakness that comes into my mind.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>I could not find a statement in the paper that the source code will be released (which is encouraged). However, an open data set was used and the clarity of the description is high, therefore, I would rate the reproducibility as high.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Just minor/out of curiosity:
From a reader’s perspective I would like to hear more about the performance of the ED/ES frame detection on the portion of the sequences which were not labeled. For patients with a regular heart beat (non-arrhythmia), is it possible to show that there is a regular pattern in the phase occurance (e.g. distance between the target frames)?</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <ul>
        <li>methodological contribution</li>
        <li>very good evaluation</li>
        <li>clarity of the paper</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper presents a method for the automatic interpretation of echocardiography videos, both the ED/ES frame selection and LVEF direct estimation. This method is based on a novel deep learning architecture incorporating both encoding for dimensionality reduction (ResNetAE) and a recurrent network used in NLP (BERT).</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The main strengths of the paper are the novel neural network construction, extensive apparent validation, and ablation study.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The main weaknesses of the paper are in the ignorance to relevant prior work and unfair comparison to cited prior work.</p>

      <p>-To the best of our knowledge…paradigm of discrete frame processing with limited temporal support – see Qin MICCAI 18 in MR (https://doi.org/10.1007/978-3-030-00934-2_53), Li MICCAI 19 in echo (https://arxiv.org/abs/1907.11292 )  and Wei MICCAI 2020 (https://link.springer.com/chapter/10.1007/978-3-030-59713-9_60 ) also on echo. Even the reference [15] itself uses r2plus1d_18 for direct estimation and which does convolve across time.</p>

      <p>-We have discussed probably the first transformer architecture that is able to analyse US videos of arbitrary length – other than [14/15] that you reference. It is unclear to this reader whether the comparisons to this prior work are fair.</p>

      <p>-It’s unclear how your Video sampling process works in the context of test-time and full video. Maybe supplemental table 1 answers the issue.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>This work may be difficult to reproduce. The hyperparameters for the ResNetAE are not specified. The Video sampling process is unclear for test time.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Small issues:</p>

      <p>-LVEF is the ratio between End-Systolic (ES) and End-Diastolic (ED) blood volumes in the left ventricle – technically the complement of that ratio, no?</p>

      <p>-[14] and [15] are the same reference.</p>

      <p>-In Table 1 caption, use `` and ‘’ for open and close quotes.</p>

      <p>-For Table 1 LVEF prediction you report [14/15]’s MAE as 7.35. I guess this comes from Extended Data Table 2, which is their r2plus1d_18 result with the whole video inferred. But your methods R and M seem to be clips centered around the labelled ED/ES frames, at least as explained in your video sampling section. The more appropriate comparison would be their “32 frame sample,” which is 4.22 and doesn’t require the labelled ED/ES frames, at least as I understand it.</p>

      <p>-Aren’t the clips in EchoNet usually ED-ES?</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>probably reject (4)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The major factors are the lack of reference to highly relevant prior work, and a seemingly unfair comparison to cited work.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors propose a combined CNN-transformer architecture for ED/ES frame detection and LVEF estimation in apical four-chamber echo videos. The per-frame encodings of the echo cine by a ResNet are fed into a BERT module to predict the targeted measurements. The public dataset of Echonet-Dynamic is used for the experiments.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>The paper has a good organization and the method is easy to comprehend.</li>
        <li>A public dataset is used for the experiments and the method has high reproducibility.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>Technical novelty: 
– Application of video transformers for video classification/regression may not be quite novel and has been explored in the community [1,2,3,4].</li>
        <li>Technical soundness: 
– There is a concern about the way LVEF is calculated. In the proposed architecture, the LVEF is obtained by averaging the LVEF predictions across the frames of the video (see Ejection Fraction Regressor block in Fig 1 and see the first 4 lines of page 5). This seems not to be clinically and technically correct. The LVEF correlates to the variations of the LV across the video. Calculating the video LVEF by averaging the LVEF predictions over the frames may not be justified.</li>
        <li>Results: 
– The proposed method noticeably underperforms the current literature (see table 1- LVEF estimation’s R2 score on the left side). The method has about 27% less R2 score (0.81 vs 0.64) compared to an existing method. The above-mentioned technical problem might be a reason for the low R2 score.</li>
        <li>Experiments:
– The results reported in table 1 for LVEF (right side) may not be an accurate comparison. The method R14 (reference [14] in the paper) uses the LV segmentation results throughout the video to find the beat-to-beat cardiac cycles. R14 runs the video LVEF calculator across multiple detected cycles and reports the average LVEF among automatically detected cycles. This way the method R14 is already compatible with variable length videos, as the cycles are automatically detected based on LV segmentation. In this setup, it seems the reported results in the paper (table 1 - LVEF prediction - right side) for full video processing are different from the full video results reported in R14. 
– The method R14, uses 32 frames in each segment of the beat-to-beat LVEF estimation network. Another valid comparison is running method R14 (Resnet 2D+1) with 128 input frames, the same input frame length used in this paper.</li>
      </ul>

      <p>References:
[1] Video Action Transformer Network, https://openaccess.thecvf.com/content_CVPR_2019/papers/Girdhar_Video_Action_Transformer_Network_CVPR_2019_paper.pdf
[2] Video Transformer Network, https://arxiv.org/pdf/2102.00719.pdf
[3] Late Temporal Modeling in 3D CNNArchitectures with BERT for Action Recognition, https://arxiv.org/pdf/2008.01232v3.pdf
[4] EchoBERT: A Transformer-Based Approach for Behavior Detection in Echograms, https://ieeexplore.ieee.org/abstract/document/9281296</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The provided details seem to be adequate and the method has good reproducibility.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>The adaptation of transformers for echo video processing seems to be an interesting path to explore; however, the paper at its current format seems to need technical improvement to properly address the tackled problem.</p>

      <p>Detailed comments:</p>
      <ul>
        <li>The method for trimming the video (into 128 frames) could be improved. Mirroring or random sampling may be a naive approach for video trimming in echo and may not be meaningful when noticing the underlying cardiac cycle patterns in echo videos.</li>
        <li>The paper claims the method is compatible with different video lengths; however, in this method, the input needs to be capped to 128 frames.</li>
        <li>The method zero-pads the videos from 112<em>112 to 128</em>128. It is not clear why the frames are not up-sampled to the higher resolution. Zero paddings may not use the full capacity of the network input.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>reject (3)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The major factors for my recommendation, as detailed in the “weaknesses” section, are the poor technical soundness, low performance, and inaccurate comparison with the prior art.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>The authors propose a combined CNN-transformer architecture for ED/ES frame detection and LVEF estimation from four-chamber echo videos. The public dataset of Echonet-Dynamic was used. Strengths: technical novelty of using a video transformer to address a classical problem of EF estimation, clarity of presentation, and well-conducted evaluation and ablation studies. Weaknesses: incomplete literature overview (reviewer 3 and 4), and technical ground of the LVEF prediction method used in this work (reviewer 4). The authors are expected present a more comprehensive literature review over related work and address the issues of LVEF estimation in response to the question raised by reviewer 4 and the issue of unfair comparison as raised by reviewer 3. The authors are encouraged to release the code for the research community (reviewer 1).</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>7</p>
    </blockquote>
  </li>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The study is well conducted, clearly written, and the authors have addressed in the rebuttal on (1) more comprehensive literature review over related work and (2) the issue of unfair comparison as raised by reviewer 3. (3) code release (reviewer 1).</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>8</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="meta-review-2">Meta-Review #2</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>

      <p>As indicated by the primary AC, the manuscript introduces a new hybrid approach for ED/ES detection. The proposed method is novel and there are sufficient experiments to explain the advantages for the proposed method. However, there are still some critical concerns after the rebuttal: 1) the literature review is incomplete. Authors response that the focus of the work is on LVEF / ESED applications and will integrate additional literatures referred by the reviewers. However, the responses didn’t explain the key technical difference to these works; and 2) quite limited information provided about the calculation of LVEF from the rebuttal. Therefore, it’s still not clear that the averaging of all the predications is the correct way to estimate the LVEF.</p>

    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Reject</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>19</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>
<h2 id="meta-review-3">Meta-Review #3</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The reviews have highlighted the fact that this paper proposes a valuable contribution in terms of methodology design.
There were some concerns regarding the way the comparison to other SotA method was done, and the citation of prior works.
In their rebuttal, the authors have provided explanations regarding both these issues, therefore I recommend acceptance for this paper.</p>

    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
</ul>

<h2><br /><br /></h2>
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We thank the reviewers and the AC for their feedback!</p>

  <p>Results comparison
We compare our LVEF prediction results with Ouyang et al. [2] as we use their dataset. They present multiple modularized approaches with different preprocessing and LVEF accuracy. Our approach takes videos “as is” without preprocessing and predicts both LVEF and ES/ED frames indices in a single forward pass. Thus, we compare our results with the closest  “All frames” evaluation from [2]. 
Indeed, we do not state clearly enough that their “beat-by-beat” method can be applied successively on all the heartbeats of a single US video. However, our method does not rely on segmentation performance and presents a novel approach to address LVEF prediction, which requires much less labelling work and shows excellent performance on the detection of the ES/ED frames. We also avoid all manual processing steps (e.g. resampling) which induce biases in the pipeline and prevent the generalization of the method to other domains. Our method is also much faster with an average processing time of 0.15s per entire video compared to 1.6s per heartbeat for EchoNet (0.05s x 32 frames).
The best results from [2] will be added to our results (Table 1) in the LVEF prediction section for “Full video”, and we will clarify the abstract to emphasize the specificity of our approach.</p>

  <p>LVEF computation
Our method computes LVEF by averaging. As the outputs from the transformer are sent into fully connected layers, the outputs of the “EF Regressor, Dense 2” layer are not bound to their frame indices (because of the averaging step). Each neuron predicts a LVEF and the averaging takes all of them into account.</p>

  <p>Literature Review: 
In the current version of our related works section we focus on LVEF and ES/ED applications. Thus, we do not elaborate on areas related in other ways to our work. We will integrate the references brought up by the reviewers and discuss their relation to our work. Of course, we will correct the duplication between references [14/15].</p>

  <p>Code release
Our code and trained models are ready to be released as a Github repository and the link will be added to the final version of the paper. We did not provide the link in the submitted version to respect the double-blind reviewing process.</p>

  <p>Test time inference
The network is trained on 128 frames videos to enable batch training. After training, the network is ready to handle videos of any length: shorter or longer. At test time, the videos are sent into the network with no modification other than scaling the pixel intensity. The number and the order of the frames is not modified. We show examples of arbitrary length videos in Figure 2 and in the supplementary figures.</p>

  <p>R1
Regarding the assessment of the ES/ED detection performance on unlabelled portions, we can see from Figure 2 and the supplement that the distance between ES/ED frames is reasonably constant, indicating that the network is detecting the sensible frames.</p>

  <p>R3
Our description of the LVEF may be misleading. To make our statement more accurate, we will change it to “The LVEF is the ratio of the stroke volume and the end-diastolic volume of the left ventricle”.</p>

  <p>To clarify the structure of the dataset, the videos have variable lengths and framerates. Most of them contain more than one full heartbeat and all of them have a single labelled ES and ED, which can come in any order (ES then ED or ED then ES).</p>

  <p>R4
Our training-time sampling methods were shown to produce convincing results at test time. “Guided random sampling” is an improved version of the usual sampling method for ES/ED prediction. We came up with temporal mirroring to improve the network capabilities on longer videos.</p>

  <p>We added padding to the frames to keep the optimal ResNetAE network configuration. Using any interpolation on such a small scale would cause the pixels to smear and would degrade the visual quality of the ultrasound images.</p>

  <p>[1] Li 2019, https://bit.ly/3hLwPQZ
[2] Ouyang 2020, https:</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0647-12-31
      -->
      <!--
      
        ,
        updated at 
        0648-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Clinical applications - Cardiac"
        class="post-category">
        Clinical applications - Cardiac
      </a> |
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - Ultrasound"
        class="post-category">
        Modalities - Ultrasound
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - Video"
        class="post-category">
        Modalities - Video
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Reynaud, Hadrien"
        class="post-category">
        Reynaud, Hadrien
      </a> |  
      
      <a href="kittywong/tags#Vlontzos, Athanasios"
        class="post-category">
        Vlontzos, Athanasios
      </a> |  
      
      <a href="kittywong/tags#Hou, Benjamin"
        class="post-category">
        Hou, Benjamin
      </a> |  
      
      <a href="kittywong/tags#Beqiri, Arian"
        class="post-category">
        Beqiri, Arian
      </a> |  
      
      <a href="kittywong/tags#Leeson, Paul"
        class="post-category">
        Leeson, Paul
      </a> |  
      
      <a href="kittywong/tags#Kainz, Bernhard"
        class="post-category">
        Kainz, Bernhard
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0648/12/31/Paper0798">
          EchoCP: An Echocardiography Dataset in Contrast Transthoracic Echocardiography for Patent Foramen Ovale Diagnosis
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0646/12/31/Paper0247">
          Distortion Energy for Deep Learning-based Volumetric Finite Element Mesh Generation for Aortic Valves
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
