<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>A Unified Hyper-GAN Model for Unpaired Multi-contrast MR Image Translation | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="A Unified Hyper-GAN Model for Unpaired Multi-contrast MR Image Translation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Heran Yang, Jian Sun, Liwei Yang, Zongben Xu Abstract Cross-contrast image translation is an important task for completing missing contrasts in clinical diagnosis. However, most existing methods learn separate translator for each pair of contrasts, which is inefficient due to many possible contrast pairs in real scenarios. In this work, we propose a unified Hyper-GAN model for effectively and efficiently translating between different contrast pairs. Hyper-GAN consists of a pair of hyper-encoder and hyper-decoder to first map from the source contrast to a common feature space, and then further map to the target contrast image. To facilitate the translation between different contrast pairs, contrast-modulators are designed to tune the hyper-encoder and hyper-decoder adaptive to different contrasts. We also design a common space loss to enforce that multi-contrast images of a subject share a common feature space, implicitly modeling the shared underlying anatomical structures. Experiments on two datasets of IXI and BraTS 2019 show that our Hyper-GAN achieves state-of-the-art results in both accuracy and efficiency, e.g., improving more than 1.47 and 1.09 dB in PSNR on two datasets with less than half the amount of parameters. Link to paper https://doi.org/10.1007/978-3-030-87199-4_12 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This work develops an unpaired method that enables efficient cross-contrast MR image translation. The authors present a hypernetwork-based approach to enable scaling of the networks filters (or normalization functions) according to the input/output contrast. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is very well written and the presentation is clear. As far as my knowledge goes, applying a hypernetwork-based approach for the synthesis of particular MRI contrasts is a novel and interesting idea, since only one encoder and decoder are necessary, while they learned filters are adapted to the particular contrast type. Good evaluation: comparison to many standard methods, ablation study concerning the losses and FS/CIN, different metrics and a significance test. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main disadvantage of the method is that it is only applied on 2D slices of the brain. I assume this is due to computational restrains, however, the 3D nature of brain MRIs is crucial. In my experience, having such a large amount of different loss-functions is typically hard to optimize. I lack the training details concerning this issue. A general drawback of the method is, that the available amount of contrasts needs to be known prior to training, i.e. four different MRI contrasts. Thus, if new sequences (e.g. from another dataset) need to be translated to the known ones, it would be impossible with this method. The authors implicitly bump into this issue in their work, since the IXI dataset has less different contrasts than the BRATS dataset. This means that a reliable training (for a clinical application) requires a training on all possible contrasts, which is highly unlikely. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance In the form the authors answered most of the questions positively. While they use openly available data, they promise to release the code on GitHub. With the released code, the reproducibility will be very good, but at this stage it would be rather hard to reproduce (i.e. because of optimization issues). Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The general idea of the work is interesting, however not “ground-breaking”. The good evaluation shows the advantages of the presented method, however the lack of 3D application limits the practical use of the method. Also, the needed a-priori knowledge about all possible contrasts, limits the possible applications of the approach. I strongly advise to make the code available as promised by the authors. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Good novel idea and good validation of the method. However, the paper is more proof-of-concept that presenting an applicable approach. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper It intends to solve cross-contrast image translation task by proposing a unified hyper-GAN model. It has two dataset IXI and BraTS2019 for evaluation. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea is interesting. It is well-written and has detailed explanation. It compares the proposed method with several baselines in the literature and also conducted ablation studies. It not only compares the performance but also compares the parameter efficiency of the model. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There are other state-of-the-art models in the literature with code such as Dual Generator Generative Adversarial Networks for Multi-domain Image-to-Image Translation. It would be great to compare with these methods. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The data are public datasets and the author promises to release the code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It would be great to compare with the other sota methods in the literature. I wonder how helpful the generated data is for the segmentation tasks. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It is an interesting paper with evaluation on two public dataset. I think it is on the borderline of acceptance. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper proposed a unified multi-constrast MRI image translation framework based on unsupervised learning. A hyper-GAN model was built which included a hyper-encoder, a hyper-decoder, two contrast-modulators, a classifier, and several discriminators. A composite objective function was also proposed for improving the performance of the proposed network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The authors built a unified multi-contrast MRI image translation network that be used to achieve any possible contrast conversion. Compared with existing multi-contrast MRI image translation method that can only achieve a certain type of contrast conversion, the proposed method worked in a multi-tasking way. The proposed network were trained unsupervisedly, which mitigated the shortcoming of the requirement of paired data. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There are two datasets used in the paper and the authors implemented their method on these two datasets separately. Since the proposed method is a unified multi-contrast method, it is unclear why the authors not choosing to implement their method by mixing the two datasets together. The result comparison with other methods only focused on the overall level. Specific contrast translation result comparison should be added to show the effectiveness of the proposed method. The authors should also point out the percentage of every type of contrast conversion in the training data. It seems that other methods for comparison like cyclegan can only implement a certain type of contrast conversion. Authors should discuss how they achieve other mentioned methods. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance This paper can be easily reproduced since authors will make their code publicly available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The authors should consider adding experiments showing the result of specific types of contrast conversion. Also, the authors are suggested to train the proposed network on the dataset by mixing the two datasets and see if better results can be obtained. Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Some factors like the novelty of the proposed method, experiment design and result analysis. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Reviewers agreed the idea of synthesizing different MR contrasts using a single encoder single decoder model is of interest. Most concerns are about the experimental validation. First, since some methods in comparison, e.g., cycleGAN, could not do arbitrary cross-contrast MR image translation, how their performance were obtained in Table 2 needs further clarification, since this is the major results of this work. Second, as Reviewer#3 pointed out, in addition to comparing the overall performance, comparing the performance of specific contrast translation is also important. Is there any discussion about this? Third, the proposed method is 2D in contrast to the 3D nature of MR images. This paper did not mention or discuss or compare with 3D unpaired medical image synthesis methods, such as 3D Cycle GAN. Could this be discussed during rebuttal since Review#1 concerned this might limit the practical use of the proposed method. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. In the rebuttal, the authors have answered most questions from Reviewer #3 including the training details of the comparison methods like cycleGAN in Table 2, additional comparison results of specific contrast translation, and additional experiments on the mixed dataset. On the other hand, the rebuttal did not address the concerns with no comparison to 3D based synthesis methods, which although does not affect the major claims of this paper. My attitude to this work is neutral. The major concern is still that although arbitrary contrast translation is of certain interest, the methodological contribution of the proposed model seems to be a bit mild. It seems to be a borderline paper After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper presents an interesting take on image translation through an invariant latent space. Overall the other reviewers, MR, and I found aspects/ideas in this paper interesting, and worthy of presentation at MICCAI. While several issues were brought up, I think they were quite well addressed in the rebuttal, such as why 2D vs 3D, how cycleGANs are tested, etc, and I believe the remaining issues brought up by the reviewers are minor. Overall, the idea bring some interesting insights that would be nice to appear at MICCAI. Of course, the authors should take all the feedback into consideration, make the necessary clarifications described in the rebuttal, as these are part of the reason for my acceptance. Minor: HyperGAN might not be the best name, as there are already popular projects named this, which will make it harder for this paper to be found. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 7 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper included good new ideas and provides a good validation. Although implementation is done in 2D, authors discuss in their rebuttal that results on 3D images are promising and show feasibility, and that the algorithm itself would not be limited to 2D given sufficient computational resources. The paper has weaknesses also cited by reviewers, e.g. hard to optimize due to complexity, somewhat experimental validation, and not really be ground breaking, but it may have merit to be discussed with the MICCAI community. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Author Feedback Q1: Clarify the implementation of CycleGAN. A1: As discussed in “Comparison with CycleGAN” on page 8. CycleGAN is for one-to-one contrast translation. For a N-contrast translation task, we trained N*(N-1)/2 CycleGAN [1], each of which is for a specific pair of contrasts. We averaged the results over different contrast pairs as final accuracy of CycleGAN. [1] Zhu, J.Y., et al. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV 2017. Q2: Compare specific contrast translation results. A2: Thanks. Due to space limit, we will try to put comparative results for specific contrast translation tasks in final version. For IXI dataset, our method achieved the highest accuracies for all 6 translation tasks in all metrics. For BraTS dataset, our method worked better than all compared methods for all 12 tasks in all metrics, except MAE score of StarGAN for FLAIR-to-T2 translation (0.0103 of StarGAN vs. 0.0105 of ours). Q3: The method is only applied on 2D slices and might limit the practical use. A3: Thanks. First, in experiments, all the compared methods are trained on 2D slices, and evaluated on 3D volumes. The results in different metrics show that the learned 2D model works well on 3D volumes. In addition, our method is not restricted to 2D and can be naturally extended to 3D in essence. As reviewer pointed out, the major bottleneck is GPU memory instead of methodology. We will discuss this in “Conclusion”, and plan to extend our approach to multiple adjacent slices or 3D patches in future work. Q4: Why not mix two datasets together? A4: In this paper, the two datasets are in different settings, i.e., one is with-skull and another is skull-stripped. As suggested, we additionally conduct experiments of training and testing on the combined dataset with 5 contrasts (T1w, T2w, PDw, T1CE, FLAIR) with both with-skull and skull-stripped brain images, which is more challenging than the separate training/testing setting. The results show that our method still performs best in all metrics (p&lt;.001) and produces 0.0127/29.02/0.902 in MAE/PSNR/SSIM, compared with 0.0142/28.60/0.873 of StarGAN, 0.0168/27.00/0.831 of SUGAN, and 0.0145/28.00/0.879 of ComboGAN. We will try to add these results in paper. Q5: The method needs to know the amount of contrasts before training, and is impossible to translate unseen sequences into the known ones. A5: We propose a unified GAN model that can flexibly translate between different contrast pairs using a unified network. It is much more efficient than the existing methods that use separate translators for different pairs of contrasts, or multiple encoders/decoders, with insufficient adaptiveness to different contrasts. All these methods including ours depend on the given contrast set. It is challenging to generalize to unseen sequences. One possible way to achieve this for our method is as follows. Different sequences can be represented by imaging parameters. We implicitly model the tissue parameters by common feature space, and imaging parameters by one-hot codes in hyper-encoder/hyper-decoder. If one-hot codes of imaging parameters can be generalized to continuous codes covering unseen sequences, our method might be able to generalize to unseen sequences. This will be left for future work. Q6: Compare with SoTA methods, e.g., DGGAN. A6: We additionally compare with DGGAN. The results show that our method works better than DGGAN in all metrics (p&lt;.001) and produces 0.0133/29.64/0.910 and 0.0070/31.90/0.930 in MAE/PSNR/SSIM on two datasets, compared with 0.0175/27.51/0.858 and 0.0081/30.81/0.908 of DGGAN. We will cite and compare DGGAN. Q7: How to optimize the training loss? A7: As discussed in “Training loss” on page 5. We set the weights lambda_{id, rec, cla} to 0.5, 10, 0.2 to make each loss term within a similar range, and trained the model in 100 epochs using an Adam optimizer with a learning rate of 0.0002 and betas of (0.5, 0.999). We will add more details in paper. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Heran Yang, Jian Sun, Liwei Yang, Zongben Xu Abstract Cross-contrast image translation is an important task for completing missing contrasts in clinical diagnosis. However, most existing methods learn separate translator for each pair of contrasts, which is inefficient due to many possible contrast pairs in real scenarios. In this work, we propose a unified Hyper-GAN model for effectively and efficiently translating between different contrast pairs. Hyper-GAN consists of a pair of hyper-encoder and hyper-decoder to first map from the source contrast to a common feature space, and then further map to the target contrast image. To facilitate the translation between different contrast pairs, contrast-modulators are designed to tune the hyper-encoder and hyper-decoder adaptive to different contrasts. We also design a common space loss to enforce that multi-contrast images of a subject share a common feature space, implicitly modeling the shared underlying anatomical structures. Experiments on two datasets of IXI and BraTS 2019 show that our Hyper-GAN achieves state-of-the-art results in both accuracy and efficiency, e.g., improving more than 1.47 and 1.09 dB in PSNR on two datasets with less than half the amount of parameters. Link to paper https://doi.org/10.1007/978-3-030-87199-4_12 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This work develops an unpaired method that enables efficient cross-contrast MR image translation. The authors present a hypernetwork-based approach to enable scaling of the networks filters (or normalization functions) according to the input/output contrast. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is very well written and the presentation is clear. As far as my knowledge goes, applying a hypernetwork-based approach for the synthesis of particular MRI contrasts is a novel and interesting idea, since only one encoder and decoder are necessary, while they learned filters are adapted to the particular contrast type. Good evaluation: comparison to many standard methods, ablation study concerning the losses and FS/CIN, different metrics and a significance test. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main disadvantage of the method is that it is only applied on 2D slices of the brain. I assume this is due to computational restrains, however, the 3D nature of brain MRIs is crucial. In my experience, having such a large amount of different loss-functions is typically hard to optimize. I lack the training details concerning this issue. A general drawback of the method is, that the available amount of contrasts needs to be known prior to training, i.e. four different MRI contrasts. Thus, if new sequences (e.g. from another dataset) need to be translated to the known ones, it would be impossible with this method. The authors implicitly bump into this issue in their work, since the IXI dataset has less different contrasts than the BRATS dataset. This means that a reliable training (for a clinical application) requires a training on all possible contrasts, which is highly unlikely. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance In the form the authors answered most of the questions positively. While they use openly available data, they promise to release the code on GitHub. With the released code, the reproducibility will be very good, but at this stage it would be rather hard to reproduce (i.e. because of optimization issues). Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The general idea of the work is interesting, however not “ground-breaking”. The good evaluation shows the advantages of the presented method, however the lack of 3D application limits the practical use of the method. Also, the needed a-priori knowledge about all possible contrasts, limits the possible applications of the approach. I strongly advise to make the code available as promised by the authors. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Good novel idea and good validation of the method. However, the paper is more proof-of-concept that presenting an applicable approach. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper It intends to solve cross-contrast image translation task by proposing a unified hyper-GAN model. It has two dataset IXI and BraTS2019 for evaluation. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea is interesting. It is well-written and has detailed explanation. It compares the proposed method with several baselines in the literature and also conducted ablation studies. It not only compares the performance but also compares the parameter efficiency of the model. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There are other state-of-the-art models in the literature with code such as Dual Generator Generative Adversarial Networks for Multi-domain Image-to-Image Translation. It would be great to compare with these methods. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The data are public datasets and the author promises to release the code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It would be great to compare with the other sota methods in the literature. I wonder how helpful the generated data is for the segmentation tasks. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It is an interesting paper with evaluation on two public dataset. I think it is on the borderline of acceptance. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper proposed a unified multi-constrast MRI image translation framework based on unsupervised learning. A hyper-GAN model was built which included a hyper-encoder, a hyper-decoder, two contrast-modulators, a classifier, and several discriminators. A composite objective function was also proposed for improving the performance of the proposed network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The authors built a unified multi-contrast MRI image translation network that be used to achieve any possible contrast conversion. Compared with existing multi-contrast MRI image translation method that can only achieve a certain type of contrast conversion, the proposed method worked in a multi-tasking way. The proposed network were trained unsupervisedly, which mitigated the shortcoming of the requirement of paired data. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There are two datasets used in the paper and the authors implemented their method on these two datasets separately. Since the proposed method is a unified multi-contrast method, it is unclear why the authors not choosing to implement their method by mixing the two datasets together. The result comparison with other methods only focused on the overall level. Specific contrast translation result comparison should be added to show the effectiveness of the proposed method. The authors should also point out the percentage of every type of contrast conversion in the training data. It seems that other methods for comparison like cyclegan can only implement a certain type of contrast conversion. Authors should discuss how they achieve other mentioned methods. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance This paper can be easily reproduced since authors will make their code publicly available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The authors should consider adding experiments showing the result of specific types of contrast conversion. Also, the authors are suggested to train the proposed network on the dataset by mixing the two datasets and see if better results can be obtained. Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Some factors like the novelty of the proposed method, experiment design and result analysis. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Reviewers agreed the idea of synthesizing different MR contrasts using a single encoder single decoder model is of interest. Most concerns are about the experimental validation. First, since some methods in comparison, e.g., cycleGAN, could not do arbitrary cross-contrast MR image translation, how their performance were obtained in Table 2 needs further clarification, since this is the major results of this work. Second, as Reviewer#3 pointed out, in addition to comparing the overall performance, comparing the performance of specific contrast translation is also important. Is there any discussion about this? Third, the proposed method is 2D in contrast to the 3D nature of MR images. This paper did not mention or discuss or compare with 3D unpaired medical image synthesis methods, such as 3D Cycle GAN. Could this be discussed during rebuttal since Review#1 concerned this might limit the practical use of the proposed method. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. In the rebuttal, the authors have answered most questions from Reviewer #3 including the training details of the comparison methods like cycleGAN in Table 2, additional comparison results of specific contrast translation, and additional experiments on the mixed dataset. On the other hand, the rebuttal did not address the concerns with no comparison to 3D based synthesis methods, which although does not affect the major claims of this paper. My attitude to this work is neutral. The major concern is still that although arbitrary contrast translation is of certain interest, the methodological contribution of the proposed model seems to be a bit mild. It seems to be a borderline paper After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper presents an interesting take on image translation through an invariant latent space. Overall the other reviewers, MR, and I found aspects/ideas in this paper interesting, and worthy of presentation at MICCAI. While several issues were brought up, I think they were quite well addressed in the rebuttal, such as why 2D vs 3D, how cycleGANs are tested, etc, and I believe the remaining issues brought up by the reviewers are minor. Overall, the idea bring some interesting insights that would be nice to appear at MICCAI. Of course, the authors should take all the feedback into consideration, make the necessary clarifications described in the rebuttal, as these are part of the reason for my acceptance. Minor: HyperGAN might not be the best name, as there are already popular projects named this, which will make it harder for this paper to be found. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 7 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper included good new ideas and provides a good validation. Although implementation is done in 2D, authors discuss in their rebuttal that results on 3D images are promising and show feasibility, and that the algorithm itself would not be limited to 2D given sufficient computational resources. The paper has weaknesses also cited by reviewers, e.g. hard to optimize due to complexity, somewhat experimental validation, and not really be ground breaking, but it may have merit to be discussed with the MICCAI community. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Author Feedback Q1: Clarify the implementation of CycleGAN. A1: As discussed in “Comparison with CycleGAN” on page 8. CycleGAN is for one-to-one contrast translation. For a N-contrast translation task, we trained N*(N-1)/2 CycleGAN [1], each of which is for a specific pair of contrasts. We averaged the results over different contrast pairs as final accuracy of CycleGAN. [1] Zhu, J.Y., et al. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV 2017. Q2: Compare specific contrast translation results. A2: Thanks. Due to space limit, we will try to put comparative results for specific contrast translation tasks in final version. For IXI dataset, our method achieved the highest accuracies for all 6 translation tasks in all metrics. For BraTS dataset, our method worked better than all compared methods for all 12 tasks in all metrics, except MAE score of StarGAN for FLAIR-to-T2 translation (0.0103 of StarGAN vs. 0.0105 of ours). Q3: The method is only applied on 2D slices and might limit the practical use. A3: Thanks. First, in experiments, all the compared methods are trained on 2D slices, and evaluated on 3D volumes. The results in different metrics show that the learned 2D model works well on 3D volumes. In addition, our method is not restricted to 2D and can be naturally extended to 3D in essence. As reviewer pointed out, the major bottleneck is GPU memory instead of methodology. We will discuss this in “Conclusion”, and plan to extend our approach to multiple adjacent slices or 3D patches in future work. Q4: Why not mix two datasets together? A4: In this paper, the two datasets are in different settings, i.e., one is with-skull and another is skull-stripped. As suggested, we additionally conduct experiments of training and testing on the combined dataset with 5 contrasts (T1w, T2w, PDw, T1CE, FLAIR) with both with-skull and skull-stripped brain images, which is more challenging than the separate training/testing setting. The results show that our method still performs best in all metrics (p&lt;.001) and produces 0.0127/29.02/0.902 in MAE/PSNR/SSIM, compared with 0.0142/28.60/0.873 of StarGAN, 0.0168/27.00/0.831 of SUGAN, and 0.0145/28.00/0.879 of ComboGAN. We will try to add these results in paper. Q5: The method needs to know the amount of contrasts before training, and is impossible to translate unseen sequences into the known ones. A5: We propose a unified GAN model that can flexibly translate between different contrast pairs using a unified network. It is much more efficient than the existing methods that use separate translators for different pairs of contrasts, or multiple encoders/decoders, with insufficient adaptiveness to different contrasts. All these methods including ours depend on the given contrast set. It is challenging to generalize to unseen sequences. One possible way to achieve this for our method is as follows. Different sequences can be represented by imaging parameters. We implicitly model the tissue parameters by common feature space, and imaging parameters by one-hot codes in hyper-encoder/hyper-decoder. If one-hot codes of imaging parameters can be generalized to continuous codes covering unseen sequences, our method might be able to generalize to unseen sequences. This will be left for future work. Q6: Compare with SoTA methods, e.g., DGGAN. A6: We additionally compare with DGGAN. The results show that our method works better than DGGAN in all metrics (p&lt;.001) and produces 0.0133/29.64/0.910 and 0.0070/31.90/0.930 in MAE/PSNR/SSIM on two datasets, compared with 0.0175/27.51/0.858 and 0.0081/30.81/0.908 of DGGAN. We will cite and compare DGGAN. Q7: How to optimize the training loss? A7: As discussed in “Training loss” on page 5. We set the weights lambda_{id, rec, cla} to 0.5, 10, 0.2 to make each loss term within a similar range, and trained the model in 100 epochs using an Adam optimizer with a learning rate of 0.0002 and betas of (0.5, 0.999). We will add more details in paper. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0311/12/31/Paper0335" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0311/12/31/Paper0335" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0311-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="A Unified Hyper-GAN Model for Unpaired Multi-contrast MR Image Translation" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0311/12/31/Paper0335"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0311/12/31/Paper0335","headline":"A Unified Hyper-GAN Model for Unpaired Multi-contrast MR Image Translation","dateModified":"0312-01-02T00:00:00-05:17","datePublished":"0311-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Heran Yang, Jian Sun, Liwei Yang, Zongben Xu Abstract Cross-contrast image translation is an important task for completing missing contrasts in clinical diagnosis. However, most existing methods learn separate translator for each pair of contrasts, which is inefficient due to many possible contrast pairs in real scenarios. In this work, we propose a unified Hyper-GAN model for effectively and efficiently translating between different contrast pairs. Hyper-GAN consists of a pair of hyper-encoder and hyper-decoder to first map from the source contrast to a common feature space, and then further map to the target contrast image. To facilitate the translation between different contrast pairs, contrast-modulators are designed to tune the hyper-encoder and hyper-decoder adaptive to different contrasts. We also design a common space loss to enforce that multi-contrast images of a subject share a common feature space, implicitly modeling the shared underlying anatomical structures. Experiments on two datasets of IXI and BraTS 2019 show that our Hyper-GAN achieves state-of-the-art results in both accuracy and efficiency, e.g., improving more than 1.47 and 1.09 dB in PSNR on two datasets with less than half the amount of parameters. Link to paper https://doi.org/10.1007/978-3-030-87199-4_12 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This work develops an unpaired method that enables efficient cross-contrast MR image translation. The authors present a hypernetwork-based approach to enable scaling of the networks filters (or normalization functions) according to the input/output contrast. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is very well written and the presentation is clear. As far as my knowledge goes, applying a hypernetwork-based approach for the synthesis of particular MRI contrasts is a novel and interesting idea, since only one encoder and decoder are necessary, while they learned filters are adapted to the particular contrast type. Good evaluation: comparison to many standard methods, ablation study concerning the losses and FS/CIN, different metrics and a significance test. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main disadvantage of the method is that it is only applied on 2D slices of the brain. I assume this is due to computational restrains, however, the 3D nature of brain MRIs is crucial. In my experience, having such a large amount of different loss-functions is typically hard to optimize. I lack the training details concerning this issue. A general drawback of the method is, that the available amount of contrasts needs to be known prior to training, i.e. four different MRI contrasts. Thus, if new sequences (e.g. from another dataset) need to be translated to the known ones, it would be impossible with this method. The authors implicitly bump into this issue in their work, since the IXI dataset has less different contrasts than the BRATS dataset. This means that a reliable training (for a clinical application) requires a training on all possible contrasts, which is highly unlikely. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance In the form the authors answered most of the questions positively. While they use openly available data, they promise to release the code on GitHub. With the released code, the reproducibility will be very good, but at this stage it would be rather hard to reproduce (i.e. because of optimization issues). Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The general idea of the work is interesting, however not “ground-breaking”. The good evaluation shows the advantages of the presented method, however the lack of 3D application limits the practical use of the method. Also, the needed a-priori knowledge about all possible contrasts, limits the possible applications of the approach. I strongly advise to make the code available as promised by the authors. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Good novel idea and good validation of the method. However, the paper is more proof-of-concept that presenting an applicable approach. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper It intends to solve cross-contrast image translation task by proposing a unified hyper-GAN model. It has two dataset IXI and BraTS2019 for evaluation. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea is interesting. It is well-written and has detailed explanation. It compares the proposed method with several baselines in the literature and also conducted ablation studies. It not only compares the performance but also compares the parameter efficiency of the model. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There are other state-of-the-art models in the literature with code such as Dual Generator Generative Adversarial Networks for Multi-domain Image-to-Image Translation. It would be great to compare with these methods. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The data are public datasets and the author promises to release the code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It would be great to compare with the other sota methods in the literature. I wonder how helpful the generated data is for the segmentation tasks. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It is an interesting paper with evaluation on two public dataset. I think it is on the borderline of acceptance. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper proposed a unified multi-constrast MRI image translation framework based on unsupervised learning. A hyper-GAN model was built which included a hyper-encoder, a hyper-decoder, two contrast-modulators, a classifier, and several discriminators. A composite objective function was also proposed for improving the performance of the proposed network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The authors built a unified multi-contrast MRI image translation network that be used to achieve any possible contrast conversion. Compared with existing multi-contrast MRI image translation method that can only achieve a certain type of contrast conversion, the proposed method worked in a multi-tasking way. The proposed network were trained unsupervisedly, which mitigated the shortcoming of the requirement of paired data. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There are two datasets used in the paper and the authors implemented their method on these two datasets separately. Since the proposed method is a unified multi-contrast method, it is unclear why the authors not choosing to implement their method by mixing the two datasets together. The result comparison with other methods only focused on the overall level. Specific contrast translation result comparison should be added to show the effectiveness of the proposed method. The authors should also point out the percentage of every type of contrast conversion in the training data. It seems that other methods for comparison like cyclegan can only implement a certain type of contrast conversion. Authors should discuss how they achieve other mentioned methods. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance This paper can be easily reproduced since authors will make their code publicly available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The authors should consider adding experiments showing the result of specific types of contrast conversion. Also, the authors are suggested to train the proposed network on the dataset by mixing the two datasets and see if better results can be obtained. Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Some factors like the novelty of the proposed method, experiment design and result analysis. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Reviewers agreed the idea of synthesizing different MR contrasts using a single encoder single decoder model is of interest. Most concerns are about the experimental validation. First, since some methods in comparison, e.g., cycleGAN, could not do arbitrary cross-contrast MR image translation, how their performance were obtained in Table 2 needs further clarification, since this is the major results of this work. Second, as Reviewer#3 pointed out, in addition to comparing the overall performance, comparing the performance of specific contrast translation is also important. Is there any discussion about this? Third, the proposed method is 2D in contrast to the 3D nature of MR images. This paper did not mention or discuss or compare with 3D unpaired medical image synthesis methods, such as 3D Cycle GAN. Could this be discussed during rebuttal since Review#1 concerned this might limit the practical use of the proposed method. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. In the rebuttal, the authors have answered most questions from Reviewer #3 including the training details of the comparison methods like cycleGAN in Table 2, additional comparison results of specific contrast translation, and additional experiments on the mixed dataset. On the other hand, the rebuttal did not address the concerns with no comparison to 3D based synthesis methods, which although does not affect the major claims of this paper. My attitude to this work is neutral. The major concern is still that although arbitrary contrast translation is of certain interest, the methodological contribution of the proposed model seems to be a bit mild. It seems to be a borderline paper After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper presents an interesting take on image translation through an invariant latent space. Overall the other reviewers, MR, and I found aspects/ideas in this paper interesting, and worthy of presentation at MICCAI. While several issues were brought up, I think they were quite well addressed in the rebuttal, such as why 2D vs 3D, how cycleGANs are tested, etc, and I believe the remaining issues brought up by the reviewers are minor. Overall, the idea bring some interesting insights that would be nice to appear at MICCAI. Of course, the authors should take all the feedback into consideration, make the necessary clarifications described in the rebuttal, as these are part of the reason for my acceptance. Minor: HyperGAN might not be the best name, as there are already popular projects named this, which will make it harder for this paper to be found. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 7 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper included good new ideas and provides a good validation. Although implementation is done in 2D, authors discuss in their rebuttal that results on 3D images are promising and show feasibility, and that the algorithm itself would not be limited to 2D given sufficient computational resources. The paper has weaknesses also cited by reviewers, e.g. hard to optimize due to complexity, somewhat experimental validation, and not really be ground breaking, but it may have merit to be discussed with the MICCAI community. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Author Feedback Q1: Clarify the implementation of CycleGAN. A1: As discussed in “Comparison with CycleGAN” on page 8. CycleGAN is for one-to-one contrast translation. For a N-contrast translation task, we trained N*(N-1)/2 CycleGAN [1], each of which is for a specific pair of contrasts. We averaged the results over different contrast pairs as final accuracy of CycleGAN. [1] Zhu, J.Y., et al. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV 2017. Q2: Compare specific contrast translation results. A2: Thanks. Due to space limit, we will try to put comparative results for specific contrast translation tasks in final version. For IXI dataset, our method achieved the highest accuracies for all 6 translation tasks in all metrics. For BraTS dataset, our method worked better than all compared methods for all 12 tasks in all metrics, except MAE score of StarGAN for FLAIR-to-T2 translation (0.0103 of StarGAN vs. 0.0105 of ours). Q3: The method is only applied on 2D slices and might limit the practical use. A3: Thanks. First, in experiments, all the compared methods are trained on 2D slices, and evaluated on 3D volumes. The results in different metrics show that the learned 2D model works well on 3D volumes. In addition, our method is not restricted to 2D and can be naturally extended to 3D in essence. As reviewer pointed out, the major bottleneck is GPU memory instead of methodology. We will discuss this in “Conclusion”, and plan to extend our approach to multiple adjacent slices or 3D patches in future work. Q4: Why not mix two datasets together? A4: In this paper, the two datasets are in different settings, i.e., one is with-skull and another is skull-stripped. As suggested, we additionally conduct experiments of training and testing on the combined dataset with 5 contrasts (T1w, T2w, PDw, T1CE, FLAIR) with both with-skull and skull-stripped brain images, which is more challenging than the separate training/testing setting. The results show that our method still performs best in all metrics (p&lt;.001) and produces 0.0127/29.02/0.902 in MAE/PSNR/SSIM, compared with 0.0142/28.60/0.873 of StarGAN, 0.0168/27.00/0.831 of SUGAN, and 0.0145/28.00/0.879 of ComboGAN. We will try to add these results in paper. Q5: The method needs to know the amount of contrasts before training, and is impossible to translate unseen sequences into the known ones. A5: We propose a unified GAN model that can flexibly translate between different contrast pairs using a unified network. It is much more efficient than the existing methods that use separate translators for different pairs of contrasts, or multiple encoders/decoders, with insufficient adaptiveness to different contrasts. All these methods including ours depend on the given contrast set. It is challenging to generalize to unseen sequences. One possible way to achieve this for our method is as follows. Different sequences can be represented by imaging parameters. We implicitly model the tissue parameters by common feature space, and imaging parameters by one-hot codes in hyper-encoder/hyper-decoder. If one-hot codes of imaging parameters can be generalized to continuous codes covering unseen sequences, our method might be able to generalize to unseen sequences. This will be left for future work. Q6: Compare with SoTA methods, e.g., DGGAN. A6: We additionally compare with DGGAN. The results show that our method works better than DGGAN in all metrics (p&lt;.001) and produces 0.0133/29.64/0.910 and 0.0070/31.90/0.930 in MAE/PSNR/SSIM on two datasets, compared with 0.0175/27.51/0.858 and 0.0081/30.81/0.908 of DGGAN. We will cite and compare DGGAN. Q7: How to optimize the training loss? A7: As discussed in “Training loss” on page 5. We set the weights lambda_{id, rec, cla} to 0.5, 10, 0.2 to make each loss term within a similar range, and trained the model in 100 epochs using an Adam optimizer with a learning rate of 0.0002 and betas of (0.5, 0.999). We will add more details in paper. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Yang, Heran,Sun, Jian,Yang, Liwei,Xu, Zongben" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>A Unified Hyper-GAN Model for Unpaired Multi-contrast MR Image Translation</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Machine Learning - Domain adaptation"
        class="post-category">
        Machine Learning - Domain adaptation
      </a>
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Yang, Heran"
        class="post-tags">
        Yang, Heran
      </a> |  
      
      <a href="kittywong/tags#Sun, Jian"
        class="post-tags">
        Sun, Jian
      </a> |  
      
      <a href="kittywong/tags#Yang, Liwei"
        class="post-tags">
        Yang, Liwei
      </a> |  
      
      <a href="kittywong/tags#Xu, Zongben"
        class="post-tags">
        Xu, Zongben
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Heran Yang, Jian Sun, Liwei Yang, Zongben Xu
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Cross-contrast image translation is an important task for completing missing contrasts in clinical diagnosis. However, most existing methods learn separate translator for each pair of contrasts, which is inefficient due to many possible contrast pairs in real scenarios. In this work, we propose a unified Hyper-GAN model for effectively and efficiently translating between different contrast pairs. Hyper-GAN consists of a pair of hyper-encoder and hyper-decoder to first map from the source contrast to a common feature space, and then further map to the target contrast image. To facilitate the translation between different contrast pairs, contrast-modulators are designed to tune the hyper-encoder and hyper-decoder adaptive to different contrasts. We also design a common space loss to enforce that multi-contrast images of a subject share a common feature space, implicitly modeling the shared underlying anatomical structures. Experiments on two datasets of IXI and BraTS 2019 show that our Hyper-GAN achieves state-of-the-art results in both accuracy and efficiency, e.g., improving more than 1.47 and 1.09 dB in PSNR on two datasets with less than half the amount of parameters.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87199-4_12">https://doi.org/10.1007/978-3-030-87199-4_12</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This work develops an unpaired method that enables efficient cross-contrast MR image translation. The authors present a hypernetwork-based approach to enable scaling of the networks filters (or normalization functions) according to the input/output contrast.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>The paper is very well written and the presentation is clear.</li>
        <li>As far as my knowledge goes, applying a hypernetwork-based approach for the synthesis of particular MRI contrasts is a novel and interesting idea, since only one encoder and decoder are necessary, while they learned filters are adapted to the particular contrast type.</li>
        <li>Good evaluation: comparison to many standard methods, ablation study concerning the losses and FS/CIN, different metrics and a significance test.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>The main disadvantage of the method is that it is only applied on 2D slices of the brain. I assume this is due to computational restrains, however, the 3D nature of brain MRIs is crucial.</li>
        <li>In my experience, having such a large amount of different loss-functions is typically hard to optimize. I lack the training details concerning this issue.</li>
        <li>A general drawback of the method is, that the available amount of  contrasts needs to be known prior to training, i.e. four different MRI contrasts. Thus, if new sequences (e.g. from another dataset) need to be translated to the known ones, it would be impossible with this method. The authors implicitly bump into this issue in their work,  since the IXI dataset has less different contrasts than the BRATS dataset. This means that a reliable training (for a clinical application) requires a training on all possible contrasts, which is highly unlikely.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <ul>
        <li>In the form the authors answered most of the questions positively. While they use openly available data, they promise to release the code on GitHub. With the released code, the reproducibility will be very good, but at this stage it would be rather hard to reproduce (i.e. because of optimization issues).</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>The general idea of the work is interesting, however not “ground-breaking”. The good evaluation shows the advantages of the presented method, however the lack of 3D application limits the practical use of the method. Also, the needed a-priori knowledge about all possible contrasts, limits the possible applications of the approach. I strongly advise to make the code available as promised by the authors.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Good novel idea and good validation of the method. However, the paper is more proof-of-concept that presenting an applicable approach.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>It intends to solve cross-contrast image translation task by proposing a unified hyper-GAN model. It has two dataset IXI and BraTS2019 for evaluation.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The idea is interesting. It is well-written and has detailed explanation. It compares the proposed method with several baselines in the literature and also conducted ablation studies. It not only compares the performance but also compares the parameter efficiency of the model.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>There are other state-of-the-art models in the literature with code such as Dual Generator Generative Adversarial Networks for Multi-domain Image-to-Image Translation. It would be great to compare with these methods.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The data are public datasets and the author promises to release the code.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>It would be great to compare with the other sota methods in the literature. 
I wonder how helpful the generated data is for the segmentation tasks.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>It is an interesting paper with evaluation on two public dataset. I think it is on the borderline of acceptance.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper proposed a unified multi-constrast MRI image translation framework based on unsupervised learning. A hyper-GAN model was built which included a hyper-encoder, a hyper-decoder, two contrast-modulators, a classifier, and several discriminators. A composite objective function was also proposed for improving the performance of the proposed network.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>The authors built a unified multi-contrast MRI image translation network that be used to achieve any possible contrast conversion. Compared with existing multi-contrast MRI image translation method that can only achieve a certain type of contrast conversion, the proposed method worked in a multi-tasking way.</li>
        <li>The proposed network were trained unsupervisedly, which mitigated the shortcoming of the requirement of paired data.</li>
      </ol>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>There are two datasets used in the paper and the authors implemented their method on these two datasets separately. Since the proposed method is a unified multi-contrast method, it is unclear why the authors not choosing to implement their method by mixing the two datasets together.</li>
        <li>The result comparison with other methods only focused on the overall level. Specific contrast translation result comparison should be added to show the effectiveness of the proposed method. The authors should also point out the percentage of every type of contrast conversion in the training data.</li>
        <li>It seems that other methods for comparison like cyclegan can only implement a certain type of contrast conversion. Authors should discuss how they achieve other mentioned methods.</li>
      </ol>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>This paper can be easily reproduced since authors will make their code publicly available.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>The authors should consider adding experiments showing the result of specific types of contrast conversion. Also, the authors are suggested to train the proposed network on the dataset by mixing the two datasets and see if better results can be obtained.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>probably reject (4)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Some factors like the novelty of the proposed method, experiment design and result analysis.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>Reviewers agreed the idea of synthesizing different MR contrasts using a single encoder single decoder model is of interest. Most concerns are about the experimental validation. First, since some methods in comparison, e.g., cycleGAN, could not do arbitrary cross-contrast MR image translation, how their performance were obtained in Table 2 needs further clarification, since this is the major results of this work. Second, as Reviewer#3 pointed out, in addition to comparing the overall performance, comparing the performance of specific contrast translation is also important. Is there any discussion about this? Third, the proposed method is 2D in contrast to the 3D nature of MR images. This paper did not mention or discuss or compare with 3D unpaired medical image synthesis methods, such as 3D Cycle GAN. Could this be discussed during rebuttal since Review#1 concerned this might limit the practical use of the proposed method.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>In the rebuttal, the authors have answered most questions from Reviewer #3 including the training details of the comparison methods like cycleGAN in Table 2, additional comparison results of specific contrast translation, and additional experiments on the mixed dataset. On the other hand, the rebuttal did not address the concerns with no comparison to 3D based synthesis methods, which although does not affect the major claims of this paper. My attitude to this work is neutral. The major concern is still that although arbitrary contrast translation is of certain interest, the methodological contribution of the proposed model seems to be a bit mild. It seems to be a borderline paper</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>11</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="meta-review-2">Meta-Review #2</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The paper presents an interesting take on image translation through an invariant latent space.</p>

      <p>Overall the other reviewers, MR, and I found aspects/ideas in this paper interesting, and worthy of presentation at MICCAI. While several issues were brought up, I think they were quite well addressed in the rebuttal, such as why 2D vs 3D, how cycleGANs are tested, etc, and I believe the remaining issues brought up by the reviewers are minor. Overall, the idea bring some interesting insights that would be nice to appear at MICCAI. Of course, the authors should take all the feedback into consideration, make the necessary clarifications described in the rebuttal, as these are part of the reason for my acceptance.</p>

      <p>Minor: HyperGAN might not be the best name, as there are already popular projects named this, which will make it harder for this paper to be found.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>7</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>
<h2 id="meta-review-3">Meta-Review #3</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>This paper included good new ideas and provides a good validation. Although implementation is done in 2D, authors discuss in their rebuttal that results on 3D images are promising and show feasibility, and that the algorithm itself would not be limited to 2D given sufficient computational resources. The paper has weaknesses also cited by reviewers, e.g. hard to optimize due to complexity, somewhat experimental validation, and not really be ground breaking, but it may have merit to be discussed with the MICCAI community.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>8</p>
    </blockquote>
  </li>
</ul>

<h2><br /><br /></h2>
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>Q1: Clarify the implementation of CycleGAN.
A1: As discussed in “Comparison with CycleGAN” on page 8. CycleGAN is for one-to-one contrast translation. For a N-contrast translation task, we trained N*(N-1)/2 CycleGAN [1], each of which is for a specific pair of contrasts. We averaged the results over different contrast pairs as final accuracy of CycleGAN.
[1] Zhu, J.Y., et al. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV 2017.</p>

  <p>Q2: Compare specific contrast translation results.
A2: Thanks. Due to space limit, we will try to put comparative results for specific contrast translation tasks in final version. For IXI dataset, our method achieved the highest accuracies for all 6 translation tasks in all metrics. For BraTS dataset, our method worked better than all compared methods for all 12 tasks in all metrics, except MAE score of StarGAN for FLAIR-to-T2 translation (0.0103 of StarGAN vs. 0.0105 of ours).</p>

  <p>Q3: The method is only applied on 2D slices and might limit the practical use.
A3: Thanks. First, in experiments, all the compared methods are trained on 2D slices, and evaluated on 3D volumes. The results in different metrics show that the learned 2D model works well on 3D volumes. In addition, our method is not restricted to 2D and can be naturally extended to 3D in essence. As reviewer pointed out, the major bottleneck is GPU memory instead of methodology. We will discuss this in “Conclusion”, and plan to extend our approach to multiple adjacent slices or 3D patches in future work.</p>

  <p>Q4: Why not mix two datasets together?
A4: In this paper, the two datasets are in different settings, i.e., one is with-skull and another is skull-stripped. As suggested, we additionally conduct experiments of training and testing on the combined dataset with 5 contrasts (T1w, T2w, PDw, T1CE, FLAIR) with both with-skull and skull-stripped brain images, which is more challenging than the separate training/testing setting. The results show that our method still performs best in all metrics (p&lt;.001) and produces 0.0127/29.02/0.902 in MAE/PSNR/SSIM, compared with 0.0142/28.60/0.873 of StarGAN, 0.0168/27.00/0.831 of SUGAN, and 0.0145/28.00/0.879 of ComboGAN. We will try to add these results in paper.</p>

  <p>Q5: The method needs to know the amount of contrasts before training, and is impossible to translate unseen sequences into the known ones.
A5: We propose a unified GAN model that can flexibly translate between different contrast pairs using a unified network. It is much more efficient than the existing methods that use separate translators for different pairs of contrasts, or multiple encoders/decoders, with insufficient adaptiveness to different contrasts. All these methods including ours depend on the given contrast set.
It is challenging to generalize to unseen sequences. One possible way to achieve this for our method is as follows. Different sequences can be represented by imaging parameters. We implicitly model the tissue parameters by common feature space, and imaging parameters by one-hot codes in hyper-encoder/hyper-decoder. If one-hot codes of imaging parameters can be generalized to continuous codes covering unseen sequences, our method might be able to generalize to unseen sequences. This will be left for future work.</p>

  <p>Q6: Compare with SoTA methods, e.g., DGGAN.
A6: We additionally compare with DGGAN. The results show that our method works better than DGGAN in all metrics (p&lt;.001) and produces 0.0133/29.64/0.910 and 0.0070/31.90/0.930 in MAE/PSNR/SSIM on two datasets, compared with 0.0175/27.51/0.858 and 0.0081/30.81/0.908 of DGGAN. We will cite and compare DGGAN.</p>

  <p>Q7: How to optimize the training loss?
A7: As discussed in “Training loss” on page 5. We set the weights lambda_{id, rec, cla} to 0.5, 10, 0.2 to make each loss term within a similar range, and trained the model in 100 epochs using an Adam optimizer with a learning rate of 0.0002 and betas of (0.5, 0.999). We will add more details in paper.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0311-12-31
      -->
      <!--
      
        ,
        updated at 
        0312-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Machine Learning - Domain adaptation"
        class="post-category">
        Machine Learning - Domain adaptation
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Yang, Heran"
        class="post-category">
        Yang, Heran
      </a> |  
      
      <a href="kittywong/tags#Sun, Jian"
        class="post-category">
        Sun, Jian
      </a> |  
      
      <a href="kittywong/tags#Yang, Liwei"
        class="post-category">
        Yang, Liwei
      </a> |  
      
      <a href="kittywong/tags#Xu, Zongben"
        class="post-category">
        Xu, Zongben
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0312/12/31/Paper0500">
          Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine MRI Synthesis
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0310/12/31/Paper0297">
          Stain Mix-up: Unsupervised Domain Generalization for Histopathology Images
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
