<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>TransPath: Transformer-based Self-supervised Learning for Histopathological Image Classification | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="TransPath: Transformer-based Self-supervised Learning for Histopathological Image Classification" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Xiyue Wang, Sen Yang, Jun Zhang, Minghui Wang, Jing Zhang, Junzhou Huang, Wei Yang, Xiao Han Abstract A large-scale labeled dataset is a key factor for the success of supervised deep learning in histopathological image analysis. However, exhaustive annotation requires a careful visual inspection by pathologists, which is extremely time-consuming and labor-intensive. Self-supervised learning (SSL) can alleviate this issue by pre-training models under the supervision of data itself, which generalizes well to various downstream tasks with limited annotations. In this work, we propose a hybrid model (TransPath) which is pre-trained in an SSL manner on massively unlabeled histopathological images to discover the inherent image property and capture domain-specific feature embedding. The TransPath can serve as a collaborative local-global feature extractor, which is designed by combining a convolutional neural network (CNN) and a modified transformer architecture. We propose a token-aggregating and excitation (TAE) module which is placed behind the self-attention of the transformer encoder for capturing more global information. We evaluate the performance of pre-trained TransPath by fine-tuning it on three downstream histopathological image classification tasks. Our experimental results indicate that TransPath outperforms state-of-the-art vision transformer networks, and the visual representations generated by SSL on domain-relevant histopathological images are more transferable than the supervised baseline on ImageNet. Our code and pre-trained models will be available at https://github.com/Xiyue-Wang/TransPath. Link to paper https://doi.org/10.1007/978-3-030-87237-3_18 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors present a self-supervised learning approach for pretraining histopathology images. To this aim, a CNN-transformer architecture is designed for making use of local and global receptive fields to extract discriminating features. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Leveraging large-scale histopathology image for pretraining is well-motivated Using less annotated data for machine learning is an important topic in medical image analysis The results demonstrate the effectiveness of the proposed results Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The detailed evaluation process is not described. Lack of novelty. Lacks comparison with existing pertaining methods. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance As the data and code will be open sourced, this work is reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The detailed evaluation process is not described. The aim of pretraining is to learn better representations so that less annotated data is required. A more detailed experiment is required to show how much annotated data is needed for down-streaming tasks after pertaining. Lack of novelty. Much of this paper is just a combination of existing approaches from computer vision. Lacks comparison with existing pertaining methods. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper address the important topic of learning better representations with less annotated data. However, it lacks novelty and more detailed experiments are required. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper presents a hybrid framework by combining self-supervised pretrained CNN with a modified transformer architecture for histology image classification. The proposed method was evaluated on multiple datasets and demonstrated with improvements over CNN and combined CNN with multi-head self-attention based transformer methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. – Application of transformers is relatively new to the histopathology domain. – The method was tested on multiple datasets. – It shows a good improvement over CNN and CNN+Trans methods with a big margin. – Pre-trained on a very large dataset, which is interesting and also challenging. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. – The novelty of this submission is limited. It’s an application of the existing works BYOL and transformers. – The paper didn’t compare with the state-of-the-art work in self-supervised learning, i.e., SimCLR, MoCo. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I carefully assessed the sensibility of the experiments, and the results seem convincing. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html – I am curious to know the performance of the method with fine-tuning on 1%, 10%, and 50% labeled data on the downstream task. Since the method has been pretrained on a very large database, it would be really interesting to know how the method performs under limited label settings? – In Table 1, it would be interesting to know the performance of CNN+Trans+SSL, without the TAE module; in this way, we could see whether the TAE module or pre-training on a larger dataset has lead to improved performance gains? – Most of the contrastive learning methods (such as SimCLR and BYOL (ref 2, 5)) are highly sensitive to data augmentations. I am curious to know why did the authors use similar data augmentation strategies as SimCLR, which are tailored for natural images. There are well-established augmentation methods [1] that have been shown to reduce variation in domain shift and improve out-of-distribution (OOD) generalization, which could be future work in this direction. [1] Tellez, David, et al. “Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology.” Medical image analysis 58 (2019): 101544. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The authors present an exciting application of transformers to the histopathology domain. Furthermore, through extensive validation on large datasets, they showed substantial improvements in image classification benchmarks. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The paper adopts a self-supervised deep learning strategy with transformers and adding one module called token-aggregating and excitation (TAE) before self-attention to capture both local and global patterns of pathology slides. The authors trained the network with TCGA data applied results on three datasets and the results were promising. The huge computational resources (3.2k V100 hour) is the backbone of this work. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Training a model in a well-known and large public datasets and showing the strangeness of the work by applying it on 3 public datasets. Adding TAE on the transformer to better global feature extraction, This simple mechanism improved the network’s accuracy considerably. Conducting an ablation study to show the power of the architecture design. This study helps the reader to understand the role of each component. Comparing with state-of-the-art models, and showing better results on three public datasets. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. TCGA consists of 22k frozen tissue and 11k FFPE tissues (diagnosis). Although both frozen and FFPE from one case (i.e. LUAC) but the pattern is totally different. Maybe removing frozen be a better idea. I anticipate seeing the state-of-the-art numbers for each dataset (even if they were better with specialized methods). This is acceptable that a method with generalized power provides slightly lower accuracy. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance They stated that the codes will be shared. It means even if the work missing reporting some parameters, which is the case, readers could find them in the shared codes. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html “The cropped histopathological image patches are usually large to capture both the cell-level structure and and tissue-level context”, large is not a proper word. Many study are working with 64x64 which is small to .. “total of 32,529 WSIs from the cancer genome atlas (TCGA)” separate frozen tissue from PFFE BN in figure 1 is not intrucuced in the text. Conclution could be expanded a little more to provide more details on the methods. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is strong. Codes and results would be helpful for other researchers in the comunity. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. All reviewers acknowledge that the paper addresses an interesting topic in medical image analysis and the benefits of the proposed method. While the technical novelty of this paper is still limited, the application of Transformer coupled with self-supervised learning for histopathological image classification is new and the results clearly show the effectiveness of the proposed method. Therefore, the paper can be interesting for MICCAI community. Please address the all the comments raised by all reviewers to further improve the paper. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We would like to thank the reviewers for their time spent on reviewing our manuscript and the overall very positive ratings. We also appreciate the insightful comments from all the reviewers, which are very helpful to further improve our manuscript and to plan our future work. Our responses to the major comments are provided below. Citations are numbered in the same order as in the final manuscript. General Responses (R2/R3) The technical novelty of this paper is still limited. We would like to point out that our model is not just a simple application of the existing works. Although our main framework is a combination of CNN, transformer, and BYOL, we have added a customized TAE module to extract global information. The designed TAE mechanism, even though simple, has improved the network performance by a large margin (cf. ablation study). Moreover, as summarized in our contributions, our model has been pre-trained on very large public datasets (approximately 2.7 million images with the size of 2048×2048 pixels), evaluated on other three public datasets, and has achieved superior performance compared with existing vision transformer networks (cf. Fig.2). Our pre-trained and to-be-released TransPath model learns histopathology-specific feature representations from a large set of histopathological images, which has the potential to be transferred to any histopathological image analysis tasks, which we also deem to be an impactful contribution. (R2/R3) Lacks comparison with existing pretraining methods. We have tried using MoCo as the pre-training method, which produced similar results as the BYOL. We did not discuss it in the manuscript due to space limitation. We will include a more thorough comparison in our extended paper in the future. (R2/R3) Performance of the method with finetuning on different numbers of labeled data. Self-supervised pre-training with a large amount of data makes it possible to use fewer annotations to achieve good performance in downstream tasks [2]. In our experiments, the MHIST dataset is about 1/30 of the other two datasets, and using the pre-trained model provided a higher performance improvement for MHIST than the other two (cf. Table 1 and Table 3). These results indirectly demonstrated the effects as asked by the reviewers. We will add more experimental results in the extended version of our manuscript in the future. Reviewer#3  See whether the TAE module or pre-training leads to performance gains? There might be a misunderstanding. The performance gains offered by TAE can be seen by comparing the results of CNN+Trans with those of CNN+Trans+TAE. The extra benefits of pre-training can be verified by comparing the results of CNN+Trans+TAE and CNN+Trans+TAE+SSL (cf. Table 1). Data augmentation strategies. Although the applied data augmentation strategies (random crop, Gaussian blur, and color distortion) were originally designed for natural images, they are also applicable to pathological images. In future work, we will include comparisons with other data augmentation methods tailored for histopathological images, like proposed in the literature [17]. Reviewer#4 Separate frozen tissue from FFPE. We agree that this strategy might improve the performance. On the other hand, the deep learning model has a sufficient number of parameters. In addition, both frozen and FFPE slides are histopathology images, which may help train a more robust model. We will compare the performance of separating frozen tissue slides from FFPE ones in our future work. The Conclusion could be expanded. We have expanded the Conclusion section by including a little more details on the methods and our future work. Minor error. We appreciate the careful review and the suggestions. We have changed the “large” to “enough” and added the BN in the title of Fig.1. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Xiyue Wang, Sen Yang, Jun Zhang, Minghui Wang, Jing Zhang, Junzhou Huang, Wei Yang, Xiao Han Abstract A large-scale labeled dataset is a key factor for the success of supervised deep learning in histopathological image analysis. However, exhaustive annotation requires a careful visual inspection by pathologists, which is extremely time-consuming and labor-intensive. Self-supervised learning (SSL) can alleviate this issue by pre-training models under the supervision of data itself, which generalizes well to various downstream tasks with limited annotations. In this work, we propose a hybrid model (TransPath) which is pre-trained in an SSL manner on massively unlabeled histopathological images to discover the inherent image property and capture domain-specific feature embedding. The TransPath can serve as a collaborative local-global feature extractor, which is designed by combining a convolutional neural network (CNN) and a modified transformer architecture. We propose a token-aggregating and excitation (TAE) module which is placed behind the self-attention of the transformer encoder for capturing more global information. We evaluate the performance of pre-trained TransPath by fine-tuning it on three downstream histopathological image classification tasks. Our experimental results indicate that TransPath outperforms state-of-the-art vision transformer networks, and the visual representations generated by SSL on domain-relevant histopathological images are more transferable than the supervised baseline on ImageNet. Our code and pre-trained models will be available at https://github.com/Xiyue-Wang/TransPath. Link to paper https://doi.org/10.1007/978-3-030-87237-3_18 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors present a self-supervised learning approach for pretraining histopathology images. To this aim, a CNN-transformer architecture is designed for making use of local and global receptive fields to extract discriminating features. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Leveraging large-scale histopathology image for pretraining is well-motivated Using less annotated data for machine learning is an important topic in medical image analysis The results demonstrate the effectiveness of the proposed results Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The detailed evaluation process is not described. Lack of novelty. Lacks comparison with existing pertaining methods. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance As the data and code will be open sourced, this work is reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The detailed evaluation process is not described. The aim of pretraining is to learn better representations so that less annotated data is required. A more detailed experiment is required to show how much annotated data is needed for down-streaming tasks after pertaining. Lack of novelty. Much of this paper is just a combination of existing approaches from computer vision. Lacks comparison with existing pertaining methods. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper address the important topic of learning better representations with less annotated data. However, it lacks novelty and more detailed experiments are required. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper presents a hybrid framework by combining self-supervised pretrained CNN with a modified transformer architecture for histology image classification. The proposed method was evaluated on multiple datasets and demonstrated with improvements over CNN and combined CNN with multi-head self-attention based transformer methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. – Application of transformers is relatively new to the histopathology domain. – The method was tested on multiple datasets. – It shows a good improvement over CNN and CNN+Trans methods with a big margin. – Pre-trained on a very large dataset, which is interesting and also challenging. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. – The novelty of this submission is limited. It’s an application of the existing works BYOL and transformers. – The paper didn’t compare with the state-of-the-art work in self-supervised learning, i.e., SimCLR, MoCo. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I carefully assessed the sensibility of the experiments, and the results seem convincing. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html – I am curious to know the performance of the method with fine-tuning on 1%, 10%, and 50% labeled data on the downstream task. Since the method has been pretrained on a very large database, it would be really interesting to know how the method performs under limited label settings? – In Table 1, it would be interesting to know the performance of CNN+Trans+SSL, without the TAE module; in this way, we could see whether the TAE module or pre-training on a larger dataset has lead to improved performance gains? – Most of the contrastive learning methods (such as SimCLR and BYOL (ref 2, 5)) are highly sensitive to data augmentations. I am curious to know why did the authors use similar data augmentation strategies as SimCLR, which are tailored for natural images. There are well-established augmentation methods [1] that have been shown to reduce variation in domain shift and improve out-of-distribution (OOD) generalization, which could be future work in this direction. [1] Tellez, David, et al. “Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology.” Medical image analysis 58 (2019): 101544. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The authors present an exciting application of transformers to the histopathology domain. Furthermore, through extensive validation on large datasets, they showed substantial improvements in image classification benchmarks. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The paper adopts a self-supervised deep learning strategy with transformers and adding one module called token-aggregating and excitation (TAE) before self-attention to capture both local and global patterns of pathology slides. The authors trained the network with TCGA data applied results on three datasets and the results were promising. The huge computational resources (3.2k V100 hour) is the backbone of this work. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Training a model in a well-known and large public datasets and showing the strangeness of the work by applying it on 3 public datasets. Adding TAE on the transformer to better global feature extraction, This simple mechanism improved the network’s accuracy considerably. Conducting an ablation study to show the power of the architecture design. This study helps the reader to understand the role of each component. Comparing with state-of-the-art models, and showing better results on three public datasets. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. TCGA consists of 22k frozen tissue and 11k FFPE tissues (diagnosis). Although both frozen and FFPE from one case (i.e. LUAC) but the pattern is totally different. Maybe removing frozen be a better idea. I anticipate seeing the state-of-the-art numbers for each dataset (even if they were better with specialized methods). This is acceptable that a method with generalized power provides slightly lower accuracy. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance They stated that the codes will be shared. It means even if the work missing reporting some parameters, which is the case, readers could find them in the shared codes. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html “The cropped histopathological image patches are usually large to capture both the cell-level structure and and tissue-level context”, large is not a proper word. Many study are working with 64x64 which is small to .. “total of 32,529 WSIs from the cancer genome atlas (TCGA)” separate frozen tissue from PFFE BN in figure 1 is not intrucuced in the text. Conclution could be expanded a little more to provide more details on the methods. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is strong. Codes and results would be helpful for other researchers in the comunity. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. All reviewers acknowledge that the paper addresses an interesting topic in medical image analysis and the benefits of the proposed method. While the technical novelty of this paper is still limited, the application of Transformer coupled with self-supervised learning for histopathological image classification is new and the results clearly show the effectiveness of the proposed method. Therefore, the paper can be interesting for MICCAI community. Please address the all the comments raised by all reviewers to further improve the paper. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We would like to thank the reviewers for their time spent on reviewing our manuscript and the overall very positive ratings. We also appreciate the insightful comments from all the reviewers, which are very helpful to further improve our manuscript and to plan our future work. Our responses to the major comments are provided below. Citations are numbered in the same order as in the final manuscript. General Responses (R2/R3) The technical novelty of this paper is still limited. We would like to point out that our model is not just a simple application of the existing works. Although our main framework is a combination of CNN, transformer, and BYOL, we have added a customized TAE module to extract global information. The designed TAE mechanism, even though simple, has improved the network performance by a large margin (cf. ablation study). Moreover, as summarized in our contributions, our model has been pre-trained on very large public datasets (approximately 2.7 million images with the size of 2048×2048 pixels), evaluated on other three public datasets, and has achieved superior performance compared with existing vision transformer networks (cf. Fig.2). Our pre-trained and to-be-released TransPath model learns histopathology-specific feature representations from a large set of histopathological images, which has the potential to be transferred to any histopathological image analysis tasks, which we also deem to be an impactful contribution. (R2/R3) Lacks comparison with existing pretraining methods. We have tried using MoCo as the pre-training method, which produced similar results as the BYOL. We did not discuss it in the manuscript due to space limitation. We will include a more thorough comparison in our extended paper in the future. (R2/R3) Performance of the method with finetuning on different numbers of labeled data. Self-supervised pre-training with a large amount of data makes it possible to use fewer annotations to achieve good performance in downstream tasks [2]. In our experiments, the MHIST dataset is about 1/30 of the other two datasets, and using the pre-trained model provided a higher performance improvement for MHIST than the other two (cf. Table 1 and Table 3). These results indirectly demonstrated the effects as asked by the reviewers. We will add more experimental results in the extended version of our manuscript in the future. Reviewer#3  See whether the TAE module or pre-training leads to performance gains? There might be a misunderstanding. The performance gains offered by TAE can be seen by comparing the results of CNN+Trans with those of CNN+Trans+TAE. The extra benefits of pre-training can be verified by comparing the results of CNN+Trans+TAE and CNN+Trans+TAE+SSL (cf. Table 1). Data augmentation strategies. Although the applied data augmentation strategies (random crop, Gaussian blur, and color distortion) were originally designed for natural images, they are also applicable to pathological images. In future work, we will include comparisons with other data augmentation methods tailored for histopathological images, like proposed in the literature [17]. Reviewer#4 Separate frozen tissue from FFPE. We agree that this strategy might improve the performance. On the other hand, the deep learning model has a sufficient number of parameters. In addition, both frozen and FFPE slides are histopathology images, which may help train a more robust model. We will compare the performance of separating frozen tissue slides from FFPE ones in our future work. The Conclusion could be expanded. We have expanded the Conclusion section by including a little more details on the methods and our future work. Minor error. We appreciate the careful review and the suggestions. We have changed the “large” to “enough” and added the BN in the title of Fig.1. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0817/12/31/Paper0503" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0817/12/31/Paper0503" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0817-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="TransPath: Transformer-based Self-supervised Learning for Histopathological Image Classification" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0817/12/31/Paper0503"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0817/12/31/Paper0503","headline":"TransPath: Transformer-based Self-supervised Learning for Histopathological Image Classification","dateModified":"0818-01-05T00:00:00-05:17","datePublished":"0817-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Xiyue Wang, Sen Yang, Jun Zhang, Minghui Wang, Jing Zhang, Junzhou Huang, Wei Yang, Xiao Han Abstract A large-scale labeled dataset is a key factor for the success of supervised deep learning in histopathological image analysis. However, exhaustive annotation requires a careful visual inspection by pathologists, which is extremely time-consuming and labor-intensive. Self-supervised learning (SSL) can alleviate this issue by pre-training models under the supervision of data itself, which generalizes well to various downstream tasks with limited annotations. In this work, we propose a hybrid model (TransPath) which is pre-trained in an SSL manner on massively unlabeled histopathological images to discover the inherent image property and capture domain-specific feature embedding. The TransPath can serve as a collaborative local-global feature extractor, which is designed by combining a convolutional neural network (CNN) and a modified transformer architecture. We propose a token-aggregating and excitation (TAE) module which is placed behind the self-attention of the transformer encoder for capturing more global information. We evaluate the performance of pre-trained TransPath by fine-tuning it on three downstream histopathological image classification tasks. Our experimental results indicate that TransPath outperforms state-of-the-art vision transformer networks, and the visual representations generated by SSL on domain-relevant histopathological images are more transferable than the supervised baseline on ImageNet. Our code and pre-trained models will be available at https://github.com/Xiyue-Wang/TransPath. Link to paper https://doi.org/10.1007/978-3-030-87237-3_18 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors present a self-supervised learning approach for pretraining histopathology images. To this aim, a CNN-transformer architecture is designed for making use of local and global receptive fields to extract discriminating features. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Leveraging large-scale histopathology image for pretraining is well-motivated Using less annotated data for machine learning is an important topic in medical image analysis The results demonstrate the effectiveness of the proposed results Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The detailed evaluation process is not described. Lack of novelty. Lacks comparison with existing pertaining methods. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance As the data and code will be open sourced, this work is reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The detailed evaluation process is not described. The aim of pretraining is to learn better representations so that less annotated data is required. A more detailed experiment is required to show how much annotated data is needed for down-streaming tasks after pertaining. Lack of novelty. Much of this paper is just a combination of existing approaches from computer vision. Lacks comparison with existing pertaining methods. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper address the important topic of learning better representations with less annotated data. However, it lacks novelty and more detailed experiments are required. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper presents a hybrid framework by combining self-supervised pretrained CNN with a modified transformer architecture for histology image classification. The proposed method was evaluated on multiple datasets and demonstrated with improvements over CNN and combined CNN with multi-head self-attention based transformer methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. – Application of transformers is relatively new to the histopathology domain. – The method was tested on multiple datasets. – It shows a good improvement over CNN and CNN+Trans methods with a big margin. – Pre-trained on a very large dataset, which is interesting and also challenging. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. – The novelty of this submission is limited. It’s an application of the existing works BYOL and transformers. – The paper didn’t compare with the state-of-the-art work in self-supervised learning, i.e., SimCLR, MoCo. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I carefully assessed the sensibility of the experiments, and the results seem convincing. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html – I am curious to know the performance of the method with fine-tuning on 1%, 10%, and 50% labeled data on the downstream task. Since the method has been pretrained on a very large database, it would be really interesting to know how the method performs under limited label settings? – In Table 1, it would be interesting to know the performance of CNN+Trans+SSL, without the TAE module; in this way, we could see whether the TAE module or pre-training on a larger dataset has lead to improved performance gains? – Most of the contrastive learning methods (such as SimCLR and BYOL (ref 2, 5)) are highly sensitive to data augmentations. I am curious to know why did the authors use similar data augmentation strategies as SimCLR, which are tailored for natural images. There are well-established augmentation methods [1] that have been shown to reduce variation in domain shift and improve out-of-distribution (OOD) generalization, which could be future work in this direction. [1] Tellez, David, et al. “Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology.” Medical image analysis 58 (2019): 101544. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The authors present an exciting application of transformers to the histopathology domain. Furthermore, through extensive validation on large datasets, they showed substantial improvements in image classification benchmarks. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The paper adopts a self-supervised deep learning strategy with transformers and adding one module called token-aggregating and excitation (TAE) before self-attention to capture both local and global patterns of pathology slides. The authors trained the network with TCGA data applied results on three datasets and the results were promising. The huge computational resources (3.2k V100 hour) is the backbone of this work. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Training a model in a well-known and large public datasets and showing the strangeness of the work by applying it on 3 public datasets. Adding TAE on the transformer to better global feature extraction, This simple mechanism improved the network’s accuracy considerably. Conducting an ablation study to show the power of the architecture design. This study helps the reader to understand the role of each component. Comparing with state-of-the-art models, and showing better results on three public datasets. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. TCGA consists of 22k frozen tissue and 11k FFPE tissues (diagnosis). Although both frozen and FFPE from one case (i.e. LUAC) but the pattern is totally different. Maybe removing frozen be a better idea. I anticipate seeing the state-of-the-art numbers for each dataset (even if they were better with specialized methods). This is acceptable that a method with generalized power provides slightly lower accuracy. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance They stated that the codes will be shared. It means even if the work missing reporting some parameters, which is the case, readers could find them in the shared codes. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html “The cropped histopathological image patches are usually large to capture both the cell-level structure and and tissue-level context”, large is not a proper word. Many study are working with 64x64 which is small to .. “total of 32,529 WSIs from the cancer genome atlas (TCGA)” separate frozen tissue from PFFE BN in figure 1 is not intrucuced in the text. Conclution could be expanded a little more to provide more details on the methods. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is strong. Codes and results would be helpful for other researchers in the comunity. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. All reviewers acknowledge that the paper addresses an interesting topic in medical image analysis and the benefits of the proposed method. While the technical novelty of this paper is still limited, the application of Transformer coupled with self-supervised learning for histopathological image classification is new and the results clearly show the effectiveness of the proposed method. Therefore, the paper can be interesting for MICCAI community. Please address the all the comments raised by all reviewers to further improve the paper. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We would like to thank the reviewers for their time spent on reviewing our manuscript and the overall very positive ratings. We also appreciate the insightful comments from all the reviewers, which are very helpful to further improve our manuscript and to plan our future work. Our responses to the major comments are provided below. Citations are numbered in the same order as in the final manuscript. General Responses (R2/R3) The technical novelty of this paper is still limited. We would like to point out that our model is not just a simple application of the existing works. Although our main framework is a combination of CNN, transformer, and BYOL, we have added a customized TAE module to extract global information. The designed TAE mechanism, even though simple, has improved the network performance by a large margin (cf. ablation study). Moreover, as summarized in our contributions, our model has been pre-trained on very large public datasets (approximately 2.7 million images with the size of 2048×2048 pixels), evaluated on other three public datasets, and has achieved superior performance compared with existing vision transformer networks (cf. Fig.2). Our pre-trained and to-be-released TransPath model learns histopathology-specific feature representations from a large set of histopathological images, which has the potential to be transferred to any histopathological image analysis tasks, which we also deem to be an impactful contribution. (R2/R3) Lacks comparison with existing pretraining methods. We have tried using MoCo as the pre-training method, which produced similar results as the BYOL. We did not discuss it in the manuscript due to space limitation. We will include a more thorough comparison in our extended paper in the future. (R2/R3) Performance of the method with finetuning on different numbers of labeled data. Self-supervised pre-training with a large amount of data makes it possible to use fewer annotations to achieve good performance in downstream tasks [2]. In our experiments, the MHIST dataset is about 1/30 of the other two datasets, and using the pre-trained model provided a higher performance improvement for MHIST than the other two (cf. Table 1 and Table 3). These results indirectly demonstrated the effects as asked by the reviewers. We will add more experimental results in the extended version of our manuscript in the future. Reviewer#3  See whether the TAE module or pre-training leads to performance gains? There might be a misunderstanding. The performance gains offered by TAE can be seen by comparing the results of CNN+Trans with those of CNN+Trans+TAE. The extra benefits of pre-training can be verified by comparing the results of CNN+Trans+TAE and CNN+Trans+TAE+SSL (cf. Table 1). Data augmentation strategies. Although the applied data augmentation strategies (random crop, Gaussian blur, and color distortion) were originally designed for natural images, they are also applicable to pathological images. In future work, we will include comparisons with other data augmentation methods tailored for histopathological images, like proposed in the literature [17]. Reviewer#4 Separate frozen tissue from FFPE. We agree that this strategy might improve the performance. On the other hand, the deep learning model has a sufficient number of parameters. In addition, both frozen and FFPE slides are histopathology images, which may help train a more robust model. We will compare the performance of separating frozen tissue slides from FFPE ones in our future work. The Conclusion could be expanded. We have expanded the Conclusion section by including a little more details on the methods and our future work. Minor error. We appreciate the careful review and the suggestions. We have changed the “large” to “enough” and added the BN in the title of Fig.1. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Wang, Xiyue,Yang, Sen,Zhang, Jun,Wang, Minghui,Zhang, Jing,Huang, Junzhou,Yang, Wei,Han, Xiao" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>TransPath: Transformer-based Self-supervised Learning for Histopathological Image Classification</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Computational (Integrative) Pathology"
        class="post-category">
        Computational (Integrative) Pathology
      </a>
      
      <a 
        href="kittywong/categories#Modalities - Histopathology"
        class="post-category">
        Modalities - Histopathology
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Wang, Xiyue"
        class="post-tags">
        Wang, Xiyue
      </a> |  
      
      <a href="kittywong/tags#Yang, Sen"
        class="post-tags">
        Yang, Sen
      </a> |  
      
      <a href="kittywong/tags#Zhang, Jun"
        class="post-tags">
        Zhang, Jun
      </a> |  
      
      <a href="kittywong/tags#Wang, Minghui"
        class="post-tags">
        Wang, Minghui
      </a> |  
      
      <a href="kittywong/tags#Zhang, Jing"
        class="post-tags">
        Zhang, Jing
      </a> |  
      
      <a href="kittywong/tags#Huang, Junzhou"
        class="post-tags">
        Huang, Junzhou
      </a> |  
      
      <a href="kittywong/tags#Yang, Wei"
        class="post-tags">
        Yang, Wei
      </a> |  
      
      <a href="kittywong/tags#Han, Xiao"
        class="post-tags">
        Han, Xiao
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Xiyue Wang, Sen Yang, Jun Zhang, Minghui Wang, Jing Zhang, Junzhou Huang, Wei Yang, Xiao Han
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>A large-scale labeled dataset is a key factor for the success of supervised deep learning in histopathological image analysis. However, exhaustive annotation requires a careful visual inspection by pathologists, which is extremely time-consuming and labor-intensive. Self-supervised learning (SSL) can alleviate this issue by pre-training models under the supervision of data itself, which generalizes well to various downstream tasks with limited annotations. In this work, we propose a hybrid model (TransPath) which is pre-trained in an SSL manner on massively unlabeled histopathological images to discover the inherent image property and capture domain-specific feature embedding. The TransPath can serve as a collaborative local-global feature extractor, which is designed by combining a convolutional neural network (CNN) and a modified transformer architecture. We propose a token-aggregating and excitation (TAE) module which is placed behind the self-attention of the transformer encoder for capturing more global information. We evaluate the performance of pre-trained TransPath by fine-tuning it on three downstream histopathological image classification tasks. Our experimental results indicate that TransPath outperforms state-of-the-art vision transformer networks, and the visual representations generated by SSL on domain-relevant histopathological images are more transferable than the supervised baseline on ImageNet. Our code and pre-trained models will be available at https://github.com/Xiyue-Wang/TransPath.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87237-3_18">https://doi.org/10.1007/978-3-030-87237-3_18</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors present a self-supervised learning approach for pretraining histopathology images. To this aim, a CNN-transformer architecture is designed for making use of local and global receptive fields to extract discriminating features.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>Leveraging large-scale histopathology image for pretraining is well-motivated</li>
        <li>Using less annotated data for machine learning is an important topic in medical image analysis</li>
        <li>The results demonstrate the effectiveness of the proposed results</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The detailed evaluation process is not described.</p>

      <p>Lack of novelty.</p>

      <p>Lacks comparison with existing pertaining methods.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>As the data and code will be open sourced, this work is reproducible.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ul>
        <li>
          <p>The detailed evaluation process is not described. The aim of pretraining is to learn better representations so that less annotated data is required. A more detailed experiment is required to show how much annotated data is needed for down-streaming tasks after pertaining.</p>
        </li>
        <li>
          <p>Lack of novelty. Much of this paper is just a combination of existing approaches from computer vision.</p>
        </li>
        <li>
          <p>Lacks comparison with existing pertaining methods.</p>
        </li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper address the important topic of learning better representations with less annotated data. However, it lacks novelty and more detailed experiments are required.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper presents a hybrid framework by combining self-supervised pretrained CNN with a modified transformer architecture for histology image classification. The proposed method was evaluated on multiple datasets and demonstrated with improvements over CNN and combined CNN with multi-head self-attention based transformer methods.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>– Application of transformers is relatively new to the histopathology domain.
– The method was tested on multiple datasets. 
– It shows a good improvement over CNN and CNN+Trans methods with a big margin.
– Pre-trained on a very large dataset, which is interesting and also challenging.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>– The novelty of this submission is limited. It’s an application of the existing works BYOL and transformers.
– The paper didn’t compare with the state-of-the-art work in self-supervised learning, i.e., SimCLR, MoCo.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>I carefully assessed the sensibility of the experiments, and the results seem convincing.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>– I am curious to know the performance of the method with fine-tuning on 1%, 10%, and 50% labeled data on the downstream task. Since the method has been pretrained on a very large database, it would be really interesting to know how the method performs under limited label settings?
– In Table 1, it would be interesting to know the performance of CNN+Trans+SSL, without the TAE module; in this way, we could see whether the TAE module or pre-training on a larger dataset has lead to improved performance gains?
– Most of the contrastive learning methods (such as SimCLR and BYOL (ref 2, 5)) are highly sensitive to data augmentations. I am curious to know why did the authors use similar data augmentation strategies as SimCLR, which are tailored for natural images. There are well-established augmentation methods [1] that have been shown to reduce variation in domain shift and improve out-of-distribution (OOD) generalization, which could be future work in this direction.</p>

      <p>[1] Tellez, David, et al. “Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology.” Medical image analysis 58 (2019): 101544.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The authors present an exciting application of transformers to the histopathology domain. Furthermore, through extensive validation on large datasets, they showed substantial improvements in image classification benchmarks.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper adopts a self-supervised deep learning strategy with transformers and adding one module called token-aggregating and excitation (TAE) before self-attention to capture both local and global patterns of pathology slides. The authors trained the network with TCGA data applied results on three datasets and the results were promising. The huge computational resources (3.2k V100 hour) is the backbone of this work.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>Training a model in a well-known and large public datasets and showing the strangeness of the work by applying it on 3 public datasets.</li>
        <li>Adding TAE on the transformer to better global feature extraction, This simple mechanism improved the network’s accuracy considerably.</li>
        <li>Conducting an ablation study to show the power of the architecture design. This study helps the reader to understand the role of each component.</li>
        <li>Comparing with state-of-the-art models, and showing better results on three public datasets.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>TCGA consists of 22k frozen tissue and  11k FFPE tissues (diagnosis). Although both frozen and FFPE from one case (i.e. LUAC) but the pattern is totally different. Maybe removing frozen be a better idea.</li>
        <li>I anticipate seeing the state-of-the-art numbers for each dataset (even if they were better with specialized methods). This is acceptable that a method with generalized power provides slightly lower accuracy.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>They stated that the codes will be shared. It means even if the work missing reporting some parameters, which is the case, readers could find them in the shared codes.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ul>
        <li>“The cropped histopathological image patches are usually large to capture both the cell-level structure and and tissue-level context”, large is not a proper word. Many study are working with 64x64 which is small to ..</li>
        <li>“total of 32,529 WSIs from the cancer genome atlas (TCGA)” separate frozen tissue from PFFE</li>
        <li>BN in figure 1 is not intrucuced in the text.</li>
        <li>Conclution could be expanded a little more to provide more details on the methods.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper is strong. Codes and results would be helpful for other researchers in the comunity.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>All reviewers acknowledge that the paper addresses an interesting topic in medical image analysis and the benefits of the proposed method. While the technical novelty of this paper is still limited, the application of Transformer coupled with 
self-supervised learning for histopathological image classification is new and the results clearly show the effectiveness of the proposed method. Therefore, the paper can be interesting for MICCAI community. Please address the all the comments raised by all reviewers to further improve the paper.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We would like to thank the reviewers for their time spent on reviewing our manuscript and the overall very positive ratings. We also appreciate the insightful comments from all the reviewers, which are very helpful to further improve our manuscript and to plan our future work. Our responses to the major comments are provided below. Citations are numbered in the same order as in the final manuscript.</p>

  <p>General Responses
(R2/R3) The technical novelty of this paper is still limited.
We would like to point out that our model is not just a simple application of the existing works. Although our main framework is a combination of CNN, transformer, and BYOL, we have added a customized TAE module to extract global information. The designed TAE mechanism, even though simple, has improved the network performance by a large margin (cf. ablation study). Moreover, as summarized in our contributions, our model has been pre-trained on very large public datasets (approximately 2.7 million images with the size of 2048×2048 pixels), evaluated on other three public datasets, and has achieved superior performance compared with existing vision transformer networks (cf. Fig.2). Our pre-trained and to-be-released TransPath model learns histopathology-specific feature representations from a large set of histopathological images, which has the potential to be transferred to any histopathological image analysis tasks, which we also deem to be an impactful contribution.</p>

  <p>(R2/R3) Lacks comparison with existing pretraining methods. We have tried using MoCo as the pre-training method, which produced similar results as the BYOL. We did not discuss it in the manuscript due to space limitation. We will include a more thorough comparison in our extended paper in the future.</p>

  <p>(R2/R3) Performance of the method with finetuning on different numbers of labeled data. Self-supervised pre-training with a large amount of data makes it possible to use fewer annotations to achieve good performance in downstream tasks [2]. In our experiments, the MHIST dataset is about 1/30 of the other two datasets, and using the pre-trained model provided a higher performance improvement for MHIST than the other two (cf. Table 1 and Table 3). These results indirectly demonstrated the effects as asked by the reviewers. We will add more experimental results in the extended version of our manuscript in the future.</p>

  <p>Reviewer#3
 See whether the TAE module or pre-training leads to performance gains? There might be a misunderstanding. The performance gains offered by TAE can be seen by comparing the results of CNN+Trans with those of CNN+Trans+TAE. The extra benefits of pre-training can be verified by comparing the results of CNN+Trans+TAE and CNN+Trans+TAE+SSL (cf. Table 1).</p>

  <p>Data augmentation strategies. Although the applied data augmentation strategies (random crop, Gaussian blur, and color distortion) were originally designed for natural images, they are also applicable to pathological images. In future work, we will include comparisons with other data augmentation methods tailored for histopathological images, like proposed in the literature [17].</p>

  <p>Reviewer#4
Separate frozen tissue from FFPE. We agree that this strategy might improve the performance. On the other hand, the deep learning model has a sufficient number of parameters. In addition, both frozen and FFPE slides are histopathology images, which may help train a more robust model. We will compare the performance of separating frozen tissue slides from FFPE ones in our future work.</p>

  <p>The Conclusion could be expanded. We have expanded the Conclusion section by including a little more details on the methods and our future work.</p>

  <p>Minor error.  We appreciate the careful review and the suggestions. We have changed the “large” to “enough” and added the BN in the title of Fig.1.</p>

</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0817-12-31
      -->
      <!--
      
        ,
        updated at 
        0818-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Computational (Integrative) Pathology"
        class="post-category">
        Computational (Integrative) Pathology
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - Histopathology"
        class="post-category">
        Modalities - Histopathology
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Wang, Xiyue"
        class="post-category">
        Wang, Xiyue
      </a> |  
      
      <a href="kittywong/tags#Yang, Sen"
        class="post-category">
        Yang, Sen
      </a> |  
      
      <a href="kittywong/tags#Zhang, Jun"
        class="post-category">
        Zhang, Jun
      </a> |  
      
      <a href="kittywong/tags#Wang, Minghui"
        class="post-category">
        Wang, Minghui
      </a> |  
      
      <a href="kittywong/tags#Zhang, Jing"
        class="post-category">
        Zhang, Jing
      </a> |  
      
      <a href="kittywong/tags#Huang, Junzhou"
        class="post-category">
        Huang, Junzhou
      </a> |  
      
      <a href="kittywong/tags#Yang, Wei"
        class="post-category">
        Yang, Wei
      </a> |  
      
      <a href="kittywong/tags#Han, Xiao"
        class="post-category">
        Han, Xiao
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0818/12/31/Paper0771">
          From Pixel to Whole Slide: Automatic Detection of Microvascular Invasion in Hepatocellular Carcinoma on Histopathological Image via Cascaded Networks
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0816/12/31/Paper0417">
          A computational geometry approach for modeling neuronal fiber pathways
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
