<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Multi-View Surgical Video Action Detection via Mixed Global View Attention | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Multi-View Surgical Video Action Detection via Mixed Global View Attention" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Adam Schmidt, Aidean Sharghi, Helene Haugerud, Daniel Oh, Omid Mohareri Abstract Automatic surgical activity detection in the operating room can enable intelligent systems that potentially lead to more efficient surgical workflow. While real-world implementations of video activity detection in the OR most likely rely on multiple video feeds observing the environment from different view points to handle occlusion and clutter, the research on the matter has been left under-explored. This is perhaps due to the lack of a suitable dataset, thus, as our first contribution, we introduce the first large-scale multi-view surgical action detection dataset that includes over 120 temporally annotated robotic surgery operations, each recorded from 4 different viewpoints, resulting in 480 full-length surgical videos. As our second contribution, we design a novel model architecture that can detect surgical actions by utilizing multiple time-synchronized videos with shared field of view to better detect the activity that is taking place at any time. We explore early, hybrid, and late fusion methods for combining data from different views. We settle on a late fusion model that remains insensitive to sensor locations and feeding order, improving over single-view performance by using a mixing in the style of attention. Our model learns how to dynamically weight and fuse information across all views. We demonstrate improvements in mean Average Precision across the board using our new model. Link to paper https://doi.org/10.1007/978-3-030-87202-1_60 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper proposes a new dataset on multi-view surgical action detection dataset that includes 120 surgery operations with four viewpoints. This is a valuable dataset for the field of surgery analysis. The other contribution is the proposed model for action detection based on the multi-view videos. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Both the dataset and action detection model are important. The dataset is more valuable since it is the first large-scale multi-view dataset for surgical action detection in this area. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. (1) It would be nice if the dataset can also provide the videos captured by normal cameras. Currently the dataset only provides the videos captured by ToF sensors which has some advantages. If the authors think the normal videos cannot be provided, please clarify the reason. (2) From Table 2, it looks that the performance gain of the proposed method compared with [20] mainly comes from the single-view model. For example, for the first row “OR”, comparing “1-view” and [20], the performance gap is almost 20 points. However, comparing “1-view” and “2-view” or comparing “2-view” and “4-view”, the performance gain is only 1-2 points. Therfore, it looks that adding more views does not help that much. Can the authors give more explanations about this phenomenon? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors do not claim that they will release the dataset or code in the paper. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see the weakness. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The multi-view dataset for action detection in surgery videos is new. I believe it will be a good research direction. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper works on activity recognition in the operating rooms by a multi-view formulation. The major contribution is a large-scale diverse multi-view video dataset, with results on this dataset showing the effectiveness of the multi-view formulation. The other contribution is an attention-based view fusion module, which gives some slight improvements in the experiments. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of multi-view is well-motivated to handle occlusion and clutter in clinical situations. A large-scale multi-view dataset, with multiple surgeons, procedure types, and operating rooms included. Experiments evidently show the performance using multi-view is better than using only single-view. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. No proper discussion on previous multi-view datasets. The authors claim that there is no existing suitable dataset for multi-view surgical activity recognition. But this reviewer thinks the CATARACTS dataset of the EndoVis challenge could be relevant. In the EndoVis 2018 CATARACTS dataset, there are two views provided, i.e., microscope videos and tray videos. And later on, EndoVis 2020 CATARACTS is for surgical workflow recognition, which is relevant to the activity recognition in the paper. Results in Table 3 show that the proposed fusion module does not significantly outperform baselines (90.26 vs. 89.13 / 90.19), which undermines the second contribution of the paper. Some experiment setup is not clear. In section 5 paragraph 4 last line, why test the model in a bidirectional manner? Does this contradict the statement to learn a model that can run online (section 4 paragraph 3)? The authors say that the single-view model outperforms [20] in Table 2 because new videos are added to the dataset. But in this case, the results will be unable to directly compare because different data is used. The writing and clarity in the method section need to be improved. See below for details. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility is good in general. But the clarity in the method section need to be improved for better reproducibility. This reviewer also encourages the authors to release the data for better reproduction. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The clarity in the method section is not as good. In sections 4 and 4.1, the process of video synchronization and alignment is not clear. Why Eq. 1,2,4 keep the superscript ‘i’? Are there multiple mixers for each view or a single mixer for all views? This question of mine might be the consequence of the unclear statement of the process of video alignment. The mixer in Fig.2, although conceptually understandable, does not match the text well. If only look at the figure, the views seem to be directly weighted by some transformation of the fused features. The attention mechanism in Eq.3 is not illustrated in the figure. -Why not use the already defined symbol ‘g_multi’ in Eq.4? -In abstract line 2, “more efficient surgical workflow and efficiency” is wordy. It will be better to have some visualizations to see how well the model works on occlusions. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This reviewer thinks the usage of multi-view and proposed large-scale dataset are clinically meaningful. And in view of that the weaknesses could be hopefully improved by a careful revise, this reviewer recommends ‘Probably accept’. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper studies the problem of multi-view surgical video action detection. A multi-view action detection framework is proposed. A new dataset is introduced. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. 1) This paper introduced a new dataset for action detection in multi-view surgical videos. 2) An attention mechanism is leveraged to fuse multi-view video information to detect actions. 3) Experiments on the new dataset shows that the proposed method outperforms existing method for surgical action detection in operation rooms. 4) Ablation study shows the proposed fusion method is effective. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. 1) The proposed mixer in Fig. 2 appears to be more than just weighted sum, followed by an fully-connected layer, which is not the same as equation (4). This needs to be clarified. 2) The paper didn’t report precision, recall, and F1 score as in [20]. Performances for different surgical procedures are also not reported. 3) More details about the model architecture and implementations should be clarified, such as the number of units in the GRUs, the number of layers, etc. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The proposed method is clear, but details are needed to reproduce the results. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Table 1, the numbers are not well formated. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper introduced a new dataset for multi-view surgical action detection. A new multi-view fusion method is proposed for this problem. The experiments results also verifies the effectiveness of the method. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Somewhat confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper received collective positive reviews. However, there are some aspects of the work that need further clarification. Please consider addressing those in the final version. These include Methodological details; A review of the literature and previous multi-view datasets for similar applications; Justification for adding more views based on the experimental results; Clarification on the experimental setup, etc. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback N/A back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Adam Schmidt, Aidean Sharghi, Helene Haugerud, Daniel Oh, Omid Mohareri Abstract Automatic surgical activity detection in the operating room can enable intelligent systems that potentially lead to more efficient surgical workflow. While real-world implementations of video activity detection in the OR most likely rely on multiple video feeds observing the environment from different view points to handle occlusion and clutter, the research on the matter has been left under-explored. This is perhaps due to the lack of a suitable dataset, thus, as our first contribution, we introduce the first large-scale multi-view surgical action detection dataset that includes over 120 temporally annotated robotic surgery operations, each recorded from 4 different viewpoints, resulting in 480 full-length surgical videos. As our second contribution, we design a novel model architecture that can detect surgical actions by utilizing multiple time-synchronized videos with shared field of view to better detect the activity that is taking place at any time. We explore early, hybrid, and late fusion methods for combining data from different views. We settle on a late fusion model that remains insensitive to sensor locations and feeding order, improving over single-view performance by using a mixing in the style of attention. Our model learns how to dynamically weight and fuse information across all views. We demonstrate improvements in mean Average Precision across the board using our new model. Link to paper https://doi.org/10.1007/978-3-030-87202-1_60 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper proposes a new dataset on multi-view surgical action detection dataset that includes 120 surgery operations with four viewpoints. This is a valuable dataset for the field of surgery analysis. The other contribution is the proposed model for action detection based on the multi-view videos. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Both the dataset and action detection model are important. The dataset is more valuable since it is the first large-scale multi-view dataset for surgical action detection in this area. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. (1) It would be nice if the dataset can also provide the videos captured by normal cameras. Currently the dataset only provides the videos captured by ToF sensors which has some advantages. If the authors think the normal videos cannot be provided, please clarify the reason. (2) From Table 2, it looks that the performance gain of the proposed method compared with [20] mainly comes from the single-view model. For example, for the first row “OR”, comparing “1-view” and [20], the performance gap is almost 20 points. However, comparing “1-view” and “2-view” or comparing “2-view” and “4-view”, the performance gain is only 1-2 points. Therfore, it looks that adding more views does not help that much. Can the authors give more explanations about this phenomenon? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors do not claim that they will release the dataset or code in the paper. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see the weakness. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The multi-view dataset for action detection in surgery videos is new. I believe it will be a good research direction. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper works on activity recognition in the operating rooms by a multi-view formulation. The major contribution is a large-scale diverse multi-view video dataset, with results on this dataset showing the effectiveness of the multi-view formulation. The other contribution is an attention-based view fusion module, which gives some slight improvements in the experiments. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of multi-view is well-motivated to handle occlusion and clutter in clinical situations. A large-scale multi-view dataset, with multiple surgeons, procedure types, and operating rooms included. Experiments evidently show the performance using multi-view is better than using only single-view. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. No proper discussion on previous multi-view datasets. The authors claim that there is no existing suitable dataset for multi-view surgical activity recognition. But this reviewer thinks the CATARACTS dataset of the EndoVis challenge could be relevant. In the EndoVis 2018 CATARACTS dataset, there are two views provided, i.e., microscope videos and tray videos. And later on, EndoVis 2020 CATARACTS is for surgical workflow recognition, which is relevant to the activity recognition in the paper. Results in Table 3 show that the proposed fusion module does not significantly outperform baselines (90.26 vs. 89.13 / 90.19), which undermines the second contribution of the paper. Some experiment setup is not clear. In section 5 paragraph 4 last line, why test the model in a bidirectional manner? Does this contradict the statement to learn a model that can run online (section 4 paragraph 3)? The authors say that the single-view model outperforms [20] in Table 2 because new videos are added to the dataset. But in this case, the results will be unable to directly compare because different data is used. The writing and clarity in the method section need to be improved. See below for details. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility is good in general. But the clarity in the method section need to be improved for better reproducibility. This reviewer also encourages the authors to release the data for better reproduction. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The clarity in the method section is not as good. In sections 4 and 4.1, the process of video synchronization and alignment is not clear. Why Eq. 1,2,4 keep the superscript ‘i’? Are there multiple mixers for each view or a single mixer for all views? This question of mine might be the consequence of the unclear statement of the process of video alignment. The mixer in Fig.2, although conceptually understandable, does not match the text well. If only look at the figure, the views seem to be directly weighted by some transformation of the fused features. The attention mechanism in Eq.3 is not illustrated in the figure. -Why not use the already defined symbol ‘g_multi’ in Eq.4? -In abstract line 2, “more efficient surgical workflow and efficiency” is wordy. It will be better to have some visualizations to see how well the model works on occlusions. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This reviewer thinks the usage of multi-view and proposed large-scale dataset are clinically meaningful. And in view of that the weaknesses could be hopefully improved by a careful revise, this reviewer recommends ‘Probably accept’. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper studies the problem of multi-view surgical video action detection. A multi-view action detection framework is proposed. A new dataset is introduced. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. 1) This paper introduced a new dataset for action detection in multi-view surgical videos. 2) An attention mechanism is leveraged to fuse multi-view video information to detect actions. 3) Experiments on the new dataset shows that the proposed method outperforms existing method for surgical action detection in operation rooms. 4) Ablation study shows the proposed fusion method is effective. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. 1) The proposed mixer in Fig. 2 appears to be more than just weighted sum, followed by an fully-connected layer, which is not the same as equation (4). This needs to be clarified. 2) The paper didn’t report precision, recall, and F1 score as in [20]. Performances for different surgical procedures are also not reported. 3) More details about the model architecture and implementations should be clarified, such as the number of units in the GRUs, the number of layers, etc. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The proposed method is clear, but details are needed to reproduce the results. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Table 1, the numbers are not well formated. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper introduced a new dataset for multi-view surgical action detection. A new multi-view fusion method is proposed for this problem. The experiments results also verifies the effectiveness of the method. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Somewhat confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper received collective positive reviews. However, there are some aspects of the work that need further clarification. Please consider addressing those in the final version. These include Methodological details; A review of the literature and previous multi-view datasets for similar applications; Justification for adding more views based on the experimental results; Clarification on the experimental setup, etc. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback N/A back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0459/12/31/Paper1533" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0459/12/31/Paper1533" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0459-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Multi-View Surgical Video Action Detection via Mixed Global View Attention" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0459/12/31/Paper1533"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0459/12/31/Paper1533","headline":"Multi-View Surgical Video Action Detection via Mixed Global View Attention","dateModified":"0460-01-02T00:00:00-05:17","datePublished":"0459-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Adam Schmidt, Aidean Sharghi, Helene Haugerud, Daniel Oh, Omid Mohareri Abstract Automatic surgical activity detection in the operating room can enable intelligent systems that potentially lead to more efficient surgical workflow. While real-world implementations of video activity detection in the OR most likely rely on multiple video feeds observing the environment from different view points to handle occlusion and clutter, the research on the matter has been left under-explored. This is perhaps due to the lack of a suitable dataset, thus, as our first contribution, we introduce the first large-scale multi-view surgical action detection dataset that includes over 120 temporally annotated robotic surgery operations, each recorded from 4 different viewpoints, resulting in 480 full-length surgical videos. As our second contribution, we design a novel model architecture that can detect surgical actions by utilizing multiple time-synchronized videos with shared field of view to better detect the activity that is taking place at any time. We explore early, hybrid, and late fusion methods for combining data from different views. We settle on a late fusion model that remains insensitive to sensor locations and feeding order, improving over single-view performance by using a mixing in the style of attention. Our model learns how to dynamically weight and fuse information across all views. We demonstrate improvements in mean Average Precision across the board using our new model. Link to paper https://doi.org/10.1007/978-3-030-87202-1_60 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper proposes a new dataset on multi-view surgical action detection dataset that includes 120 surgery operations with four viewpoints. This is a valuable dataset for the field of surgery analysis. The other contribution is the proposed model for action detection based on the multi-view videos. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Both the dataset and action detection model are important. The dataset is more valuable since it is the first large-scale multi-view dataset for surgical action detection in this area. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. (1) It would be nice if the dataset can also provide the videos captured by normal cameras. Currently the dataset only provides the videos captured by ToF sensors which has some advantages. If the authors think the normal videos cannot be provided, please clarify the reason. (2) From Table 2, it looks that the performance gain of the proposed method compared with [20] mainly comes from the single-view model. For example, for the first row “OR”, comparing “1-view” and [20], the performance gap is almost 20 points. However, comparing “1-view” and “2-view” or comparing “2-view” and “4-view”, the performance gain is only 1-2 points. Therfore, it looks that adding more views does not help that much. Can the authors give more explanations about this phenomenon? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors do not claim that they will release the dataset or code in the paper. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see the weakness. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The multi-view dataset for action detection in surgery videos is new. I believe it will be a good research direction. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper works on activity recognition in the operating rooms by a multi-view formulation. The major contribution is a large-scale diverse multi-view video dataset, with results on this dataset showing the effectiveness of the multi-view formulation. The other contribution is an attention-based view fusion module, which gives some slight improvements in the experiments. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of multi-view is well-motivated to handle occlusion and clutter in clinical situations. A large-scale multi-view dataset, with multiple surgeons, procedure types, and operating rooms included. Experiments evidently show the performance using multi-view is better than using only single-view. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. No proper discussion on previous multi-view datasets. The authors claim that there is no existing suitable dataset for multi-view surgical activity recognition. But this reviewer thinks the CATARACTS dataset of the EndoVis challenge could be relevant. In the EndoVis 2018 CATARACTS dataset, there are two views provided, i.e., microscope videos and tray videos. And later on, EndoVis 2020 CATARACTS is for surgical workflow recognition, which is relevant to the activity recognition in the paper. Results in Table 3 show that the proposed fusion module does not significantly outperform baselines (90.26 vs. 89.13 / 90.19), which undermines the second contribution of the paper. Some experiment setup is not clear. In section 5 paragraph 4 last line, why test the model in a bidirectional manner? Does this contradict the statement to learn a model that can run online (section 4 paragraph 3)? The authors say that the single-view model outperforms [20] in Table 2 because new videos are added to the dataset. But in this case, the results will be unable to directly compare because different data is used. The writing and clarity in the method section need to be improved. See below for details. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility is good in general. But the clarity in the method section need to be improved for better reproducibility. This reviewer also encourages the authors to release the data for better reproduction. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The clarity in the method section is not as good. In sections 4 and 4.1, the process of video synchronization and alignment is not clear. Why Eq. 1,2,4 keep the superscript ‘i’? Are there multiple mixers for each view or a single mixer for all views? This question of mine might be the consequence of the unclear statement of the process of video alignment. The mixer in Fig.2, although conceptually understandable, does not match the text well. If only look at the figure, the views seem to be directly weighted by some transformation of the fused features. The attention mechanism in Eq.3 is not illustrated in the figure. -Why not use the already defined symbol ‘g_multi’ in Eq.4? -In abstract line 2, “more efficient surgical workflow and efficiency” is wordy. It will be better to have some visualizations to see how well the model works on occlusions. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This reviewer thinks the usage of multi-view and proposed large-scale dataset are clinically meaningful. And in view of that the weaknesses could be hopefully improved by a careful revise, this reviewer recommends ‘Probably accept’. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper studies the problem of multi-view surgical video action detection. A multi-view action detection framework is proposed. A new dataset is introduced. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. 1) This paper introduced a new dataset for action detection in multi-view surgical videos. 2) An attention mechanism is leveraged to fuse multi-view video information to detect actions. 3) Experiments on the new dataset shows that the proposed method outperforms existing method for surgical action detection in operation rooms. 4) Ablation study shows the proposed fusion method is effective. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. 1) The proposed mixer in Fig. 2 appears to be more than just weighted sum, followed by an fully-connected layer, which is not the same as equation (4). This needs to be clarified. 2) The paper didn’t report precision, recall, and F1 score as in [20]. Performances for different surgical procedures are also not reported. 3) More details about the model architecture and implementations should be clarified, such as the number of units in the GRUs, the number of layers, etc. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The proposed method is clear, but details are needed to reproduce the results. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Table 1, the numbers are not well formated. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper introduced a new dataset for multi-view surgical action detection. A new multi-view fusion method is proposed for this problem. The experiments results also verifies the effectiveness of the method. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Somewhat confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper received collective positive reviews. However, there are some aspects of the work that need further clarification. Please consider addressing those in the final version. These include Methodological details; A review of the literature and previous multi-view datasets for similar applications; Justification for adding more views based on the experimental results; Clarification on the experimental setup, etc. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback N/A back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Schmidt, Adam,Sharghi, Aidean,Haugerud, Helene,Oh, Daniel,Mohareri, Omid" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Multi-View Surgical Video Action Detection via Mixed Global View Attention</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Surgical Skill and Work Flow Analysis"
        class="post-category">
        Surgical Skill and Work Flow Analysis
      </a>
      
      <a 
        href="kittywong/categories#Image-Guided Interventions and Surgery"
        class="post-category">
        Image-Guided Interventions and Surgery
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a>
      
      <a 
        href="kittywong/categories#Modalities - Video"
        class="post-category">
        Modalities - Video
      </a>
      
      <a 
        href="kittywong/categories#Surgical Data Science"
        class="post-category">
        Surgical Data Science
      </a>
      
      <a 
        href="kittywong/categories#Surgical Planning and Simulation"
        class="post-category">
        Surgical Planning and Simulation
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Schmidt, Adam"
        class="post-tags">
        Schmidt, Adam
      </a> |  
      
      <a href="kittywong/tags#Sharghi, Aidean"
        class="post-tags">
        Sharghi, Aidean
      </a> |  
      
      <a href="kittywong/tags#Haugerud, Helene"
        class="post-tags">
        Haugerud, Helene
      </a> |  
      
      <a href="kittywong/tags#Oh, Daniel"
        class="post-tags">
        Oh, Daniel
      </a> |  
      
      <a href="kittywong/tags#Mohareri, Omid"
        class="post-tags">
        Mohareri, Omid
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Adam Schmidt, Aidean Sharghi, Helene Haugerud, Daniel Oh, Omid Mohareri
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Automatic surgical activity detection in the operating room can enable intelligent systems that potentially lead to more efficient surgical workflow. While real-world implementations of video activity detection in the OR most likely rely on multiple video feeds observing the environment from different view points to handle occlusion and clutter, the research on the matter has been left under-explored.
This is perhaps due to the lack of a suitable dataset, thus, as our first contribution, we introduce the first large-scale multi-view surgical action detection dataset that includes over 120 temporally annotated robotic surgery operations, each recorded from 4 different viewpoints, resulting in 480 full-length surgical videos. As our second contribution, we design a novel model architecture that can detect surgical actions by utilizing multiple time-synchronized videos with shared field of view to better detect the activity that is taking place at any time.
We explore early, hybrid, and late fusion methods for combining data from different views.
We settle on a late fusion model that remains insensitive to sensor locations and feeding order, improving over single-view performance by using a mixing in the style of attention.
Our model learns how to dynamically weight and fuse information across all views.
We demonstrate improvements in mean Average Precision across the board using our new model. 
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87202-1_60">https://doi.org/10.1007/978-3-030-87202-1_60</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper proposes a new dataset on multi-view surgical action detection dataset that includes 120 surgery operations with four viewpoints. This is a valuable dataset for the field of surgery analysis. The other contribution is the proposed model for action detection based on the multi-view videos.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>Both the dataset and action detection model are important. The dataset is more valuable since it is the first large-scale multi-view dataset for surgical action detection in this area.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>(1)	It would be nice if the dataset can also provide the videos captured by normal cameras.  Currently the dataset only provides the videos captured by ToF sensors which has some advantages. If the authors think the normal videos cannot be provided, please clarify the reason. 
(2)	From Table 2, it looks that the performance gain of the proposed method compared with [20] mainly comes from the single-view model. For example, for the first row “OR”, comparing “1-view” and [20],  the performance gap is almost 20 points. However, comparing “1-view” and “2-view” or comparing “2-view” and “4-view”, the performance gain is only 1-2 points. Therfore, it looks that adding more views does not help that much. Can the authors give more explanations about this phenomenon?</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The authors do not claim that they will release the dataset or code in the paper.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Please see the weakness.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The multi-view dataset for action detection in surgery videos is new. I believe it will be a good research direction.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper works on activity recognition in the operating rooms by a multi-view formulation. The major contribution is a large-scale diverse multi-view video dataset, with results on this dataset showing the effectiveness of the multi-view formulation. The other contribution is an attention-based view fusion module, which gives some slight improvements in the experiments.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>
          <p>The idea of multi-view is well-motivated to handle occlusion and clutter in clinical situations.</p>
        </li>
        <li>
          <p>A large-scale multi-view dataset, with multiple surgeons, procedure types, and operating rooms included.</p>
        </li>
        <li>
          <p>Experiments evidently show the performance using multi-view is better than using only single-view.</p>
        </li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>
          <p>No proper discussion on previous multi-view datasets. The authors claim that there is no existing suitable dataset for multi-view surgical activity recognition. But this reviewer thinks the CATARACTS dataset of the EndoVis challenge could be relevant. In the EndoVis 2018 CATARACTS dataset, there are two views provided, i.e., microscope videos and tray videos. And later on, EndoVis 2020 CATARACTS is for surgical workflow recognition, which is relevant to the activity recognition in the paper.</p>
        </li>
        <li>
          <p>Results in Table 3 show that the proposed fusion module does not significantly outperform baselines (90.26 vs. 89.13 / 90.19), which undermines the second contribution of the paper.</p>
        </li>
        <li>Some experiment setup is not clear.
          <ul>
            <li>In section 5 paragraph 4 last line, why test the model in a bidirectional manner? Does this contradict the statement to learn a model that can run online (section 4 paragraph 3)?</li>
            <li>The authors say that the single-view model outperforms [20] in Table 2 because new videos are added to the dataset. But in this case, the results will be unable to directly compare because different data is used.</li>
          </ul>
        </li>
        <li>The writing and clarity in the method section need to be improved. 
See below for details.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Satisfactory</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The reproducibility is good in general. But the clarity in the method section need to be improved for better reproducibility. This reviewer also encourages the authors to release the data for better reproduction.</p>

    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ol>
        <li>The clarity in the method section is not as good.
          <ul>
            <li>In sections 4 and 4.1, the process of video synchronization and alignment is not clear.</li>
            <li>Why Eq. 1,2,4 keep the superscript ‘i’? Are there multiple mixers for each view or a single mixer for all views? This question of mine might be the consequence of the unclear statement of the process of video alignment.</li>
            <li>The mixer in Fig.2, although conceptually understandable, does not match the text well. If only look at the figure, the views seem to be directly weighted by some transformation of the fused features. The attention mechanism in Eq.3 is not illustrated in the figure.
-Why not use the already defined symbol ‘g_multi’ in Eq.4?
-In abstract line 2, “more efficient surgical workflow and
efficiency” is wordy.</li>
          </ul>
        </li>
        <li>It will be better to have some visualizations to see how well the model works on occlusions.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>This reviewer thinks the usage of multi-view and proposed large-scale dataset are clinically meaningful. And in view of that the weaknesses could be hopefully improved by a careful revise, this reviewer recommends ‘Probably accept’.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper studies the problem of multi-view surgical video action detection. A multi-view action detection framework is proposed. A new dataset is introduced.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>1) This paper introduced a new dataset for action detection in multi-view surgical videos.
2) An attention mechanism is leveraged to fuse multi-view video information to detect actions.
3) Experiments on the new dataset shows that the proposed method outperforms existing method for surgical action detection in operation rooms.
4) Ablation study shows the proposed fusion method is effective.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>1) The proposed mixer in Fig. 2 appears to be more than just weighted sum, followed by an fully-connected layer, which is not the same as equation (4). This needs to be clarified.
2) The paper didn’t report precision, recall, and F1 score as in [20]. Performances for different surgical procedures are also not reported.
3) More details about the model architecture and implementations should be clarified, such as the number of units in the GRUs, the number of layers, etc.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The proposed method is clear, but details are needed to reproduce the results.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>In Table 1, the numbers are not well formated.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper introduced a new dataset for multi-view surgical action detection. A new multi-view fusion method is proposed for this problem. The experiments results also verifies the effectiveness of the method.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Somewhat confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>The paper received collective positive reviews. However, there are some aspects of the work that need further clarification. Please consider addressing those in the final version. These include Methodological details; A review of the literature and previous multi-view datasets for similar applications; Justification for adding more views based on the experimental results; Clarification on the experimental setup, etc.</p>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>N/A</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0459-12-31
      -->
      <!--
      
        ,
        updated at 
        0460-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Surgical Skill and Work Flow Analysis"
        class="post-category">
        Surgical Skill and Work Flow Analysis
      </a> |
      
      <a 
        href="kittywong/categories#Image-Guided Interventions and Surgery"
        class="post-category">
        Image-Guided Interventions and Surgery
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - Video"
        class="post-category">
        Modalities - Video
      </a> |
      
      <a 
        href="kittywong/categories#Surgical Data Science"
        class="post-category">
        Surgical Data Science
      </a> |
      
      <a 
        href="kittywong/categories#Surgical Planning and Simulation"
        class="post-category">
        Surgical Planning and Simulation
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Schmidt, Adam"
        class="post-category">
        Schmidt, Adam
      </a> |  
      
      <a href="kittywong/tags#Sharghi, Aidean"
        class="post-category">
        Sharghi, Aidean
      </a> |  
      
      <a href="kittywong/tags#Haugerud, Helene"
        class="post-category">
        Haugerud, Helene
      </a> |  
      
      <a href="kittywong/tags#Oh, Daniel"
        class="post-category">
        Oh, Daniel
      </a> |  
      
      <a href="kittywong/tags#Mohareri, Omid"
        class="post-category">
        Mohareri, Omid
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0460/12/31/Paper1892">
          Interhemispheric functional connectivity in the primary motor cortex distinguishes between training on a physical and a virtual surgical simulator
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0458/12/31/Paper0849">
          Surgical Workflow Anticipation using Instrument Interaction
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
