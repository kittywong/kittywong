<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Orthogonal Ensemble Networks for Biomedical Image Segmentation | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Orthogonal Ensemble Networks for Biomedical Image Segmentation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Agostina J. Larrazabal, César Martínez, Jose Dolz, Enzo Ferrante Abstract Despite the astonishing performance of deep-learning based approaches for visual tasks such as semantic segmentation, they are known to produce miscalibrated predictions, which could be harmful for critical decision-making processes. Ensemble learning has shown to not only boost the performance of individual models but also reduce their miscalibration by averaging the independent predictions. In this scenario, model diversity has become a key factor, which facilitates individual models converging to different functional solutions. In this work, we introduce Orthogonal Ensemble Networks (OEN), a novel framework to explicitly enforce model diversity by means of orthogonal constraints. The proposed method is based on the hypothesis that inducing orthogonality among the constituents of the ensemble will increase the overall model diversity. We resort to a new pairwise orthogonality constraint which can be used to regularize a sequential ensemble training process, resulting on improved predictive performance and better calibrated model outputs. We benchmark the proposed framework in two challenging brain lesion segmentation tasks –brain tumor and white matter hyper-intensity segmentation in MR images. The experimental results show that our approach produces more robust and well-calibrated ensemble models and can deal with challenging tasks in the context of biomedical image segmentation. Link to paper https://doi.org/10.1007/978-3-030-87199-4_56 Link to the code repository https://github.com/agosl/Orthogonal_Ensemble_Networks Link to the dataset(s) https://www.med.upenn.edu/cbica/brats2020/data.html https://wmh.isi.uu.nl/data/ Reviews Review #1 Please describe the contribution of the paper The paper introduces orthogonality constraints and a corresponding learning strategy to improve calibration and segmentation performance of an ensemble of deep neural networks. Orthogonality constraints are used to decorrelate layers within a given model as well as among different base models in an ensemble. Such constraints enable the sequential learning of more diverse (i.e. decorrelated) base models. Experimental evaluation on BRATS 2020 dataset and a WMH dataset demonstrate a clear improvement in calibration and segmentation performance. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. • Relevance: the authors propose a method for improving calibration of deep neural network. Clinical decisions inevitably depend on well-calibrated probabilities. Calibration of deep neural networks is in my opinion still an understudied topic and better methods are desperately needed. • Novelty: the introduction of orthogonality constraints to enforce diversity among base models in an ensemble appears to be sensible and novel. • Breadth: in addition to introducing orthogonality constraints, the authors provide a strategy to train the base models of the ensemble, evaluate their approach on two relevant medical imaging segmentation tasks and chose meaningful baseline methods to experimentally prove their point. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. • Impact on model weights: While the authors provide results on calibration and segmentation performance, an investigation on the impact of the decorrelation constraints on the actual model weights is missing. I would have liked to see a visualization (e.g. in Supplementary Materials) which confirms that the constraints are inducing the desired outcome (decorrelation). (see also detailed comments) Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No code is explicitly provided. However, the proposed meta-learning algorithm is very well explained. Therefore, I think it reasonable to say that based on the paper alone (and maybe some correspondence with the authors) one could easily reproduce the method. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html • From my point of view, the hypothesis presented in this paper consists of two parts: i) orthogonality constraints yield more diverse (i.e. decorrelated) models, ii) diverse models yield better calibration and segmentation performance. The authors clearly succeeded in proving part ii). While orthogonality constraints have been studied previously and their impact of filters has been presented, I would say that a verification of part i) is still necessary (at least provided as Supplementary Material). • Regarding the inter-orthogonality term, I am wondering if the restriction of the computation to a given layer l is sensible (apart from practical reasons). It is not obvious to me if the same “feature” is encoded for a given layer l among multiple models. Therefore, I am curious if it makes sense to compute an orthogonality for models as a whole rather than in a per-layer manner? • The sequential learning of “informed” models reminds me of traditional boosting methods (e.g. AdaBoost). In the case of boosting, the “informing” is conducted via sample weighting. In the case of the author’s methodology, it is conducted via the inter-orthogonality terms. From a theoretical point of view, it would be very interesting to work out if a connection of the author’s methodology with boosting exists. • On the BRATS dataset, the 5-net ensemble with self-orthogonality performed worse than the ensemble with random initialization. I wonder why? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novelty. Breadth of the paper including a very convincing experimental evaluation. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper Deep learning models are well-known for its overconfident predictions, which make them unsuitable for applications that require good probability estimations. A simple solution is to use an ensemble of deep models and average their predictions. But, for an ensemble to work properly, its components need to be decorrelated. The paper proposes a method to enforce the diversity in ensembles of deep neural networks by training the networks sequentially and encouraging that the weights of the filters of each network are mutually orthogonal with the weights of the filters at the same layer of all previously trained networks. While it is unclear if this orthogonality is leading to actual model diversity in the ensembles, it is shown experimentally that the performance of an ensemble improves, sometimes by a large margin, when trained with the proposed orthogonality loss. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. S1. The method is simple, clean and reasonable. The related work section seems relevant. The paper is well written, concise and very clear. S2. While orthogonality losses have been used before, I am not aware of any previous method that applies it to encourage model diversity in ensembles. To the extent of my knowledge, this is a novel contribution. S3. The experimental section is convincing and shows clear advantages of the proposed method. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. I found a few minor issues: W1. The paper does not analyze the complexity of computing the orthogonality loss, O(Nn^2), which seems to be quite high. How long does the training take compared to the training of the random ensemble? W2. The hypothesis of the paper is that the orthogonal constraints will bring diversity to the ensemble. This is not shown or proved. Is the increase in performance a consequence of a more diverse ensemble, or a consequence of better individual models obtained thanks to the orthogonality term? W3. How sensitive is the method to the hyperparameter lambda? A sensitivity analysis would be very helpful, as methods with high sensitivity to the hyperparameters are harder to use in practice. W4. How does the proposed method compare to other methods to produce ensembles of networks, such as monte-carlo dropout or boosting? I miss a comparison to alternative previous methods to clarify where the proposed method stands. W5. No qualitative results shown. W6. It is not completely clear to me how the inter orthogonality loss is applied in the 1-Network case of Figures 1 and 2. If I understood correctly, an ensemble of 10 models is trained and then one of them is randomly selected for evaluation? Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I have no concerns regarding the reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I suggest that authors address my weaknesses to make the paper stronger. For W1, a training time comparison would suffice. For W2, authors could show the variance in the predictions of the components of the ensemble with and without the orthogonal losses. If the hypothesis is correct, the variance of the ensemble should increase when the new losses are used. For W3, a plot of performance vs. lambda would help to understand its influence. For W4, authors should add at least one additional baseline of ensemble of networks. Also, the paper would benefit from some qualitative results (W5) that show examples where the proposed method performs better than baselines, and a few failure cases. Two minor typos: Page 3, end of first paragraph: “…in the performance on the benchmark , “ Remove the space before the comma. Page 8, second paragraph: “less than $1e^{-3}$”. This should be $10^{-3}$ (or $1e-3$, but the former is more elegant). Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I think this is a good paper, and I consider the weaknesses listed above to be minor. Even considering the possibility that a comparison with another baseline (W4) showed the proposed method as inferior, the paper still introduces an elegant and theoretically sound method and presents convincing experiments that will be interesting for the community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. A solid and well demonstrated work that provides an interesting solution to an important problem. Suggestions proposed for additional experiments and improved clarification will certainly be beneficial to the future development of the paper What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback N/A back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Agostina J. Larrazabal, César Martínez, Jose Dolz, Enzo Ferrante Abstract Despite the astonishing performance of deep-learning based approaches for visual tasks such as semantic segmentation, they are known to produce miscalibrated predictions, which could be harmful for critical decision-making processes. Ensemble learning has shown to not only boost the performance of individual models but also reduce their miscalibration by averaging the independent predictions. In this scenario, model diversity has become a key factor, which facilitates individual models converging to different functional solutions. In this work, we introduce Orthogonal Ensemble Networks (OEN), a novel framework to explicitly enforce model diversity by means of orthogonal constraints. The proposed method is based on the hypothesis that inducing orthogonality among the constituents of the ensemble will increase the overall model diversity. We resort to a new pairwise orthogonality constraint which can be used to regularize a sequential ensemble training process, resulting on improved predictive performance and better calibrated model outputs. We benchmark the proposed framework in two challenging brain lesion segmentation tasks –brain tumor and white matter hyper-intensity segmentation in MR images. The experimental results show that our approach produces more robust and well-calibrated ensemble models and can deal with challenging tasks in the context of biomedical image segmentation. Link to paper https://doi.org/10.1007/978-3-030-87199-4_56 Link to the code repository https://github.com/agosl/Orthogonal_Ensemble_Networks Link to the dataset(s) https://www.med.upenn.edu/cbica/brats2020/data.html https://wmh.isi.uu.nl/data/ Reviews Review #1 Please describe the contribution of the paper The paper introduces orthogonality constraints and a corresponding learning strategy to improve calibration and segmentation performance of an ensemble of deep neural networks. Orthogonality constraints are used to decorrelate layers within a given model as well as among different base models in an ensemble. Such constraints enable the sequential learning of more diverse (i.e. decorrelated) base models. Experimental evaluation on BRATS 2020 dataset and a WMH dataset demonstrate a clear improvement in calibration and segmentation performance. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. • Relevance: the authors propose a method for improving calibration of deep neural network. Clinical decisions inevitably depend on well-calibrated probabilities. Calibration of deep neural networks is in my opinion still an understudied topic and better methods are desperately needed. • Novelty: the introduction of orthogonality constraints to enforce diversity among base models in an ensemble appears to be sensible and novel. • Breadth: in addition to introducing orthogonality constraints, the authors provide a strategy to train the base models of the ensemble, evaluate their approach on two relevant medical imaging segmentation tasks and chose meaningful baseline methods to experimentally prove their point. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. • Impact on model weights: While the authors provide results on calibration and segmentation performance, an investigation on the impact of the decorrelation constraints on the actual model weights is missing. I would have liked to see a visualization (e.g. in Supplementary Materials) which confirms that the constraints are inducing the desired outcome (decorrelation). (see also detailed comments) Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No code is explicitly provided. However, the proposed meta-learning algorithm is very well explained. Therefore, I think it reasonable to say that based on the paper alone (and maybe some correspondence with the authors) one could easily reproduce the method. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html • From my point of view, the hypothesis presented in this paper consists of two parts: i) orthogonality constraints yield more diverse (i.e. decorrelated) models, ii) diverse models yield better calibration and segmentation performance. The authors clearly succeeded in proving part ii). While orthogonality constraints have been studied previously and their impact of filters has been presented, I would say that a verification of part i) is still necessary (at least provided as Supplementary Material). • Regarding the inter-orthogonality term, I am wondering if the restriction of the computation to a given layer l is sensible (apart from practical reasons). It is not obvious to me if the same “feature” is encoded for a given layer l among multiple models. Therefore, I am curious if it makes sense to compute an orthogonality for models as a whole rather than in a per-layer manner? • The sequential learning of “informed” models reminds me of traditional boosting methods (e.g. AdaBoost). In the case of boosting, the “informing” is conducted via sample weighting. In the case of the author’s methodology, it is conducted via the inter-orthogonality terms. From a theoretical point of view, it would be very interesting to work out if a connection of the author’s methodology with boosting exists. • On the BRATS dataset, the 5-net ensemble with self-orthogonality performed worse than the ensemble with random initialization. I wonder why? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novelty. Breadth of the paper including a very convincing experimental evaluation. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper Deep learning models are well-known for its overconfident predictions, which make them unsuitable for applications that require good probability estimations. A simple solution is to use an ensemble of deep models and average their predictions. But, for an ensemble to work properly, its components need to be decorrelated. The paper proposes a method to enforce the diversity in ensembles of deep neural networks by training the networks sequentially and encouraging that the weights of the filters of each network are mutually orthogonal with the weights of the filters at the same layer of all previously trained networks. While it is unclear if this orthogonality is leading to actual model diversity in the ensembles, it is shown experimentally that the performance of an ensemble improves, sometimes by a large margin, when trained with the proposed orthogonality loss. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. S1. The method is simple, clean and reasonable. The related work section seems relevant. The paper is well written, concise and very clear. S2. While orthogonality losses have been used before, I am not aware of any previous method that applies it to encourage model diversity in ensembles. To the extent of my knowledge, this is a novel contribution. S3. The experimental section is convincing and shows clear advantages of the proposed method. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. I found a few minor issues: W1. The paper does not analyze the complexity of computing the orthogonality loss, O(Nn^2), which seems to be quite high. How long does the training take compared to the training of the random ensemble? W2. The hypothesis of the paper is that the orthogonal constraints will bring diversity to the ensemble. This is not shown or proved. Is the increase in performance a consequence of a more diverse ensemble, or a consequence of better individual models obtained thanks to the orthogonality term? W3. How sensitive is the method to the hyperparameter lambda? A sensitivity analysis would be very helpful, as methods with high sensitivity to the hyperparameters are harder to use in practice. W4. How does the proposed method compare to other methods to produce ensembles of networks, such as monte-carlo dropout or boosting? I miss a comparison to alternative previous methods to clarify where the proposed method stands. W5. No qualitative results shown. W6. It is not completely clear to me how the inter orthogonality loss is applied in the 1-Network case of Figures 1 and 2. If I understood correctly, an ensemble of 10 models is trained and then one of them is randomly selected for evaluation? Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I have no concerns regarding the reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I suggest that authors address my weaknesses to make the paper stronger. For W1, a training time comparison would suffice. For W2, authors could show the variance in the predictions of the components of the ensemble with and without the orthogonal losses. If the hypothesis is correct, the variance of the ensemble should increase when the new losses are used. For W3, a plot of performance vs. lambda would help to understand its influence. For W4, authors should add at least one additional baseline of ensemble of networks. Also, the paper would benefit from some qualitative results (W5) that show examples where the proposed method performs better than baselines, and a few failure cases. Two minor typos: Page 3, end of first paragraph: “…in the performance on the benchmark , “ Remove the space before the comma. Page 8, second paragraph: “less than $1e^{-3}$”. This should be $10^{-3}$ (or $1e-3$, but the former is more elegant). Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I think this is a good paper, and I consider the weaknesses listed above to be minor. Even considering the possibility that a comparison with another baseline (W4) showed the proposed method as inferior, the paper still introduces an elegant and theoretically sound method and presents convincing experiments that will be interesting for the community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. A solid and well demonstrated work that provides an interesting solution to an important problem. Suggestions proposed for additional experiments and improved clarification will certainly be beneficial to the future development of the paper What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback N/A back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0355/12/31/Paper0969" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0355/12/31/Paper0969" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0355-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Orthogonal Ensemble Networks for Biomedical Image Segmentation" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0355/12/31/Paper0969"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0355/12/31/Paper0969","headline":"Orthogonal Ensemble Networks for Biomedical Image Segmentation","dateModified":"0356-01-02T00:00:00-05:17","datePublished":"0355-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Agostina J. Larrazabal, César Martínez, Jose Dolz, Enzo Ferrante Abstract Despite the astonishing performance of deep-learning based approaches for visual tasks such as semantic segmentation, they are known to produce miscalibrated predictions, which could be harmful for critical decision-making processes. Ensemble learning has shown to not only boost the performance of individual models but also reduce their miscalibration by averaging the independent predictions. In this scenario, model diversity has become a key factor, which facilitates individual models converging to different functional solutions. In this work, we introduce Orthogonal Ensemble Networks (OEN), a novel framework to explicitly enforce model diversity by means of orthogonal constraints. The proposed method is based on the hypothesis that inducing orthogonality among the constituents of the ensemble will increase the overall model diversity. We resort to a new pairwise orthogonality constraint which can be used to regularize a sequential ensemble training process, resulting on improved predictive performance and better calibrated model outputs. We benchmark the proposed framework in two challenging brain lesion segmentation tasks –brain tumor and white matter hyper-intensity segmentation in MR images. The experimental results show that our approach produces more robust and well-calibrated ensemble models and can deal with challenging tasks in the context of biomedical image segmentation. Link to paper https://doi.org/10.1007/978-3-030-87199-4_56 Link to the code repository https://github.com/agosl/Orthogonal_Ensemble_Networks Link to the dataset(s) https://www.med.upenn.edu/cbica/brats2020/data.html https://wmh.isi.uu.nl/data/ Reviews Review #1 Please describe the contribution of the paper The paper introduces orthogonality constraints and a corresponding learning strategy to improve calibration and segmentation performance of an ensemble of deep neural networks. Orthogonality constraints are used to decorrelate layers within a given model as well as among different base models in an ensemble. Such constraints enable the sequential learning of more diverse (i.e. decorrelated) base models. Experimental evaluation on BRATS 2020 dataset and a WMH dataset demonstrate a clear improvement in calibration and segmentation performance. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. • Relevance: the authors propose a method for improving calibration of deep neural network. Clinical decisions inevitably depend on well-calibrated probabilities. Calibration of deep neural networks is in my opinion still an understudied topic and better methods are desperately needed. • Novelty: the introduction of orthogonality constraints to enforce diversity among base models in an ensemble appears to be sensible and novel. • Breadth: in addition to introducing orthogonality constraints, the authors provide a strategy to train the base models of the ensemble, evaluate their approach on two relevant medical imaging segmentation tasks and chose meaningful baseline methods to experimentally prove their point. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. • Impact on model weights: While the authors provide results on calibration and segmentation performance, an investigation on the impact of the decorrelation constraints on the actual model weights is missing. I would have liked to see a visualization (e.g. in Supplementary Materials) which confirms that the constraints are inducing the desired outcome (decorrelation). (see also detailed comments) Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No code is explicitly provided. However, the proposed meta-learning algorithm is very well explained. Therefore, I think it reasonable to say that based on the paper alone (and maybe some correspondence with the authors) one could easily reproduce the method. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html • From my point of view, the hypothesis presented in this paper consists of two parts: i) orthogonality constraints yield more diverse (i.e. decorrelated) models, ii) diverse models yield better calibration and segmentation performance. The authors clearly succeeded in proving part ii). While orthogonality constraints have been studied previously and their impact of filters has been presented, I would say that a verification of part i) is still necessary (at least provided as Supplementary Material). • Regarding the inter-orthogonality term, I am wondering if the restriction of the computation to a given layer l is sensible (apart from practical reasons). It is not obvious to me if the same “feature” is encoded for a given layer l among multiple models. Therefore, I am curious if it makes sense to compute an orthogonality for models as a whole rather than in a per-layer manner? • The sequential learning of “informed” models reminds me of traditional boosting methods (e.g. AdaBoost). In the case of boosting, the “informing” is conducted via sample weighting. In the case of the author’s methodology, it is conducted via the inter-orthogonality terms. From a theoretical point of view, it would be very interesting to work out if a connection of the author’s methodology with boosting exists. • On the BRATS dataset, the 5-net ensemble with self-orthogonality performed worse than the ensemble with random initialization. I wonder why? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novelty. Breadth of the paper including a very convincing experimental evaluation. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper Deep learning models are well-known for its overconfident predictions, which make them unsuitable for applications that require good probability estimations. A simple solution is to use an ensemble of deep models and average their predictions. But, for an ensemble to work properly, its components need to be decorrelated. The paper proposes a method to enforce the diversity in ensembles of deep neural networks by training the networks sequentially and encouraging that the weights of the filters of each network are mutually orthogonal with the weights of the filters at the same layer of all previously trained networks. While it is unclear if this orthogonality is leading to actual model diversity in the ensembles, it is shown experimentally that the performance of an ensemble improves, sometimes by a large margin, when trained with the proposed orthogonality loss. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. S1. The method is simple, clean and reasonable. The related work section seems relevant. The paper is well written, concise and very clear. S2. While orthogonality losses have been used before, I am not aware of any previous method that applies it to encourage model diversity in ensembles. To the extent of my knowledge, this is a novel contribution. S3. The experimental section is convincing and shows clear advantages of the proposed method. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. I found a few minor issues: W1. The paper does not analyze the complexity of computing the orthogonality loss, O(Nn^2), which seems to be quite high. How long does the training take compared to the training of the random ensemble? W2. The hypothesis of the paper is that the orthogonal constraints will bring diversity to the ensemble. This is not shown or proved. Is the increase in performance a consequence of a more diverse ensemble, or a consequence of better individual models obtained thanks to the orthogonality term? W3. How sensitive is the method to the hyperparameter lambda? A sensitivity analysis would be very helpful, as methods with high sensitivity to the hyperparameters are harder to use in practice. W4. How does the proposed method compare to other methods to produce ensembles of networks, such as monte-carlo dropout or boosting? I miss a comparison to alternative previous methods to clarify where the proposed method stands. W5. No qualitative results shown. W6. It is not completely clear to me how the inter orthogonality loss is applied in the 1-Network case of Figures 1 and 2. If I understood correctly, an ensemble of 10 models is trained and then one of them is randomly selected for evaluation? Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I have no concerns regarding the reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I suggest that authors address my weaknesses to make the paper stronger. For W1, a training time comparison would suffice. For W2, authors could show the variance in the predictions of the components of the ensemble with and without the orthogonal losses. If the hypothesis is correct, the variance of the ensemble should increase when the new losses are used. For W3, a plot of performance vs. lambda would help to understand its influence. For W4, authors should add at least one additional baseline of ensemble of networks. Also, the paper would benefit from some qualitative results (W5) that show examples where the proposed method performs better than baselines, and a few failure cases. Two minor typos: Page 3, end of first paragraph: “…in the performance on the benchmark , “ Remove the space before the comma. Page 8, second paragraph: “less than $1e^{-3}$”. This should be $10^{-3}$ (or $1e-3$, but the former is more elegant). Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I think this is a good paper, and I consider the weaknesses listed above to be minor. Even considering the possibility that a comparison with another baseline (W4) showed the proposed method as inferior, the paper still introduces an elegant and theoretically sound method and presents convincing experiments that will be interesting for the community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. A solid and well demonstrated work that provides an interesting solution to an important problem. Suggestions proposed for additional experiments and improved clarification will certainly be beneficial to the future development of the paper What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback N/A back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Larrazabal, Agostina J.,Martínez, César,Dolz, Jose,Ferrante, Enzo" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Orthogonal Ensemble Networks for Biomedical Image Segmentation</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Image Segmentation"
        class="post-category">
        Image Segmentation
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Neuroimaging - Others"
        class="post-category">
        Clinical applications - Neuroimaging - Others
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Oncology"
        class="post-category">
        Clinical applications - Oncology
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Uncertainty"
        class="post-category">
        Machine Learning - Uncertainty
      </a>
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Larrazabal, Agostina J."
        class="post-tags">
        Larrazabal, Agostina J.
      </a> |  
      
      <a href="kittywong/tags#Martínez, César"
        class="post-tags">
        Martínez, César
      </a> |  
      
      <a href="kittywong/tags#Dolz, Jose"
        class="post-tags">
        Dolz, Jose
      </a> |  
      
      <a href="kittywong/tags#Ferrante, Enzo"
        class="post-tags">
        Ferrante, Enzo
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Agostina J. Larrazabal, César Martínez, Jose Dolz, Enzo Ferrante
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Despite the astonishing performance of deep-learning based approaches for visual tasks such as semantic segmentation, they are known to produce miscalibrated predictions, which could be harmful for critical decision-making processes. Ensemble learning has shown to not only boost the performance of individual models but also reduce their miscalibration by averaging the independent predictions. In this scenario, model diversity has become a key factor, which facilitates individual models converging to different functional solutions. In this work, we introduce Orthogonal Ensemble Networks (OEN), a novel framework to explicitly enforce model diversity by means of orthogonal constraints. The proposed method is based on the hypothesis that inducing orthogonality among the constituents of the ensemble will increase the overall model diversity. We resort to a new pairwise orthogonality constraint which can be used to regularize a sequential ensemble training process, resulting on improved predictive performance and better calibrated model outputs. We benchmark the proposed framework in two challenging brain lesion segmentation tasks –brain tumor and white matter hyper-intensity segmentation in MR images. The experimental results show that our approach produces more robust and well-calibrated ensemble models and can deal with challenging tasks in the context of biomedical image segmentation.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87199-4_56">https://doi.org/10.1007/978-3-030-87199-4_56</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/agosl/Orthogonal_Ensemble_Networks
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>https://www.med.upenn.edu/cbica/brats2020/data.html
https://wmh.isi.uu.nl/data/
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper introduces orthogonality constraints and a corresponding learning strategy to improve calibration and segmentation performance of an ensemble of deep neural networks. Orthogonality constraints are used to decorrelate layers within a given model as well as among different base models in an ensemble. Such constraints enable the sequential learning of more diverse (i.e. decorrelated) base models. Experimental evaluation on BRATS 2020 dataset and a WMH dataset demonstrate a clear improvement in calibration and segmentation performance.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>• Relevance: the authors propose a method for improving calibration of deep neural network. Clinical decisions inevitably depend on well-calibrated probabilities. Calibration of deep neural networks is in my opinion still an understudied topic and better methods are desperately needed.
• Novelty: the introduction of orthogonality constraints to enforce diversity among base models in an ensemble appears to be sensible and novel.
• Breadth: in addition to introducing orthogonality constraints, the authors provide a strategy to train the base models of the ensemble, evaluate their approach on two relevant medical imaging segmentation tasks and chose meaningful baseline methods to experimentally prove their point.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>• Impact on model weights: While the authors provide results on calibration and segmentation performance, an investigation on the impact of the decorrelation constraints on the actual model weights is missing. I would have liked to see a visualization (e.g. in Supplementary Materials) which confirms that the constraints are inducing the desired outcome (decorrelation). (see also detailed comments)</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>No code is explicitly provided. However, the proposed meta-learning algorithm is very well explained. Therefore, I think it reasonable to say that based on the paper alone (and maybe some correspondence with the authors) one could easily reproduce the method.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>• From my point of view, the hypothesis presented in this paper consists of two parts: i) orthogonality constraints yield more diverse (i.e. decorrelated) models, ii) diverse models yield better calibration and segmentation performance. The authors clearly succeeded in proving part ii). While orthogonality constraints have been studied previously and their impact of filters has been presented, I would say that a verification of part i) is still necessary (at least provided as Supplementary Material).
• Regarding the inter-orthogonality term, I am wondering if the restriction of the computation to a given layer l is sensible (apart from practical reasons). It is not obvious to me if the same “feature” is encoded for a given layer l among multiple models. Therefore, I am curious if it makes sense to compute an orthogonality for models as a whole rather than in a per-layer manner?
• The sequential learning of “informed” models reminds me of traditional boosting methods (e.g. AdaBoost). In the case of boosting, the “informing” is conducted via sample weighting. In the case of the author’s methodology, it is conducted via the inter-orthogonality terms. From a theoretical point of view, it would be very interesting to work out if a connection of the author’s methodology with boosting exists.
• On the BRATS dataset, the 5-net ensemble with self-orthogonality performed worse than the ensemble with random initialization. I wonder why?</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Novelty. Breadth of the paper including a very convincing experimental evaluation.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>

      <p>Deep learning models are well-known for its overconfident predictions, which
make them unsuitable for applications that require good probability estimations.
A simple solution is to use an ensemble of deep models and average their
predictions. But, for an ensemble to work properly, its components need to be
decorrelated. The paper proposes a method to enforce the diversity in ensembles
of deep neural networks by training the networks sequentially and encouraging
that the weights of the filters of each network are mutually orthogonal with the
weights of the filters at the same layer of all previously trained networks.
While it is unclear if this orthogonality is leading to actual model diversity
in the ensembles, it is shown experimentally that the performance of an ensemble
improves, sometimes by a large margin, when trained with the proposed
orthogonality loss.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>

      <p>S1. The method is simple, clean and reasonable. The related work section seems
relevant. The paper is well written, concise and very clear.</p>

      <p>S2. While orthogonality losses have been used before, I am not aware of any
previous method that applies it to encourage model diversity in ensembles. To
the extent of my knowledge, this is a novel contribution.</p>

      <p>S3. The experimental section is convincing and shows clear advantages of the
proposed method.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>

      <p>I found a few minor issues:</p>

      <p>W1. The paper does not analyze the complexity of computing the orthogonality
loss, O(Nn^2), which seems to be quite high. How long does the training take
compared to the training of the random ensemble?</p>

      <p>W2. The hypothesis of the paper is that the orthogonal constraints will bring
diversity to the ensemble. This is not shown or proved. Is the increase in
performance a consequence of a more diverse ensemble, or a consequence of better
individual models obtained thanks to the orthogonality term?</p>

      <p>W3. How sensitive is the method to the hyperparameter lambda? A sensitivity
analysis would be very helpful, as methods with high sensitivity to the
hyperparameters are harder to use in practice.</p>

      <p>W4. How does the proposed method compare to other methods to produce ensembles
of networks, such as monte-carlo dropout or boosting? I miss a comparison to
alternative previous methods to clarify where the proposed method stands.</p>

      <p>W5. No qualitative results shown.</p>

      <p>W6. It is not completely clear to me how the inter orthogonality loss is applied
in the 1-Network case of Figures 1 and 2. If I understood correctly, an ensemble
of 10 models is trained and then one of them is randomly selected for
evaluation?</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>

      <p>I have no concerns regarding the reproducibility.</p>

    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>

      <p>I suggest that authors address my weaknesses to make the paper stronger. For W1,
a training time comparison would suffice. For W2, authors could show the
variance in the predictions of the components of the ensemble with and without
the orthogonal losses. If the hypothesis is correct, the variance of the
ensemble should increase when the new losses are used. For W3, a plot of
performance vs. lambda would help to understand its influence. For W4, authors
should add at least one additional baseline of ensemble of networks. Also, the
paper would benefit from some qualitative results (W5) that show examples where
the proposed method performs better than baselines, and a few failure cases.</p>

      <p>Two minor typos:</p>

      <ol>
        <li>
          <p>Page 3, end of first paragraph: “…in the performance on the benchmark , “
Remove the space before the comma.</p>
        </li>
        <li>
          <p>Page 8, second paragraph: “less than $1e^{-3}$”. This should be $10^{-3}$ (or
$1e-3$, but the former is more elegant).</p>
        </li>
      </ol>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>

      <p>I think this is a good paper, and I consider the weaknesses listed above to be
minor. Even considering the possibility that a comparison with another baseline
(W4) showed the proposed method as inferior, the paper still introduces an
elegant and theoretically sound method and presents convincing experiments that
will be interesting for the community.</p>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>A solid and well demonstrated work that provides an interesting solution to an important problem. Suggestions proposed for additional experiments and improved clarification will certainly be beneficial to the future development of the paper</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>N/A</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0355-12-31
      -->
      <!--
      
        ,
        updated at 
        0356-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Image Segmentation"
        class="post-category">
        Image Segmentation
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Neuroimaging - Others"
        class="post-category">
        Clinical applications - Neuroimaging - Others
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Oncology"
        class="post-category">
        Clinical applications - Oncology
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Uncertainty"
        class="post-category">
        Machine Learning - Uncertainty
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Larrazabal, Agostina J."
        class="post-category">
        Larrazabal, Agostina J.
      </a> |  
      
      <a href="kittywong/tags#Martínez, César"
        class="post-category">
        Martínez, César
      </a> |  
      
      <a href="kittywong/tags#Dolz, Jose"
        class="post-category">
        Dolz, Jose
      </a> |  
      
      <a href="kittywong/tags#Ferrante, Enzo"
        class="post-category">
        Ferrante, Enzo
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0356/12/31/Paper1009">
          Learning to Predict Error for MRI Reconstruction
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0354/12/31/Paper0611">
          Confidence-aware Cascaded Network for Fetal Brain Segmentation on MR Images
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
