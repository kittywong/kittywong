<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Flip Learning: Erase to Segment | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Flip Learning: Erase to Segment" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Yuhao Huang, Xin Yang, Yuxin Zou, Chaoyu Chen, Jian Wang, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Jianqiao Zhou, Dong Ni Abstract Nodule segmentation from breast ultrasound images is challenging yet essential for the diagnosis. Weakly-supervised segmentation (WSS) can help reduce time-consuming and cumbersome manual annotation. Unlike existing weakly-supervised approaches, in this study, we propose a novel and general WSS framework called Flip Learning, which only needs the box annotation. Specifically, the target in the label box will be erased gradually to flip the classification tag, and the erased region will be considered as the segmentation result finally. Our contribution is three-fold. First, our proposed approach erases superpixel level using a Multi-agent Reinforcement Learning framework to exploit the prior boundary knowledge and accelerate the learning process. Second, we design two rewards: classification score and intensity distribution reward, to avoid under- and over-segmentation, respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the residual errors and improve the segmentation performance. Extensively validated on a large dataset, our proposed approach achieves competitive performance and shows great potential to narrow the gap between fully-supervised and weakly-supervised learning. Link to paper https://doi.org/10.1007/978-3-030-87193-2_47 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors propose a novel and general Flip Learning framework for WSS based on BBox. According to the authors, the contribution is three-fold. First, the erasing process via Multi-agent Reinforcement Learning (MARL) is based on superpixels, which can capture the prior boundary information and improve learning efficiency. Second, the design of two rewards for guiding the agents accurately. Specifically, the classification score reward (CSR) is used for pushing the agents’ erasing for label flipping, while the intensity distribution reward (IDR) is employed to limit the over-segmentation. Third, the use of a coarse- to-fine (C2F) strategy to simplify agents’ learning for residuals decreasing and segmentation performance improvement. Validation experiments demonstrated that the proposed Flip Learning framework could achieve high accuracy in the nodule segmentation task. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Problem is very relevant in the scientific community and the experimental framework seems robust Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The dataset used in the experiments is not public, thus the reproducibility is not so straigthforward Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Private dataset without comment about making it free and public Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Authors should revise Table 1 and comments about it. It is confusing to have U-net results in the same table and say that their approach is better (there is contradiction in table 1) Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed method is not clearly explained, even when the experimental frameworks seems to be carefully designed. It is not easy to follow and read the whole article. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper proposed a weakly supervised algorithm for image segmentation using classification and bounding box annotations. An image classifier was trained. Then a segmentation agent was trained such that it flipped the classifier’s prediction by filling a region with the background. The training of the segmentation used reinforcement learning, where the action was to fill a superpixel and the reward was the flip of prediction. The authors further improved the performance by using two agents to fill superpixels simultaneously; penalizing intensity distribution change; adding a stage with finer superpixels. The experiments on 2D US images outperformed other weakly supervised methods and matched fully supervised methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The flip learning with reinforcement learning (RL) idea is innovative. One of the common problems in RL is the definition of reward as an agent might learn to cheat. Using a trained classifier and the intensity distribution different, the proposed rewards are meaningful and simple. The results are strong. Without pixel-level annotations, the proposed method reached similar performance as a fully supervised segmentation network. The proposed method largely outperformed other weakly supervised methods. The clinical value is great. This method only requires class and bounding box annotations, instead of precise mask labels. It largely reduced annotation cost. This method is not specific to a particular image type, thus potentially useful for other modalities. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. It is not clear if the evaluation is on a separate subset. This would impact the entire paper. The trained image classifier should use the same training data as the segmentation agent. The evaluation of the segmentation agent should be on a separate dataset, containing no training data. Although reinforcement learning often train and test on the same environments, for segmentation, the evaluation should not be on a training set. If the evaluation is on the training set, then the obtained results could be not credible. Segmentation on unseen data is less efficient compared to end-to-end methods. First, it requires binary label and bounding box annotation. Second, the prediction is performed via reinforcement learning, therefore the agent has to go through superpixels sequentially. The prediction is limited by the quality of superpixels. Superpixels may have coarse boundaries, making the predicted mask unnatural. The proposed background calculation is limited, as mentioned in the conclusion. Linearly mixing neighbor patches may not be meaningful. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The data will not be accessible. Models were not well explained. The values used for hyper-parameter N and beta for reinforcement learning terminal signals were not given. The architectures for the image classifier and segmentation agent were not given. Evaluation process was not well explained. It is unclear if the authors evaluated on a separate test data set. Although the authors claimed to provide the details of train / validation / test splits in the checklist. The architecture of the fully supervised U-Net was not given. Code would be released according to checklist. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html On page 4, agents, It would be better to explain further on the multi-agent training, for instance: Did these two agents share a same network? How the superpixels were assigned to the agents, randomly or left and right? How did the agent traverse the superpixels, randomly or with a heuristic order? The order might affect the final predicted shape. On page 6, section 3, it would be clearer if the authors could briefly state what types of labels did other weakly supervised method use. If other methods did not use the bounding box annotations, then it is expected that these methods performed worse. If other methods did use the bounding box annotations, then this makes the results stronger. Minor: On page 3, the definition of w_f and w_g were not explained, given a bounding box, how could we define these two values? On page 6, section 3, please cite the white paper of PyTorch https://arxiv.org/abs/1912.01703. On page 7, table 1, the proposed method was not better than U-Net (the supervised baseline), although marked by blue color. It would be better to clarify in the title that it was the best among weakly supervised methods, and it matched the U-Net. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed flip learning idea with multi-agent reinforcement learning is innovative and simple. However, important details such as dataset split are missing, reducing the credibility of the results significantly. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper proposes a novel and general Flip Learning framework for weakly-supervised segmentation based on BBox. The proposed erasing process via Multi-agent Reinforcement Learning is based on superpixels, capturing the prior boundary information and improving learning efficiency. Two rewards for guiding the agents accurately are designed for pushing the agents ’ erasing for label flipping, and the intensity distribution reward is employed to limit the over-segmentation. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is well-written and easy to follow. The proposed method is novel and interesting. The experimental results seem to be convincing. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The whole algorithm is too complex to be entirely described within an 8-page paper, so it would be better to provide more details, such as making the source codes available after acceptance. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No source code is provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see “the main weaknesses”. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall, it is a good paper and my rating is “accept”. My only concern is that the implementation details are not perfectly elaborated within an 8-page paper. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Two reviewers recommend acceptance of this paper and the third one recommends rejection. While all reviewers agree on the novelty and interest of the technical contribution, they all express serious doubts about the experimental setup. In particular, reviewers are concerned because the dataset in which experiments are conducted is not public and hence the experimental framework and the results cannot be independently verified. The rebuttal should address this points thoroughly and should also clarify the description of the model (R1, R2). What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The rebuttal adequately addresses all concerns pointed out by the reviewers. The availability of public code and data significantly increases the value of this work for the MICCAI community. The final version should include the comments made by the reviewers. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors have thoroughly responded to the comments of the authors. Specifically, they have commented on the data-split of the experimentation which was unclear and potentially conclusion-changing. In addition, the authors have made a considerable effort detailing how their method will be reproducable. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper has good novelty acknowledged by all reviewers, but some complaints about its reproducibility. With the statement in the rebuttal on publicly releasing all code, model and dataset, this should largely resolve the reproducibility issue. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Author Feedback We provide explanations to address the main concerns of the reviewers and will improve the writing as suggested. Q1. Dataset release. (R2, R3) We have discussed with our cooperating hospitals about the dataset release affairs. With their support, we have applied to the IRB. After obtaining the IRB approval, we will release it as the first open-access breast ultrasound MICCAI Challenge. Q2. Code release. (R3, R5) After acceptance, we will release the source code at (anonymized) https://github.com/miccai-1545/flip-learning. We have uploaded demos, testing software and images to this repository. We believe that this can help improve the reproducibility of the paper. Q3. Dataset division. (R2, R3) We apologize for the missing description of the dataset split. For experiments, we carefully keep the consistency of dataset division in both classification and segmentation tasks. The dataset was split into 1278, 100 and 345 images for training, validation and independent testing at the patient level with no overlap. The classifier and agents used the same training, validation and testing set. Q4. Models and the details of agents. (R2, R3, R5) We provide more details for clearer explanation. First, the architecture for both the classifier and agents is ResNet18. Two agents share the parameters in the convolution layers for knowledge sharing, and have independent fully connected layers for decision-making. Second, all superpixels are indexed from 1 to S by the OpenCV function from the top to bottom, from left to right. Both agents start from the center superpixel with index S/2. One agent traverses the superpixels from S/2 to S, the other one traverses reversely from index S/2 to 1. The traverse order has limited impact on our system since the agents take the whole BBox region as the environment. The order we adopted is preferred for simplicity. Third, for agents’ terminal signals, we set N and beta as 2 and 0.05, respectively. Q5. Comparison experiments /Table 1 (R2, R3) In Table 1, we compared both the fully- (U-net with architecture provided in [10]) and weakly-supervised segmentation (WSS) methods. The result of U-net is referred to compare these two types of methods intuitively. We will change the caption of Table 1 to “The best results of WSS methods are shown in blue” to remove any confusion. Besides, all the other compared WSS methods use the same BBox annotations for labels as our method uses, thus we consider the comparisons are fair. Q6. Model efficiency. (R3) We need to make it clear that our method only needs BBox annotations, which indicate nodule existence. By using object detection model in our future work, manual BBox annotation will be further discarded through automatic localization to improve efficiency. We agree that the RL-based method is not as efficient as the end-to-end ones. In this regard, we introduced superpixel and multi-agent strategies, which significantly accelerate the inference by more than ten times compared with the pixel-level and single-agent approaches. We will focus on simplifying the RL-based segmentation in the future. Q7. Quality and assignment of superpixels. (R3) To generate high-quality superpixels, we have tried several advanced methods, including SEEDS, SLIC, MSLIC, SLICO, LSC, etc. Based on our tasks, SEEDS is the superior one. Besides, we also adopted a coarse-to-fine strategy to generate fine superpixels, making the predicted segmentation as accurate as possible. Approximately half of the superpixels were assigned to each agent based on the center index (Details refer to Q4). Q8. Background calculation and parameter definition. (R3) The background calculation method we proposed is tractable, simple and efficient for our task. Given a BBox, both the w_f and w_g are defined as equal to the width of BBox. In the future, we will adopt advanced methods (e.g., GAN) for better and general background generation. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Yuhao Huang, Xin Yang, Yuxin Zou, Chaoyu Chen, Jian Wang, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Jianqiao Zhou, Dong Ni Abstract Nodule segmentation from breast ultrasound images is challenging yet essential for the diagnosis. Weakly-supervised segmentation (WSS) can help reduce time-consuming and cumbersome manual annotation. Unlike existing weakly-supervised approaches, in this study, we propose a novel and general WSS framework called Flip Learning, which only needs the box annotation. Specifically, the target in the label box will be erased gradually to flip the classification tag, and the erased region will be considered as the segmentation result finally. Our contribution is three-fold. First, our proposed approach erases superpixel level using a Multi-agent Reinforcement Learning framework to exploit the prior boundary knowledge and accelerate the learning process. Second, we design two rewards: classification score and intensity distribution reward, to avoid under- and over-segmentation, respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the residual errors and improve the segmentation performance. Extensively validated on a large dataset, our proposed approach achieves competitive performance and shows great potential to narrow the gap between fully-supervised and weakly-supervised learning. Link to paper https://doi.org/10.1007/978-3-030-87193-2_47 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors propose a novel and general Flip Learning framework for WSS based on BBox. According to the authors, the contribution is three-fold. First, the erasing process via Multi-agent Reinforcement Learning (MARL) is based on superpixels, which can capture the prior boundary information and improve learning efficiency. Second, the design of two rewards for guiding the agents accurately. Specifically, the classification score reward (CSR) is used for pushing the agents’ erasing for label flipping, while the intensity distribution reward (IDR) is employed to limit the over-segmentation. Third, the use of a coarse- to-fine (C2F) strategy to simplify agents’ learning for residuals decreasing and segmentation performance improvement. Validation experiments demonstrated that the proposed Flip Learning framework could achieve high accuracy in the nodule segmentation task. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Problem is very relevant in the scientific community and the experimental framework seems robust Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The dataset used in the experiments is not public, thus the reproducibility is not so straigthforward Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Private dataset without comment about making it free and public Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Authors should revise Table 1 and comments about it. It is confusing to have U-net results in the same table and say that their approach is better (there is contradiction in table 1) Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed method is not clearly explained, even when the experimental frameworks seems to be carefully designed. It is not easy to follow and read the whole article. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper proposed a weakly supervised algorithm for image segmentation using classification and bounding box annotations. An image classifier was trained. Then a segmentation agent was trained such that it flipped the classifier’s prediction by filling a region with the background. The training of the segmentation used reinforcement learning, where the action was to fill a superpixel and the reward was the flip of prediction. The authors further improved the performance by using two agents to fill superpixels simultaneously; penalizing intensity distribution change; adding a stage with finer superpixels. The experiments on 2D US images outperformed other weakly supervised methods and matched fully supervised methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The flip learning with reinforcement learning (RL) idea is innovative. One of the common problems in RL is the definition of reward as an agent might learn to cheat. Using a trained classifier and the intensity distribution different, the proposed rewards are meaningful and simple. The results are strong. Without pixel-level annotations, the proposed method reached similar performance as a fully supervised segmentation network. The proposed method largely outperformed other weakly supervised methods. The clinical value is great. This method only requires class and bounding box annotations, instead of precise mask labels. It largely reduced annotation cost. This method is not specific to a particular image type, thus potentially useful for other modalities. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. It is not clear if the evaluation is on a separate subset. This would impact the entire paper. The trained image classifier should use the same training data as the segmentation agent. The evaluation of the segmentation agent should be on a separate dataset, containing no training data. Although reinforcement learning often train and test on the same environments, for segmentation, the evaluation should not be on a training set. If the evaluation is on the training set, then the obtained results could be not credible. Segmentation on unseen data is less efficient compared to end-to-end methods. First, it requires binary label and bounding box annotation. Second, the prediction is performed via reinforcement learning, therefore the agent has to go through superpixels sequentially. The prediction is limited by the quality of superpixels. Superpixels may have coarse boundaries, making the predicted mask unnatural. The proposed background calculation is limited, as mentioned in the conclusion. Linearly mixing neighbor patches may not be meaningful. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The data will not be accessible. Models were not well explained. The values used for hyper-parameter N and beta for reinforcement learning terminal signals were not given. The architectures for the image classifier and segmentation agent were not given. Evaluation process was not well explained. It is unclear if the authors evaluated on a separate test data set. Although the authors claimed to provide the details of train / validation / test splits in the checklist. The architecture of the fully supervised U-Net was not given. Code would be released according to checklist. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html On page 4, agents, It would be better to explain further on the multi-agent training, for instance: Did these two agents share a same network? How the superpixels were assigned to the agents, randomly or left and right? How did the agent traverse the superpixels, randomly or with a heuristic order? The order might affect the final predicted shape. On page 6, section 3, it would be clearer if the authors could briefly state what types of labels did other weakly supervised method use. If other methods did not use the bounding box annotations, then it is expected that these methods performed worse. If other methods did use the bounding box annotations, then this makes the results stronger. Minor: On page 3, the definition of w_f and w_g were not explained, given a bounding box, how could we define these two values? On page 6, section 3, please cite the white paper of PyTorch https://arxiv.org/abs/1912.01703. On page 7, table 1, the proposed method was not better than U-Net (the supervised baseline), although marked by blue color. It would be better to clarify in the title that it was the best among weakly supervised methods, and it matched the U-Net. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed flip learning idea with multi-agent reinforcement learning is innovative and simple. However, important details such as dataset split are missing, reducing the credibility of the results significantly. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper proposes a novel and general Flip Learning framework for weakly-supervised segmentation based on BBox. The proposed erasing process via Multi-agent Reinforcement Learning is based on superpixels, capturing the prior boundary information and improving learning efficiency. Two rewards for guiding the agents accurately are designed for pushing the agents ’ erasing for label flipping, and the intensity distribution reward is employed to limit the over-segmentation. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is well-written and easy to follow. The proposed method is novel and interesting. The experimental results seem to be convincing. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The whole algorithm is too complex to be entirely described within an 8-page paper, so it would be better to provide more details, such as making the source codes available after acceptance. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No source code is provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see “the main weaknesses”. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall, it is a good paper and my rating is “accept”. My only concern is that the implementation details are not perfectly elaborated within an 8-page paper. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Two reviewers recommend acceptance of this paper and the third one recommends rejection. While all reviewers agree on the novelty and interest of the technical contribution, they all express serious doubts about the experimental setup. In particular, reviewers are concerned because the dataset in which experiments are conducted is not public and hence the experimental framework and the results cannot be independently verified. The rebuttal should address this points thoroughly and should also clarify the description of the model (R1, R2). What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The rebuttal adequately addresses all concerns pointed out by the reviewers. The availability of public code and data significantly increases the value of this work for the MICCAI community. The final version should include the comments made by the reviewers. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors have thoroughly responded to the comments of the authors. Specifically, they have commented on the data-split of the experimentation which was unclear and potentially conclusion-changing. In addition, the authors have made a considerable effort detailing how their method will be reproducable. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper has good novelty acknowledged by all reviewers, but some complaints about its reproducibility. With the statement in the rebuttal on publicly releasing all code, model and dataset, this should largely resolve the reproducibility issue. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Author Feedback We provide explanations to address the main concerns of the reviewers and will improve the writing as suggested. Q1. Dataset release. (R2, R3) We have discussed with our cooperating hospitals about the dataset release affairs. With their support, we have applied to the IRB. After obtaining the IRB approval, we will release it as the first open-access breast ultrasound MICCAI Challenge. Q2. Code release. (R3, R5) After acceptance, we will release the source code at (anonymized) https://github.com/miccai-1545/flip-learning. We have uploaded demos, testing software and images to this repository. We believe that this can help improve the reproducibility of the paper. Q3. Dataset division. (R2, R3) We apologize for the missing description of the dataset split. For experiments, we carefully keep the consistency of dataset division in both classification and segmentation tasks. The dataset was split into 1278, 100 and 345 images for training, validation and independent testing at the patient level with no overlap. The classifier and agents used the same training, validation and testing set. Q4. Models and the details of agents. (R2, R3, R5) We provide more details for clearer explanation. First, the architecture for both the classifier and agents is ResNet18. Two agents share the parameters in the convolution layers for knowledge sharing, and have independent fully connected layers for decision-making. Second, all superpixels are indexed from 1 to S by the OpenCV function from the top to bottom, from left to right. Both agents start from the center superpixel with index S/2. One agent traverses the superpixels from S/2 to S, the other one traverses reversely from index S/2 to 1. The traverse order has limited impact on our system since the agents take the whole BBox region as the environment. The order we adopted is preferred for simplicity. Third, for agents’ terminal signals, we set N and beta as 2 and 0.05, respectively. Q5. Comparison experiments /Table 1 (R2, R3) In Table 1, we compared both the fully- (U-net with architecture provided in [10]) and weakly-supervised segmentation (WSS) methods. The result of U-net is referred to compare these two types of methods intuitively. We will change the caption of Table 1 to “The best results of WSS methods are shown in blue” to remove any confusion. Besides, all the other compared WSS methods use the same BBox annotations for labels as our method uses, thus we consider the comparisons are fair. Q6. Model efficiency. (R3) We need to make it clear that our method only needs BBox annotations, which indicate nodule existence. By using object detection model in our future work, manual BBox annotation will be further discarded through automatic localization to improve efficiency. We agree that the RL-based method is not as efficient as the end-to-end ones. In this regard, we introduced superpixel and multi-agent strategies, which significantly accelerate the inference by more than ten times compared with the pixel-level and single-agent approaches. We will focus on simplifying the RL-based segmentation in the future. Q7. Quality and assignment of superpixels. (R3) To generate high-quality superpixels, we have tried several advanced methods, including SEEDS, SLIC, MSLIC, SLICO, LSC, etc. Based on our tasks, SEEDS is the superior one. Besides, we also adopted a coarse-to-fine strategy to generate fine superpixels, making the predicted segmentation as accurate as possible. Approximately half of the superpixels were assigned to each agent based on the center index (Details refer to Q4). Q8. Background calculation and parameter definition. (R3) The background calculation method we proposed is tractable, simple and efficient for our task. Given a BBox, both the w_f and w_g are defined as equal to the width of BBox. In the future, we will adopt advanced methods (e.g., GAN) for better and general background generation. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0146/12/31/Paper1545" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0146/12/31/Paper1545" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0146-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Flip Learning: Erase to Segment" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0146/12/31/Paper1545"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0146/12/31/Paper1545","headline":"Flip Learning: Erase to Segment","dateModified":"0146-12-31T00:00:00-05:17","datePublished":"0146-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Yuhao Huang, Xin Yang, Yuxin Zou, Chaoyu Chen, Jian Wang, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Jianqiao Zhou, Dong Ni Abstract Nodule segmentation from breast ultrasound images is challenging yet essential for the diagnosis. Weakly-supervised segmentation (WSS) can help reduce time-consuming and cumbersome manual annotation. Unlike existing weakly-supervised approaches, in this study, we propose a novel and general WSS framework called Flip Learning, which only needs the box annotation. Specifically, the target in the label box will be erased gradually to flip the classification tag, and the erased region will be considered as the segmentation result finally. Our contribution is three-fold. First, our proposed approach erases superpixel level using a Multi-agent Reinforcement Learning framework to exploit the prior boundary knowledge and accelerate the learning process. Second, we design two rewards: classification score and intensity distribution reward, to avoid under- and over-segmentation, respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the residual errors and improve the segmentation performance. Extensively validated on a large dataset, our proposed approach achieves competitive performance and shows great potential to narrow the gap between fully-supervised and weakly-supervised learning. Link to paper https://doi.org/10.1007/978-3-030-87193-2_47 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors propose a novel and general Flip Learning framework for WSS based on BBox. According to the authors, the contribution is three-fold. First, the erasing process via Multi-agent Reinforcement Learning (MARL) is based on superpixels, which can capture the prior boundary information and improve learning efficiency. Second, the design of two rewards for guiding the agents accurately. Specifically, the classification score reward (CSR) is used for pushing the agents’ erasing for label flipping, while the intensity distribution reward (IDR) is employed to limit the over-segmentation. Third, the use of a coarse- to-fine (C2F) strategy to simplify agents’ learning for residuals decreasing and segmentation performance improvement. Validation experiments demonstrated that the proposed Flip Learning framework could achieve high accuracy in the nodule segmentation task. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Problem is very relevant in the scientific community and the experimental framework seems robust Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The dataset used in the experiments is not public, thus the reproducibility is not so straigthforward Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Private dataset without comment about making it free and public Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Authors should revise Table 1 and comments about it. It is confusing to have U-net results in the same table and say that their approach is better (there is contradiction in table 1) Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed method is not clearly explained, even when the experimental frameworks seems to be carefully designed. It is not easy to follow and read the whole article. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper proposed a weakly supervised algorithm for image segmentation using classification and bounding box annotations. An image classifier was trained. Then a segmentation agent was trained such that it flipped the classifier’s prediction by filling a region with the background. The training of the segmentation used reinforcement learning, where the action was to fill a superpixel and the reward was the flip of prediction. The authors further improved the performance by using two agents to fill superpixels simultaneously; penalizing intensity distribution change; adding a stage with finer superpixels. The experiments on 2D US images outperformed other weakly supervised methods and matched fully supervised methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The flip learning with reinforcement learning (RL) idea is innovative. One of the common problems in RL is the definition of reward as an agent might learn to cheat. Using a trained classifier and the intensity distribution different, the proposed rewards are meaningful and simple. The results are strong. Without pixel-level annotations, the proposed method reached similar performance as a fully supervised segmentation network. The proposed method largely outperformed other weakly supervised methods. The clinical value is great. This method only requires class and bounding box annotations, instead of precise mask labels. It largely reduced annotation cost. This method is not specific to a particular image type, thus potentially useful for other modalities. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. It is not clear if the evaluation is on a separate subset. This would impact the entire paper. The trained image classifier should use the same training data as the segmentation agent. The evaluation of the segmentation agent should be on a separate dataset, containing no training data. Although reinforcement learning often train and test on the same environments, for segmentation, the evaluation should not be on a training set. If the evaluation is on the training set, then the obtained results could be not credible. Segmentation on unseen data is less efficient compared to end-to-end methods. First, it requires binary label and bounding box annotation. Second, the prediction is performed via reinforcement learning, therefore the agent has to go through superpixels sequentially. The prediction is limited by the quality of superpixels. Superpixels may have coarse boundaries, making the predicted mask unnatural. The proposed background calculation is limited, as mentioned in the conclusion. Linearly mixing neighbor patches may not be meaningful. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The data will not be accessible. Models were not well explained. The values used for hyper-parameter N and beta for reinforcement learning terminal signals were not given. The architectures for the image classifier and segmentation agent were not given. Evaluation process was not well explained. It is unclear if the authors evaluated on a separate test data set. Although the authors claimed to provide the details of train / validation / test splits in the checklist. The architecture of the fully supervised U-Net was not given. Code would be released according to checklist. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html On page 4, agents, It would be better to explain further on the multi-agent training, for instance: Did these two agents share a same network? How the superpixels were assigned to the agents, randomly or left and right? How did the agent traverse the superpixels, randomly or with a heuristic order? The order might affect the final predicted shape. On page 6, section 3, it would be clearer if the authors could briefly state what types of labels did other weakly supervised method use. If other methods did not use the bounding box annotations, then it is expected that these methods performed worse. If other methods did use the bounding box annotations, then this makes the results stronger. Minor: On page 3, the definition of w_f and w_g were not explained, given a bounding box, how could we define these two values? On page 6, section 3, please cite the white paper of PyTorch https://arxiv.org/abs/1912.01703. On page 7, table 1, the proposed method was not better than U-Net (the supervised baseline), although marked by blue color. It would be better to clarify in the title that it was the best among weakly supervised methods, and it matched the U-Net. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed flip learning idea with multi-agent reinforcement learning is innovative and simple. However, important details such as dataset split are missing, reducing the credibility of the results significantly. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper proposes a novel and general Flip Learning framework for weakly-supervised segmentation based on BBox. The proposed erasing process via Multi-agent Reinforcement Learning is based on superpixels, capturing the prior boundary information and improving learning efficiency. Two rewards for guiding the agents accurately are designed for pushing the agents ’ erasing for label flipping, and the intensity distribution reward is employed to limit the over-segmentation. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is well-written and easy to follow. The proposed method is novel and interesting. The experimental results seem to be convincing. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The whole algorithm is too complex to be entirely described within an 8-page paper, so it would be better to provide more details, such as making the source codes available after acceptance. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No source code is provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see “the main weaknesses”. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall, it is a good paper and my rating is “accept”. My only concern is that the implementation details are not perfectly elaborated within an 8-page paper. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Two reviewers recommend acceptance of this paper and the third one recommends rejection. While all reviewers agree on the novelty and interest of the technical contribution, they all express serious doubts about the experimental setup. In particular, reviewers are concerned because the dataset in which experiments are conducted is not public and hence the experimental framework and the results cannot be independently verified. The rebuttal should address this points thoroughly and should also clarify the description of the model (R1, R2). What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The rebuttal adequately addresses all concerns pointed out by the reviewers. The availability of public code and data significantly increases the value of this work for the MICCAI community. The final version should include the comments made by the reviewers. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors have thoroughly responded to the comments of the authors. Specifically, they have commented on the data-split of the experimentation which was unclear and potentially conclusion-changing. In addition, the authors have made a considerable effort detailing how their method will be reproducable. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper has good novelty acknowledged by all reviewers, but some complaints about its reproducibility. With the statement in the rebuttal on publicly releasing all code, model and dataset, this should largely resolve the reproducibility issue. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Author Feedback We provide explanations to address the main concerns of the reviewers and will improve the writing as suggested. Q1. Dataset release. (R2, R3) We have discussed with our cooperating hospitals about the dataset release affairs. With their support, we have applied to the IRB. After obtaining the IRB approval, we will release it as the first open-access breast ultrasound MICCAI Challenge. Q2. Code release. (R3, R5) After acceptance, we will release the source code at (anonymized) https://github.com/miccai-1545/flip-learning. We have uploaded demos, testing software and images to this repository. We believe that this can help improve the reproducibility of the paper. Q3. Dataset division. (R2, R3) We apologize for the missing description of the dataset split. For experiments, we carefully keep the consistency of dataset division in both classification and segmentation tasks. The dataset was split into 1278, 100 and 345 images for training, validation and independent testing at the patient level with no overlap. The classifier and agents used the same training, validation and testing set. Q4. Models and the details of agents. (R2, R3, R5) We provide more details for clearer explanation. First, the architecture for both the classifier and agents is ResNet18. Two agents share the parameters in the convolution layers for knowledge sharing, and have independent fully connected layers for decision-making. Second, all superpixels are indexed from 1 to S by the OpenCV function from the top to bottom, from left to right. Both agents start from the center superpixel with index S/2. One agent traverses the superpixels from S/2 to S, the other one traverses reversely from index S/2 to 1. The traverse order has limited impact on our system since the agents take the whole BBox region as the environment. The order we adopted is preferred for simplicity. Third, for agents’ terminal signals, we set N and beta as 2 and 0.05, respectively. Q5. Comparison experiments /Table 1 (R2, R3) In Table 1, we compared both the fully- (U-net with architecture provided in [10]) and weakly-supervised segmentation (WSS) methods. The result of U-net is referred to compare these two types of methods intuitively. We will change the caption of Table 1 to “The best results of WSS methods are shown in blue” to remove any confusion. Besides, all the other compared WSS methods use the same BBox annotations for labels as our method uses, thus we consider the comparisons are fair. Q6. Model efficiency. (R3) We need to make it clear that our method only needs BBox annotations, which indicate nodule existence. By using object detection model in our future work, manual BBox annotation will be further discarded through automatic localization to improve efficiency. We agree that the RL-based method is not as efficient as the end-to-end ones. In this regard, we introduced superpixel and multi-agent strategies, which significantly accelerate the inference by more than ten times compared with the pixel-level and single-agent approaches. We will focus on simplifying the RL-based segmentation in the future. Q7. Quality and assignment of superpixels. (R3) To generate high-quality superpixels, we have tried several advanced methods, including SEEDS, SLIC, MSLIC, SLICO, LSC, etc. Based on our tasks, SEEDS is the superior one. Besides, we also adopted a coarse-to-fine strategy to generate fine superpixels, making the predicted segmentation as accurate as possible. Approximately half of the superpixels were assigned to each agent based on the center index (Details refer to Q4). Q8. Background calculation and parameter definition. (R3) The background calculation method we proposed is tractable, simple and efficient for our task. Given a BBox, both the w_f and w_g are defined as equal to the width of BBox. In the future, we will adopt advanced methods (e.g., GAN) for better and general background generation. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Huang, Yuhao,Yang, Xin,Zou, Yuxin,Chen, Chaoyu,Wang, Jian,Dou, Haoran,Ravikumar, Nishant,Frangi, Alejandro F.,Zhou, Jianqiao,Ni, Dong" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Flip Learning: Erase to Segment</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Image Segmentation"
        class="post-category">
        Image Segmentation
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Reinforcement learning"
        class="post-category">
        Machine Learning - Reinforcement learning
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Weakly supervised learning"
        class="post-category">
        Machine Learning - Weakly supervised learning
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Huang, Yuhao"
        class="post-tags">
        Huang, Yuhao
      </a> |  
      
      <a href="kittywong/tags#Yang, Xin"
        class="post-tags">
        Yang, Xin
      </a> |  
      
      <a href="kittywong/tags#Zou, Yuxin"
        class="post-tags">
        Zou, Yuxin
      </a> |  
      
      <a href="kittywong/tags#Chen, Chaoyu"
        class="post-tags">
        Chen, Chaoyu
      </a> |  
      
      <a href="kittywong/tags#Wang, Jian"
        class="post-tags">
        Wang, Jian
      </a> |  
      
      <a href="kittywong/tags#Dou, Haoran"
        class="post-tags">
        Dou, Haoran
      </a> |  
      
      <a href="kittywong/tags#Ravikumar, Nishant"
        class="post-tags">
        Ravikumar, Nishant
      </a> |  
      
      <a href="kittywong/tags#Frangi, Alejandro F."
        class="post-tags">
        Frangi, Alejandro F.
      </a> |  
      
      <a href="kittywong/tags#Zhou, Jianqiao"
        class="post-tags">
        Zhou, Jianqiao
      </a> |  
      
      <a href="kittywong/tags#Ni, Dong"
        class="post-tags">
        Ni, Dong
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Yuhao Huang, Xin Yang, Yuxin Zou, Chaoyu Chen, Jian Wang, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Jianqiao Zhou, Dong Ni
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Nodule segmentation from breast ultrasound images is challenging yet essential for the diagnosis. Weakly-supervised segmentation (WSS) can help reduce time-consuming and cumbersome manual annotation. Unlike existing weakly-supervised approaches, in this study, we propose a novel and general WSS framework called Flip Learning, which only needs the box annotation. Specifically, the target in the label box will be erased gradually to flip the classification tag, and the erased region will be considered as the segmentation result finally. Our contribution is three-fold. First, our proposed approach erases superpixel level using a Multi-agent Reinforcement Learning framework to exploit the prior boundary knowledge and accelerate the learning process. Second, we design two rewards: classification score and intensity distribution reward, to avoid under- and over-segmentation, respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the residual errors and improve the segmentation performance. Extensively validated on a large dataset, our proposed approach achieves competitive performance and shows great potential to narrow the gap between fully-supervised and weakly-supervised learning.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87193-2_47">https://doi.org/10.1007/978-3-030-87193-2_47</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors propose a novel and general Flip Learning framework for
WSS based on BBox. According to the authors, the contribution is three-fold. First,
the erasing process via Multi-agent Reinforcement Learning (MARL) is based
on superpixels, which can capture the prior boundary information and improve
learning efficiency. Second, the design of two rewards for guiding the agents
accurately. Specifically, the classification score reward (CSR) is used for pushing
the agents’ erasing for label flipping, while the intensity distribution reward
(IDR) is employed to limit the over-segmentation. Third, the use of a coarse-
to-fine (C2F) strategy to simplify agents’ learning for residuals decreasing and
segmentation performance improvement. Validation experiments demonstrated
that the proposed Flip Learning framework could achieve high accuracy in the
nodule segmentation task.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>Problem is very relevant in the scientific community and the experimental framework seems robust</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The dataset used in the experiments is not public, thus the reproducibility is not so straigthforward</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Private dataset without comment about making it free and public</p>

    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Authors should revise Table 1 and comments about it. It is confusing to have U-net results in the same table and say that their approach is better (there is contradiction in table 1)</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline reject (5)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The proposed method is not clearly explained, even when the experimental frameworks seems to be carefully designed. It is not easy to follow and read the whole article.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper proposed a weakly supervised algorithm for image segmentation using classification and bounding box annotations. An image classifier was trained. Then a segmentation agent was trained such that it flipped the classifier’s prediction by filling a region with the background. The training of the segmentation used reinforcement learning, where the action was to fill a superpixel and the reward was the flip of prediction. The authors further improved the performance by using two agents to fill superpixels simultaneously; penalizing intensity distribution change; adding a stage with finer superpixels. The experiments on 2D US images outperformed other weakly supervised methods and matched fully supervised methods.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The flip learning with reinforcement learning (RL) idea is innovative.</p>
      <ul>
        <li>One of the common problems in RL is the definition of reward as an agent might learn to cheat. Using a trained classifier and the intensity distribution different, the proposed rewards are meaningful and simple.</li>
      </ul>

      <p>The results are strong.</p>
      <ul>
        <li>Without pixel-level annotations, the proposed method reached similar performance as a fully supervised segmentation network.</li>
        <li>The proposed method largely outperformed other weakly supervised methods.</li>
      </ul>

      <p>The clinical value is great.</p>
      <ul>
        <li>This method only requires class and bounding box annotations, instead of precise mask labels. It largely reduced annotation cost.</li>
        <li>This method is not specific to a particular image type, thus potentially useful for other modalities.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>It is not clear if the evaluation is on a separate subset. This would impact the entire paper.</p>
      <ul>
        <li>The trained image classifier should use the same training data as the segmentation agent.</li>
        <li>The evaluation of the segmentation agent should be on a separate dataset, containing no training data. Although reinforcement learning often train and test on the same environments, for segmentation, the evaluation should not be on a training set. If the evaluation is on the training set, then the obtained results could be not credible.</li>
      </ul>

      <p>Segmentation on unseen data is less efficient compared to end-to-end methods.</p>
      <ul>
        <li>First, it requires binary label and bounding box annotation.</li>
        <li>Second, the prediction is performed via reinforcement learning, therefore the agent has to go through superpixels sequentially.</li>
      </ul>

      <p>The prediction is limited by the quality of superpixels.</p>
      <ul>
        <li>Superpixels may have coarse boundaries, making the predicted mask unnatural.</li>
      </ul>

      <p>The proposed background calculation is limited, as mentioned in the conclusion.</p>
      <ul>
        <li>Linearly mixing neighbor patches may not be meaningful.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Satisfactory</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <ul>
        <li>The data will not be accessible.</li>
        <li>Models were not well explained. The values used for hyper-parameter N and beta for reinforcement learning terminal signals were not given. The architectures for the image classifier and segmentation agent were not given.</li>
        <li>Evaluation process was not well explained. It is unclear if the authors evaluated on a separate test data set. Although the authors claimed to provide the details of train / validation / test splits in the checklist. The architecture of the fully supervised U-Net was not given.</li>
        <li>Code would be released according to checklist.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>On page 4, agents, It would be better to explain further on the multi-agent training, for instance:</p>
      <ul>
        <li>Did these two agents share a same network?</li>
        <li>How the superpixels were assigned to the agents, randomly or left and right?</li>
        <li>How did the agent traverse the superpixels, randomly or with a heuristic order? The order might affect the final predicted shape.</li>
      </ul>

      <p>On page 6, section 3, it would be clearer if the authors could briefly state what types of labels did other weakly supervised method use.</p>
      <ul>
        <li>If other methods did not use the bounding box annotations, then it is expected that these methods performed worse.</li>
        <li>If other methods did use the bounding box annotations, then this makes the results stronger.</li>
      </ul>

      <p>Minor:</p>
      <ul>
        <li>On page 3, the definition of w_f and w_g were not explained, given a bounding box, how could we define these two values?</li>
        <li>On page 6, section 3, please cite the white paper of PyTorch https://arxiv.org/abs/1912.01703.</li>
        <li>On page 7, table 1, the proposed method was not better than U-Net (the supervised baseline), although marked by blue color. It would be better to clarify in the title that it was the best among weakly supervised methods, and it matched the U-Net.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The proposed flip learning idea with multi-agent reinforcement learning is innovative and simple.
However, important details such as dataset split are missing, reducing the credibility of the results significantly.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper proposes a novel and general Flip Learning framework for weakly-supervised segmentation based on BBox. The proposed erasing process via Multi-agent Reinforcement Learning is based on superpixels, capturing the prior boundary information and improving learning efficiency. Two rewards for guiding the agents accurately are designed for pushing the agents ’ erasing for label flipping, and the intensity distribution reward is employed to limit the over-segmentation.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>The paper is well-written and easy to follow.</li>
        <li>The proposed method is novel and interesting.</li>
        <li>The experimental results seem to be convincing.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The whole algorithm is too complex to be entirely described within an 8-page paper, so it would be better to provide more details, such as making the source codes available after acceptance.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>No source code is provided.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Please see “the main weaknesses”.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Overall, it is a good paper and my rating is “accept”. My only concern is that  the implementation details are not perfectly elaborated within an 8-page paper.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>Two reviewers recommend acceptance of this paper and the third one recommends rejection. While all reviewers agree on the novelty and interest of the technical contribution, they all express serious doubts about the experimental setup. In particular, reviewers are concerned because the dataset in which experiments are conducted is not public and hence the experimental framework and the results cannot be independently verified.  The rebuttal should address this points thoroughly and should also clarify the description of the model (R1, R2).</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The rebuttal adequately addresses all concerns pointed out by the reviewers. The availability of public code and data significantly increases the value of this work for the MICCAI community. The final version should include the comments made by the reviewers.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>8</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="meta-review-2">Meta-Review #2</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The authors have thoroughly responded to the comments of the authors. Specifically, they have commented on the data-split of the experimentation which was unclear and potentially conclusion-changing. In addition, the authors have made a considerable effort detailing how their method will be reproducable.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>
<h2 id="meta-review-3">Meta-Review #3</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>This paper has good novelty acknowledged by all reviewers, but some complaints about its reproducibility. With the statement in the rebuttal on publicly releasing all code, model and dataset, this should largely resolve the reproducibility issue.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>11</p>
    </blockquote>
  </li>
</ul>

<h2><br /><br /></h2>
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We provide explanations to address the main concerns of the reviewers and will improve the writing as suggested.</p>

  <p>Q1. Dataset release. (R2, R3)
We have discussed with our cooperating hospitals about the dataset release affairs. With their support, we have applied to the IRB. After obtaining the IRB approval, we will release it as the first open-access breast ultrasound MICCAI Challenge.</p>

  <p>Q2. Code release. (R3, R5)
After acceptance, we will release the source code at (anonymized) https://github.com/miccai-1545/flip-learning. We have uploaded demos, testing software and images to this repository. We believe that this can help improve the reproducibility of the paper.</p>

  <p>Q3. Dataset division. (R2, R3)
We apologize for the missing description of the dataset split. For experiments, we carefully keep the consistency of dataset division in both classification and segmentation tasks. The dataset was split into 1278, 100 and 345 images for training, validation and independent testing at the patient level with no overlap. The classifier and agents used the same training, validation and testing set.</p>

  <p>Q4. Models and the details of agents. (R2, R3, R5)
We provide more details for clearer explanation. First, the architecture for both the classifier and agents is ResNet18. Two agents share the parameters in the convolution layers for knowledge sharing, and have independent fully connected layers for decision-making. Second, all superpixels are indexed from 1 to S by the OpenCV function from the top to bottom, from left to right. Both agents start from the center superpixel with index S/2. One agent traverses the superpixels from S/2 to S, the other one traverses reversely from index S/2 to 1. The traverse order has limited impact on our system since the agents take the whole BBox region as the environment. The order we adopted is preferred for simplicity. Third, for agents’ terminal signals, we set N and beta as 2 and 0.05, respectively.</p>

  <p>Q5. Comparison experiments /Table 1 (R2, R3)
In Table 1, we compared both the fully- (U-net with architecture provided in [10]) and weakly-supervised segmentation (WSS) methods. The result of U-net is referred to compare these two types of methods intuitively. We will change the caption of Table 1 to “The best results of WSS methods are shown in blue” to remove any confusion. Besides, all the other compared WSS methods use the same BBox annotations for labels as our method uses, thus we consider the comparisons are fair.</p>

  <p>Q6. Model efficiency. (R3)
We need to make it clear that our method only needs BBox annotations, which indicate nodule existence. By using object detection model in our future work, manual BBox annotation will be further discarded through automatic localization to improve efficiency. We agree that the RL-based method is not as efficient as the end-to-end ones. In this regard, we introduced superpixel and multi-agent strategies, which significantly accelerate the inference by more than ten times compared with the pixel-level and single-agent approaches. We will focus on simplifying the RL-based segmentation in the future.</p>

  <p>Q7. Quality and assignment of superpixels. (R3)
To generate high-quality superpixels, we have tried several advanced methods, including SEEDS, SLIC, MSLIC, SLICO, LSC, etc. Based on our tasks, SEEDS is the superior one. Besides, we also adopted a coarse-to-fine strategy to generate fine superpixels, making the predicted segmentation as accurate as possible. Approximately half of the superpixels were assigned to each agent based on the center index (Details refer to Q4).</p>

  <p>Q8. Background calculation and parameter definition. (R3)
The background calculation method we proposed is tractable, simple and efficient for our task. Given a BBox, both the w_f and w_g are defined as equal to the width of BBox. In the future, we will adopt advanced methods (e.g., GAN) for better and general background generation.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0146-12-31
      -->
      <!--
      
        ,
        updated at 
        0147-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Image Segmentation"
        class="post-category">
        Image Segmentation
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Reinforcement learning"
        class="post-category">
        Machine Learning - Reinforcement learning
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Weakly supervised learning"
        class="post-category">
        Machine Learning - Weakly supervised learning
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Huang, Yuhao"
        class="post-category">
        Huang, Yuhao
      </a> |  
      
      <a href="kittywong/tags#Yang, Xin"
        class="post-category">
        Yang, Xin
      </a> |  
      
      <a href="kittywong/tags#Zou, Yuxin"
        class="post-category">
        Zou, Yuxin
      </a> |  
      
      <a href="kittywong/tags#Chen, Chaoyu"
        class="post-category">
        Chen, Chaoyu
      </a> |  
      
      <a href="kittywong/tags#Wang, Jian"
        class="post-category">
        Wang, Jian
      </a> |  
      
      <a href="kittywong/tags#Dou, Haoran"
        class="post-category">
        Dou, Haoran
      </a> |  
      
      <a href="kittywong/tags#Ravikumar, Nishant"
        class="post-category">
        Ravikumar, Nishant
      </a> |  
      
      <a href="kittywong/tags#Frangi, Alejandro F."
        class="post-category">
        Frangi, Alejandro F.
      </a> |  
      
      <a href="kittywong/tags#Zhou, Jianqiao"
        class="post-category">
        Zhou, Jianqiao
      </a> |  
      
      <a href="kittywong/tags#Ni, Dong"
        class="post-category">
        Ni, Dong
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0147/12/31/Paper1554">
          DC-Net: Dual Context Network for 2D Medical Image Segmentation
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0145/12/31/Paper1516">
          Learning to Address Intra-segment Misclassification in Retinal Imaging
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
