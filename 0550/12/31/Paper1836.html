<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>nnDetection: A Self-configuring Method for Medical Object Detection | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="nnDetection: A Self-configuring Method for Medical Object Detection" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Michael Baumgartner, Paul F. Jäger, Fabian Isensee, Klaus H. Maier-Hein Abstract Simultaneous localisation and categorization of objects in medical images, also referred to as medical object detection, is of high clinical relevance because diagnostic decisions often depend on rating of objects rather than e.g. pixels. For this task, the cumbersome and iterative process of method configuration constitutes a major research bottleneck. Recently, nnU-Net has tackled this challenge for the task of image segmentation with great success. Following nnU-Net’s agenda, in this work we systematize and automate the configuration process for medical object detection. The resulting self-configuring method, nnDetection, adapts itself without any manual intervention to arbitrary medical detection problems while achieving results en par with or superior to the state-of-the-art. We demonstrate the effectiveness of nnDetection on two public benchmarks, ADAM and LUNA16, and propose 10 further medical object detection tasks on public data sets for comprehensive method evaluation. Link to paper https://doi.org/10.1007/978-3-030-87240-3_51 Link to the code repository https://github.com/MIC-DKFZ/nnDetection Link to the dataset(s) http://medicaldecathlon.com/ https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI https://ribfrac.grand-challenge.org/ https://cada.grand-challenge.org/Introduction/ https://adam.isi.uu.nl/ https://kits19.grand-challenge.org/ https://wiki.cancerimagingarchive.net/display/Public/SPIE-AAPM-NCI+PROSTATEx+Challenges https://wiki.cancerimagingarchive.net/display/Public/CT+Lymph+Nodes https://luna16.grand-challenge.org/Home/ Reviews Review #1 Please describe the contribution of the paper The paper proposes an extension of the nnUnet framework to provide medical object detection. It customizes base=architecture Retina-UNet so that the final framework can provide object detection on any new medical dataset with minimum intervention. The authors also provided 12 new datasets for the evaluation of medical object detection methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper presents 12 new datasets for medical object detection. The authors made significant effort to make their work reproducible. They have open sourced their code and pre-trained models. The paper provides a way to create Deep LEarning frameworks, that are designed to solve a particular problem and can generalize over datasets and new similar tasks. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Overall paper is weel written. But at multiple instances the authors rely on readers familiarity with mmUNet to understand this paper. To make this manuscript stand-alone, please provide adequate details to completely understand the method. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors provided link to gitHub and made substantial efforts for reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The inverted commas (“ “) are all reversed. Figure.1. Data augmentation, resampling strategy, - please provide adequate details to completely understand the process. Figure 1., the full form for FPN, NMS and WBC are missing. In Rule-based parameters, the authors mentioned “iterative optimization process to determine network topology parameters”. Please provide additional details. Its nor clear, what exactly the author meant by “anchor configuration”. More details are required to understand the process. Figure 2, the results from the proposed method are kind of lost in all other methods. Please use a different color or format to make the proposed method visible. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper provides a way to create Deep LEarning frameworks, that are designed to solve a particular problem and can generalize over datasets and new similar tasks. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 7 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper proposed a systematized nnDetection that integrates flexible configurations for state-of-the-art object detection methods on medical imaging. ADAM and LUNA16 datasets are conducted to demonstrate the performance of the proposed self-configuring method. The code will be made publicly available. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The nnDetection method can benefit the researchers in exploring the medical object detection tasks with easy configuration and various options. A large-scale benchmark with 12 data sets is proposed to enabling sufficiently diverse evaluation of medical object detection methods Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. More discussion about the baseline Retina-UNet. An overview of the proposed method is needed including the detection pipeline, 2D vs. 3D, one-stage vs. two-stage. To save the space, it can be integrated into Fig. 1. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The code of the proposed method is available and the benchmark will also be available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It is well known that U-Net dominates the segmentation task. More discussions are needed on why the author choose the Retina U-Net for medical object detection as their baseline. From the manuscript, it is not unknown that the proposed methods are based on 2D or 3D. Are both the one-stage or two-stage detectors available? For the dataset, the NIH DeepLesion dataset [1] can additionally demonstrate the generalization of the proposed detection method in universal lesion detection. Ref [1] Yan, K., Wang, X., Lu, L., Summers, R.M.: DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning. J. Med. Imaging 5(3), 036501 (2018) Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed method can benefit the researchers in exploring the various medical object detection tasks and have a good contribution to the medical imaging society. The proposed model show a strong performance on both detection and segmentation benchmarks. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The authors propose a self-configured code base for medical image detection, showing excellent results over several benchmarks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -The method is clearly presented. -Provide a good code base for the community of medical image analysis. -Experimental setup described in sufficient detail. -Well paper formatting and writing Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -The academic novelty is limited, and does not address any academic issues. -From an engineering point of view, it can be seen as a codebase that brings together multiple training strategies. where are the benefits of self-configuration highlighted by the authors? The authors should compare with other publicly available codebase Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Yes, it can be reproduced Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -It will be better to compare with other popular detection codebases (include 2d and 3d), -2D detection is more popular in the mia, it will be better for the author to supplement the experiments of 2D tasks, for example: https://www.kaggle.com/c/rsna-pneumonia-detection-challenge or PolyP detection. -It will be better if authors can provide some quantitive metrics such as speed/resource consumption. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? -The academic novelty is limited. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposed an extension of nnUnet framework on the medical object detection task and extensive experiments on benchmarks demonstrate good performance. Overall, the reviewers gave positive comments on the well-established framework and comprehensive experimental comparison, which will be helpful in research community of medical object detection. The issues raised by reviewers could be addressed in the final version. Therefore, a recommendation of provisional accept is recommended. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We would like to thank all reviewers for their valuable time and constructive feedback. We will incorporate the clarifications into the final version of the paper to improve the clarity of some of our descriptions. While we won’t be able to add all the requested comparisons and features (2D vs 3D networks, one stage vs two stage, an additional 2D dataset pool) due to space constraints, we hope to be able to provide an extended journal submission which will cover most of the requests in the future. Furthermore, we would like to elaborate on some specific concerns raised in the reviews: Retina U-Net Our pool of data sets covers a large range of data set sizes and target structures (i.e. objects). Specifically, it also incorporates data sets with only ~100-150 objects in total which makes it incredibly difficult to train pure bounding box based object detectors(ablation studies on toy data sets can be found in the original Retina U-Net publication by Jaeger et. al.). Furthermore, nnU-Net showed that simple architectures can achieve SOTA results when configured properly. Following the same philosophy, Retina U-Net is a simple extension of the commonly used Retina Net detector which (both) showed competitive results compared to more complex two stage methods. 2D or 3D Networks We ran all of our experiments with the 3D version of Retina U-Net since our preliminary 2D results did not show any benefit (his is also reflected in the results of the original nnU-Net publication and the original Retina U-Net publication). nnU-Net experiments were run with all of its configurations (2D, 3D, 3D cascade). Compute Resources for training All configurations are designed for GPUs with 10.9 GB of VRAM (NVIDIA RTX 2080ti) and training of a single network takes around 2 days. Data sets Finally, we would like to note that we are not planning to host the data sets ourselves. While all of them are publicly accessible (some of them with restricted access), all rights are reserved by the original curators of the data sets and need to be acknowledged by future work. nnDetection will include guides, manually corrected labels and scripts to convert the datasets to the desired input format which make it easy for future researchers to use them. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Michael Baumgartner, Paul F. Jäger, Fabian Isensee, Klaus H. Maier-Hein Abstract Simultaneous localisation and categorization of objects in medical images, also referred to as medical object detection, is of high clinical relevance because diagnostic decisions often depend on rating of objects rather than e.g. pixels. For this task, the cumbersome and iterative process of method configuration constitutes a major research bottleneck. Recently, nnU-Net has tackled this challenge for the task of image segmentation with great success. Following nnU-Net’s agenda, in this work we systematize and automate the configuration process for medical object detection. The resulting self-configuring method, nnDetection, adapts itself without any manual intervention to arbitrary medical detection problems while achieving results en par with or superior to the state-of-the-art. We demonstrate the effectiveness of nnDetection on two public benchmarks, ADAM and LUNA16, and propose 10 further medical object detection tasks on public data sets for comprehensive method evaluation. Link to paper https://doi.org/10.1007/978-3-030-87240-3_51 Link to the code repository https://github.com/MIC-DKFZ/nnDetection Link to the dataset(s) http://medicaldecathlon.com/ https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI https://ribfrac.grand-challenge.org/ https://cada.grand-challenge.org/Introduction/ https://adam.isi.uu.nl/ https://kits19.grand-challenge.org/ https://wiki.cancerimagingarchive.net/display/Public/SPIE-AAPM-NCI+PROSTATEx+Challenges https://wiki.cancerimagingarchive.net/display/Public/CT+Lymph+Nodes https://luna16.grand-challenge.org/Home/ Reviews Review #1 Please describe the contribution of the paper The paper proposes an extension of the nnUnet framework to provide medical object detection. It customizes base=architecture Retina-UNet so that the final framework can provide object detection on any new medical dataset with minimum intervention. The authors also provided 12 new datasets for the evaluation of medical object detection methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper presents 12 new datasets for medical object detection. The authors made significant effort to make their work reproducible. They have open sourced their code and pre-trained models. The paper provides a way to create Deep LEarning frameworks, that are designed to solve a particular problem and can generalize over datasets and new similar tasks. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Overall paper is weel written. But at multiple instances the authors rely on readers familiarity with mmUNet to understand this paper. To make this manuscript stand-alone, please provide adequate details to completely understand the method. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors provided link to gitHub and made substantial efforts for reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The inverted commas (“ “) are all reversed. Figure.1. Data augmentation, resampling strategy, - please provide adequate details to completely understand the process. Figure 1., the full form for FPN, NMS and WBC are missing. In Rule-based parameters, the authors mentioned “iterative optimization process to determine network topology parameters”. Please provide additional details. Its nor clear, what exactly the author meant by “anchor configuration”. More details are required to understand the process. Figure 2, the results from the proposed method are kind of lost in all other methods. Please use a different color or format to make the proposed method visible. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper provides a way to create Deep LEarning frameworks, that are designed to solve a particular problem and can generalize over datasets and new similar tasks. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 7 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper proposed a systematized nnDetection that integrates flexible configurations for state-of-the-art object detection methods on medical imaging. ADAM and LUNA16 datasets are conducted to demonstrate the performance of the proposed self-configuring method. The code will be made publicly available. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The nnDetection method can benefit the researchers in exploring the medical object detection tasks with easy configuration and various options. A large-scale benchmark with 12 data sets is proposed to enabling sufficiently diverse evaluation of medical object detection methods Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. More discussion about the baseline Retina-UNet. An overview of the proposed method is needed including the detection pipeline, 2D vs. 3D, one-stage vs. two-stage. To save the space, it can be integrated into Fig. 1. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The code of the proposed method is available and the benchmark will also be available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It is well known that U-Net dominates the segmentation task. More discussions are needed on why the author choose the Retina U-Net for medical object detection as their baseline. From the manuscript, it is not unknown that the proposed methods are based on 2D or 3D. Are both the one-stage or two-stage detectors available? For the dataset, the NIH DeepLesion dataset [1] can additionally demonstrate the generalization of the proposed detection method in universal lesion detection. Ref [1] Yan, K., Wang, X., Lu, L., Summers, R.M.: DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning. J. Med. Imaging 5(3), 036501 (2018) Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed method can benefit the researchers in exploring the various medical object detection tasks and have a good contribution to the medical imaging society. The proposed model show a strong performance on both detection and segmentation benchmarks. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The authors propose a self-configured code base for medical image detection, showing excellent results over several benchmarks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -The method is clearly presented. -Provide a good code base for the community of medical image analysis. -Experimental setup described in sufficient detail. -Well paper formatting and writing Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -The academic novelty is limited, and does not address any academic issues. -From an engineering point of view, it can be seen as a codebase that brings together multiple training strategies. where are the benefits of self-configuration highlighted by the authors? The authors should compare with other publicly available codebase Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Yes, it can be reproduced Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -It will be better to compare with other popular detection codebases (include 2d and 3d), -2D detection is more popular in the mia, it will be better for the author to supplement the experiments of 2D tasks, for example: https://www.kaggle.com/c/rsna-pneumonia-detection-challenge or PolyP detection. -It will be better if authors can provide some quantitive metrics such as speed/resource consumption. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? -The academic novelty is limited. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposed an extension of nnUnet framework on the medical object detection task and extensive experiments on benchmarks demonstrate good performance. Overall, the reviewers gave positive comments on the well-established framework and comprehensive experimental comparison, which will be helpful in research community of medical object detection. The issues raised by reviewers could be addressed in the final version. Therefore, a recommendation of provisional accept is recommended. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We would like to thank all reviewers for their valuable time and constructive feedback. We will incorporate the clarifications into the final version of the paper to improve the clarity of some of our descriptions. While we won’t be able to add all the requested comparisons and features (2D vs 3D networks, one stage vs two stage, an additional 2D dataset pool) due to space constraints, we hope to be able to provide an extended journal submission which will cover most of the requests in the future. Furthermore, we would like to elaborate on some specific concerns raised in the reviews: Retina U-Net Our pool of data sets covers a large range of data set sizes and target structures (i.e. objects). Specifically, it also incorporates data sets with only ~100-150 objects in total which makes it incredibly difficult to train pure bounding box based object detectors(ablation studies on toy data sets can be found in the original Retina U-Net publication by Jaeger et. al.). Furthermore, nnU-Net showed that simple architectures can achieve SOTA results when configured properly. Following the same philosophy, Retina U-Net is a simple extension of the commonly used Retina Net detector which (both) showed competitive results compared to more complex two stage methods. 2D or 3D Networks We ran all of our experiments with the 3D version of Retina U-Net since our preliminary 2D results did not show any benefit (his is also reflected in the results of the original nnU-Net publication and the original Retina U-Net publication). nnU-Net experiments were run with all of its configurations (2D, 3D, 3D cascade). Compute Resources for training All configurations are designed for GPUs with 10.9 GB of VRAM (NVIDIA RTX 2080ti) and training of a single network takes around 2 days. Data sets Finally, we would like to note that we are not planning to host the data sets ourselves. While all of them are publicly accessible (some of them with restricted access), all rights are reserved by the original curators of the data sets and need to be acknowledged by future work. nnDetection will include guides, manually corrected labels and scripts to convert the datasets to the desired input format which make it easy for future researchers to use them. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0550/12/31/Paper1836" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0550/12/31/Paper1836" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0550-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="nnDetection: A Self-configuring Method for Medical Object Detection" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0550/12/31/Paper1836"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0550/12/31/Paper1836","headline":"nnDetection: A Self-configuring Method for Medical Object Detection","dateModified":"0551-01-03T00:00:00-05:17","datePublished":"0550-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Michael Baumgartner, Paul F. Jäger, Fabian Isensee, Klaus H. Maier-Hein Abstract Simultaneous localisation and categorization of objects in medical images, also referred to as medical object detection, is of high clinical relevance because diagnostic decisions often depend on rating of objects rather than e.g. pixels. For this task, the cumbersome and iterative process of method configuration constitutes a major research bottleneck. Recently, nnU-Net has tackled this challenge for the task of image segmentation with great success. Following nnU-Net’s agenda, in this work we systematize and automate the configuration process for medical object detection. The resulting self-configuring method, nnDetection, adapts itself without any manual intervention to arbitrary medical detection problems while achieving results en par with or superior to the state-of-the-art. We demonstrate the effectiveness of nnDetection on two public benchmarks, ADAM and LUNA16, and propose 10 further medical object detection tasks on public data sets for comprehensive method evaluation. Link to paper https://doi.org/10.1007/978-3-030-87240-3_51 Link to the code repository https://github.com/MIC-DKFZ/nnDetection Link to the dataset(s) http://medicaldecathlon.com/ https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI https://ribfrac.grand-challenge.org/ https://cada.grand-challenge.org/Introduction/ https://adam.isi.uu.nl/ https://kits19.grand-challenge.org/ https://wiki.cancerimagingarchive.net/display/Public/SPIE-AAPM-NCI+PROSTATEx+Challenges https://wiki.cancerimagingarchive.net/display/Public/CT+Lymph+Nodes https://luna16.grand-challenge.org/Home/ Reviews Review #1 Please describe the contribution of the paper The paper proposes an extension of the nnUnet framework to provide medical object detection. It customizes base=architecture Retina-UNet so that the final framework can provide object detection on any new medical dataset with minimum intervention. The authors also provided 12 new datasets for the evaluation of medical object detection methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper presents 12 new datasets for medical object detection. The authors made significant effort to make their work reproducible. They have open sourced their code and pre-trained models. The paper provides a way to create Deep LEarning frameworks, that are designed to solve a particular problem and can generalize over datasets and new similar tasks. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Overall paper is weel written. But at multiple instances the authors rely on readers familiarity with mmUNet to understand this paper. To make this manuscript stand-alone, please provide adequate details to completely understand the method. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors provided link to gitHub and made substantial efforts for reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The inverted commas (“ “) are all reversed. Figure.1. Data augmentation, resampling strategy, - please provide adequate details to completely understand the process. Figure 1., the full form for FPN, NMS and WBC are missing. In Rule-based parameters, the authors mentioned “iterative optimization process to determine network topology parameters”. Please provide additional details. Its nor clear, what exactly the author meant by “anchor configuration”. More details are required to understand the process. Figure 2, the results from the proposed method are kind of lost in all other methods. Please use a different color or format to make the proposed method visible. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper provides a way to create Deep LEarning frameworks, that are designed to solve a particular problem and can generalize over datasets and new similar tasks. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 7 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper proposed a systematized nnDetection that integrates flexible configurations for state-of-the-art object detection methods on medical imaging. ADAM and LUNA16 datasets are conducted to demonstrate the performance of the proposed self-configuring method. The code will be made publicly available. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The nnDetection method can benefit the researchers in exploring the medical object detection tasks with easy configuration and various options. A large-scale benchmark with 12 data sets is proposed to enabling sufficiently diverse evaluation of medical object detection methods Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. More discussion about the baseline Retina-UNet. An overview of the proposed method is needed including the detection pipeline, 2D vs. 3D, one-stage vs. two-stage. To save the space, it can be integrated into Fig. 1. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The code of the proposed method is available and the benchmark will also be available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It is well known that U-Net dominates the segmentation task. More discussions are needed on why the author choose the Retina U-Net for medical object detection as their baseline. From the manuscript, it is not unknown that the proposed methods are based on 2D or 3D. Are both the one-stage or two-stage detectors available? For the dataset, the NIH DeepLesion dataset [1] can additionally demonstrate the generalization of the proposed detection method in universal lesion detection. Ref [1] Yan, K., Wang, X., Lu, L., Summers, R.M.: DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning. J. Med. Imaging 5(3), 036501 (2018) Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed method can benefit the researchers in exploring the various medical object detection tasks and have a good contribution to the medical imaging society. The proposed model show a strong performance on both detection and segmentation benchmarks. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The authors propose a self-configured code base for medical image detection, showing excellent results over several benchmarks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -The method is clearly presented. -Provide a good code base for the community of medical image analysis. -Experimental setup described in sufficient detail. -Well paper formatting and writing Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -The academic novelty is limited, and does not address any academic issues. -From an engineering point of view, it can be seen as a codebase that brings together multiple training strategies. where are the benefits of self-configuration highlighted by the authors? The authors should compare with other publicly available codebase Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Yes, it can be reproduced Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -It will be better to compare with other popular detection codebases (include 2d and 3d), -2D detection is more popular in the mia, it will be better for the author to supplement the experiments of 2D tasks, for example: https://www.kaggle.com/c/rsna-pneumonia-detection-challenge or PolyP detection. -It will be better if authors can provide some quantitive metrics such as speed/resource consumption. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? -The academic novelty is limited. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposed an extension of nnUnet framework on the medical object detection task and extensive experiments on benchmarks demonstrate good performance. Overall, the reviewers gave positive comments on the well-established framework and comprehensive experimental comparison, which will be helpful in research community of medical object detection. The issues raised by reviewers could be addressed in the final version. Therefore, a recommendation of provisional accept is recommended. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We would like to thank all reviewers for their valuable time and constructive feedback. We will incorporate the clarifications into the final version of the paper to improve the clarity of some of our descriptions. While we won’t be able to add all the requested comparisons and features (2D vs 3D networks, one stage vs two stage, an additional 2D dataset pool) due to space constraints, we hope to be able to provide an extended journal submission which will cover most of the requests in the future. Furthermore, we would like to elaborate on some specific concerns raised in the reviews: Retina U-Net Our pool of data sets covers a large range of data set sizes and target structures (i.e. objects). Specifically, it also incorporates data sets with only ~100-150 objects in total which makes it incredibly difficult to train pure bounding box based object detectors(ablation studies on toy data sets can be found in the original Retina U-Net publication by Jaeger et. al.). Furthermore, nnU-Net showed that simple architectures can achieve SOTA results when configured properly. Following the same philosophy, Retina U-Net is a simple extension of the commonly used Retina Net detector which (both) showed competitive results compared to more complex two stage methods. 2D or 3D Networks We ran all of our experiments with the 3D version of Retina U-Net since our preliminary 2D results did not show any benefit (his is also reflected in the results of the original nnU-Net publication and the original Retina U-Net publication). nnU-Net experiments were run with all of its configurations (2D, 3D, 3D cascade). Compute Resources for training All configurations are designed for GPUs with 10.9 GB of VRAM (NVIDIA RTX 2080ti) and training of a single network takes around 2 days. Data sets Finally, we would like to note that we are not planning to host the data sets ourselves. While all of them are publicly accessible (some of them with restricted access), all rights are reserved by the original curators of the data sets and need to be acknowledged by future work. nnDetection will include guides, manually corrected labels and scripts to convert the datasets to the desired input format which make it easy for future researchers to use them. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Baumgartner, Michael,Jäger, Paul F.,Isensee, Fabian,Maier-Hein, Klaus H." />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>nnDetection: A Self-configuring Method for Medical Object Detection</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Baumgartner, Michael"
        class="post-tags">
        Baumgartner, Michael
      </a> |  
      
      <a href="kittywong/tags#Jäger, Paul F."
        class="post-tags">
        Jäger, Paul F.
      </a> |  
      
      <a href="kittywong/tags#Isensee, Fabian"
        class="post-tags">
        Isensee, Fabian
      </a> |  
      
      <a href="kittywong/tags#Maier-Hein, Klaus H."
        class="post-tags">
        Maier-Hein, Klaus H.
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Michael Baumgartner, Paul F. Jäger, Fabian Isensee, Klaus H. Maier-Hein
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Simultaneous localisation and categorization of objects in medical images, also referred to as medical object detection, is of high clinical relevance because diagnostic decisions often depend on rating of objects rather than e.g. pixels. For this task, the cumbersome and iterative process of method configuration constitutes a major research bottleneck. Recently, nnU-Net has tackled this challenge for the task of image segmentation with great success. Following nnU-Net’s agenda, in this work we systematize and automate the configuration process for medical object detection. The resulting self-configuring method, nnDetection, adapts itself without any manual intervention to arbitrary medical detection problems while achieving results en par with or superior to the state-of-the-art. We demonstrate the effectiveness of nnDetection on two public benchmarks, ADAM and LUNA16, and propose 10 further medical object detection tasks on public data sets for comprehensive method evaluation.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87240-3_51">https://doi.org/10.1007/978-3-030-87240-3_51</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/MIC-DKFZ/nnDetection
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>http://medicaldecathlon.com/
https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI
https://ribfrac.grand-challenge.org/
https://cada.grand-challenge.org/Introduction/
https://adam.isi.uu.nl/
https://kits19.grand-challenge.org/
https://wiki.cancerimagingarchive.net/display/Public/SPIE-AAPM-NCI+PROSTATEx+Challenges
https://wiki.cancerimagingarchive.net/display/Public/CT+Lymph+Nodes
https://luna16.grand-challenge.org/Home/
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper proposes an extension of the nnUnet framework to provide medical object detection. It customizes base=architecture Retina-UNet so that the final framework can provide object detection on any new medical dataset with minimum intervention. The authors also provided 12 new datasets for the evaluation of medical object detection methods.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The paper presents 12 new datasets for medical object detection.</p>

      <p>The authors made significant effort to make their work reproducible. They have open sourced their code and pre-trained models.</p>

      <p>The paper provides a way to create Deep LEarning frameworks, that are designed to solve a particular problem and can generalize over datasets and new similar tasks.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>Overall paper is weel written. But at multiple instances the authors rely on readers familiarity with mmUNet to understand this paper. To make this manuscript stand-alone, please provide adequate details to completely understand the method.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The authors provided link to gitHub and made substantial efforts for  reproducibility.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>The inverted commas (“ “) are all reversed.</p>

      <p>Figure.1. Data augmentation, resampling strategy, - please provide adequate details to completely understand the process.</p>

      <p>Figure 1., the full form for FPN, NMS and WBC are missing.</p>

      <p>In Rule-based parameters, the authors mentioned “iterative optimization process to determine network topology parameters”. Please provide additional details.</p>

      <p>Its nor clear, what exactly the author meant by “anchor configuration”. More details are required to understand the process.</p>

      <p>Figure 2, the results from the proposed method are kind of lost in all other methods. Please use a different color or format to make the proposed method visible.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper provides a way to create Deep LEarning frameworks, that are designed to solve a particular problem and can generalize over datasets and new similar tasks.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>7</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper proposed a systematized nnDetection that integrates flexible configurations for state-of-the-art object detection methods on medical imaging. ADAM and LUNA16 datasets are conducted to demonstrate the performance of the proposed self-configuring method. The code will be made publicly available.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>The nnDetection method can benefit the researchers in exploring the medical object detection tasks with easy configuration and various options.</li>
        <li>A large-scale benchmark with 12 data sets is proposed to enabling sufficiently diverse evaluation of medical object detection methods</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>More discussion about the baseline Retina-UNet.</li>
        <li>An overview of the proposed method is needed including the detection pipeline, 2D vs. 3D, one-stage vs. two-stage. To save the space, it can be integrated into Fig. 1.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The code of the proposed method is available and the benchmark will also be available.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ol>
        <li>It is well known that U-Net dominates the segmentation task. More discussions are needed on why the author choose the Retina U-Net for medical object detection as their baseline.</li>
        <li>From the manuscript, it is not unknown that the proposed methods are based on 2D or 3D. Are both the one-stage or two-stage detectors available?</li>
        <li>For the dataset, the NIH DeepLesion dataset [1] can additionally demonstrate the generalization of the proposed detection method in universal lesion detection. 
Ref [1] Yan, K., Wang, X., Lu, L., Summers, R.M.: DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning. J. Med. Imaging 5(3), 036501 (2018)</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <ol>
        <li>The proposed method can benefit the researchers in exploring the various medical object detection tasks and have a good contribution to the medical imaging society.</li>
        <li>The proposed model show a strong performance on both detection and segmentation benchmarks.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors propose a self-configured code base for medical image detection, showing excellent results over several benchmarks.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>-The method is clearly presented.
-Provide a good code base for the community of medical image analysis.
-Experimental setup described in sufficient detail. 
-Well paper formatting and writing</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>-The academic novelty is limited, and does not address any academic issues.
-From an engineering point of view, it can be seen as a codebase that brings together multiple training strategies. where are the benefits of self-configuration highlighted by the authors? The authors should compare with other publicly available codebase</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Yes, it can be reproduced</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>-It will be better to compare with other popular detection codebases (include 2d and 3d),
-2D detection is more popular in the mia, it will be better for the author to supplement the experiments of 2D tasks, for example:
https://www.kaggle.com/c/rsna-pneumonia-detection-challenge or PolyP detection.
-It will be better if authors can provide some quantitive metrics such as speed/resource consumption.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>-The academic novelty is limited.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>This paper proposed an extension of nnUnet framework on the medical object detection task and extensive experiments on benchmarks demonstrate good performance. Overall, the reviewers gave positive comments on the well-established framework and comprehensive experimental comparison, which will be helpful in research community of medical object detection. The issues raised by reviewers could be addressed in the final version. Therefore, a recommendation of provisional accept is recommended.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We would like to thank all reviewers for their valuable time and constructive feedback. We will incorporate the clarifications into the final version of the paper to improve the clarity of some of our descriptions. While we won’t be able to add all the requested comparisons and features (2D vs 3D networks, one stage vs two stage, an additional 2D dataset pool) due to space constraints, we hope to be able to provide an extended journal submission which will cover most of the requests in the future.</p>

  <p>Furthermore, we would like to elaborate on some specific concerns raised in the reviews:</p>

  <ol>
    <li>
      <p>Retina U-Net
Our pool of data sets covers a large range of data set sizes and target structures (i.e. objects).  Specifically, it also incorporates data sets with only ~100-150 objects in total which makes it incredibly difficult to train pure bounding box based object detectors(ablation studies on toy data sets can be found in the original Retina U-Net publication by Jaeger et. al.).
Furthermore, nnU-Net showed that simple architectures can achieve SOTA results when configured properly. Following the same philosophy, Retina U-Net is a simple extension of the commonly used Retina Net detector which (both) showed competitive results compared to more complex two stage methods.</p>
    </li>
    <li>
      <p>2D or 3D Networks
We ran all of our experiments with the 3D version of Retina U-Net since our preliminary 2D results did not show any benefit (his is also reflected in the results of the original nnU-Net publication and the original Retina U-Net publication). nnU-Net experiments were run with all of its configurations (2D, 3D, 3D cascade).</p>
    </li>
    <li>
      <p>Compute Resources for training
All configurations are designed for GPUs with 10.9 GB of VRAM (NVIDIA RTX 2080ti) and training of a single network takes around 2 days.</p>
    </li>
    <li>
      <p>Data sets
Finally, we would like to note that we are not planning to host the data sets ourselves. While all of them are publicly accessible (some of them with restricted access), all rights are reserved by the original curators of the data sets and need to be acknowledged by future work. nnDetection will include guides, manually corrected labels and scripts to convert the datasets to the desired input format which make it easy for future researchers to use them.</p>
    </li>
  </ol>

</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0550-12-31
      -->
      <!--
      
        ,
        updated at 
        0551-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Baumgartner, Michael"
        class="post-category">
        Baumgartner, Michael
      </a> |  
      
      <a href="kittywong/tags#Jäger, Paul F."
        class="post-category">
        Jäger, Paul F.
      </a> |  
      
      <a href="kittywong/tags#Isensee, Fabian"
        class="post-category">
        Isensee, Fabian
      </a> |  
      
      <a href="kittywong/tags#Maier-Hein, Klaus H."
        class="post-category">
        Maier-Hein, Klaus H.
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0551/12/31/Paper1844">
          Automating Embryo Development Stage Detection in Time-Lapse Imaging with Synergic Loss and Temporal Learning
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0549/12/31/Paper1834">
          Unsupervised Representation Learning Meets Pseudo-Label Supervised Self-Distillation: A New Approach to Rare Disease Classification
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
