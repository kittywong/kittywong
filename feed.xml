<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://kittywong.github.io/kittywong/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kittywong.github.io/kittywong/" rel="alternate" type="text/html" /><updated>2021-08-14T01:37:48-04:00</updated><id>https://kittywong.github.io/kittywong/feed.xml</id><title type="html">MICCAI 2021 - Accepted Papers and Reviews</title><subtitle></subtitle><entry><title type="html">Rethinking Ultrasound Augmentation: A Physics-Inspired Approach</title><link href="https://kittywong.github.io/kittywong/0865/12/31/Paper2428" rel="alternate" type="text/html" title="Rethinking Ultrasound Augmentation: A Physics-Inspired Approach" /><published>0865-12-31T23:58:56-05:17</published><updated>0866-01-05T00:00:00-05:17</updated><id>https://kittywong.github.io/kittywong/0865/12/31/Paper2428</id><content type="html" xml:base="https://kittywong.github.io/kittywong/0865/12/31/Paper2428">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#author-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Paper Info&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#review-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Reviews&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#metareview-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Meta-review(s)&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#authorFeedback-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Author Feedback&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;author-id&quot;&gt;Authors&lt;/h1&gt;
&lt;p&gt;Maria Tirindelli, Christine Eilers, Walter Simson, Magdalini Paschali, Mohammad Farid Azampour, Nassir Navab
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-id&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Medical Ultrasound (US), despite its wide use, is characterized by artefacts and operator dependency. Those attributes hinder the gathering and utilization of US datasets for the training of deep neural networks used for computer-assisted intervention systems. Data augmentation is commonly used to enhance model generalization and performance. However, common data augmentation techniques, such as affine transformations do not align with the physics of US and, when used carelessly can lead to unrealistic US images. To this end, we propose a set of physics-inspired transformations, including deformation, reverb and signal-to-noise ratio, that we apply on US B-mode images for data augmentation. We evaluate our method on a new spine US dataset for the tasks of bone segmentation and classification. 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;link-id&quot;&gt;Link to paper&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1007/978-3-030-87237-3_66&quot;&gt;https://doi.org/10.1007/978-3-030-87237-3_66&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;code-id&quot;&gt;Link to the code repository&lt;/h1&gt;
&lt;p&gt;https://github.com/mariatirindelli/UltrasoundAugmentation
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dataset-id&quot;&gt;Link to the dataset(s)&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;review-id&quot;&gt;Reviews&lt;/h1&gt;

&lt;h3 id=&quot;review-1&quot;&gt;Review #1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper proposes a data augmentation strategy for ultrasound, which mimics ultrasound physics better than classic augmentation strategies (rotation, flip, etc..), therefore can be considered more realistic. In particular, the authors propose 3 strategies: deformation (due to ultrasound probe pressure and bone structures), reverberation (based on the bone centroid) and signal-to-noise ratio (also based on bone structures).
The method is evaluated on two tasks: bone segmentation and bone classification, however results show a marginal improvement of 0.001 in Dice for segmentation and 0.0 in accuracy for classification.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;It is a novel and easy to implement data augmentation strategy that could potentially improve the outcomes of some clinical applications. Classic data augmentation techniques include: rotation, translation, scaling etc. However these techniques may not be suitable for ultrasound, and the physics behind the creation of the images is very different. The strategies proposed in the paper, which are based on physics, could help improving the outcomes of tasks involving bone visualisation.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;
          &lt;p&gt;Marginal improvement compared to classic data augmentation strategies. Authors report a dice score (standard deviation) of 0.625 (0.03) with a classical data augmentation approach and 0.626 (0.01) with the proposed deformation approach and 0.626 (0.02) with the proposed reverberation approach on ultrasound segmentation. They also report an accuracy of 0.883 (0.04) with a classical data augmentation approach and an accuracy of 0.883 (0.03) with the proposed reverberation approach.&lt;/p&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;Limited application: although the authors claim that are proposing a general ultrasound augmentation (title, abstract, conclusions), this strategy is designed for bone detection and classification as it requires bone masks as input. It is evaluated on bone-related tasks only (detection and classification). It may be difficult to use this data augmentation on other applications such as breast, prostate, cardiac, etc..&lt;/p&gt;
        &lt;/li&gt;
      &lt;/ul&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Excellent&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Some details are missing: image size, number of epochs.
Code and data will be made available.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Reverberation:
The paper proposes to include reverberation as a strategy to provide data augmentation. However, the question is, how realistic will the images be after adding reverberation artifacts? One would imagine that if reverberation occurs, the original image will contain the artifacts already.&lt;/p&gt;

      &lt;p&gt;Results:
Looking at the results, the improvement is marginal and therefore not convincing that the proposed technique works better than a classical approach for segmentation (0.625 (0.03) vs. 0.626 (0.01), respectively) or classification (0.883 (0.04) vs. 0.883 (0.03)).
With such small improvement, a statistical analysis would be needed to claim that “The proposed transformations of Deformation and SNR outperform the classical augmentation”&lt;/p&gt;

      &lt;p&gt;The authors may need to look to other datasets to show more convincing results and on how useful this technique could be.&lt;/p&gt;

      &lt;p&gt;Minor: Figure 1 is not referenced in the text.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;reject (3)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Results do not demonstrate that the physics-based approach improves classic data augmentation approaches.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;3&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-2&quot;&gt;Review #2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper describes three physics-inspired transformations of ultrasound images that can be used to augment learning of DNN. The three are 1) Deformation based on a simple linear deformation above bone, and zero deformation below bone; 2) Reverberation which duplicates the bone echo at multiples of depths; 3) SNR which changes the multiples of bone/non-bone pixels. The new augmentation is shown to have some advantages on a classic DNN bone segmentation task, although the benefit of using all of the proposed augmentations is not clear.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The main strength of this paper is the widespread and acknowledged need to augment DNN leaning with realistic transformations of ultrasound images that are very different than photographic image transforms used in other standard image DNN augmentations. I expect many ultrasound researchers in MICCAI would be interested in this paper and likely to generate much debate. I would also look forward to such a debate.&lt;/p&gt;

      &lt;p&gt;The paper is well written, very clear, good and appropriate references and honest description of the results and conclusions.&lt;/p&gt;

      &lt;p&gt;I also appreciate the demonstrated bone segmentation task which is a growing area of research at MICCAI in a number of different clinical applications using ultrasound.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;An obvious weakness is that only three possible augmentation methods are proposed whereas real variations in ultrasound images go far beyond just simple models of deformation, reverberation and SNR. All of the many reconfigurable parameters on the console of an ultrasound scanner also change the images such as TGC, gain, focus, depth, THI, etc. The authors acknowledge this in the literature review but do not really justify the choice of these three variations to start.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Excellent&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;It would be very easy to replicate the methods in this paper. The three augmentation techniques are clearly described and are simple (although the simplicity is also a weakness because it doesn’t capture the true ultrasound physics)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;All three of the augmentation methods require a priori knowledge of the bone location. Since the goal of the DNN is to give the segmentation, this becomes a chicken-and-egg problem: which comes first? The authors should acknowledge the need for bone locations more clearly in all three augmentation methods and describe what happens when there is uncertainty/errors in that knowledge.  Some justification should be offered for the three chosen augmentations.&lt;/p&gt;

      &lt;p&gt;The authors should define the variable H in equation (1) and be more explicit about how the deformation is performed. It seems to me that Algorithm 1 simply scales linearly the depth of tissue above bone and keeps the bone location unchanged. But what about axial versus lateral motion? Is the tissue considered incompressible? Moreover, if the tissue is deformed, it would suggest a physically incorrect point spread function. Finally, why smooth the deformation field after modeling it, doesn’t that result in a deformation below bone that is physically incorrect?&lt;/p&gt;

      &lt;p&gt;For reverberation, if the bright bone echo is repeated at a multiple of depth, doesn’t the augmentation overlap with existing reverberations effects? Also justify the kernel and sigma parameters, i.e. try to relate to ultrasound physics to see how to select such parameters for other ultrasound transducers.&lt;/p&gt;

      &lt;p&gt;Also defend the range of augmentation ranges in Table 1.&lt;/p&gt;

      &lt;p&gt;Also be more clear about the definition of “All” in Table 2: is it both Classical and the newly proposed augmentations?&lt;/p&gt;

      &lt;p&gt;Minor point: remove unnecessary capitalization, especially in abstract&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;accept (8)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;I am arguing in favour of this paper because I believe it is one of the first of likely many papers such on how to properly augment ultrasound images. This is of wide interest to all ultrasound AI/ML researchers. I’m not convinced the three proposed augmentations are the best, but it is a good start and I look forward to debating where to go next.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-3&quot;&gt;Review #3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;In many computer vision tasks, data augmentation is applied to overcome the lack of training data. Conventional image modifications, such as random scaling, translations, rotations, and Gaussian noise additions, are applied to the dataset. However, the paper claims that these classical augmentations are based on mechanisms behind optical cameras which strongly differs from the principles of US. They suggest three augmentation techniques, using a set of US image modifications considering realistic sources of variability in US. The proposed method augments the US image using deformation, reverberation, or signal-to-noise ratio. At last, they evaluated the proposed methods on the tasks of bone segmentation and classification.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The unrealistic nature of conventional augmentation methods is recognized by the authors. I do agree that the existing augmentation methods do not consider the main characteristics of the medical US images.&lt;/li&gt;
        &lt;li&gt;Deformation is an interesting idea considering.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The paper describes a segmentation task and a classification task for the US image containing bones to verify the novelty. However, the tasks performed has limited use in practice. In addition, the three proposed augmentation methods are valid only for such tasks and can not be applied for general purposes.&lt;/li&gt;
        &lt;li&gt;This paper proposes three main augmentation methods that utilize realistic sources of variability of US. However, it is questionable how realistic the implementation of each method is. In the method of implementing SNR tuning, it is not realistic to adjust the intensity in the image domain since the actual B-mode image is generated through the delay-and-sum(DAS) algorithm of the rf data.
          &lt;ul&gt;
            &lt;li&gt;The paper claims classical transformation is an unrealistic method that does not consider the physical properties of the medical US imaging. However, as shown in experimental results, classical methods are also effective for training DNNs. On the other hand, the suggested method did not show improvement in performance compared to the classical method. In particular, the proposed transformations such as Deformation and SNR in the classification task demonstrated inferior performance as compared to the classical method.&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors provides adequate information for the reproducibility.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;When conducting an experiment to evaluate data augmentation, it is necessary to specify the size of dataset created. In addition, no description is provided for the dataset creation for the evaluation of the classical augmentation method.&lt;/li&gt;
        &lt;li&gt;For each task, there exits differences in the performance of deformation, reverberation, and SNR. it would help readers to better understand why such difference exists. For example, there should be some explanation why Deformation and SNR transformations are beneficial for bone segmentation and Reverberation for classification task.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;reject (3)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The motives and solutions presented in this paper are interesting, but the results are not impressive as compared to the existing methods. In addition, the description on the experimental methods and variable control are insufficient.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;metareview-id&quot;&gt;Meta-Review(s)&lt;/h1&gt;

&lt;h2 id=&quot;primary-meta-review&quot;&gt;Primary Meta-Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The work proposes an ultrasound (US) physics-based data augmentation method specifically in the context of bone imaging using ultrasound. US-based orthopedic procedures can benefit from the method to enlarge the small dataset size. All the reviewers find the physics-guided augmentation novel. However, they also raised concerns regarding the impact of the work as quantitative results did not show a significant improvement over the classical augmentation approach. I believe this work is important for ultrasound researchers at MICCAI and I would like to invite the authors for a rebuttal.&lt;/p&gt;

      &lt;p&gt;I think the major weakness of the work is that the quantitative results do not show significant improvement over classical augmentation approaches (Table 2). A statistical significance test should be reported especially for classical vs proposed augmentations. Without this authors claim ‘The proposed transformations of Deformation and SNR outperform the classical augmentation’ is not valid.&lt;/p&gt;

      &lt;p&gt;It is also not clear how many images were generated by using the proposed augmentation, and classical augmentation method. This information is important and should be included. Please provide information on how many images were generated for each proposed augmentation, all augmentations (all three proposed augmentations), and the classical augmentation method.&lt;/p&gt;

      &lt;p&gt;Why are different physics-based augmentation methods performing differently? For example deformation and SNR perform the same for bone segmentation but if you use reverberation it drops the success for segmentation but improves the success of classification. Does this mean the reverberation model is not accurate? Also by investigating Fig4 a bone reverberation artifact will not appear that deep in the imaging direction. Shouldn’t the reverberation artifacts be closer to the bone surface?&lt;/p&gt;

      &lt;p&gt;Why were these three augmentations chosen?&lt;/p&gt;

      &lt;p&gt;More explanation is required regarding the deformation under the bone (Rev2) and the calculation of SNR not obeying the DAS image formation process(Rev 3). If the proposed method is based on observations obtained from the image only this should be clearly explained and maybe the physics-based approach should be reworded as a bone feature appearance-based approach.&lt;/p&gt;

      &lt;p&gt;A better justification should be provided about the parameters used (Table 1) in the proposed method (Rev2)&lt;/p&gt;

      &lt;p&gt;Minor:
Fig2: Normally for bone US data you wouldn’t perform vertical flip as that is against the concept of data collection. One would never collect bone ultrasound data where the bone image would be flipped like the one shown in Fig2-b.&lt;/p&gt;

      &lt;p&gt;Classification: If the intended application of US is for imaging bones why would an operator collect an ultrasound scan without the appearance of a bone feature? Do the authors mean a quality control (or scan adequacy) where the classification would omit US scans with bad quality bone features?&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;11&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The rebuttal addressed some of the major concerns. Although very important work in the right direction for US data augmentation, unfortunately, in its current form it can not be accepted. Statistical significance is not reported in the rebuttal and by looking at the quantitative results presented in the work there is no significant difference between the proposed and classical augmentation methods. As the authors mention on a larger dataset the proposed work could potentially outperform classical augmentation. However, this is not shown in this work. Finally, it would be important to evaluate the proposed work against GAN-based data augmentation to further improve the strength of this work. I am looking forward to reading a more updated version of this work.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Reject&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;18&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;meta-review-2&quot;&gt;Meta-Review #2&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;I agree this is an interesting and important application wrt. ultrasound image computing. I also would like to add that negative results should not be the reason to reject a technically sound paper.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;13&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;meta-review-3&quot;&gt;Meta-Review #3&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper, titled “Rethinking Ultrasound Augmentation: A Physics-Inspired Approach” received polarizing recommendations from 3 senior reviewers:&lt;/p&gt;

      &lt;p&gt;R1	strength	- novelty
	weakness	- incremental improvement (i.e. no significance)
				- limited application
R2	strength	- appropriateness to miccai
				- limited application
				- evaluation
	weakness	- missing technical details
R3	strength	- novelty
	weakness	- limited application
				- incremental improvement&lt;/p&gt;

      &lt;p&gt;All agreed the writing quality is excellent and the approach interesting/novel. As summarized by the primary AC, the main limitation was the incremental improvement over the more classical/traditional methods, thus the significance of this work is not demonstrated. While the clarity was rated as being excellent by all reviewers, the Primary AC raised certain key questions. The reproducibility was also being rated as good by all reviewers.&lt;/p&gt;

      &lt;p&gt;Based on my personal reading of the manuscript, reviews and meta-reviews, and the rebuttal, I support the decision to accept this paper. I agree with authors’ rebuttal that the physics-based approach is perhaps the “scientific-correct” way for data augmentation, despite the incremental improvement based on the current implementation. I also with R2 that this is “one of the first of likely many papers” on physics-based data augmentation. This topic is appropriate for all MICCAI audience who works with deep-learning approach for US processing.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;6&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;br /&gt;&lt;br /&gt;&lt;/h2&gt;
&lt;h1 id=&quot;authorFeedback-id&quot;&gt;Author Feedback&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;We thank the reviewers for their constructive feedback. We appreciate that they recognize the novelty of the approach, impact for the MICCAI community (MR1, R1, R2) and the clarity and reproducibility of our method (R2, R3).&lt;/p&gt;

  &lt;p&gt;Our major contribution is the physical basis of the augmentations compared to vision-inspired transformations directly transferred from the field of computer vision. As also shown in the paper, data augmentation does not in general have a major impact on the results. We therefore did not expect significant improvement of the final results, when using different augmentation methods. However, our augmentations reflect the physics of ultrasound acquisitions and we believe that this is the scientifically correct way of manipulating US data to generate augmentations. Physics-inspired augmentations prevent the network from being exposed to random, unrealistic data, allowing not only for better modeling, but also for better interpretability and understanding of the model behavior under extreme conditions. The proposed method could provide an  interpretable failure case of the model in case of improper ultrasound acquisitions or processing, while the unrealistic traditional augmentation methods borrowed from other communities do not provide such scientific paths. We therefore believe that instead of only looking at the improvement in the outcome results, we also need to pay attention to the correctness of concept and scientific foundations of the design and deployment of augmentations.
Interestingly, MR1 points out that vertical flips are not realistic for US imaging. However, such transformations are still employed in the literature [9]. These unrealistic transformations are one of the main reasons we believe our proposed physics-inspired augmentation is fundamentally better suited for US images. We also believe that our augmentations will have a stronger effect on larger datasets.&lt;/p&gt;

  &lt;p&gt;Regarding the depth of the reverb artefact, we would like to point out that we consider reverb artefacts arising from multiple reflections between the US probe and the bony tissue. The proposed method can be easily extended to other tissue interfaces like bone-fascial layers.
We assume the “bone is a static body without deformation or transformation” (sec. 2.1), thus we set the deformation below it to 0.&lt;/p&gt;

  &lt;p&gt;Regarding the choice of the augmentation transformations, we would like to stress that this work focuses on physics-inspired transformations, easily applicable to B-mode data. We acknowledge that other parameters such as TGC and frequency are relevant but would require access to more  information than simply the B-mode data for defining complex models suitable for the RF domain.
Main advantage of our method is that it can be applied to public US datasets that only provide B-modes without access to an US console or corresponding RF data.&lt;/p&gt;

  &lt;p&gt;On the consistency between the generation of the SNR artefact and DAS image formation (MR1, R3), we would like to point out that the SNR augmentation aims to simulate variations in tissue echogenicity in the image domain as defined in [Janesick, J. (2007). Photon Transfer, SPIE]. Variations in the RF domain have a direct effect on the image domain, thus variations in the image domain are also realistic.&lt;/p&gt;

  &lt;p&gt;We agree that to specify the content of the paper, it could be renamed to “Physics-Inspired Ultrasound Augmentation: A first application in bone segmentation”.&lt;/p&gt;

  &lt;p&gt;The parameter values were empirically chosen. For classical augmentations, we use parameter values from [6-14]. Augmentations were generated on the fly during training with 30% probability.&lt;/p&gt;

  &lt;p&gt;Concerning the application of bone classification, scan adequacy is indeed a possible use case. Other applications include robotic control and sonographer training.&lt;/p&gt;

  &lt;p&gt;Minor comments will be clarified in the paper.&lt;/p&gt;

  &lt;p&gt;Overall, the novelty of the method was acknowledged by all reviewers and we believe that this work offers new paths to the MICCAI community.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;back to top&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Modalities - Ultrasound" /><category term="Tirindelli, Maria" /><category term="Eilers, Christine" /><category term="Simson, Walter" /><category term="Paschali, Magdalini" /><category term="Azampour, Mohammad Farid" /><category term="Navab, Nassir" /><summary type="html">Paper Info Reviews Meta-review(s) Author Feedback Authors Maria Tirindelli, Christine Eilers, Walter Simson, Magdalini Paschali, Mohammad Farid Azampour, Nassir Navab Abstract Medical Ultrasound (US), despite its wide use, is characterized by artefacts and operator dependency. Those attributes hinder the gathering and utilization of US datasets for the training of deep neural networks used for computer-assisted intervention systems. Data augmentation is commonly used to enhance model generalization and performance. However, common data augmentation techniques, such as affine transformations do not align with the physics of US and, when used carelessly can lead to unrealistic US images. To this end, we propose a set of physics-inspired transformations, including deformation, reverb and signal-to-noise ratio, that we apply on US B-mode images for data augmentation. We evaluate our method on a new spine US dataset for the tasks of bone segmentation and classification. Link to paper https://doi.org/10.1007/978-3-030-87237-3_66 Link to the code repository https://github.com/mariatirindelli/UltrasoundAugmentation Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper proposes a data augmentation strategy for ultrasound, which mimics ultrasound physics better than classic augmentation strategies (rotation, flip, etc..), therefore can be considered more realistic. In particular, the authors propose 3 strategies: deformation (due to ultrasound probe pressure and bone structures), reverberation (based on the bone centroid) and signal-to-noise ratio (also based on bone structures). The method is evaluated on two tasks: bone segmentation and bone classification, however results show a marginal improvement of 0.001 in Dice for segmentation and 0.0 in accuracy for classification. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It is a novel and easy to implement data augmentation strategy that could potentially improve the outcomes of some clinical applications. Classic data augmentation techniques include: rotation, translation, scaling etc. However these techniques may not be suitable for ultrasound, and the physics behind the creation of the images is very different. The strategies proposed in the paper, which are based on physics, could help improving the outcomes of tasks involving bone visualisation. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Marginal improvement compared to classic data augmentation strategies. Authors report a dice score (standard deviation) of 0.625 (0.03) with a classical data augmentation approach and 0.626 (0.01) with the proposed deformation approach and 0.626 (0.02) with the proposed reverberation approach on ultrasound segmentation. They also report an accuracy of 0.883 (0.04) with a classical data augmentation approach and an accuracy of 0.883 (0.03) with the proposed reverberation approach. Limited application: although the authors claim that are proposing a general ultrasound augmentation (title, abstract, conclusions), this strategy is designed for bone detection and classification as it requires bone masks as input. It is evaluated on bone-related tasks only (detection and classification). It may be difficult to use this data augmentation on other applications such as breast, prostate, cardiac, etc.. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Some details are missing: image size, number of epochs. Code and data will be made available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Reverberation: The paper proposes to include reverberation as a strategy to provide data augmentation. However, the question is, how realistic will the images be after adding reverberation artifacts? One would imagine that if reverberation occurs, the original image will contain the artifacts already. Results: Looking at the results, the improvement is marginal and therefore not convincing that the proposed technique works better than a classical approach for segmentation (0.625 (0.03) vs. 0.626 (0.01), respectively) or classification (0.883 (0.04) vs. 0.883 (0.03)). With such small improvement, a statistical analysis would be needed to claim that “The proposed transformations of Deformation and SNR outperform the classical augmentation” The authors may need to look to other datasets to show more convincing results and on how useful this technique could be. Minor: Figure 1 is not referenced in the text. Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Results do not demonstrate that the physics-based approach improves classic data augmentation approaches. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 3 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper describes three physics-inspired transformations of ultrasound images that can be used to augment learning of DNN. The three are 1) Deformation based on a simple linear deformation above bone, and zero deformation below bone; 2) Reverberation which duplicates the bone echo at multiples of depths; 3) SNR which changes the multiples of bone/non-bone pixels. The new augmentation is shown to have some advantages on a classic DNN bone segmentation task, although the benefit of using all of the proposed augmentations is not clear. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strength of this paper is the widespread and acknowledged need to augment DNN leaning with realistic transformations of ultrasound images that are very different than photographic image transforms used in other standard image DNN augmentations. I expect many ultrasound researchers in MICCAI would be interested in this paper and likely to generate much debate. I would also look forward to such a debate. The paper is well written, very clear, good and appropriate references and honest description of the results and conclusions. I also appreciate the demonstrated bone segmentation task which is a growing area of research at MICCAI in a number of different clinical applications using ultrasound. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. An obvious weakness is that only three possible augmentation methods are proposed whereas real variations in ultrasound images go far beyond just simple models of deformation, reverberation and SNR. All of the many reconfigurable parameters on the console of an ultrasound scanner also change the images such as TGC, gain, focus, depth, THI, etc. The authors acknowledge this in the literature review but do not really justify the choice of these three variations to start. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance It would be very easy to replicate the methods in this paper. The three augmentation techniques are clearly described and are simple (although the simplicity is also a weakness because it doesn’t capture the true ultrasound physics) Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html All three of the augmentation methods require a priori knowledge of the bone location. Since the goal of the DNN is to give the segmentation, this becomes a chicken-and-egg problem: which comes first? The authors should acknowledge the need for bone locations more clearly in all three augmentation methods and describe what happens when there is uncertainty/errors in that knowledge. Some justification should be offered for the three chosen augmentations. The authors should define the variable H in equation (1) and be more explicit about how the deformation is performed. It seems to me that Algorithm 1 simply scales linearly the depth of tissue above bone and keeps the bone location unchanged. But what about axial versus lateral motion? Is the tissue considered incompressible? Moreover, if the tissue is deformed, it would suggest a physically incorrect point spread function. Finally, why smooth the deformation field after modeling it, doesn’t that result in a deformation below bone that is physically incorrect? For reverberation, if the bright bone echo is repeated at a multiple of depth, doesn’t the augmentation overlap with existing reverberations effects? Also justify the kernel and sigma parameters, i.e. try to relate to ultrasound physics to see how to select such parameters for other ultrasound transducers. Also defend the range of augmentation ranges in Table 1. Also be more clear about the definition of “All” in Table 2: is it both Classical and the newly proposed augmentations? Minor point: remove unnecessary capitalization, especially in abstract Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I am arguing in favour of this paper because I believe it is one of the first of likely many papers such on how to properly augment ultrasound images. This is of wide interest to all ultrasound AI/ML researchers. I’m not convinced the three proposed augmentations are the best, but it is a good start and I look forward to debating where to go next. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper In many computer vision tasks, data augmentation is applied to overcome the lack of training data. Conventional image modifications, such as random scaling, translations, rotations, and Gaussian noise additions, are applied to the dataset. However, the paper claims that these classical augmentations are based on mechanisms behind optical cameras which strongly differs from the principles of US. They suggest three augmentation techniques, using a set of US image modifications considering realistic sources of variability in US. The proposed method augments the US image using deformation, reverberation, or signal-to-noise ratio. At last, they evaluated the proposed methods on the tasks of bone segmentation and classification. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The unrealistic nature of conventional augmentation methods is recognized by the authors. I do agree that the existing augmentation methods do not consider the main characteristics of the medical US images. Deformation is an interesting idea considering. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The paper describes a segmentation task and a classification task for the US image containing bones to verify the novelty. However, the tasks performed has limited use in practice. In addition, the three proposed augmentation methods are valid only for such tasks and can not be applied for general purposes. This paper proposes three main augmentation methods that utilize realistic sources of variability of US. However, it is questionable how realistic the implementation of each method is. In the method of implementing SNR tuning, it is not realistic to adjust the intensity in the image domain since the actual B-mode image is generated through the delay-and-sum(DAS) algorithm of the rf data. The paper claims classical transformation is an unrealistic method that does not consider the physical properties of the medical US imaging. However, as shown in experimental results, classical methods are also effective for training DNNs. On the other hand, the suggested method did not show improvement in performance compared to the classical method. In particular, the proposed transformations such as Deformation and SNR in the classification task demonstrated inferior performance as compared to the classical method. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors provides adequate information for the reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html When conducting an experiment to evaluate data augmentation, it is necessary to specify the size of dataset created. In addition, no description is provided for the dataset creation for the evaluation of the classical augmentation method. For each task, there exits differences in the performance of deformation, reverberation, and SNR. it would help readers to better understand why such difference exists. For example, there should be some explanation why Deformation and SNR transformations are beneficial for bone segmentation and Reverberation for classification task. Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The motives and solutions presented in this paper are interesting, but the results are not impressive as compared to the existing methods. In addition, the description on the experimental methods and variable control are insufficient. What is the ranking of this paper in your review stack? 5 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The work proposes an ultrasound (US) physics-based data augmentation method specifically in the context of bone imaging using ultrasound. US-based orthopedic procedures can benefit from the method to enlarge the small dataset size. All the reviewers find the physics-guided augmentation novel. However, they also raised concerns regarding the impact of the work as quantitative results did not show a significant improvement over the classical augmentation approach. I believe this work is important for ultrasound researchers at MICCAI and I would like to invite the authors for a rebuttal. I think the major weakness of the work is that the quantitative results do not show significant improvement over classical augmentation approaches (Table 2). A statistical significance test should be reported especially for classical vs proposed augmentations. Without this authors claim ‘The proposed transformations of Deformation and SNR outperform the classical augmentation’ is not valid. It is also not clear how many images were generated by using the proposed augmentation, and classical augmentation method. This information is important and should be included. Please provide information on how many images were generated for each proposed augmentation, all augmentations (all three proposed augmentations), and the classical augmentation method. Why are different physics-based augmentation methods performing differently? For example deformation and SNR perform the same for bone segmentation but if you use reverberation it drops the success for segmentation but improves the success of classification. Does this mean the reverberation model is not accurate? Also by investigating Fig4 a bone reverberation artifact will not appear that deep in the imaging direction. Shouldn’t the reverberation artifacts be closer to the bone surface? Why were these three augmentations chosen? More explanation is required regarding the deformation under the bone (Rev2) and the calculation of SNR not obeying the DAS image formation process(Rev 3). If the proposed method is based on observations obtained from the image only this should be clearly explained and maybe the physics-based approach should be reworded as a bone feature appearance-based approach. A better justification should be provided about the parameters used (Table 1) in the proposed method (Rev2) Minor: Fig2: Normally for bone US data you wouldn’t perform vertical flip as that is against the concept of data collection. One would never collect bone ultrasound data where the bone image would be flipped like the one shown in Fig2-b. Classification: If the intended application of US is for imaging bones why would an operator collect an ultrasound scan without the appearance of a bone feature? Do the authors mean a quality control (or scan adequacy) where the classification would omit US scans with bad quality bone features? What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The rebuttal addressed some of the major concerns. Although very important work in the right direction for US data augmentation, unfortunately, in its current form it can not be accepted. Statistical significance is not reported in the rebuttal and by looking at the quantitative results presented in the work there is no significant difference between the proposed and classical augmentation methods. As the authors mention on a larger dataset the proposed work could potentially outperform classical augmentation. However, this is not shown in this work. Finally, it would be important to evaluate the proposed work against GAN-based data augmentation to further improve the strength of this work. I am looking forward to reading a more updated version of this work. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 18 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. I agree this is an interesting and important application wrt. ultrasound image computing. I also would like to add that negative results should not be the reason to reject a technically sound paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 13 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper, titled “Rethinking Ultrasound Augmentation: A Physics-Inspired Approach” received polarizing recommendations from 3 senior reviewers: R1 strength - novelty weakness - incremental improvement (i.e. no significance) - limited application R2 strength - appropriateness to miccai - limited application - evaluation weakness - missing technical details R3 strength - novelty weakness - limited application - incremental improvement All agreed the writing quality is excellent and the approach interesting/novel. As summarized by the primary AC, the main limitation was the incremental improvement over the more classical/traditional methods, thus the significance of this work is not demonstrated. While the clarity was rated as being excellent by all reviewers, the Primary AC raised certain key questions. The reproducibility was also being rated as good by all reviewers. Based on my personal reading of the manuscript, reviews and meta-reviews, and the rebuttal, I support the decision to accept this paper. I agree with authors’ rebuttal that the physics-based approach is perhaps the “scientific-correct” way for data augmentation, despite the incremental improvement based on the current implementation. I also with R2 that this is “one of the first of likely many papers” on physics-based data augmentation. This topic is appropriate for all MICCAI audience who works with deep-learning approach for US processing. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Author Feedback We thank the reviewers for their constructive feedback. We appreciate that they recognize the novelty of the approach, impact for the MICCAI community (MR1, R1, R2) and the clarity and reproducibility of our method (R2, R3). Our major contribution is the physical basis of the augmentations compared to vision-inspired transformations directly transferred from the field of computer vision. As also shown in the paper, data augmentation does not in general have a major impact on the results. We therefore did not expect significant improvement of the final results, when using different augmentation methods. However, our augmentations reflect the physics of ultrasound acquisitions and we believe that this is the scientifically correct way of manipulating US data to generate augmentations. Physics-inspired augmentations prevent the network from being exposed to random, unrealistic data, allowing not only for better modeling, but also for better interpretability and understanding of the model behavior under extreme conditions. The proposed method could provide an interpretable failure case of the model in case of improper ultrasound acquisitions or processing, while the unrealistic traditional augmentation methods borrowed from other communities do not provide such scientific paths. We therefore believe that instead of only looking at the improvement in the outcome results, we also need to pay attention to the correctness of concept and scientific foundations of the design and deployment of augmentations. Interestingly, MR1 points out that vertical flips are not realistic for US imaging. However, such transformations are still employed in the literature [9]. These unrealistic transformations are one of the main reasons we believe our proposed physics-inspired augmentation is fundamentally better suited for US images. We also believe that our augmentations will have a stronger effect on larger datasets. Regarding the depth of the reverb artefact, we would like to point out that we consider reverb artefacts arising from multiple reflections between the US probe and the bony tissue. The proposed method can be easily extended to other tissue interfaces like bone-fascial layers. We assume the “bone is a static body without deformation or transformation” (sec. 2.1), thus we set the deformation below it to 0. Regarding the choice of the augmentation transformations, we would like to stress that this work focuses on physics-inspired transformations, easily applicable to B-mode data. We acknowledge that other parameters such as TGC and frequency are relevant but would require access to more information than simply the B-mode data for defining complex models suitable for the RF domain. Main advantage of our method is that it can be applied to public US datasets that only provide B-modes without access to an US console or corresponding RF data. On the consistency between the generation of the SNR artefact and DAS image formation (MR1, R3), we would like to point out that the SNR augmentation aims to simulate variations in tissue echogenicity in the image domain as defined in [Janesick, J. (2007). Photon Transfer, SPIE]. Variations in the RF domain have a direct effect on the image domain, thus variations in the image domain are also realistic. We agree that to specify the content of the paper, it could be renamed to “Physics-Inspired Ultrasound Augmentation: A first application in bone segmentation”. The parameter values were empirically chosen. For classical augmentations, we use parameter values from [6-14]. Augmentations were generated on the fly during training with 30% probability. Concerning the application of bone classification, scan adequacy is indeed a possible use case. Other applications include robotic control and sonographer training. Minor comments will be clarified in the paper. Overall, the novelty of the method was acknowledged by all reviewers and we believe that this work offers new paths to the MICCAI community. back to top</summary></entry><entry><title type="html">Training Deep Networks for Prostate Cancer Diagnosis Using Coarse Histopathological Labels</title><link href="https://kittywong.github.io/kittywong/0864/12/31/Paper2126" rel="alternate" type="text/html" title="Training Deep Networks for Prostate Cancer Diagnosis Using Coarse Histopathological Labels" /><published>0864-12-31T23:58:56-05:17</published><updated>0865-01-05T00:00:00-05:17</updated><id>https://kittywong.github.io/kittywong/0864/12/31/Paper2126</id><content type="html" xml:base="https://kittywong.github.io/kittywong/0864/12/31/Paper2126">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#author-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Paper Info&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#review-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Reviews&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#metareview-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Meta-review(s)&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#authorFeedback-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Author Feedback&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;author-id&quot;&gt;Authors&lt;/h1&gt;
&lt;p&gt;Golara Javadi, Samareh Samadi, Sharareh Bayat, Samira Sojoudi, Antonio Hurtado, Silvia Chang, Peter Black, Parvin Mousavi, Purang Abolmaesumi
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-id&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Motivation: Accurate detection of prostate cancer using ultrasound data is a challenging yet highly relevant clinical question. A significant roadblock for training accurate models for cancer detection is the lack of histopathology labels with high resolution that correspond to the presence of cancer in the entire imaging or biopsy planes. Histopathology reports only provide a coarse, representation of cancer distribution in an image region; the distribution of cancer in itself is only approximately reported, making labels generated from these reports very noisy. Method: We propose a multi-constraint optimization method in a co-teaching framework with two deep neural networks. These networks are simultaneously and jointly trained, where each network uses data identified by its peer network as less noisy, to update itself. We propose two additional constraints based on the statistics of cancer distribution and noisy nature of labels to the conventional co-teaching framework. Results: We demonstrate the effectiveness of the proposed learning methodology using a challenging ultrasound dataset with 380 biopsy cores obtained from 89 patients during systematic prostate biopsy. Our results show that our proposed multi-constraint optimization method leads to a significant improvements in terms of area under the curve and balanced accuracy over baseline co-teaching method for detection of prostate cancer.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;link-id&quot;&gt;Link to paper&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1007/978-3-030-87237-3_65&quot;&gt;https://doi.org/10.1007/978-3-030-87237-3_65&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;code-id&quot;&gt;Link to the code repository&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dataset-id&quot;&gt;Link to the dataset(s)&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;review-id&quot;&gt;Reviews&lt;/h1&gt;

&lt;h3 id=&quot;review-1&quot;&gt;Review #1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The prostate cancer classification network in the ultrasound imaging is proposed. The weakly supervised problem of prostate cancer detection is reformulated as a multi-constraint optimization problem. The co-teaching framework and an adaptive weighted loss are combined to analyze the sequence of the ultrasound image.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;Combining co-teaching method and an adaptive weighted loss to manage noisy labels.&lt;/li&gt;
        &lt;li&gt;The cancer involvement information is used for adaptive weighted loss to reflect the uncertainty associated with the reported histopathology.&lt;/li&gt;
        &lt;li&gt;In Fig. 3, the portion of the red regions on each colormap is correlated with the involvement value for that core.&lt;/li&gt;
        &lt;li&gt;In table 1, the proposed method outperforms the baseline co-teaching method.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;There is no explanation why the temporal ultrasound data (especially 200 frames) is chosen as an input to the network.&lt;/li&gt;
        &lt;li&gt;The performance improvement is not significant when applying the max constraint proposed as the main technique. The AUC of the methods with co-teaching (baseline) and the max constraint are 0.61 and 0.64, respectively.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Satisfactory&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Provided information is limited and thus the reproducibility is limited.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;lack of description on the network architecture. Details of the CNN and dense layers described in figure 1 should be provided. In addition, the dimension of the input data should be described.&lt;/li&gt;
        &lt;li&gt;A reason for using ultrasonic temporal data as the input to the classification network should be supported by evidence.&lt;/li&gt;
        &lt;li&gt;Details on how the adaptively weighted loss from the estimated involvement probability can be applied should be provided.&lt;/li&gt;
        &lt;li&gt;In section 2.2, ROI is set to 2 mm by 18 mm. However, the ROI presented in figure 3 is different. Details on how the colormap with variable ROI size is obtained should be provided.&lt;/li&gt;
        &lt;li&gt;The result of conventional prostate cancer classification model should be added as an ablation study.&lt;/li&gt;
        &lt;li&gt;Detail on the measurement hardware should be provided.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;borderline reject (5)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;It is reasonable to use co-teaching methods and adaptive weight loss based on involvement information to solve the problem of the prostate cancer classification. However, since no comparison with the existing prostate cancer classification network is provided, the superiority of the proposed method is unclear. In addition, effectiveness of the max-constraint method presented as the main contribution seems very limited.
The details of the proposed method are are limited in general. For example, 1) how the weight of the loss is adjusted, 2) why the proposed network architecture is chosen, 3) why temporal data is used as an input to the network.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-2&quot;&gt;Review #2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper aims to apply Co-teaching to the Prostate Cancer detection task, with several improvements. The histopathology report is used as a coarse label for learning, and the coarse label is used to restrict the model, including Max Constraint and Involvement Constraint. Compared with standard Co-teaching (Baseline) on private datasets, the authors claim the significant improvements can be observed.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;Interesting and novel cross-modality learning paradigm&lt;/li&gt;
        &lt;li&gt;Good performance for the PC detection task&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The details of the dataset is missing, which makes this paper less convincing;&lt;/li&gt;
        &lt;li&gt;Through the co-learning is a strong baseline, many other cross-modality learning appraoches can be used, which are not discussed in this paper.&lt;/li&gt;
        &lt;li&gt;The proposed method has several hyper-parameters such as lambda, a set of layers. However, the authors did not provide how they select these hyperparameters. What was the search range?&lt;/li&gt;
        &lt;li&gt;The authors would be great to improve Table and Figure. The analysis with Fig. 2 look interesting but some missing information might make the reader difficult to fully understand the flowchart.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;No code and data link are provided.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The image size input to the neural network does not seem to be given in the
paper.&lt;/li&gt;
        &lt;li&gt;The batch size is set as 2024, is it wrong?&lt;/li&gt;
        &lt;li&gt;Does different feature generator model are used? Such as, using the ResNet or DenseNet?&lt;/li&gt;
        &lt;li&gt;The paper mentioned that the loss function can use any loss function. It is interesting to compare other loss functions, with the goal to evaluate the purposed improvements generalization performance?&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;borderline accept (6)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good paper organization while the implementation details are missing.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Confident but not absolutely certain&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-3&quot;&gt;Review #3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper presents a weakly supervised learning method to train a network using noisy lable.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The contribution of the paper worth publishing in miccai. The proposed loss function sounds reasonable.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper is difficult to follow. The discription of the method is also difficult to undrestand. More visual results could be added.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;They provided the details of the training but no code/demo is provided.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1-Eq 1 why max(y^)=y~?
2-One line before Eq 2: “R(e) is the ratio of a batch that would be selected in each epoch and increases” I think it is decrease not increase.
3-please clarify that the network is 1D or 2D? ROI is 2D but 1D convolutions are used.
4-the discription of the loss function is difficult to follow.
5-please include an image similar to Fig.3 that compares previous co-teaching with this method.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Probably accept (7)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;I think the contribution and the novelty are good but the paper is difficult to follow especially the loss function they used.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;3&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Somewhat confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;metareview-id&quot;&gt;Meta-Review(s)&lt;/h1&gt;

&lt;h2 id=&quot;primary-meta-review&quot;&gt;Primary Meta-Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Although the Reviewer 3 gave a high score but without strong justification, while reviewer 1 raised concerns on the performance interpretation. Therefore, i would like to invite the authors to rebuttal, in particular, to clarify the results demonstrating the significance, both clinically and statistically, of the added value - with respect to this relatively small data set. In addition, address comments from reviewers 1 and 2.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Overall, this study described an important and interesting application with a promising learning based approach. The rebuttal argued 89 patients is not a small data set citing difficulties in data acquisition - an irrelevant argument as the reviewers questioned this from a modern deep learning perspective in which large data set is required for training. However, the study showed convincingly improved results based on this small data set. 
I also would like to acknowledge the complex clinical issues surrounding the prostate cancer diagnosis, where pinpoint the clinical relevance based on a single fixed detection algorithm can be challenging or arguably beyond the scope of this technical paper. The rebuttal compares the results with that from PROMIS trial which adopted a much more saturating biopsy sampling, an oversimplification in my opinion considering the difference in quality of reference standard. Nonetheless, this work presents the state-of-the-art results in applying learning based methods in assisting prostate cancer diagnosis on temporal ultrasound imaging, with clear and structured presentation, therefore, i do recommend to share it with the miccai community.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;8&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;meta-review-2&quot;&gt;Meta-Review #2&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Reviewers raise questions about using temporal ultrasound data, the significance of results, small sample size. The rebuttal provides clarification to these issues. The proposed method combines co-teaching and adaptive weighted loss, which is interesting and practical. While the dataset size seems small, the authors stated that it is the largest ultrasound data for prostate cancer from sextant biopsy patients. Taking into account reviewers’ and authors’ information, I recommend accepting this paper.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;8&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;meta-review-3&quot;&gt;Meta-Review #3&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors statement about Max Constraint in the Experiments and Results section in pages 8-9 “the Max constraint, the network is forced strongly to predict higher probability of benign than cancer for all benign signals” raises a concern about the network bias. Also, the results with the “Involvement constraint” as the main technique is another point to see which of the constraints has more effect or domination. For the comparison to the methods in [3], and [4] to be fair it should be on the same data sets not similar data sets as mentioned in the authors rebuttal.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Reject&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;12&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;br /&gt;&lt;br /&gt;&lt;/h2&gt;
&lt;h1 id=&quot;authorFeedback-id&quot;&gt;Author Feedback&lt;/h1&gt;
&lt;blockquote&gt;

  &lt;p&gt;The reviewers noted that our approach is a novel learning paradigm, which achieves good performance for prostate cancer detection. They asked for clarification of the results demonstrating their statistical significance with respect to dataset size, comparison with existing methods, and additional details in terms of implementation, the network structure, and the hardware for data acquisition.&lt;/p&gt;

  &lt;p&gt;CLARIFICATION OF RESULTS 
The main optimization technique proposed in our work is a combination of two constraints: a) max constraint that limits the detected cancer ratio in benign cores, which is crucial in our optimization problem because it helps increase the specificity of the model. Low specificity is also a well-known issue in detection of prostate cancer with other imaging modalities such as multi-parametric MRI [1] and b) involvement, as reported by pathology which constrains the fraction of detected cancer in each biopsy core. In an ablation study (Figure 4), we show the effect of each of these constraints. Starting with a baseline AUC of 61% with co-teaching, the results improved to 64% by incorporating the max constraint. The results further improved to 70% by applying the involvement constraint, which further shrinks the search space for the max constraint.&lt;/p&gt;

  &lt;p&gt;DATASET SIZE
Our work is in the context of image-guided intervention, where significant effort is normally invested to experimentally acquire the data. Our dataset of 89 patients has been obtained over a period of two years and is the largest temporal ultrasound data for prostate cancer to-date from sextant biopsy patients. We do not consider our dataset small within this context.&lt;/p&gt;

  &lt;p&gt;COMPARISON WITH EXISTING METHODS
The main approaches used in the weak supervision literature are: (a) incomplete supervision, when there are many unlabeled samples; (b) inexact supervision, when labels are available only at image-level vs. pixel-level; and (c) inaccurate supervision, when labels are noisy to some extent [2]. Our proposed approach is a remedy to inaccurate supervision, with key contributions that address the specific problems related to prostate cancer. Our weak supervision approach differs from cross-modality learning methods in the literature since our focus is on learning from the noisy labels of one modality rather than learning from two image modalities.&lt;/p&gt;

  &lt;p&gt;A review of the literature shows that temporal ultrasound data is promising for detection of prostate cancer [3,4] compared to methods that only use a single ultrasound image. As a result, we use this protocol as a base for our data analysis. We have compared our learning paradigm against recent studies [3,4] on similar datasets. Our results indicate that we statistically significantly improve the performance using the proposed method. We will add these details to the paper. 
The clinical standard for evaluation of the performance of an imaging technology for prostate cancer detection is set by the PROMIS study (740 patients, 11 clinical sites) [1] comparing mp-MRI with ultrasound-guided sextant biopsy. In that study, the reported sensitivity and specificity of mp-MRI for detection of aggressive cancer was 88% and 45%, respectively, vs. those of 48% and 99% for the sextant biopsy, respectively. In comparison, with temporal ultrasound, at sensitivity of 87%, the specificity of our proposed network is equal to 50%.&lt;/p&gt;

  &lt;p&gt;ADDITIONAL DETAILS
The reviewers asked for additional implementation details including the network structure and the hardware for data acquisition, which we will add to the paper and to supplementary material in the form of a figure. We will expand the description of the figure captions to further elaborate on the details of the methodology.&lt;/p&gt;

  &lt;p&gt;[1]	Ahmed, H., et al.The Lancet, vol. 389, pp. 815-822
[2]	Zhou, Z National Science Review vol. 5, pp. 44–53
[3]	Javadi, G., et al. MICCAI. (2020), pp 524-533
[4]	Javadi, G., et al. IJCARS vol. 15, pp 1023-1031&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;back to top&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Modalities - Ultrasound" /><category term="Machine Learning - Weakly supervised learning" /><category term="Javadi, Golara" /><category term="Samadi, Samareh" /><category term="Bayat, Sharareh" /><category term="Sojoudi, Samira" /><category term="Hurtado, Antonio" /><category term="Chang, Silvia" /><category term="Black, Peter" /><category term="Mousavi, Parvin" /><category term="Abolmaesumi, Purang" /><summary type="html">Paper Info Reviews Meta-review(s) Author Feedback Authors Golara Javadi, Samareh Samadi, Sharareh Bayat, Samira Sojoudi, Antonio Hurtado, Silvia Chang, Peter Black, Parvin Mousavi, Purang Abolmaesumi Abstract Motivation: Accurate detection of prostate cancer using ultrasound data is a challenging yet highly relevant clinical question. A significant roadblock for training accurate models for cancer detection is the lack of histopathology labels with high resolution that correspond to the presence of cancer in the entire imaging or biopsy planes. Histopathology reports only provide a coarse, representation of cancer distribution in an image region; the distribution of cancer in itself is only approximately reported, making labels generated from these reports very noisy. Method: We propose a multi-constraint optimization method in a co-teaching framework with two deep neural networks. These networks are simultaneously and jointly trained, where each network uses data identified by its peer network as less noisy, to update itself. We propose two additional constraints based on the statistics of cancer distribution and noisy nature of labels to the conventional co-teaching framework. Results: We demonstrate the effectiveness of the proposed learning methodology using a challenging ultrasound dataset with 380 biopsy cores obtained from 89 patients during systematic prostate biopsy. Our results show that our proposed multi-constraint optimization method leads to a significant improvements in terms of area under the curve and balanced accuracy over baseline co-teaching method for detection of prostate cancer. Link to paper https://doi.org/10.1007/978-3-030-87237-3_65 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The prostate cancer classification network in the ultrasound imaging is proposed. The weakly supervised problem of prostate cancer detection is reformulated as a multi-constraint optimization problem. The co-teaching framework and an adaptive weighted loss are combined to analyze the sequence of the ultrasound image. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Combining co-teaching method and an adaptive weighted loss to manage noisy labels. The cancer involvement information is used for adaptive weighted loss to reflect the uncertainty associated with the reported histopathology. In Fig. 3, the portion of the red regions on each colormap is correlated with the involvement value for that core. In table 1, the proposed method outperforms the baseline co-teaching method. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There is no explanation why the temporal ultrasound data (especially 200 frames) is chosen as an input to the network. The performance improvement is not significant when applying the max constraint proposed as the main technique. The AUC of the methods with co-teaching (baseline) and the max constraint are 0.61 and 0.64, respectively. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Provided information is limited and thus the reproducibility is limited. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html lack of description on the network architecture. Details of the CNN and dense layers described in figure 1 should be provided. In addition, the dimension of the input data should be described. A reason for using ultrasonic temporal data as the input to the classification network should be supported by evidence. Details on how the adaptively weighted loss from the estimated involvement probability can be applied should be provided. In section 2.2, ROI is set to 2 mm by 18 mm. However, the ROI presented in figure 3 is different. Details on how the colormap with variable ROI size is obtained should be provided. The result of conventional prostate cancer classification model should be added as an ablation study. Detail on the measurement hardware should be provided. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It is reasonable to use co-teaching methods and adaptive weight loss based on involvement information to solve the problem of the prostate cancer classification. However, since no comparison with the existing prostate cancer classification network is provided, the superiority of the proposed method is unclear. In addition, effectiveness of the max-constraint method presented as the main contribution seems very limited. The details of the proposed method are are limited in general. For example, 1) how the weight of the loss is adjusted, 2) why the proposed network architecture is chosen, 3) why temporal data is used as an input to the network. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper aims to apply Co-teaching to the Prostate Cancer detection task, with several improvements. The histopathology report is used as a coarse label for learning, and the coarse label is used to restrict the model, including Max Constraint and Involvement Constraint. Compared with standard Co-teaching (Baseline) on private datasets, the authors claim the significant improvements can be observed. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Interesting and novel cross-modality learning paradigm Good performance for the PC detection task Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The details of the dataset is missing, which makes this paper less convincing; Through the co-learning is a strong baseline, many other cross-modality learning appraoches can be used, which are not discussed in this paper. The proposed method has several hyper-parameters such as lambda, a set of layers. However, the authors did not provide how they select these hyperparameters. What was the search range? The authors would be great to improve Table and Figure. The analysis with Fig. 2 look interesting but some missing information might make the reader difficult to fully understand the flowchart. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No code and data link are provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The image size input to the neural network does not seem to be given in the paper. The batch size is set as 2024, is it wrong? Does different feature generator model are used? Such as, using the ResNet or DenseNet? The paper mentioned that the loss function can use any loss function. It is interesting to compare other loss functions, with the goal to evaluate the purposed improvements generalization performance? Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Good paper organization while the implementation details are missing. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The paper presents a weakly supervised learning method to train a network using noisy lable. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The contribution of the paper worth publishing in miccai. The proposed loss function sounds reasonable. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The paper is difficult to follow. The discription of the method is also difficult to undrestand. More visual results could be added. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance They provided the details of the training but no code/demo is provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html 1-Eq 1 why max(y^)=y~? 2-One line before Eq 2: “R(e) is the ratio of a batch that would be selected in each epoch and increases” I think it is decrease not increase. 3-please clarify that the network is 1D or 2D? ROI is 2D but 1D convolutions are used. 4-the discription of the loss function is difficult to follow. 5-please include an image similar to Fig.3 that compares previous co-teaching with this method. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I think the contribution and the novelty are good but the paper is difficult to follow especially the loss function they used. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Somewhat confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Although the Reviewer 3 gave a high score but without strong justification, while reviewer 1 raised concerns on the performance interpretation. Therefore, i would like to invite the authors to rebuttal, in particular, to clarify the results demonstrating the significance, both clinically and statistically, of the added value - with respect to this relatively small data set. In addition, address comments from reviewers 1 and 2. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. Overall, this study described an important and interesting application with a promising learning based approach. The rebuttal argued 89 patients is not a small data set citing difficulties in data acquisition - an irrelevant argument as the reviewers questioned this from a modern deep learning perspective in which large data set is required for training. However, the study showed convincingly improved results based on this small data set. I also would like to acknowledge the complex clinical issues surrounding the prostate cancer diagnosis, where pinpoint the clinical relevance based on a single fixed detection algorithm can be challenging or arguably beyond the scope of this technical paper. The rebuttal compares the results with that from PROMIS trial which adopted a much more saturating biopsy sampling, an oversimplification in my opinion considering the difference in quality of reference standard. Nonetheless, this work presents the state-of-the-art results in applying learning based methods in assisting prostate cancer diagnosis on temporal ultrasound imaging, with clear and structured presentation, therefore, i do recommend to share it with the miccai community. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. Reviewers raise questions about using temporal ultrasound data, the significance of results, small sample size. The rebuttal provides clarification to these issues. The proposed method combines co-teaching and adaptive weighted loss, which is interesting and practical. While the dataset size seems small, the authors stated that it is the largest ultrasound data for prostate cancer from sextant biopsy patients. Taking into account reviewers’ and authors’ information, I recommend accepting this paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 8 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors statement about Max Constraint in the Experiments and Results section in pages 8-9 “the Max constraint, the network is forced strongly to predict higher probability of benign than cancer for all benign signals” raises a concern about the network bias. Also, the results with the “Involvement constraint” as the main technique is another point to see which of the constraints has more effect or domination. For the comparison to the methods in [3], and [4] to be fair it should be on the same data sets not similar data sets as mentioned in the authors rebuttal. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 12 Author Feedback The reviewers noted that our approach is a novel learning paradigm, which achieves good performance for prostate cancer detection. They asked for clarification of the results demonstrating their statistical significance with respect to dataset size, comparison with existing methods, and additional details in terms of implementation, the network structure, and the hardware for data acquisition. CLARIFICATION OF RESULTS The main optimization technique proposed in our work is a combination of two constraints: a) max constraint that limits the detected cancer ratio in benign cores, which is crucial in our optimization problem because it helps increase the specificity of the model. Low specificity is also a well-known issue in detection of prostate cancer with other imaging modalities such as multi-parametric MRI [1] and b) involvement, as reported by pathology which constrains the fraction of detected cancer in each biopsy core. In an ablation study (Figure 4), we show the effect of each of these constraints. Starting with a baseline AUC of 61% with co-teaching, the results improved to 64% by incorporating the max constraint. The results further improved to 70% by applying the involvement constraint, which further shrinks the search space for the max constraint. DATASET SIZE Our work is in the context of image-guided intervention, where significant effort is normally invested to experimentally acquire the data. Our dataset of 89 patients has been obtained over a period of two years and is the largest temporal ultrasound data for prostate cancer to-date from sextant biopsy patients. We do not consider our dataset small within this context. COMPARISON WITH EXISTING METHODS The main approaches used in the weak supervision literature are: (a) incomplete supervision, when there are many unlabeled samples; (b) inexact supervision, when labels are available only at image-level vs. pixel-level; and (c) inaccurate supervision, when labels are noisy to some extent [2]. Our proposed approach is a remedy to inaccurate supervision, with key contributions that address the specific problems related to prostate cancer. Our weak supervision approach differs from cross-modality learning methods in the literature since our focus is on learning from the noisy labels of one modality rather than learning from two image modalities. A review of the literature shows that temporal ultrasound data is promising for detection of prostate cancer [3,4] compared to methods that only use a single ultrasound image. As a result, we use this protocol as a base for our data analysis. We have compared our learning paradigm against recent studies [3,4] on similar datasets. Our results indicate that we statistically significantly improve the performance using the proposed method. We will add these details to the paper. The clinical standard for evaluation of the performance of an imaging technology for prostate cancer detection is set by the PROMIS study (740 patients, 11 clinical sites) [1] comparing mp-MRI with ultrasound-guided sextant biopsy. In that study, the reported sensitivity and specificity of mp-MRI for detection of aggressive cancer was 88% and 45%, respectively, vs. those of 48% and 99% for the sextant biopsy, respectively. In comparison, with temporal ultrasound, at sensitivity of 87%, the specificity of our proposed network is equal to 50%. ADDITIONAL DETAILS The reviewers asked for additional implementation details including the network structure and the hardware for data acquisition, which we will add to the paper and to supplementary material in the form of a figure. We will expand the description of the figure captions to further elaborate on the details of the methodology. [1] Ahmed, H., et al.The Lancet, vol. 389, pp. 815-822 [2] Zhou, Z National Science Review vol. 5, pp. 44–53 [3] Javadi, G., et al. MICCAI. (2020), pp 524-533 [4] Javadi, G., et al. IJCARS vol. 15, pp 1023-1031 back to top</summary></entry><entry><title type="html">Visual-Assisted Probe Movement Guidance for Obstetric Ultrasound Scanning using Landmark Retrieval</title><link href="https://kittywong.github.io/kittywong/0863/12/31/Paper1291" rel="alternate" type="text/html" title="Visual-Assisted Probe Movement Guidance for Obstetric Ultrasound Scanning using Landmark Retrieval" /><published>0863-12-31T23:58:56-05:17</published><updated>0864-01-05T00:00:00-05:17</updated><id>https://kittywong.github.io/kittywong/0863/12/31/Paper1291</id><content type="html" xml:base="https://kittywong.github.io/kittywong/0863/12/31/Paper1291">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#author-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Paper Info&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#review-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Reviews&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#metareview-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Meta-review(s)&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#authorFeedback-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Author Feedback&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;author-id&quot;&gt;Authors&lt;/h1&gt;
&lt;p&gt;Cheng Zhao, Richard Droste, Lior Drukker, Aris T. Papageorghiou, J. Alison Noble
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-id&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Automated ultrasound (US)-probe movement guidance is desirable to assist inexperienced human operators during obstetric US scanning. 
In this paper, we present a new visual-assisted probe movement technique using automated landmark retrieval for assistive obstetric US scanning. 
In a first step, a set of landmarks is constructed uniformly around a virtual 3D fetal model. 
Then, during obstetric scanning, a deep neural network (DNN) model locates the nearest landmark through descriptor search between the current observation and landmarks. 
The global position cues are visualised in real-time on a monitor to assist the human operator in probe movement. 
A Transformer-VLAD network is proposed to learn a global descriptor to represent each US image. 
This method abandons the need for deep parameter regression to enhance the generalization ability of the network. 
To avoid prohibitively expensive human annotation, anchor-positive-negative US image-pairs are automatically constructed through a KD-Tree search of 3D probe positions. 
This leads to an end-to-end network trained in a self-supervised way through contrastive learning.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;link-id&quot;&gt;Link to paper&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1007/978-3-030-87237-3_64&quot;&gt;https://doi.org/10.1007/978-3-030-87237-3_64&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;code-id&quot;&gt;Link to the code repository&lt;/h1&gt;
&lt;p&gt;https://github.com/dachengxiaocheng/Visual-Assisted-Probe-Movement.git
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dataset-id&quot;&gt;Link to the dataset(s)&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;review-id&quot;&gt;Reviews&lt;/h1&gt;

&lt;h3 id=&quot;review-1&quot;&gt;Review #1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors developed a method to direct a user of 2D ultrasound to move it to a specific pose during obstetrical imaging of the fetus.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The algorithm appears novel and the analysis is also appropriate; however, it is not clear why a 15mm distance was used as a metric.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Since you need to determine the pose of the acquired images, why not generate a 3D ultrasound image using the motion tracker and then examine the 3D image for the features required for evaluating the fetus. 3D ultrasound imaging in obstetrics is widely used and there are very many publications on this topic.&lt;/p&gt;

      &lt;p&gt;How accurately must you know the pose information using the motion tracker? Adding a motion tracker adds cost to the system. What motion tracker did you use.&lt;/p&gt;

      &lt;p&gt;If I understand correctly, the physician will need to acquire a large number of images for each patient, from which you will generate about 400 evenly distributed images as patients may be at different points during the pregnancy, fetal abnormalities may appear very differently, and the fetus may be in a different position at each examination. How long does that take? Does this aspect require special training and can the images be acquired freely with no specific method to scan the abdomen?&lt;/p&gt;

      &lt;p&gt;For performance evaluation, why did you use 15mm as the distance target for a correctly retrieved landmark? This distance appears quite large if specific abnormalities are needed to be viewed. Is one retrieved landmark sufficient for clinical use? The rotation around a retrieved landmark may not be correct.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors will provide the code&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;See comments in the “weaknesses” section.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;borderline accept (6)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The main issue is the utility of the method with questions what not use 3D ultrasound images and issues around the motion tracking.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Not Confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-2&quot;&gt;Review #2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper proposed a triplet-learning framework based on transformer and NetVLAD for landmark retrieval in probe movement guidance. The topN recall is promising based on the proposed method.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Strength:&lt;/p&gt;
      &lt;ol&gt;
        &lt;li&gt;A good application paper based on the integration of Transformer, NetVLAD and triplet learning for landmark retrieval.&lt;/li&gt;
        &lt;li&gt;The TopN recall is good compared to baselines.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Weakness:&lt;/p&gt;
      &lt;ol&gt;
        &lt;li&gt;The novelty is limited;&lt;/li&gt;
        &lt;li&gt;The claims of this work are not fully convincing, especially for the self-supervised part. Please refer to the detailed comments.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The reproducibility of this work seems OK. According to the implementation part, this work follows the network architecture of  transformer and NetVALD. The hyperparameters are also presented in details.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper proposed a triplet framework for landmark retrieval. For techical contribution, the novelty is limited since the proposed method is a direct combination of transformer, NetVLAD and triplet learning. However, there are also several issues to be concerned:&lt;/p&gt;
      &lt;ol&gt;
        &lt;li&gt;The use of NetVLAD seems not essential. Why the authors choose NetVLAD as the feature aggregation module? The only support is the good accuracy in Table 2. Do you think this is due to the special application in this work or it is a general observations? NetVLAD also has several extentions including TEN [1]. Since NetVLAD is not orginally proposed in this paper, I suggest that more aggeration layers should be evaluated.&lt;/li&gt;
        &lt;li&gt;This work claims that it is a “Self-supervised Network Training” method. However, this is actually the way to construct the triplet (P,N,A) for model training based on the KD-Tree. For ‘self-supervised learning’, it usually requires no supervision which cannot fully match the pipeline of this work.&lt;/li&gt;
      &lt;/ol&gt;

      &lt;p&gt;[1] Zhang, et.al ‘Deep TEN’, CVPR 2016&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;probably reject (4)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The novelty of this method is limited and the experiment setup is not convincing.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Confident but not absolutely certain&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-3&quot;&gt;Review #3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;Developing an algorithm that assists the US-probe movement by applying landmark retrieval.&lt;/li&gt;
        &lt;li&gt;Proposing a network that generates a generalized descriptor for automatic landmark retrieval.&lt;/li&gt;
        &lt;li&gt;Contrastive learning, a form of self-supervised learning, is used to avoid human annotations.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;A new approach providing visual-supported navigation unlike conventional methods that provide measured parameters only. Human annotation is minimized by using self-supervised learning, which is an effective method in the area of medical imaging where ground truth is hard to acquire. The network itself was designed by transforming conventional networks to accomplish the targeted specific tasks.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The clinical importance of the proposed work is controversial. Untrained individuals can be assisted by the proposed system but many trained sonographers can accomplish what they want without such assistance.&lt;/li&gt;
        &lt;li&gt;The performance comparison with the CNN SOTA network seems insufficient. Other network types should be considered.&lt;/li&gt;
        &lt;li&gt;There is no evidence or explanation as to whether the VGG network pre-trained with ImageNet is effective for ultrasound images.&lt;/li&gt;
        &lt;li&gt;Five test cases were prepared, but only three results were shown for the 3rd trimester.&lt;/li&gt;
        &lt;li&gt;The reason for using the attention feature is not clear.&lt;/li&gt;
        &lt;li&gt;It is claimed to work in real-time, but no data on the execution time is provided. 7. 7. There is no information on how the test set is obtained.&lt;/li&gt;
        &lt;li&gt;There are two typos (per-trained, SOAT).&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Authors say the results will be released as an open-source project to contribute to the US research community.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The approach is impressive and the understanding level of the neural network seems very high. 
It would be good if more test case images are provided. Although detailed explanations of the network were provided, adding motivation or justification of choosing such a network will help readers.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;borderline accept (6)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The clinical importance of the proposed work is controversial. Untrained individuals can be assisted by the proposed system but many trained sonographers can accomplish what they want without such assistance. 
The main academic contribution of this work is applying self-supervised learning in the new application of probe navigation. However, more rigorous explanation/justification/comparison of the proposed network should be given.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Confident but not absolutely certain&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;metareview-id&quot;&gt;Meta-Review(s)&lt;/h1&gt;

&lt;h2 id=&quot;primary-meta-review&quot;&gt;Primary Meta-Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;All reviewers agreed this paper presented an interesting application and the proposed model is of certain interest. However, they also requested further clarification about the following issues. First, the proposed approach needs to be better motivated against 3D Ultrasound imaging widely used in obstetrics (Reviewer #1), and the concern about its clinic importance needs to be addressed (Reviewer #3). Second, more rigorous explanation or justification of the proposed network needs to be provided to facilitate understanding (Reviewer #2 and Reviewer #3). Third, please respond to Reviewer #2’s concern about the technical contribution. Fourth, please discuss the sensitivity of the proposed approach to the accuracy of pose information and explain why a 15mm distance was used as a metric in the evaluation (Reviewer #1).&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper addressed an interesting application that provides guide to the probe movement during obstetric ultrasound scanning, and formulates this problem as a landmark retrieval problem solved by the proposed Transformer-VLAD model. It contains several interesting seeds in the less studied application problem, the solution, and the SOTA network model, which could be of certain research interest to the community of MICCAI. In the rebuttal, the authors answered the major questions from the reviewers, especially with a new comparison with trans-TEN as requested by Reviewer#2. If it could be accepted for publication, the authors should highlight their response to the motivation and the clinical importance, and incorporate the clarifications to other major concerns in their final paper.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;10&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;meta-review-2&quot;&gt;Meta-Review #2&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper has mixed reviews, and the meta-review lists many questions that the authors needed to address in the rebuttal.  Unfortunately, it seems that the authors did not answer the AC’s questions and focused on other technical questions.  Given that AC’s questions remain unanswered, I believe the paper needs to go through another round of revision, which means that I recommend it to be rejected at this stage.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Reject&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;18&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;meta-review-3&quot;&gt;Meta-Review #3&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper receives conflicting comments: 2 borderline acc, 1 prob rej. (R2) 
The rebuttal has addressed most of R2’s concerns (self-supervised learning, limited novelty) and therefore I will downplay R2’s opinions. The rebuttal also clarified various subtle points raised by R1 and R3. Overall, I think that the paper addresses a clinical important problem with a sensible approach and satisfactory results.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;9&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;br /&gt;&lt;br /&gt;&lt;/h2&gt;
&lt;h1 id=&quot;authorFeedback-id&quot;&gt;Author Feedback&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;We thank the reviewers for their valuable comments and respond to the main queries below.&lt;/p&gt;

  &lt;p&gt;Why not pose as a 3DUS problem(R1)
Our motivation is to support uptake of the new generation of lower-cost 2D US probes outside of traditional hospital setting, where users are not highly trained. Assistive guidance tools are potentially useful then. 3DUS is typically based in hospitals and performed by highly trained sonographers. This is why we pose the problem as a 2D US one. The proposed method could be adapted to the 3DUS case. The only required architecture modification is to replace the 2D CNN encoder with a 3D CNN one.&lt;/p&gt;

  &lt;p&gt;Clinical importance(R3)
Simplifying US to be more accessible to non-expert operators is a recognized priority for wider deployment of US in clinical practice. This paper aims to address this challenge, investigating technical feasibility of a novel image-retrieval based solution to provide helpful visualization cues to guide an operator during 2D fetal US scanning. The solution is implemented using data from a realistic data-based US simulator as a proof-of-principle. Future work would address issues in adapting the solution to a clinical setting and assessing its usability and usefulness to guide operators in practice.&lt;/p&gt;

  &lt;p&gt;More rigorous network justification(R2, R3)
Refer to Table2 and Sec.3.3; SOTA baseline comparisons[2][4] are reported. The patch-style encoder[2] lost some local geometry cues compared to CNN-backbone we used. The Transformer improves performance compared with its absence[1] as the attention mechanism inside extracts co-contextual cues within the feature vector. For US image retrieval, VLAD aggregation including more statistics mechanism performs better than maxpooling aggregation[4] commonly used in large-scale public benchmarks. A potential reason is it depends on dataset scale. Performance difference between maxpooling and VLAD may decrease with increased dataset scale. We find it also happens on the 3D point-cloud/lidar retrieval. Following R2, we report performance of Trans-TEN: r@1=82.5%, r@5=91.5%, r@10=94.0% as a supplement. DeepTEN has a similar mechanism to NetVLAD, hence similar performances.&lt;/p&gt;

  &lt;p&gt;Technical novelty is limited(R2)
We respectively disagree. We propose a new way to formulate US-probe movement guidance via automated landmark retrieval achieved by global descriptor learning and matching. Technical novelties: (1) the global descriptor is learned via contrastive learning using self-constructed A-P-N data-pairs by a probe position KD-Tree without human annotation; and (2) the proposed network is forerunner Transformer research investigating its image retrieval use. Independently [4] published a little earlier(arXiv not accepted,10/02/2021) c.f. MICCAI deadline 03/03/21. Our work focuses on US images, while [4] focuses on natural images. We will release the code as an open-source project on paper acceptance.&lt;/p&gt;

  &lt;p&gt;Why is it self-supervised learning(R2)
Unsupervised learning learns without supervision while self-supervision learns under the supervision from data itself without human annotation. The global descriptor is learned via contrastive learning using automatically constructed data pairs according to the probe-position KD-Tree without human annotation. We name it self-supervised learning as the supervision comes from the data only.&lt;/p&gt;

  &lt;p&gt;Why use a 15mm distance in evaluation; sensitivity of method to pose information(R1)
The hyper-parameter 15mm is empirically set according to the number of landmarks and 3D volume i.e. density of landmarks. It can be adjusted according to the specific clinical task. We have not used a motion tracker. The images come from a real clinical data-based US simulator(ScanTrainer) so the 3D relation between probe position and image is known. This 3D-to-2D image relation is only used in training. At test, and hence for use, you only need a 2D image(no motion tracker).&lt;/p&gt;

  &lt;p&gt;Other minor points of clarification will be carefully addressed in the final paper.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;back to top&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Modalities - Ultrasound" /><category term="Clinical applications - Fetal Imaging" /><category term="Zhao, Cheng" /><category term="Droste, Richard" /><category term="Drukker, Lior" /><category term="Papageorghiou, Aris T." /><category term="Noble, J. Alison" /><summary type="html">Paper Info Reviews Meta-review(s) Author Feedback Authors Cheng Zhao, Richard Droste, Lior Drukker, Aris T. Papageorghiou, J. Alison Noble Abstract Automated ultrasound (US)-probe movement guidance is desirable to assist inexperienced human operators during obstetric US scanning. In this paper, we present a new visual-assisted probe movement technique using automated landmark retrieval for assistive obstetric US scanning. In a first step, a set of landmarks is constructed uniformly around a virtual 3D fetal model. Then, during obstetric scanning, a deep neural network (DNN) model locates the nearest landmark through descriptor search between the current observation and landmarks. The global position cues are visualised in real-time on a monitor to assist the human operator in probe movement. A Transformer-VLAD network is proposed to learn a global descriptor to represent each US image. This method abandons the need for deep parameter regression to enhance the generalization ability of the network. To avoid prohibitively expensive human annotation, anchor-positive-negative US image-pairs are automatically constructed through a KD-Tree search of 3D probe positions. This leads to an end-to-end network trained in a self-supervised way through contrastive learning. Link to paper https://doi.org/10.1007/978-3-030-87237-3_64 Link to the code repository https://github.com/dachengxiaocheng/Visual-Assisted-Probe-Movement.git Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors developed a method to direct a user of 2D ultrasound to move it to a specific pose during obstetrical imaging of the fetus. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The algorithm appears novel and the analysis is also appropriate; however, it is not clear why a 15mm distance was used as a metric. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Since you need to determine the pose of the acquired images, why not generate a 3D ultrasound image using the motion tracker and then examine the 3D image for the features required for evaluating the fetus. 3D ultrasound imaging in obstetrics is widely used and there are very many publications on this topic. How accurately must you know the pose information using the motion tracker? Adding a motion tracker adds cost to the system. What motion tracker did you use. If I understand correctly, the physician will need to acquire a large number of images for each patient, from which you will generate about 400 evenly distributed images as patients may be at different points during the pregnancy, fetal abnormalities may appear very differently, and the fetus may be in a different position at each examination. How long does that take? Does this aspect require special training and can the images be acquired freely with no specific method to scan the abdomen? For performance evaluation, why did you use 15mm as the distance target for a correctly retrieved landmark? This distance appears quite large if specific abnormalities are needed to be viewed. Is one retrieved landmark sufficient for clinical use? The rotation around a retrieved landmark may not be correct. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors will provide the code Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html See comments in the “weaknesses” section. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The main issue is the utility of the method with questions what not use 3D ultrasound images and issues around the motion tracking. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 2 Reviewer confidence Not Confident Review #2 Please describe the contribution of the paper This paper proposed a triplet-learning framework based on transformer and NetVLAD for landmark retrieval in probe movement guidance. The topN recall is promising based on the proposed method. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Strength: A good application paper based on the integration of Transformer, NetVLAD and triplet learning for landmark retrieval. The TopN recall is good compared to baselines. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Weakness: The novelty is limited; The claims of this work are not fully convincing, especially for the self-supervised part. Please refer to the detailed comments. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility of this work seems OK. According to the implementation part, this work follows the network architecture of transformer and NetVALD. The hyperparameters are also presented in details. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html This paper proposed a triplet framework for landmark retrieval. For techical contribution, the novelty is limited since the proposed method is a direct combination of transformer, NetVLAD and triplet learning. However, there are also several issues to be concerned: The use of NetVLAD seems not essential. Why the authors choose NetVLAD as the feature aggregation module? The only support is the good accuracy in Table 2. Do you think this is due to the special application in this work or it is a general observations? NetVLAD also has several extentions including TEN [1]. Since NetVLAD is not orginally proposed in this paper, I suggest that more aggeration layers should be evaluated. This work claims that it is a “Self-supervised Network Training” method. However, this is actually the way to construct the triplet (P,N,A) for model training based on the KD-Tree. For ‘self-supervised learning’, it usually requires no supervision which cannot fully match the pipeline of this work. [1] Zhang, et.al ‘Deep TEN’, CVPR 2016 Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The novelty of this method is limited and the experiment setup is not convincing. What is the ranking of this paper in your review stack? 5 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper Developing an algorithm that assists the US-probe movement by applying landmark retrieval. Proposing a network that generates a generalized descriptor for automatic landmark retrieval. Contrastive learning, a form of self-supervised learning, is used to avoid human annotations. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. A new approach providing visual-supported navigation unlike conventional methods that provide measured parameters only. Human annotation is minimized by using self-supervised learning, which is an effective method in the area of medical imaging where ground truth is hard to acquire. The network itself was designed by transforming conventional networks to accomplish the targeted specific tasks. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The clinical importance of the proposed work is controversial. Untrained individuals can be assisted by the proposed system but many trained sonographers can accomplish what they want without such assistance. The performance comparison with the CNN SOTA network seems insufficient. Other network types should be considered. There is no evidence or explanation as to whether the VGG network pre-trained with ImageNet is effective for ultrasound images. Five test cases were prepared, but only three results were shown for the 3rd trimester. The reason for using the attention feature is not clear. It is claimed to work in real-time, but no data on the execution time is provided. 7. 7. There is no information on how the test set is obtained. There are two typos (per-trained, SOAT). Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Authors say the results will be released as an open-source project to contribute to the US research community. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The approach is impressive and the understanding level of the neural network seems very high. It would be good if more test case images are provided. Although detailed explanations of the network were provided, adding motivation or justification of choosing such a network will help readers. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The clinical importance of the proposed work is controversial. Untrained individuals can be assisted by the proposed system but many trained sonographers can accomplish what they want without such assistance. The main academic contribution of this work is applying self-supervised learning in the new application of probe navigation. However, more rigorous explanation/justification/comparison of the proposed network should be given. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. All reviewers agreed this paper presented an interesting application and the proposed model is of certain interest. However, they also requested further clarification about the following issues. First, the proposed approach needs to be better motivated against 3D Ultrasound imaging widely used in obstetrics (Reviewer #1), and the concern about its clinic importance needs to be addressed (Reviewer #3). Second, more rigorous explanation or justification of the proposed network needs to be provided to facilitate understanding (Reviewer #2 and Reviewer #3). Third, please respond to Reviewer #2’s concern about the technical contribution. Fourth, please discuss the sensitivity of the proposed approach to the accuracy of pose information and explain why a 15mm distance was used as a metric in the evaluation (Reviewer #1). What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper addressed an interesting application that provides guide to the probe movement during obstetric ultrasound scanning, and formulates this problem as a landmark retrieval problem solved by the proposed Transformer-VLAD model. It contains several interesting seeds in the less studied application problem, the solution, and the SOTA network model, which could be of certain research interest to the community of MICCAI. In the rebuttal, the authors answered the major questions from the reviewers, especially with a new comparison with trans-TEN as requested by Reviewer#2. If it could be accepted for publication, the authors should highlight their response to the motivation and the clinical importance, and incorporate the clarifications to other major concerns in their final paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper has mixed reviews, and the meta-review lists many questions that the authors needed to address in the rebuttal. Unfortunately, it seems that the authors did not answer the AC’s questions and focused on other technical questions. Given that AC’s questions remain unanswered, I believe the paper needs to go through another round of revision, which means that I recommend it to be rejected at this stage. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 18 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper receives conflicting comments: 2 borderline acc, 1 prob rej. (R2) The rebuttal has addressed most of R2’s concerns (self-supervised learning, limited novelty) and therefore I will downplay R2’s opinions. The rebuttal also clarified various subtle points raised by R1 and R3. Overall, I think that the paper addresses a clinical important problem with a sensible approach and satisfactory results. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 9 Author Feedback We thank the reviewers for their valuable comments and respond to the main queries below. Why not pose as a 3DUS problem(R1) Our motivation is to support uptake of the new generation of lower-cost 2D US probes outside of traditional hospital setting, where users are not highly trained. Assistive guidance tools are potentially useful then. 3DUS is typically based in hospitals and performed by highly trained sonographers. This is why we pose the problem as a 2D US one. The proposed method could be adapted to the 3DUS case. The only required architecture modification is to replace the 2D CNN encoder with a 3D CNN one. Clinical importance(R3) Simplifying US to be more accessible to non-expert operators is a recognized priority for wider deployment of US in clinical practice. This paper aims to address this challenge, investigating technical feasibility of a novel image-retrieval based solution to provide helpful visualization cues to guide an operator during 2D fetal US scanning. The solution is implemented using data from a realistic data-based US simulator as a proof-of-principle. Future work would address issues in adapting the solution to a clinical setting and assessing its usability and usefulness to guide operators in practice. More rigorous network justification(R2, R3) Refer to Table2 and Sec.3.3; SOTA baseline comparisons[2][4] are reported. The patch-style encoder[2] lost some local geometry cues compared to CNN-backbone we used. The Transformer improves performance compared with its absence[1] as the attention mechanism inside extracts co-contextual cues within the feature vector. For US image retrieval, VLAD aggregation including more statistics mechanism performs better than maxpooling aggregation[4] commonly used in large-scale public benchmarks. A potential reason is it depends on dataset scale. Performance difference between maxpooling and VLAD may decrease with increased dataset scale. We find it also happens on the 3D point-cloud/lidar retrieval. Following R2, we report performance of Trans-TEN: r@1=82.5%, r@5=91.5%, r@10=94.0% as a supplement. DeepTEN has a similar mechanism to NetVLAD, hence similar performances. Technical novelty is limited(R2) We respectively disagree. We propose a new way to formulate US-probe movement guidance via automated landmark retrieval achieved by global descriptor learning and matching. Technical novelties: (1) the global descriptor is learned via contrastive learning using self-constructed A-P-N data-pairs by a probe position KD-Tree without human annotation; and (2) the proposed network is forerunner Transformer research investigating its image retrieval use. Independently [4] published a little earlier(arXiv not accepted,10/02/2021) c.f. MICCAI deadline 03/03/21. Our work focuses on US images, while [4] focuses on natural images. We will release the code as an open-source project on paper acceptance. Why is it self-supervised learning(R2) Unsupervised learning learns without supervision while self-supervision learns under the supervision from data itself without human annotation. The global descriptor is learned via contrastive learning using automatically constructed data pairs according to the probe-position KD-Tree without human annotation. We name it self-supervised learning as the supervision comes from the data only. Why use a 15mm distance in evaluation; sensitivity of method to pose information(R1) The hyper-parameter 15mm is empirically set according to the number of landmarks and 3D volume i.e. density of landmarks. It can be adjusted according to the specific clinical task. We have not used a motion tracker. The images come from a real clinical data-based US simulator(ScanTrainer) so the 3D relation between probe position and image is known. This 3D-to-2D image relation is only used in training. At test, and hence for use, you only need a 2D image(no motion tracker). Other minor points of clarification will be carefully addressed in the final paper. back to top</summary></entry><entry><title type="html">Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images</title><link href="https://kittywong.github.io/kittywong/0862/12/31/Paper1222" rel="alternate" type="text/html" title="Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images" /><published>0862-12-31T23:58:56-05:17</published><updated>0863-01-05T00:00:00-05:17</updated><id>https://kittywong.github.io/kittywong/0862/12/31/Paper1222</id><content type="html" xml:base="https://kittywong.github.io/kittywong/0862/12/31/Paper1222">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#author-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Paper Info&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#review-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Reviews&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#metareview-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Meta-review(s)&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#authorFeedback-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Author Feedback&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;author-id&quot;&gt;Authors&lt;/h1&gt;
&lt;p&gt;Devavrat Tomar, Lin Zhang, Tiziano Portenier, Orcun Goksel
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-id&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Interactive simulation of ultrasound imaging greatly facilitates sonography training. Although ray-tracing based methods have shown promising results, obtaining realistic images requires substantial modeling effort and manual parameter tuning. In addition, current techniques still result in a significant appearance gap between simulated images and real clinical scans. Herein we introduce a novel content-preserving image translation framework (ConPres) to bridge this appearance gap, while maintaining the simulated anatomical layout. We achieve this goal by leveraging both simulated images with semantic segmentations and unpaired in-vivo ultrasound scans. Our framework is based on recent contrastive unpaired translation techniques and we propose a regularization approach by learning an auxiliary segmentation-to-real image translation task, which encourages the disentanglement of content and style. In addition, we extend the generator to be class-conditional, which enables the incorporation of additional losses, in particular a cyclic consistency loss, to further improve the translation quality. Qualitative and quantitative comparisons against state-of-the-art unpaired translation methods demonstrate the superiority of our proposed framework.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;link-id&quot;&gt;Link to paper&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1007/978-3-030-87237-3_63&quot;&gt;https://doi.org/10.1007/978-3-030-87237-3_63&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;code-id&quot;&gt;Link to the code repository&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dataset-id&quot;&gt;Link to the dataset(s)&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;review-id&quot;&gt;Reviews&lt;/h1&gt;

&lt;h3 id=&quot;review-1&quot;&gt;Review #1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper describes an image translation framework synthesizing from simulated ultrasound images to realistic ones. The proposed network consists of a single generator and a single discriminator. The generator takes simulated image, its segmentation and an real image as input, computing contrastive loss and noise contrastive estimation loss which was proposed in previous work, sematic-consistent loss with the mask, and finally a cycle consistent loss using the same G twice in contrast to cycleGAN-like models.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper leverages recent advances of the domain adaptation work, such as contrastive loss for training, and kernel inception distance for evaluation which I found are interesting.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Unfortunately there are quite some and crucial weakness in this paper.
First,  lack of novelty. The only differences between this paper and starGAN are contrastive loss and semantic-consistent loss. The contrastive loss comes from Park, T. [16]. The semantic-consistent loss is just replacing X by S in the contrastive loss, since S is just another source image for generating images in Y’s domain. The overall novelty is very limited.
Second, the experiment design is poor and even wrong. The data splitting is improper which causes a high risk of ‘training and testing on a same image’. Please see comments part for details.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Reproducing can be hard, mainly because the simulation algorithm details are not listed (either main or supplementary script). Without the details of simulated images, reproducing the work can be very challenging.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Overall this paper is well written and organized. Advantages of many recent proposed works have been incorporated. Besides the lack of novelty issue, my biggest concern is on the data splitting part in Section 3 Experiments.&lt;/p&gt;

      &lt;ol&gt;
        &lt;li&gt;
          &lt;p&gt;For real in-vivo images, the authors collected 22 ultrasound sequences from 8 patients, each of them is only several seconds long. However, the author puts all frames together and makes an 80%-10%-10% data splitting. First of all, it is not possible to make such splitting over 8 patients, so this splitting is really done over all frames.  So there will be overlap in terms patients data between training/validation and testing. In addition, each sequence is only few seconds long. This means the variation in the data is very limited. Overlapping data, limited variation, this significantly weakens the claims of the experiments, especially for GAN related work.&lt;/p&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;I don’t think the segmentation-consistency regularization term is well validated by the experiments. From Fig 3., comparing between CUT and CUTS, on the 4th column, the segmentation corresponding area is lost in CUTS. In Table 1, CUTS is consistently worse than CUT for all metrics. Then what is the advantage of adding this term? Please comment.&lt;/p&gt;
        &lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;reject (3)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Reject. Lack of novelty. Proposed segmentation regularization is not well justified. Problematic data splitting in numerical experiments.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-2&quot;&gt;Review #2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors proposed a novel image translation framework to bridge significant appearance gap between simulated images and real clinical scans, which has two contributions: constrain the generator with the accompanying semantic labels of simulated images by learning an auxiliary segmentation-to-real image translation task and apply a class-conditional generator, which in turn enables the incorporation of a cyclic loss.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;-The proposed method is novel. Based on contrastive unpaired translation framework, the authors used the semantic labels of simulated images by learning an auxiliary segmentation-to-real image task to constrain the generator and extended a class-conditional generator to enable a cyclic loss.
-The qualitative and quantitative metrics proved that this framework closed the appearance gap between simulated and real images, which had a significant influence on the US images generation. And a user study was performed to evaluate the realism of translated images by US experts.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;-A more clear outline of the next steps in research would be appropriate.
-There are fewer data sets and all act on fetal images.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The details described in the paper and the reproducibility are good. No code and data set are provided.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;-A more clear outline of the next steps in research would be appropriate, some discussion but vague. There are already some new work based on CUT.
-The author should try to experiment on a larger data set, and based on different types of images, which is more convincing, now it is only fetal images.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Probably accept (7)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;-From the experimental results, the framework proposed by the authors can indeed bridge appearance gap between simulated images and real clinical scans to a certain extent.
-The algorithm has a certain degree of innovation.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-3&quot;&gt;Review #3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper takes a bag of losses to boost the image-to-image translation. Extensive experiments compared to SOTA methods on realistic datasets evaluated the effectiveness of the proposed method to improve the realism of computationally simulated US images.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper tries to use various losses in the image-to-image translation field to improve the realism of the generated images. According to the experimental results, the simulated images are much more realistic than other methods. The authors use typical evaluation metrics and invite experts to rate the qualities. The semantic-consistent regularization is well designed.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1 As claimed in the Abstract, the authors try to improve the quality of simulated US images, which can facilitate sonography training. The authors should evaluate how the generated images by the proposed method help sonography training, compared to the traditional methods.&lt;/p&gt;

      &lt;p&gt;2 More ablation studies and visualizations should be explored to show the effectiveness of the losses (loss of GAN, CLS, CYC, NCE)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Excellent&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;As the dataset is private and the code is not going be released, I am not sure this work can be reproduced.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Please see the “Weakness” part.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;borderline accept (6)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;I choose the “borderline accept” because of the great performance shown in the experimental results. However, due to the lack of reproducibility, I am not sure if this paper contributes more to society. On the other hand, It needs more experiments to evaluate the clinical effectiveness of those realistic simulated ultrasound images.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Somewhat confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;metareview-id&quot;&gt;Meta-Review(s)&lt;/h1&gt;

&lt;h2 id=&quot;primary-meta-review&quot;&gt;Primary Meta-Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;6&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers.&lt;/p&gt;

      &lt;p&gt;The rebuttal explains why the employed  data splitting is justified: “the goal is not to reproduce the real data, but to abstract and apply the style of the real training data.” This somehow alleviate the issue, however, it is still questionable in my view. For example, a common use of image synthesis is through a subsequent application such as classification, segmentation, etc. In this regard, a strict data splitting is necessary. The rebuttal provides the results based on patient-wise splitting and it seems that similar performance improvements are reported. The rebuttal also provides more ablation studies.&lt;/p&gt;

      &lt;p&gt;Overall, the paper meets the MICCAI acceptance level if the authors can make necessary changes per rebuttal.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;11&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;meta-review-2&quot;&gt;Meta-Review #2&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors propose a method to simulate realistic ultrasound images, by taking the structure from input simulated images, and the appearance from real images.  The method clearly work in translating the appearance to the generated images.
Authors have addressed the methodological criticisms, although I am not sure they have correctly addressed the criticisms regarding novelty.&lt;/p&gt;

      &lt;p&gt;Moreover, I am not sure they have addressed at all the issues related to clinical utility (for example raised by R3).&lt;/p&gt;

      &lt;p&gt;I have concerns about the user survey too. Authors said that experts were asked to rank images by “their likelihood for being an image from this machine”.  Even if the answer to this is highly positive, it fails to demonstrate that images look realistic, because they don’t. As pointed out by the authors themselves, “the simulated and the real images have substantially different anatomical contents”, i.e. the simulated fetal geometries are highly unrealistic. This has two implications: first, because of the lack of realism, the clinical utility is at best arguable (reason for which authors have not been able to rebut this point). Second, the challenge of obtaining realistic fetal geometries to generate realistic looking images might be greater than that of simulating realistic images. In their rebuttal, authors address one of the main criticisms (lack of novelty) by saying that “ our novelty is to enforce semantic consistency “, however this is precisely (as I pointed out) the main flaw. As a result, I cannot recommend acceptance.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Reject&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;15&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;meta-review-3&quot;&gt;Meta-Review #3&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The solution provided by the authors is interesting and the strongest arguments raised by the reviewers (novelty and data splitting) were properly explained and justified in the rebuttal phase. The rebuttal by the authors is clear, well argued and complete, and provides clarifications to the mayor points raised by the reviewers. The weakest point in the rebuttal is the justification and highlighting of the work novelty, yet novelty is detailed in the manuscript.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;10&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;br /&gt;&lt;br /&gt;&lt;/h2&gt;
&lt;h1 id=&quot;authorFeedback-id&quot;&gt;Author Feedback&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;We thank the reviewers for their valuable feedback. Major comments are addressed below, and will be integrated shortly in the paper:&lt;/p&gt;

  &lt;p&gt;** Data splitting (R1)
Other image analysis tasks such as classification and segmentation require evaluation on unseen patients to avoid bias. However, using train and/or test sets, even together, for computing GAN distribution metrics (FID/KID) is common practice [2,8, StyleGAN-Ada]. This is indeed more justified in an &lt;em&gt;unpaired&lt;/em&gt; setting as in our appearance transfer task, where the goal is not to reproduce the real data, but to abstract and apply the style of the real training data.&lt;/p&gt;

  &lt;p&gt;For computing FID/KID, we followed the approach from [8, StyleGAN-Ada] that introduced / used these metrics; i.e., we computed on the train set, since set distribution metrics require large samples for robust estimates. Indeed, adding the test set does not result in any difference (CUTS-C* KID: 0.24/0.24, FID 1.51/1.52 for train/all). The test set alone would be too small to compute these metrics robustly.&lt;/p&gt;

  &lt;p&gt;Moreover, if there were bias from data splitting, this would also affect our baselines, against which our method is still seen to be superior. Indeed, larger models in our baselines might even profit more, if there were any such bias.&lt;/p&gt;

  &lt;p&gt;That said, we have conducted an additional, patient-wise splitting experiment (6 patients for train). This yields a KID of 0.24, the same as earlier random data splitting.&lt;/p&gt;

  &lt;p&gt;** Limited real data variability (R1), More datasets (R3)
For transferring appearance, large variability in real data is not necessary. Indeed, even a single image in the target domain can be sufficient under certain circumstances (cf. [16]). Here we used thousands of real images to capture the appearance of a specific US machine.&lt;/p&gt;

  &lt;p&gt;Our goal here is task-specific, i.e., enhancing US simulation of the common 20-week fetal exams with real-machine like image appearance. Conceptually, there is no limitation in translating other US scenes (although a practical requirement would be the availability of detailed 3D models).&lt;/p&gt;

  &lt;p&gt;** Semantic-consistent regularization not well justified (R1)
We assume a misunderstanding regarding the ablated models: CUTS is CUT with an auxiliary segmentation to real translation, but without the proposed semantic-consistent regularization. As seen in Fig.3 and pointed out by R1, learning this auxiliary task alone does not improve CUT. However, by adding the regularization term (CUTS-C), the translated images are semantically more close to simulated ones, c.f. Fig. 3 (CUT vs. CUTS-C). This is also corroborated with the better SSIM of CUTS-C, clearly demonstrating the benefit of our proposed regularization approach. As the namings may be confusing w.r.t. ablations, and we will rename our method.&lt;/p&gt;

  &lt;p&gt;** Limited Novelty (R1)
For content preservation during appearance transfer, our novelty is to enforce semantic consistency with the proposed regularization during a multi-domain translation. Note that semantic regularization is not “replacing X by S in the contrastive loss”, but enforcing G to generate the same output for X and S.&lt;/p&gt;

  &lt;p&gt;** Future works (R3&amp;amp;R4), Clinical experiments (R3)
Future works include: clinical evaluation of the effects of translated images on US training; improving seg-to-real image translation to bypass any expensive rendering entirely; evaluations with other US/non-US scenes.&lt;/p&gt;

  &lt;p&gt;** More ablations for other losses (R4)
The experiments in the submission demonstrate the benefits of all proposed components. For completeness, we report additional comparisons here (FID/KID/SSIM): proposed model (1.51/0.24/72.13), without NCE (2.22/0.43/64.77), without CYC (2.33/0.45/84.77). The results show that these components are indeed essential. Without CYC, the translated images look similar to the simulated ones. GAN loss is what promotes realism; and without CLS, the discriminator would have no info on domain-dependent classification [5], so these are essential.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;back to top&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Modalities - Ultrasound" /><category term="Surgical Planning and Simulation" /><category term="Surgical Visualization and Mixed, Augmented and Virtual Reality" /><category term="Tomar, Devavrat" /><category term="Zhang, Lin" /><category term="Portenier, Tiziano" /><category term="Goksel, Orcun" /><summary type="html">Paper Info Reviews Meta-review(s) Author Feedback Authors Devavrat Tomar, Lin Zhang, Tiziano Portenier, Orcun Goksel Abstract Interactive simulation of ultrasound imaging greatly facilitates sonography training. Although ray-tracing based methods have shown promising results, obtaining realistic images requires substantial modeling effort and manual parameter tuning. In addition, current techniques still result in a significant appearance gap between simulated images and real clinical scans. Herein we introduce a novel content-preserving image translation framework (ConPres) to bridge this appearance gap, while maintaining the simulated anatomical layout. We achieve this goal by leveraging both simulated images with semantic segmentations and unpaired in-vivo ultrasound scans. Our framework is based on recent contrastive unpaired translation techniques and we propose a regularization approach by learning an auxiliary segmentation-to-real image translation task, which encourages the disentanglement of content and style. In addition, we extend the generator to be class-conditional, which enables the incorporation of additional losses, in particular a cyclic consistency loss, to further improve the translation quality. Qualitative and quantitative comparisons against state-of-the-art unpaired translation methods demonstrate the superiority of our proposed framework. Link to paper https://doi.org/10.1007/978-3-030-87237-3_63 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper describes an image translation framework synthesizing from simulated ultrasound images to realistic ones. The proposed network consists of a single generator and a single discriminator. The generator takes simulated image, its segmentation and an real image as input, computing contrastive loss and noise contrastive estimation loss which was proposed in previous work, sematic-consistent loss with the mask, and finally a cycle consistent loss using the same G twice in contrast to cycleGAN-like models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper leverages recent advances of the domain adaptation work, such as contrastive loss for training, and kernel inception distance for evaluation which I found are interesting. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Unfortunately there are quite some and crucial weakness in this paper. First, lack of novelty. The only differences between this paper and starGAN are contrastive loss and semantic-consistent loss. The contrastive loss comes from Park, T. [16]. The semantic-consistent loss is just replacing X by S in the contrastive loss, since S is just another source image for generating images in Y’s domain. The overall novelty is very limited. Second, the experiment design is poor and even wrong. The data splitting is improper which causes a high risk of ‘training and testing on a same image’. Please see comments part for details. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Reproducing can be hard, mainly because the simulation algorithm details are not listed (either main or supplementary script). Without the details of simulated images, reproducing the work can be very challenging. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Overall this paper is well written and organized. Advantages of many recent proposed works have been incorporated. Besides the lack of novelty issue, my biggest concern is on the data splitting part in Section 3 Experiments. For real in-vivo images, the authors collected 22 ultrasound sequences from 8 patients, each of them is only several seconds long. However, the author puts all frames together and makes an 80%-10%-10% data splitting. First of all, it is not possible to make such splitting over 8 patients, so this splitting is really done over all frames. So there will be overlap in terms patients data between training/validation and testing. In addition, each sequence is only few seconds long. This means the variation in the data is very limited. Overlapping data, limited variation, this significantly weakens the claims of the experiments, especially for GAN related work. I don’t think the segmentation-consistency regularization term is well validated by the experiments. From Fig 3., comparing between CUT and CUTS, on the 4th column, the segmentation corresponding area is lost in CUTS. In Table 1, CUTS is consistently worse than CUT for all metrics. Then what is the advantage of adding this term? Please comment. Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Reject. Lack of novelty. Proposed segmentation regularization is not well justified. Problematic data splitting in numerical experiments. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The authors proposed a novel image translation framework to bridge significant appearance gap between simulated images and real clinical scans, which has two contributions: constrain the generator with the accompanying semantic labels of simulated images by learning an auxiliary segmentation-to-real image translation task and apply a class-conditional generator, which in turn enables the incorporation of a cyclic loss. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -The proposed method is novel. Based on contrastive unpaired translation framework, the authors used the semantic labels of simulated images by learning an auxiliary segmentation-to-real image task to constrain the generator and extended a class-conditional generator to enable a cyclic loss. -The qualitative and quantitative metrics proved that this framework closed the appearance gap between simulated and real images, which had a significant influence on the US images generation. And a user study was performed to evaluate the realism of translated images by US experts. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -A more clear outline of the next steps in research would be appropriate. -There are fewer data sets and all act on fetal images. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The details described in the paper and the reproducibility are good. No code and data set are provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -A more clear outline of the next steps in research would be appropriate, some discussion but vague. There are already some new work based on CUT. -The author should try to experiment on a larger data set, and based on different types of images, which is more convincing, now it is only fetal images. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? -From the experimental results, the framework proposed by the authors can indeed bridge appearance gap between simulated images and real clinical scans to a certain extent. -The algorithm has a certain degree of innovation. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper takes a bag of losses to boost the image-to-image translation. Extensive experiments compared to SOTA methods on realistic datasets evaluated the effectiveness of the proposed method to improve the realism of computationally simulated US images. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper tries to use various losses in the image-to-image translation field to improve the realism of the generated images. According to the experimental results, the simulated images are much more realistic than other methods. The authors use typical evaluation metrics and invite experts to rate the qualities. The semantic-consistent regularization is well designed. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. 1 As claimed in the Abstract, the authors try to improve the quality of simulated US images, which can facilitate sonography training. The authors should evaluate how the generated images by the proposed method help sonography training, compared to the traditional methods. 2 More ablation studies and visualizations should be explored to show the effectiveness of the losses (loss of GAN, CLS, CYC, NCE) Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance As the dataset is private and the code is not going be released, I am not sure this work can be reproduced. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see the “Weakness” part. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I choose the “borderline accept” because of the great performance shown in the experimental results. However, due to the lack of reproducibility, I am not sure if this paper contributes more to society. On the other hand, It needs more experiments to evaluate the clinical effectiveness of those realistic simulated ultrasound images. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 2 Reviewer confidence Somewhat confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers. The rebuttal explains why the employed data splitting is justified: “the goal is not to reproduce the real data, but to abstract and apply the style of the real training data.” This somehow alleviate the issue, however, it is still questionable in my view. For example, a common use of image synthesis is through a subsequent application such as classification, segmentation, etc. In this regard, a strict data splitting is necessary. The rebuttal provides the results based on patient-wise splitting and it seems that similar performance improvements are reported. The rebuttal also provides more ablation studies. Overall, the paper meets the MICCAI acceptance level if the authors can make necessary changes per rebuttal. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors propose a method to simulate realistic ultrasound images, by taking the structure from input simulated images, and the appearance from real images. The method clearly work in translating the appearance to the generated images. Authors have addressed the methodological criticisms, although I am not sure they have correctly addressed the criticisms regarding novelty. Moreover, I am not sure they have addressed at all the issues related to clinical utility (for example raised by R3). I have concerns about the user survey too. Authors said that experts were asked to rank images by “their likelihood for being an image from this machine”. Even if the answer to this is highly positive, it fails to demonstrate that images look realistic, because they don’t. As pointed out by the authors themselves, “the simulated and the real images have substantially different anatomical contents”, i.e. the simulated fetal geometries are highly unrealistic. This has two implications: first, because of the lack of realism, the clinical utility is at best arguable (reason for which authors have not been able to rebut this point). Second, the challenge of obtaining realistic fetal geometries to generate realistic looking images might be greater than that of simulating realistic images. In their rebuttal, authors address one of the main criticisms (lack of novelty) by saying that “ our novelty is to enforce semantic consistency “, however this is precisely (as I pointed out) the main flaw. As a result, I cannot recommend acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 15 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The solution provided by the authors is interesting and the strongest arguments raised by the reviewers (novelty and data splitting) were properly explained and justified in the rebuttal phase. The rebuttal by the authors is clear, well argued and complete, and provides clarifications to the mayor points raised by the reviewers. The weakest point in the rebuttal is the justification and highlighting of the work novelty, yet novelty is detailed in the manuscript. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Author Feedback We thank the reviewers for their valuable feedback. Major comments are addressed below, and will be integrated shortly in the paper: ** Data splitting (R1) Other image analysis tasks such as classification and segmentation require evaluation on unseen patients to avoid bias. However, using train and/or test sets, even together, for computing GAN distribution metrics (FID/KID) is common practice [2,8, StyleGAN-Ada]. This is indeed more justified in an unpaired setting as in our appearance transfer task, where the goal is not to reproduce the real data, but to abstract and apply the style of the real training data. For computing FID/KID, we followed the approach from [8, StyleGAN-Ada] that introduced / used these metrics; i.e., we computed on the train set, since set distribution metrics require large samples for robust estimates. Indeed, adding the test set does not result in any difference (CUTS-C* KID: 0.24/0.24, FID 1.51/1.52 for train/all). The test set alone would be too small to compute these metrics robustly. Moreover, if there were bias from data splitting, this would also affect our baselines, against which our method is still seen to be superior. Indeed, larger models in our baselines might even profit more, if there were any such bias. That said, we have conducted an additional, patient-wise splitting experiment (6 patients for train). This yields a KID of 0.24, the same as earlier random data splitting. ** Limited real data variability (R1), More datasets (R3) For transferring appearance, large variability in real data is not necessary. Indeed, even a single image in the target domain can be sufficient under certain circumstances (cf. [16]). Here we used thousands of real images to capture the appearance of a specific US machine. Our goal here is task-specific, i.e., enhancing US simulation of the common 20-week fetal exams with real-machine like image appearance. Conceptually, there is no limitation in translating other US scenes (although a practical requirement would be the availability of detailed 3D models). ** Semantic-consistent regularization not well justified (R1) We assume a misunderstanding regarding the ablated models: CUTS is CUT with an auxiliary segmentation to real translation, but without the proposed semantic-consistent regularization. As seen in Fig.3 and pointed out by R1, learning this auxiliary task alone does not improve CUT. However, by adding the regularization term (CUTS-C), the translated images are semantically more close to simulated ones, c.f. Fig. 3 (CUT vs. CUTS-C). This is also corroborated with the better SSIM of CUTS-C, clearly demonstrating the benefit of our proposed regularization approach. As the namings may be confusing w.r.t. ablations, and we will rename our method. ** Limited Novelty (R1) For content preservation during appearance transfer, our novelty is to enforce semantic consistency with the proposed regularization during a multi-domain translation. Note that semantic regularization is not “replacing X by S in the contrastive loss”, but enforcing G to generate the same output for X and S. ** Future works (R3&amp;amp;R4), Clinical experiments (R3) Future works include: clinical evaluation of the effects of translated images on US training; improving seg-to-real image translation to bypass any expensive rendering entirely; evaluations with other US/non-US scenes. ** More ablations for other losses (R4) The experiments in the submission demonstrate the benefits of all proposed components. For completeness, we report additional comparisons here (FID/KID/SSIM): proposed model (1.51/0.24/72.13), without NCE (2.22/0.43/64.77), without CYC (2.33/0.45/84.77). The results show that these components are indeed essential. Without CYC, the translated images look similar to the simulated ones. GAN loss is what promotes realism; and without CLS, the discriminator would have no info on domain-dependent classification [5], so these are essential. back to top</summary></entry><entry><title type="html">Weakly-Supervised Ultrasound Video Segmentation with Minimal Annotations</title><link href="https://kittywong.github.io/kittywong/0861/12/31/Paper0403" rel="alternate" type="text/html" title="Weakly-Supervised Ultrasound Video Segmentation with Minimal Annotations" /><published>0861-12-31T23:58:56-05:17</published><updated>0862-01-05T00:00:00-05:17</updated><id>https://kittywong.github.io/kittywong/0861/12/31/Paper0403</id><content type="html" xml:base="https://kittywong.github.io/kittywong/0861/12/31/Paper0403">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#author-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Paper Info&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#review-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Reviews&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#metareview-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Meta-review(s)&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#authorFeedback-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Author Feedback&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;author-id&quot;&gt;Authors&lt;/h1&gt;
&lt;p&gt;Ruiheng Chang, Dong Wang, Haiyan Guo, Jia Ding, Liwei Wang
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-id&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Ultrasound segmentation models provide powerful tools for the diagnosis process of ultrasound examinations. However, developing such models for ultrasound videos requires densely annotated segmentation masks for all frames in a dataset, which is unpractical and unaffordable. Therefore, we propose a weakly-supervised learning (WSL) approach to accomplish the goal of video-based ultrasound segmentation. By only annotating the location of the start and end frames of the lesions, we obtain frame-level binary labels for WSL. We design Video Co-Attention Network to learn the correspondence between frames, where CAM and co-CAM will be obtained to perform lesion localization. Moreover, we find that the essential factor to the success of extracting video-level information is applying our proposed consistency regularization between CAM and co-CAM. Our method achieves an mIoU score of 45.43\% in the breast ultrasound dataset, which significantly outperforms the baseline methods. The codes of our models will be released.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;link-id&quot;&gt;Link to paper&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1007/978-3-030-87237-3_62&quot;&gt;https://doi.org/10.1007/978-3-030-87237-3_62&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;code-id&quot;&gt;Link to the code repository&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dataset-id&quot;&gt;Link to the dataset(s)&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;review-id&quot;&gt;Reviews&lt;/h1&gt;

&lt;h3 id=&quot;review-1&quot;&gt;Review #1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper presents a weakly-supervised ultrasound video segmentation method in the context of breast lesion. The locations of the start and end frames of the lesions are the only source of supervision for the task of lesion segmentation, hence this work falls in the category of “weakly-supervised learning”. The author proposes a co-attention mechanism that learns the frame-wise correspondence and utilizes and optimize class activation maps (CAM) to segment lesion. The proposed method outperforms baseline models in terms of the mIoU score.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The strengths of the paper are listed as follows in terms of technical contributions and clinical relevance:&lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;
          &lt;p&gt;The paper is relatively easy to follow in terms of technical expression. It is well organized and self-contained. Most details are well conveyed. The investigated issue involves segmenting lesions from ultrasound videos, which is of certain clinical relevance to provide real-time feedbacks during breast ultrasound examination .&lt;/p&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;The proposed video co-attention network is of certain technical novelty. The author introduces a co-attention module that models correlation between the CAM of two frames. By applying a consistency loss, CAM’s bias towards larger region is alleviated to some extent.&lt;/p&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The weakness of the paper is listed as follows in terms of technical details:&lt;/p&gt;

      &lt;ul&gt;
        &lt;li&gt;In terms of experimental results, it is usually expected to conduct cross-validation and statistical hypothesis testing for in-house dataset. This part is missing in the paper.&lt;/li&gt;
        &lt;li&gt;Section 4.4 Ablation study is actually about hyper-parameter tuning. The reviewer is expected that the impact of three loss terms were analyzed respectively. How important is the nGWP structure as a practical design choice?&lt;/li&gt;
        &lt;li&gt;In video-related analysis, the processing speed in terms of frame-per-second is a key factor to consider. This is missing in the experimental section.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The author claims to open-source the code. The reviewer wonders whether the in-house dataset shall be released as well. The architecture is well explained. Overall, the reproducibility should be good.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;The reviewer suggests to enrich the experimental results with cross-validation and further ablation studies as stated in Question 5.&lt;/li&gt;
        &lt;li&gt;The baselines do not contain any published work, such as literature [1] and [20]. It is expected to benchmark more widely, especially for a in-house dataset.&lt;/li&gt;
        &lt;li&gt;Put at least one figure of segmentation results that contain comparisons between baselines in the main paper, not the supplementary material.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Probably accept (7)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The concern on technical novelty and relevance to clinical applications are the major factors that lead to the reviewer’s overall score.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-2&quot;&gt;Review #2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper proposed a weakly-supervised video segmentation method for ultrasound video. The main contribution of the paper is the Consistency Loss between CAM (Class Attention Map) and co-CAM (Co-Attention CAM).&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;The proposed consistency loss is very effective as shown in the experimental results.&lt;/li&gt;
        &lt;li&gt;The authors promise to release code, which will be very helpful for reproductivity purpose.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Because CAM and co-CAM are proposed by prior works, so “CAM and co-CAM without consistency loss” should be one of the baselines in Table 1.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The code will be released.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Please give the full name of CAM and co-CAM in introduction.&lt;/li&gt;
        &lt;li&gt;In 4th paragraph in Page 5, “discriminate” should be “discriminative”.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;borderline accept (6)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;The consistency loss is very effective as shown by the experiments.&lt;/li&gt;
        &lt;li&gt;One concern is the lack of more convincing explanation why CAM and co-CAM without consistency works for natural video tasks but totally unhelpful in this paper.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;3&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;3&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Confident but not absolutely certain&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-3&quot;&gt;Review #3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper proposed a weakly supervised segmentation network for ultrasound video. CAM and co-attention are adopted in the network to generate pseudo labels, a consistency loss between cam and co-cam is proposed.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;This paper is well written and organized.&lt;/li&gt;
        &lt;li&gt;The consistency loss between cam and co-cam seems novel and their statements of the rationale is reasonable.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Both cam and co-cam are from published work (Ref [28] and Ref[20]).&lt;/li&gt;
        &lt;li&gt;No comparison with other published SOTA.&lt;/li&gt;
      &lt;/ul&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper provides sufficient details about the models/algorithms, datasets, and evaluation.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Since both cam and Co-cam predicted larger region than the ROI. Why Co-cam make aggressive predictions?&lt;/li&gt;
        &lt;li&gt;Cam and Co-cam predicted larger region, will it be possible that the consistency loss between these two module lead to a pesudo label with larger ROI?&lt;/li&gt;
        &lt;li&gt;Authors should compare their method with SOTA.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Probably accept (7)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;The presentation of this paper is clear.&lt;/li&gt;
        &lt;li&gt;The method is some how novel.&lt;/li&gt;
        &lt;li&gt;They used ultrasound video instead of 2D images, which is more relevant to clinical.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;metareview-id&quot;&gt;Meta-Review(s)&lt;/h1&gt;

&lt;h2 id=&quot;primary-meta-review&quot;&gt;Primary Meta-Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper received three reviews with positive feedback. The reviewers collectively raise several concerns. The authors are encouraged to address those in the final version. These issues include statistical hypothesis testing, proper ablation study, qualitative comparisons between baselines, and proper selection of baselines.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;3&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;authorFeedback-id&quot;&gt;Author Feedback&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;N/A&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;back to top&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Modalities - Ultrasound" /><category term="Clinical applications - Breast" /><category term="Machine Learning - Weakly supervised learning" /><category term="Chang, Ruiheng" /><category term="Wang, Dong" /><category term="Guo, Haiyan" /><category term="Ding, Jia" /><category term="Wang, Liwei" /><summary type="html">Paper Info Reviews Meta-review(s) Author Feedback Authors Ruiheng Chang, Dong Wang, Haiyan Guo, Jia Ding, Liwei Wang Abstract Ultrasound segmentation models provide powerful tools for the diagnosis process of ultrasound examinations. However, developing such models for ultrasound videos requires densely annotated segmentation masks for all frames in a dataset, which is unpractical and unaffordable. Therefore, we propose a weakly-supervised learning (WSL) approach to accomplish the goal of video-based ultrasound segmentation. By only annotating the location of the start and end frames of the lesions, we obtain frame-level binary labels for WSL. We design Video Co-Attention Network to learn the correspondence between frames, where CAM and co-CAM will be obtained to perform lesion localization. Moreover, we find that the essential factor to the success of extracting video-level information is applying our proposed consistency regularization between CAM and co-CAM. Our method achieves an mIoU score of 45.43\% in the breast ultrasound dataset, which significantly outperforms the baseline methods. The codes of our models will be released. Link to paper https://doi.org/10.1007/978-3-030-87237-3_62 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper presents a weakly-supervised ultrasound video segmentation method in the context of breast lesion. The locations of the start and end frames of the lesions are the only source of supervision for the task of lesion segmentation, hence this work falls in the category of “weakly-supervised learning”. The author proposes a co-attention mechanism that learns the frame-wise correspondence and utilizes and optimize class activation maps (CAM) to segment lesion. The proposed method outperforms baseline models in terms of the mIoU score. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The strengths of the paper are listed as follows in terms of technical contributions and clinical relevance: The paper is relatively easy to follow in terms of technical expression. It is well organized and self-contained. Most details are well conveyed. The investigated issue involves segmenting lesions from ultrasound videos, which is of certain clinical relevance to provide real-time feedbacks during breast ultrasound examination . The proposed video co-attention network is of certain technical novelty. The author introduces a co-attention module that models correlation between the CAM of two frames. By applying a consistency loss, CAM’s bias towards larger region is alleviated to some extent. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The weakness of the paper is listed as follows in terms of technical details: In terms of experimental results, it is usually expected to conduct cross-validation and statistical hypothesis testing for in-house dataset. This part is missing in the paper. Section 4.4 Ablation study is actually about hyper-parameter tuning. The reviewer is expected that the impact of three loss terms were analyzed respectively. How important is the nGWP structure as a practical design choice? In video-related analysis, the processing speed in terms of frame-per-second is a key factor to consider. This is missing in the experimental section. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The author claims to open-source the code. The reviewer wonders whether the in-house dataset shall be released as well. The architecture is well explained. Overall, the reproducibility should be good. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The reviewer suggests to enrich the experimental results with cross-validation and further ablation studies as stated in Question 5. The baselines do not contain any published work, such as literature [1] and [20]. It is expected to benchmark more widely, especially for a in-house dataset. Put at least one figure of segmentation results that contain comparisons between baselines in the main paper, not the supplementary material. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The concern on technical novelty and relevance to clinical applications are the major factors that lead to the reviewer’s overall score. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The paper proposed a weakly-supervised video segmentation method for ultrasound video. The main contribution of the paper is the Consistency Loss between CAM (Class Attention Map) and co-CAM (Co-Attention CAM). Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The proposed consistency loss is very effective as shown in the experimental results. The authors promise to release code, which will be very helpful for reproductivity purpose. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Because CAM and co-CAM are proposed by prior works, so “CAM and co-CAM without consistency loss” should be one of the baselines in Table 1. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The code will be released. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please give the full name of CAM and co-CAM in introduction. In 4th paragraph in Page 5, “discriminate” should be “discriminative”. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The consistency loss is very effective as shown by the experiments. One concern is the lack of more convincing explanation why CAM and co-CAM without consistency works for natural video tasks but totally unhelpful in this paper. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper proposed a weakly supervised segmentation network for ultrasound video. CAM and co-attention are adopted in the network to generate pseudo labels, a consistency loss between cam and co-cam is proposed. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper is well written and organized. The consistency loss between cam and co-cam seems novel and their statements of the rationale is reasonable. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Both cam and co-cam are from published work (Ref [28] and Ref[20]). No comparison with other published SOTA. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper provides sufficient details about the models/algorithms, datasets, and evaluation. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Since both cam and Co-cam predicted larger region than the ROI. Why Co-cam make aggressive predictions? Cam and Co-cam predicted larger region, will it be possible that the consistency loss between these two module lead to a pesudo label with larger ROI? Authors should compare their method with SOTA. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The presentation of this paper is clear. The method is some how novel. They used ultrasound video instead of 2D images, which is more relevant to clinical. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper received three reviews with positive feedback. The reviewers collectively raise several concerns. The authors are encouraged to address those in the final version. These issues include statistical hypothesis testing, proper ablation study, qualitative comparisons between baselines, and proper selection of baselines. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback N/A back to top</summary></entry><entry><title type="html">Identifying Quantitative and Explanatory Tumor Indexes from Dynamic Contrast Enhanced Ultrasound</title><link href="https://kittywong.github.io/kittywong/0860/12/31/Paper0135" rel="alternate" type="text/html" title="Identifying Quantitative and Explanatory Tumor Indexes from Dynamic Contrast Enhanced Ultrasound" /><published>0860-12-31T23:58:56-05:17</published><updated>0861-01-05T00:00:00-05:17</updated><id>https://kittywong.github.io/kittywong/0860/12/31/Paper0135</id><content type="html" xml:base="https://kittywong.github.io/kittywong/0860/12/31/Paper0135">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#author-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Paper Info&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#review-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Reviews&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#metareview-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Meta-review(s)&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#authorFeedback-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Author Feedback&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;author-id&quot;&gt;Authors&lt;/h1&gt;
&lt;p&gt;Peng Wan, Chunrui Liu, Fang Chen, Jing Qin, Daoqiang Zhang
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-id&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Contrast-enhanced ultrasound (CEUS) has been one of the most promising imaging techniques in tumor differential diagnosis since the real-time view of intra-tumor blood microcirculation.  Existing studies primarily focus on extracting those discriminative imaging features whereas lack medical explanations. However, accurate quantitation of some clinical experience-driven indexes regarding intra-tumor vascularity, such as tumor infiltration and heterogeneity, still faces significant limitations. To tackle this problem, we present a novel scheme to identify quantitative and explanatory tumor indexes from dynamic CEUS sequences. Specifically, our method mainly comprises three steps: 1) extracting the stable pixel-level perfusion pattern from dynamic CEUS imaging using an improved stable principal component pursuit (SPCP) algorithm; 2) performing local perfusion variation comparison by the proposed Phase-constrained Wasserstein (PCW) distance; 3) estimating three clinical knowledge-induced tumor indexes, i.e. infiltration, regularity, and heterogeneity. The effectiveness of this method was evaluated on our collected CEUS dataset of thyroid nodules, and the resulting infiltration and heterogeneity index with p &amp;lt; 0.05 between different pathological types validated the efficacy of this quantitation scheme in thyroid nodule diagnosis.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;link-id&quot;&gt;Link to paper&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1007/978-3-030-87237-3_61&quot;&gt;https://doi.org/10.1007/978-3-030-87237-3_61&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;code-id&quot;&gt;Link to the code repository&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dataset-id&quot;&gt;Link to the dataset(s)&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;review-id&quot;&gt;Reviews&lt;/h1&gt;

&lt;h3 id=&quot;review-1&quot;&gt;Review #1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Using Dynamic Contrast Enhanced Ultrasound to identify quantitative and explanatory tumor indexes. (CEU)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;Novel method : using DCEU for quantitative evaluation indexes of tumor.&lt;/li&gt;
        &lt;li&gt;Boxplot diagram helps to explain the result achievement.&lt;/li&gt;
        &lt;li&gt;Methods to be used were explained comprehensively.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;Experimental setup of the research explanation is not enough for understanding of the process which can lead to confusion and misunderstanding. Maybe more explanation such as written for section method needed here.&lt;/li&gt;
        &lt;li&gt;Tools used for the experimentation were not mentioned, difficult to fathom how results were achieved.
3.Does author did comparison study of the method and how to achieve the indexes presented.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;No reproducibility criteria was selected.
Citation for existing dataset were included in the paper writing.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;Very comprehensive methods used for the research were explained in detail.&lt;/li&gt;
        &lt;li&gt;It would be helpful to include diagram in the explaination of method.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;accept (8)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;Experiment done and results discussed in the paper help justify the proposed method.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-2&quot;&gt;Review #2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;A method to quantify dynamic contrast-enhanced ultrasound (CEUS) images with is proposed. Three new measures are derived from the acquired signal on a pixel-by-pixel and analyzed for their usefulness as tumor indexes (tumor infiltration, shape regularity, and perfusion heterogeneity). The method is applied to the data acquired from 55 subjects with three pathological types of thyroid nodules showing some of the indexes can discriminate benign from malignant.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper has the following strengths:&lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;Quantitative measures from CEUS, including novel algorithms to extract stable perfusion pattern to account for irregular microbubble destruction leading to inconsistent intensity changes.&lt;/li&gt;
        &lt;li&gt;Use of phase-constrained Wasserstein distance for perfusion pattern analysis.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Some weaknesses:&lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;(minor) Some more details about the dataset could have been included (US equipment/vendor; same or different operators; acquisition parameters)&lt;/li&gt;
        &lt;li&gt;(trivial) Need to define which of the three pathological types made up the benign and malignant classes.&lt;/li&gt;
        &lt;li&gt;(minor) boxplots are presented for showing tumor indexes between three types but no statistical comparisons (though the boxplots seem to imply no statistically significant differences).&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The work will be difficult to reproduce as the data and code are not available and there are some parameters used without values specified.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Overall a well-written paper. Addressing the noted weaknesses would lead to an excellent paper.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;accept (8)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Novel application for generating tumor indexes from dynamic CEUS.
Presentation of methods and results is organized and well written.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;3&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-3&quot;&gt;Review #3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper proposed an approach in which three quantitative tumor indexes are extracted from contrast-enhanced ultrasound. The introduced indexes might be useful in classification of patients which is important in timely diagnosis and treatment of diseases.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The introduced indexes are novel while a well designed approach for extraction is proposed.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;The proposed indexes are not visualized in the feature domain to see how distinguishable they are.&lt;/li&gt;
        &lt;li&gt;It is not clear why do the authors call the proposed indexes as clinical knowledge induced tumor indexes!&lt;/li&gt;
        &lt;li&gt;The assumptions of the statistical analysis are not investigated to see whether the result of applied test is valid or not.&lt;/li&gt;
        &lt;li&gt;The comparison between proposed method and other approached for tumor classification is not completed.&lt;/li&gt;
      &lt;/ul&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;I would think that the method is well explained and the results are reproducable.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Here are my comments/questions:&lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;What does the abbreviation AUC stand for? is it area under the perfusion curve?&lt;/li&gt;
        &lt;li&gt;I would be beneficial to compare the performance of the proposed method with other approached for tumor classification such as quantitative ultrasound, electrography, etc.&lt;/li&gt;
        &lt;li&gt;It is necessary to investigate the distribution of your data. Are the data normally distributed? The corresponding test has to be completed before running t-test. I am not sure whether your result are valid or not.&lt;/li&gt;
        &lt;li&gt;Details regarding your imaging settings such as transmit center frequency, sampling frequency, imaging technique, etc are missing.&lt;/li&gt;
        &lt;li&gt;More information regarding the preprocessing steps that you have applied on the original Bmode images helps readers to better understand your method. The current explanation is vague.&lt;/li&gt;
        &lt;li&gt;The proposed indexes have to be visualized in the feature domain to see how distinguishable they are.&lt;/li&gt;
      &lt;/ul&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;accept (8)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Please see the answers to questions 3,4, and 7.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Somewhat confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;metareview-id&quot;&gt;Meta-Review(s)&lt;/h1&gt;

&lt;h2 id=&quot;primary-meta-review&quot;&gt;Primary Meta-Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper proposes a novel scheme for quantitative evaluation of contrast enhanced ultrasound (CEUS). Generally, the paper touches base on an interesting topic to the medical image computing, and there is a consensus among all reviewers about the novelty, clarity and merits, of the presented work. The paper is also well-organized and easy to follow. A few comments and suggestions related to datasets details and experiments settings (for reproducibility) should be addressed in the camera-ready version. Please add summary of the quantitative results to the abstract. I also agree with R3 to include a feature-domain visualization for the extracted index as well as a comparison with other approaches for tumor classification such as quantitative ultrasound. Please also define abbreviation (e.g., AUC)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;authorFeedback-id&quot;&gt;Author Feedback&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;N/A&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;back to top&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Modalities - Ultrasound" /><category term="Clinical applications - Abdomen" /><category term="Wan, Peng" /><category term="Liu, Chunrui" /><category term="Chen, Fang" /><category term="Qin, Jing" /><category term="Zhang, Daoqiang" /><summary type="html">Paper Info Reviews Meta-review(s) Author Feedback Authors Peng Wan, Chunrui Liu, Fang Chen, Jing Qin, Daoqiang Zhang Abstract Contrast-enhanced ultrasound (CEUS) has been one of the most promising imaging techniques in tumor differential diagnosis since the real-time view of intra-tumor blood microcirculation. Existing studies primarily focus on extracting those discriminative imaging features whereas lack medical explanations. However, accurate quantitation of some clinical experience-driven indexes regarding intra-tumor vascularity, such as tumor infiltration and heterogeneity, still faces significant limitations. To tackle this problem, we present a novel scheme to identify quantitative and explanatory tumor indexes from dynamic CEUS sequences. Specifically, our method mainly comprises three steps: 1) extracting the stable pixel-level perfusion pattern from dynamic CEUS imaging using an improved stable principal component pursuit (SPCP) algorithm; 2) performing local perfusion variation comparison by the proposed Phase-constrained Wasserstein (PCW) distance; 3) estimating three clinical knowledge-induced tumor indexes, i.e. infiltration, regularity, and heterogeneity. The effectiveness of this method was evaluated on our collected CEUS dataset of thyroid nodules, and the resulting infiltration and heterogeneity index with p &amp;lt; 0.05 between different pathological types validated the efficacy of this quantitation scheme in thyroid nodule diagnosis. Link to paper https://doi.org/10.1007/978-3-030-87237-3_61 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper Using Dynamic Contrast Enhanced Ultrasound to identify quantitative and explanatory tumor indexes. (CEU) Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Novel method : using DCEU for quantitative evaluation indexes of tumor. Boxplot diagram helps to explain the result achievement. Methods to be used were explained comprehensively. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Experimental setup of the research explanation is not enough for understanding of the process which can lead to confusion and misunderstanding. Maybe more explanation such as written for section method needed here. Tools used for the experimentation were not mentioned, difficult to fathom how results were achieved. 3.Does author did comparison study of the method and how to achieve the indexes presented. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No reproducibility criteria was selected. Citation for existing dataset were included in the paper writing. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Very comprehensive methods used for the research were explained in detail. It would be helpful to include diagram in the explaination of method. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Experiment done and results discussed in the paper help justify the proposed method. What is the ranking of this paper in your review stack? 5 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper A method to quantify dynamic contrast-enhanced ultrasound (CEUS) images with is proposed. Three new measures are derived from the acquired signal on a pixel-by-pixel and analyzed for their usefulness as tumor indexes (tumor infiltration, shape regularity, and perfusion heterogeneity). The method is applied to the data acquired from 55 subjects with three pathological types of thyroid nodules showing some of the indexes can discriminate benign from malignant. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper has the following strengths: Quantitative measures from CEUS, including novel algorithms to extract stable perfusion pattern to account for irregular microbubble destruction leading to inconsistent intensity changes. Use of phase-constrained Wasserstein distance for perfusion pattern analysis. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Some weaknesses: (minor) Some more details about the dataset could have been included (US equipment/vendor; same or different operators; acquisition parameters) (trivial) Need to define which of the three pathological types made up the benign and malignant classes. (minor) boxplots are presented for showing tumor indexes between three types but no statistical comparisons (though the boxplots seem to imply no statistically significant differences). Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The work will be difficult to reproduce as the data and code are not available and there are some parameters used without values specified. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Overall a well-written paper. Addressing the noted weaknesses would lead to an excellent paper. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novel application for generating tumor indexes from dynamic CEUS. Presentation of methods and results is organized and well written. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper proposed an approach in which three quantitative tumor indexes are extracted from contrast-enhanced ultrasound. The introduced indexes might be useful in classification of patients which is important in timely diagnosis and treatment of diseases. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The introduced indexes are novel while a well designed approach for extraction is proposed. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The proposed indexes are not visualized in the feature domain to see how distinguishable they are. It is not clear why do the authors call the proposed indexes as clinical knowledge induced tumor indexes! The assumptions of the statistical analysis are not investigated to see whether the result of applied test is valid or not. The comparison between proposed method and other approached for tumor classification is not completed. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I would think that the method is well explained and the results are reproducable. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Here are my comments/questions: What does the abbreviation AUC stand for? is it area under the perfusion curve? I would be beneficial to compare the performance of the proposed method with other approached for tumor classification such as quantitative ultrasound, electrography, etc. It is necessary to investigate the distribution of your data. Are the data normally distributed? The corresponding test has to be completed before running t-test. I am not sure whether your result are valid or not. Details regarding your imaging settings such as transmit center frequency, sampling frequency, imaging technique, etc are missing. More information regarding the preprocessing steps that you have applied on the original Bmode images helps readers to better understand your method. The current explanation is vague. The proposed indexes have to be visualized in the feature domain to see how distinguishable they are. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Please see the answers to questions 3,4, and 7. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Somewhat confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper proposes a novel scheme for quantitative evaluation of contrast enhanced ultrasound (CEUS). Generally, the paper touches base on an interesting topic to the medical image computing, and there is a consensus among all reviewers about the novelty, clarity and merits, of the presented work. The paper is also well-organized and easy to follow. A few comments and suggestions related to datasets details and experiments settings (for reproducibility) should be addressed in the camera-ready version. Please add summary of the quantitative results to the abstract. I also agree with R3 to include a feature-domain visualization for the extracted index as well as a comparison with other approaches for tumor classification such as quantitative ultrasound. Please also define abbreviation (e.g., AUC) What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback N/A back to top</summary></entry><entry><title type="html">USCL: Pretraining Deep Ultrasound Image Diagnosis Model through Video Contrastive Representation Learning</title><link href="https://kittywong.github.io/kittywong/0859/12/31/Paper0040" rel="alternate" type="text/html" title="USCL: Pretraining Deep Ultrasound Image Diagnosis Model through Video Contrastive Representation Learning" /><published>0859-12-31T23:58:56-05:17</published><updated>0860-01-05T00:00:00-05:17</updated><id>https://kittywong.github.io/kittywong/0859/12/31/Paper0040</id><content type="html" xml:base="https://kittywong.github.io/kittywong/0859/12/31/Paper0040">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#author-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Paper Info&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#review-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Reviews&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#metareview-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Meta-review(s)&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#authorFeedback-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Author Feedback&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;author-id&quot;&gt;Authors&lt;/h1&gt;
&lt;p&gt;Yixiong Chen, Chunhui Zhang, Li Liu, Cheng Feng, Changfeng Dong, Yongfang Luo, Xiang Wan
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-id&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Most deep neural networks (DNNs) based ultrasound (US) medical image analysis models use pretrained backbones (e.g. ImageNet) for better model generalization. However, the domain gap between natural and medical images causes an inevitable performance bottleneck. To alleviate this problem, an US dataset named US-4 is constructed for direct pretraining on the same domain. It contains over 23,000 images from four US video sub-datasets. To learn robust features from US-4, we propose an US semi-supervised contrastive learning method, named USCL, for pretraining. In order to avoid high similarities between negative pairs as well as mine abundant visual features from limited US videos, USCL adopts a sample pair generation method to enrich the feature involved in a single step of contrastive optimization. Extensive experiments on several downstream tasks show the superiority of USCL pretraining against ImageNet pretraining and other state-of-the-art (SOTA) pretraining approaches. In particular, USCL pretrained backbone achieves fine-tuning accuracy of over 94% on POCUS dataset, which is 10% higher than 84% of the ImageNet pretrained model. The source codes of this work are available at https://github.com/983632847/USCL.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;link-id&quot;&gt;Link to paper&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1007/978-3-030-87237-3_60&quot;&gt;https://doi.org/10.1007/978-3-030-87237-3_60&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;code-id&quot;&gt;Link to the code repository&lt;/h1&gt;
&lt;p&gt;https://github.com/983632847/USCL
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dataset-id&quot;&gt;Link to the dataset(s)&lt;/h1&gt;
&lt;p&gt;https://github.com/983632847/USCL
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;review-id&quot;&gt;Reviews&lt;/h1&gt;

&lt;h3 id=&quot;review-1&quot;&gt;Review #1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper proposes a novel methodology for pretraining a deep neural network to solve ultrasound-related tasks such as classification, detection and segmentation, with the aim to have a better model generalisation. The proposed method in based on a semi-supervised contrastive learning approach, where frames from the same video are considered as a positive pair (i.e., similar) and frames from different videos are considered as a negative pair (i.e., different). The pretrained network is trained on ultrasound images from lung and liver, and the evaluation is performed on two ultrasound datasets, one composed of lung images and the other one of breast images.
The ablation study compares the proposed pretraining methodology with a network pretrained on natural images (ImageNet) and shows an improvement in all downstream tasks (classification, detection and segmentation).&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Novel idea for pretraining deep neural networks to solve ultrasound-related tasks&lt;/li&gt;
        &lt;li&gt;Simple but original methodology to solve the similarity conflict in a semi-supervised contrastive representation learning, including an enriched approach to create positive pairs (i.e., pairs of similar images).&lt;/li&gt;
        &lt;li&gt;Code and data will be made public, which could facilitate the use of this methodology and be of interest in the research community as currently there is a lack of pre-trained models for ultrasound.&lt;/li&gt;
      &lt;/ul&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Limited evaluation and discussion on what combination of datasets may work better. For instance, is it preferable to pretrain the method on a collection of datasets that are very different to each other (e.g., lung, liver) ? Or is it preferred to create a dataset with similar characteristics as the task to be solved (e.g., lung centre 1, lung centre 2)? The ablation study provided in the Annex shows results on different dataset combinations, however it is not clear if the increase in accuracy is due to the increase in data size or due to the increase in image variation.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Excellent&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Some details are missing, for instance: fine-tuning is used on two datasets: POCUS (lung) and UDIAT-B (breast), but the fine-tuning learning rate is not mentioned. The amount of images used for training and validation from the POCUS dataset is not mentioned.
On a positive note, code and data will be made public.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Pretraining data variety:
The pretraining data variety experiment is limited. From the results it is not clear if the increase in accuracy is due to the addition of a dataset or to the addition of more data in general. 
One way to answer this question could be by selecting a dataset with X number of images from Butterfly + LUSMS and another dataset with X number of images from CLUST + Liver Fibrosis and compare the results on the POCUS dataset and breast dataset. I believe this experiment would give the reader and idea of what types of dataset may work better in a different application.
Data:
Important information regarding the datasets is missing in the main paper. For instance:&lt;/p&gt;
      &lt;ul&gt;
        &lt;li&gt;The POCUS dataset is mentioned in the abstract but not defined/explained, please consider adding a short description.&lt;/li&gt;
        &lt;li&gt;The POCUS dataset is missing a reference in the main text&lt;/li&gt;
        &lt;li&gt;The UDIAT-B dataset is also not well defined: please include that this is a breast dataset to help the reader better understand the variability between the images used in the pretrained method and the fine-tune experiments.
Some information is included in the Annex. However, crucial information is still missing. For example:&lt;/li&gt;
        &lt;li&gt;A reference for the CLUST dataset is given, but the authors should mention that is a liver dataset. This information may help the reader understand the variability included in the pretrained model.&lt;/li&gt;
        &lt;li&gt;What do the different categories (F0, F1, F2, F3) in the Liver Fibrosis dataset mean?
Also, it should be mentioned if the videos come from different patients or there may be multiple videos from a patient.
Results:&lt;/li&gt;
        &lt;li&gt;Figure 4, please include the full definition of RS, RSOFV, etc. in the caption.&lt;/li&gt;
        &lt;li&gt;Figure 5: The strategy followed to select the cases shown is figure 5 is not explained. Are these a collection of the best examples? Or the examples were randomly selected? Also, please include the original label, and the predicted labels for ImageNet and USCL.&lt;/li&gt;
        &lt;li&gt;Classification: I assume the values reported in table 2 correspond to the accuracy? This should be included in the caption of the table and accuracy metric should be defined.&lt;/li&gt;
        &lt;li&gt;Detection and segmentation: the COCO metric AP is referenced but not defined. Since this is not a very common metric to use in medical imaging, a short definition should be included in the paper. Also, the results report values 38~45 for detection and 42~52 in segmentation. Are not these values too low? Is this expected?
Minor:
Figure 3 is referenced before figure 2.&lt;/li&gt;
      &lt;/ul&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;accept (8)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Pre-trained models for ultrasound are scarce.&lt;/p&gt;

      &lt;p&gt;This is a simple but effective way to provide pretrained models which seems to work better than models pretrained on natural images (such as ImageNet).&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;3&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Confident but not absolutely certain&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-2&quot;&gt;Review #2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;In the paper “USCL: Pretraining Deep Ultrasound Image Diagnosis Model through Video Contrastive Representation Learning”, first a new dataset US-4 is constructed from four ultrasound video datasets. Then a semi-supervised contrastive learning method is used to pre-train a model, which is fine-tuned to perform downstream tasks such as image classification and segmentation/detection. For contrastive learning, the authors propose a sample pair generation method to define positive and negative image pairs from ultrasound videos. The method is compared with existing semi-supervised and self-supervised methods, and ImageNet pretraining.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The problem is well-formulated to use fewer annotations in a semi-supervised way. Models are pre-trained via contrastive learning which can be used for other downstream applications.&lt;/li&gt;
        &lt;li&gt;The work provides strong evaluations with existing works and ImageNet pre-training baseline.&lt;/li&gt;
        &lt;li&gt;The work shows promising results for ultrasound classification and detection downstream tasks. It can be useful when small number of labelled samples are available for training.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The authors combine 4 video datasets into one US-4 dataset. There are different image and video characteristics of these 4 datasets (can be seen in Fig.2 and Table 1). It is not clear if there were any measures to normalize the data across datasets and prevent the model from learning dataset-specific features. For e.g., different frame rates, image sizes, and image qualities can lead to bias in the learnt model.&lt;/li&gt;
        &lt;li&gt;Several frames are extracted from the raw videos. The authors could explain how only meaningful frames are selected for learning the model, as the ultrasound video may contain background frames, blurring, shadowing, and other artefacts.&lt;/li&gt;
        &lt;li&gt;The domain used for pre-training consists of liver and lung ultrasound videos. The downstream tasks are performed on lung and breast datasets. The choice of datasets for pre-training and fine-tuning seems arbitrary, so the authors may want to discuss how close or far the domains should be for pretrained and finetuning tasks. This would be useful to adapt the method to other domains.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors have stated that the constructed US-4 dataset and source code of this work will be made public.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;In the introduction, readers may not clearly understand the stated conflict with respect to the ultrasound tasks being addressed. For e.g. it is not clear from the text why samples from ‘different videos’ of the same structure would form different clusters in the feature space. If this refers to videos with different labels (of different structures), it should be clearly stated.&lt;/li&gt;
        &lt;li&gt;The authors say that ‘We can see that the US-4 dataset is relatively balanced, where most videos contain tens of US images.’ These are balanced with respect to which quantity? Number of videos or images in each dataset do not appear to be balanced.&lt;/li&gt;
        &lt;li&gt;The authors pretrained the model with US-4 dataset. They fine-tuned the model with POCUS and UDIAT-B datasets, and a part of these two datasets were used for testing. What happens if the pre-trained datasets were directly used on the new datasets without fine-tuning?&lt;/li&gt;
        &lt;li&gt;Please check typos such as ‘COIVD-19’&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;borderline accept (6)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper presents a novel method for semi-supervised contrastive learning using ultrasound videos. The idea of contrastive learning is well-known. The paper proposes a method to generate positive and negative pairs from US videos in a semi-supervised way to pre-train the models using small number of labelled samples. The model is shown to perform well on downstream tasks such as detection/segmentation and classification, and outperforms existing semi-supervised and self-supervised methods. There are a few comments (see above) which need to be addressed for a better clarity to the readers.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Confident but not absolutely certain&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-3&quot;&gt;Review #3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;In this paper, the authors release a novel dataset, then explore  a Contrastive Representation Learning for the pretraining of ultrasound image analysis models.  The idea makes sense and sounds very interesting.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The paper has a clear organization and task definition;&lt;/li&gt;
        &lt;li&gt;The authors also plan to release a Dataset, which can be very helpful to promote the research in this field;&lt;/li&gt;
        &lt;li&gt;This paper has a good technical contribution, and contrastive semi-supervised learning is under-explored in previous studies;&lt;/li&gt;
        &lt;li&gt;Convincing results on different tasks&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt; The sub dataset comes from different domains, it is interesting to know the domain gap can decrease the performance of down-stream tasks.&lt;/li&gt;
        &lt;li&gt;Only ResNet18 is used in this study. It is interesting to know similar performance improvements can be observed using different backbone architectures.&lt;/li&gt;
        &lt;li&gt;The experimental results are as expected, which is not surprising.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;No code is provided.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;None&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Probably accept (7)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;None&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Confident but not absolutely certain&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;metareview-id&quot;&gt;Meta-Review(s)&lt;/h1&gt;

&lt;h2 id=&quot;primary-meta-review&quot;&gt;Primary Meta-Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper introduces a novel ultrasound dataset and shows results of  contrastive learning with this dataset. The new dataset would be a great contribution to the research community. The paper seems to be written very clearly.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;authorFeedback-id&quot;&gt;Author Feedback&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;We would like to thank the chairs and reviewers for their efforts in reviewing this paper. Key comments and responses are summarized as follows. Due to the limited space, for other issues (i.e., typos and presentation problems), we promise to correct them in the revised version.&lt;/p&gt;

  &lt;p&gt;Q1: Evaluation and discussion of data size and image variation (R1, R2 and R3)&lt;/p&gt;

  &lt;p&gt;Response: The size and domain variation of the US-4 dataset are both beneficial to our USCL pretraining. (1) For data size, we have analyzed its effect in the current version, please refer to Supplementary Material Tab. 3. (2) For domain variation, we simply treat different organs as different domains and conduct some new pretraining experiments. Results and brief analysis are as follows (benchmarking on POCUS dataset):
1) Single domain: Butterfly (85.0%), CLUST (89.7%), Liver Fibrosis (90.4%), COVID19-LUSMS (90.6%). Bigger dataset can achieve better model performance.
2) Two domains: Butterfly+CLUST (88.5%), Liver Fibrosis+COVID19-LUSMS (90.4%), CLUST+Liver Fibrosis (90.8%), Butterfly+COVID19-LUSMS (91.5%), CLUST+COVID19-LUSMS (92.3%), Butterfly+Liver Fibrosis (92.7%). Combining lung sub-dataset (Butterfly) and liver sub-dataset (Liver Fibrosis), we achieved the best accuracy 92.7%, which was higher than the combination between the same organs (Butterfly+COVID19-LUSMS or CLUST+Liver Fibrosis, which have the data size similar to Butterfly+Liver Fibrosis).&lt;/p&gt;

  &lt;p&gt;Above results demonstrate that increasing both the size and variety of data can promote our USCL pre-training. As for how close or far the domains should be for pretraining and fine-tuning, we used the POCUS lung dataset to clarify the effectiveness of our method because it has the similar domain as US-4 pretraining dataset (both contain convex-probe data of lungs), and used UDIAT-B (linear probe) breast dataset to further indicate the generalization of the method facing larger domain gap. USCL worked well on both cases.&lt;/p&gt;

  &lt;p&gt;Q2: Reproducibility and details of the experimental settings (R1, R3)&lt;/p&gt;

  &lt;p&gt;Response: We fine-tuned the last 3 layers of our US-4 pretrained backbone (ResNet18) on POCUS dataset and all layers on UDIAT-B dataset. On POCUS and UDIAT-B, the learning rates were 0.01 and 0.005, respectively. The training, testing code and US-4 dataset are available on GitHub.&lt;/p&gt;

  &lt;p&gt;Q3: It is not clear if there were any measures to normalize the data across datasets and prevent the model from learning dataset-specific features (R2)&lt;/p&gt;

  &lt;p&gt;Response: Because of the moderate domain differences between sub-datasets, we normalized the whole pretraining datasets w.r.t the overall mean and variance instead of special measures. We also found that the risk of learning dataset-specific features wasn’t obvious due to the robust encoding ability of contrastive learning with data variation.&lt;/p&gt;

  &lt;p&gt;Q4: How only meaningful frames are selected for learning the model (R2)&lt;/p&gt;

  &lt;p&gt;Response: (1) In fact, it is not certain that only meaningful frames are selected, but due to our careful data collection, there should exist few meaningless frames in our US-4 dataset. (2) In addition, the frame mix-up scheme can further reduce the chance that the final samples are meaningless.&lt;/p&gt;

  &lt;p&gt;Q5: Evaluation without fine-tuning (R2)&lt;/p&gt;

  &lt;p&gt;Response: We followed the prevailing contrastive learning setting, first pre-training on US-4 dataset, and then fine-tuning on downstream tasks (i.e., POCUS, UDIAT-B) to evaluate the performance of the pretrained backbones. Moreover, the reason why we did not directly use pretraining model to new dataset without finetuning is that this requires new dataset and pretraining dataset having the same categories in our experimental setting.&lt;/p&gt;

  &lt;p&gt;Q6: Result using different backbone architectures (R3)&lt;/p&gt;

  &lt;p&gt;Response: In addition to ResNet18, we additionally tried new experiments using the ResNet34, reaching 94.0% accuracy on POCUS, which is comparable to the accuracy (94.2%) achieved by ResNet18.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;back to top&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Modalities - Ultrasound" /><category term="Clinical applications - Lung" /><category term="Computer Aided Diagnosis" /><category term="Image Segmentation" /><category term="Machine Learning - Domain adaptation" /><category term="Machine Learning - Semi-supervised learning" /><category term="Chen, Yixiong" /><category term="Zhang, Chunhui" /><category term="Liu, Li" /><category term="Feng, Cheng" /><category term="Dong, Changfeng" /><category term="Luo, Yongfang" /><category term="Wan, Xiang" /><summary type="html">Paper Info Reviews Meta-review(s) Author Feedback Authors Yixiong Chen, Chunhui Zhang, Li Liu, Cheng Feng, Changfeng Dong, Yongfang Luo, Xiang Wan Abstract Most deep neural networks (DNNs) based ultrasound (US) medical image analysis models use pretrained backbones (e.g. ImageNet) for better model generalization. However, the domain gap between natural and medical images causes an inevitable performance bottleneck. To alleviate this problem, an US dataset named US-4 is constructed for direct pretraining on the same domain. It contains over 23,000 images from four US video sub-datasets. To learn robust features from US-4, we propose an US semi-supervised contrastive learning method, named USCL, for pretraining. In order to avoid high similarities between negative pairs as well as mine abundant visual features from limited US videos, USCL adopts a sample pair generation method to enrich the feature involved in a single step of contrastive optimization. Extensive experiments on several downstream tasks show the superiority of USCL pretraining against ImageNet pretraining and other state-of-the-art (SOTA) pretraining approaches. In particular, USCL pretrained backbone achieves fine-tuning accuracy of over 94% on POCUS dataset, which is 10% higher than 84% of the ImageNet pretrained model. The source codes of this work are available at https://github.com/983632847/USCL. Link to paper https://doi.org/10.1007/978-3-030-87237-3_60 Link to the code repository https://github.com/983632847/USCL Link to the dataset(s) https://github.com/983632847/USCL Reviews Review #1 Please describe the contribution of the paper This paper proposes a novel methodology for pretraining a deep neural network to solve ultrasound-related tasks such as classification, detection and segmentation, with the aim to have a better model generalisation. The proposed method in based on a semi-supervised contrastive learning approach, where frames from the same video are considered as a positive pair (i.e., similar) and frames from different videos are considered as a negative pair (i.e., different). The pretrained network is trained on ultrasound images from lung and liver, and the evaluation is performed on two ultrasound datasets, one composed of lung images and the other one of breast images. The ablation study compares the proposed pretraining methodology with a network pretrained on natural images (ImageNet) and shows an improvement in all downstream tasks (classification, detection and segmentation). Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Novel idea for pretraining deep neural networks to solve ultrasound-related tasks Simple but original methodology to solve the similarity conflict in a semi-supervised contrastive representation learning, including an enriched approach to create positive pairs (i.e., pairs of similar images). Code and data will be made public, which could facilitate the use of this methodology and be of interest in the research community as currently there is a lack of pre-trained models for ultrasound. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Limited evaluation and discussion on what combination of datasets may work better. For instance, is it preferable to pretrain the method on a collection of datasets that are very different to each other (e.g., lung, liver) ? Or is it preferred to create a dataset with similar characteristics as the task to be solved (e.g., lung centre 1, lung centre 2)? The ablation study provided in the Annex shows results on different dataset combinations, however it is not clear if the increase in accuracy is due to the increase in data size or due to the increase in image variation. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Some details are missing, for instance: fine-tuning is used on two datasets: POCUS (lung) and UDIAT-B (breast), but the fine-tuning learning rate is not mentioned. The amount of images used for training and validation from the POCUS dataset is not mentioned. On a positive note, code and data will be made public. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Pretraining data variety: The pretraining data variety experiment is limited. From the results it is not clear if the increase in accuracy is due to the addition of a dataset or to the addition of more data in general. One way to answer this question could be by selecting a dataset with X number of images from Butterfly + LUSMS and another dataset with X number of images from CLUST + Liver Fibrosis and compare the results on the POCUS dataset and breast dataset. I believe this experiment would give the reader and idea of what types of dataset may work better in a different application. Data: Important information regarding the datasets is missing in the main paper. For instance: The POCUS dataset is mentioned in the abstract but not defined/explained, please consider adding a short description. The POCUS dataset is missing a reference in the main text The UDIAT-B dataset is also not well defined: please include that this is a breast dataset to help the reader better understand the variability between the images used in the pretrained method and the fine-tune experiments. Some information is included in the Annex. However, crucial information is still missing. For example: A reference for the CLUST dataset is given, but the authors should mention that is a liver dataset. This information may help the reader understand the variability included in the pretrained model. What do the different categories (F0, F1, F2, F3) in the Liver Fibrosis dataset mean? Also, it should be mentioned if the videos come from different patients or there may be multiple videos from a patient. Results: Figure 4, please include the full definition of RS, RSOFV, etc. in the caption. Figure 5: The strategy followed to select the cases shown is figure 5 is not explained. Are these a collection of the best examples? Or the examples were randomly selected? Also, please include the original label, and the predicted labels for ImageNet and USCL. Classification: I assume the values reported in table 2 correspond to the accuracy? This should be included in the caption of the table and accuracy metric should be defined. Detection and segmentation: the COCO metric AP is referenced but not defined. Since this is not a very common metric to use in medical imaging, a short definition should be included in the paper. Also, the results report values 38~45 for detection and 42~52 in segmentation. Are not these values too low? Is this expected? Minor: Figure 3 is referenced before figure 2. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Pre-trained models for ultrasound are scarce. This is a simple but effective way to provide pretrained models which seems to work better than models pretrained on natural images (such as ImageNet). What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper In the paper “USCL: Pretraining Deep Ultrasound Image Diagnosis Model through Video Contrastive Representation Learning”, first a new dataset US-4 is constructed from four ultrasound video datasets. Then a semi-supervised contrastive learning method is used to pre-train a model, which is fine-tuned to perform downstream tasks such as image classification and segmentation/detection. For contrastive learning, the authors propose a sample pair generation method to define positive and negative image pairs from ultrasound videos. The method is compared with existing semi-supervised and self-supervised methods, and ImageNet pretraining. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The problem is well-formulated to use fewer annotations in a semi-supervised way. Models are pre-trained via contrastive learning which can be used for other downstream applications. The work provides strong evaluations with existing works and ImageNet pre-training baseline. The work shows promising results for ultrasound classification and detection downstream tasks. It can be useful when small number of labelled samples are available for training. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors combine 4 video datasets into one US-4 dataset. There are different image and video characteristics of these 4 datasets (can be seen in Fig.2 and Table 1). It is not clear if there were any measures to normalize the data across datasets and prevent the model from learning dataset-specific features. For e.g., different frame rates, image sizes, and image qualities can lead to bias in the learnt model. Several frames are extracted from the raw videos. The authors could explain how only meaningful frames are selected for learning the model, as the ultrasound video may contain background frames, blurring, shadowing, and other artefacts. The domain used for pre-training consists of liver and lung ultrasound videos. The downstream tasks are performed on lung and breast datasets. The choice of datasets for pre-training and fine-tuning seems arbitrary, so the authors may want to discuss how close or far the domains should be for pretrained and finetuning tasks. This would be useful to adapt the method to other domains. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have stated that the constructed US-4 dataset and source code of this work will be made public. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In the introduction, readers may not clearly understand the stated conflict with respect to the ultrasound tasks being addressed. For e.g. it is not clear from the text why samples from ‘different videos’ of the same structure would form different clusters in the feature space. If this refers to videos with different labels (of different structures), it should be clearly stated. The authors say that ‘We can see that the US-4 dataset is relatively balanced, where most videos contain tens of US images.’ These are balanced with respect to which quantity? Number of videos or images in each dataset do not appear to be balanced. The authors pretrained the model with US-4 dataset. They fine-tuned the model with POCUS and UDIAT-B datasets, and a part of these two datasets were used for testing. What happens if the pre-trained datasets were directly used on the new datasets without fine-tuning? Please check typos such as ‘COIVD-19’ Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper presents a novel method for semi-supervised contrastive learning using ultrasound videos. The idea of contrastive learning is well-known. The paper proposes a method to generate positive and negative pairs from US videos in a semi-supervised way to pre-train the models using small number of labelled samples. The model is shown to perform well on downstream tasks such as detection/segmentation and classification, and outperforms existing semi-supervised and self-supervised methods. There are a few comments (see above) which need to be addressed for a better clarity to the readers. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper In this paper, the authors release a novel dataset, then explore a Contrastive Representation Learning for the pretraining of ultrasound image analysis models. The idea makes sense and sounds very interesting. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper has a clear organization and task definition; The authors also plan to release a Dataset, which can be very helpful to promote the research in this field; This paper has a good technical contribution, and contrastive semi-supervised learning is under-explored in previous studies; Convincing results on different tasks Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.  The sub dataset comes from different domains, it is interesting to know the domain gap can decrease the performance of down-stream tasks. Only ResNet18 is used in this study. It is interesting to know similar performance improvements can be observed using different backbone architectures. The experimental results are as expected, which is not surprising. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No code is provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html None Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? None What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper introduces a novel ultrasound dataset and shows results of contrastive learning with this dataset. The new dataset would be a great contribution to the research community. The paper seems to be written very clearly. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We would like to thank the chairs and reviewers for their efforts in reviewing this paper. Key comments and responses are summarized as follows. Due to the limited space, for other issues (i.e., typos and presentation problems), we promise to correct them in the revised version. Q1: Evaluation and discussion of data size and image variation (R1, R2 and R3) Response: The size and domain variation of the US-4 dataset are both beneficial to our USCL pretraining. (1) For data size, we have analyzed its effect in the current version, please refer to Supplementary Material Tab. 3. (2) For domain variation, we simply treat different organs as different domains and conduct some new pretraining experiments. Results and brief analysis are as follows (benchmarking on POCUS dataset): 1) Single domain: Butterfly (85.0%), CLUST (89.7%), Liver Fibrosis (90.4%), COVID19-LUSMS (90.6%). Bigger dataset can achieve better model performance. 2) Two domains: Butterfly+CLUST (88.5%), Liver Fibrosis+COVID19-LUSMS (90.4%), CLUST+Liver Fibrosis (90.8%), Butterfly+COVID19-LUSMS (91.5%), CLUST+COVID19-LUSMS (92.3%), Butterfly+Liver Fibrosis (92.7%). Combining lung sub-dataset (Butterfly) and liver sub-dataset (Liver Fibrosis), we achieved the best accuracy 92.7%, which was higher than the combination between the same organs (Butterfly+COVID19-LUSMS or CLUST+Liver Fibrosis, which have the data size similar to Butterfly+Liver Fibrosis). Above results demonstrate that increasing both the size and variety of data can promote our USCL pre-training. As for how close or far the domains should be for pretraining and fine-tuning, we used the POCUS lung dataset to clarify the effectiveness of our method because it has the similar domain as US-4 pretraining dataset (both contain convex-probe data of lungs), and used UDIAT-B (linear probe) breast dataset to further indicate the generalization of the method facing larger domain gap. USCL worked well on both cases. Q2: Reproducibility and details of the experimental settings (R1, R3) Response: We fine-tuned the last 3 layers of our US-4 pretrained backbone (ResNet18) on POCUS dataset and all layers on UDIAT-B dataset. On POCUS and UDIAT-B, the learning rates were 0.01 and 0.005, respectively. The training, testing code and US-4 dataset are available on GitHub. Q3: It is not clear if there were any measures to normalize the data across datasets and prevent the model from learning dataset-specific features (R2) Response: Because of the moderate domain differences between sub-datasets, we normalized the whole pretraining datasets w.r.t the overall mean and variance instead of special measures. We also found that the risk of learning dataset-specific features wasn’t obvious due to the robust encoding ability of contrastive learning with data variation. Q4: How only meaningful frames are selected for learning the model (R2) Response: (1) In fact, it is not certain that only meaningful frames are selected, but due to our careful data collection, there should exist few meaningless frames in our US-4 dataset. (2) In addition, the frame mix-up scheme can further reduce the chance that the final samples are meaningless. Q5: Evaluation without fine-tuning (R2) Response: We followed the prevailing contrastive learning setting, first pre-training on US-4 dataset, and then fine-tuning on downstream tasks (i.e., POCUS, UDIAT-B) to evaluate the performance of the pretrained backbones. Moreover, the reason why we did not directly use pretraining model to new dataset without finetuning is that this requires new dataset and pretraining dataset having the same categories in our experimental setting. Q6: Result using different backbone architectures (R3) Response: In addition to ResNet18, we additionally tried new experiments using the ResNet34, reaching 94.0% accuracy on POCUS, which is comparable to the accuracy (94.2%) achieved by ResNet18. back to top</summary></entry><entry><title type="html">A Multi-attribute Controllable Generative Model for Histopathology Image Synthesis</title><link href="https://kittywong.github.io/kittywong/0858/12/31/Paper2599" rel="alternate" type="text/html" title="A Multi-attribute Controllable Generative Model for Histopathology Image Synthesis" /><published>0858-12-31T23:58:56-05:17</published><updated>0859-01-05T00:00:00-05:17</updated><id>https://kittywong.github.io/kittywong/0858/12/31/Paper2599</id><content type="html" xml:base="https://kittywong.github.io/kittywong/0858/12/31/Paper2599">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#author-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Paper Info&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#review-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Reviews&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#metareview-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Meta-review(s)&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#authorFeedback-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Author Feedback&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;author-id&quot;&gt;Authors&lt;/h1&gt;
&lt;p&gt;Jiarong Ye, Yuan Xue, Peter Liu, Richard Zaino, Keith C. Cheng, Xiaolei Huang
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-id&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Generative models have been applied in the medical imaging domain for various image recognition and synthesis tasks, however, a more controllable and interpretable image synthesis model is still lacking yet necessary towards important applications such as assisting in medical training. In this work, we leverage the efficient self-attention and contrastive learning modules and build upon state-of-the-art generative adversarial networks (GANs) to achieve an attribute-aware image synthesis model, termed AttributeGAN, which can generate high-quality histopathology images based on multi-attribute inputs. In comparison to existing single-attribute conditional generative models, our proposed model better reflects input attributes and enables smoother interpolation among attribute values. We conduct experiments on a histopathology dataset containing stained H&amp;amp;E images of urothelial carcinoma and demonstrate the effectiveness of our proposed model via comprehensive quantitative and qualitative comparisons with state-of-the-art models as well as different variants of our model.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;link-id&quot;&gt;Link to paper&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1007/978-3-030-87237-3_59&quot;&gt;https://doi.org/10.1007/978-3-030-87237-3_59&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;code-id&quot;&gt;Link to the code repository&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dataset-id&quot;&gt;Link to the dataset(s)&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;review-id&quot;&gt;Reviews&lt;/h1&gt;

&lt;h3 id=&quot;review-1&quot;&gt;Review #1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper presents a deep conditional generative model (GAN based) which is able to generate H&amp;amp;E patches conditioned on different cellular attributes like cell crowding,
cell polarity, mitosis, prominence of nucleoli and state of nuclear pleomorphism. The paper combines recent approaches like self supervised techniques and attention models.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;It is interesting how the authors get patch information from text based annotation.
Additionally the use of contrastive loss terms to encourage similar features intra attributes seems to me as a good idea.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The idea of conditional generation with multiple attributes is not new at least in the computer vision community. 
On the other hand, the work seems to force the introduction of new techniques like self supervised and attention. The performance seems to increase by using them but no analysis was made on why is this the case. As an example, it could have been interesting to analyze the  attention maps to see visually that is doing something relevant e.g. looking at individual cells.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The data used is publicly available which is a very good thing. In addition in the reproducibility checklist the authors engage themselves to share the code.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper is concise and it is a new application of generative models to histopathology imaging. Using attributes extracted from text is also a positive point. On the other hand, it seems that the authors wanted to use the latest approaches in the computer vision technology (self supervised learning /attention) which is also interesting. However, the analysis of these techniques was quite limited and only focused on performance. A deeper analysis of these modules could significantly improve the paper. For example what is the attention model focusing at? are the self supervised learnt features relevant for other tasks such as classification?&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Probably accept (7)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The application of this type of techniques is relatively new to histopathology data. It is interesting to be able to control medical relevant attributes when generating synthetic images. My main objection of the paper is that it seems it just wants to use the latest approaches without providing a clear analysis on why it can be relevant for the task at hand other than just by performance.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;3&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Confident but not absolutely certain&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-2&quot;&gt;Review #2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper proposes AttributeGAN, a model that aims to synthesize high-resolution histopathology images whose appearance can be influenced by five different parameters (cell crowding, cell polarity, mitosis, prominence of nucleoli, state of nuclear plenomorphism). AttributeGAN combines concepts from three different areas: generative modelling, self-supervised learning and attention mechanisms. The synthesized images are compared by experts against the originals on a stained dataset, and empirically compared vs. BigGAN.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;+The paper is generally well-written and its language is easy to follow
+The paragraph on efficient attention was interesting and highlights how the batch size (that needs to be sufficiently high for contrastive-learning) can be increased.
+To my knowledge, this is the first attempt to conditionally generate realistic histopathology images.
+The proposed technical contributions (contrastive loss, attention module) appear to improve performance.
+The architecture makes use of modern components: SLE modules, GLU, attention modules, contrastive loss.
+The graphics are good quality and help to understand the general aspects of the paper
+The synthesized images are given to medical experts who found the quality to be “remarkably good”&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;-It is good to see that AttributeGAN is compared against a third-party network (Big-GAN) and also that there are a couple of ablations (w/o attention module, contrastive learning) it is compared against. But a more appropriate comparison would be a model from the InfoGAN [1] cosmos, or something that comes close To Fader Networks [2] which tackle the same problem directly.&lt;/p&gt;

      &lt;p&gt;-The number of provided images is a bit meager, I would have expected considerably more synthesized images, especially in the supplementary material&lt;/p&gt;

      &lt;p&gt;-Frechet Inception Distance (FID) is not really suitable to medical imaging, as the benchmark network is trained on natural imagery.&lt;/p&gt;

      &lt;p&gt;[1] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., &amp;amp; Abbeel, P. (2016). Infogan: Interpretable representation learning by information maximizing generative adversarial nets. arXiv preprint arXiv:1606.03657.
[2] Lample, G., Zeghidour, N., Usunier, N., Bordes, A., Denoyer, L., &amp;amp; Ranzato, M. A. (2017). Fader networks: Manipulating images by sliding attributes. arXiv preprint arXiv:1706.00409.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Excellent&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors have described their method in reasonable detail, although some details are missing. The code/data/models have not been made public. Medium/low reproducibility.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;As mentioned above, the FID score is not an appropriate metric for histopathology images, but as there is no readily available alternative I can understand its use.&lt;/p&gt;

      &lt;p&gt;The main shortcoming of the paper in my opinion is the missing comparison to well-known attribute-controlling architectures such as InfoGAN, Fader Networks, StyleGAN v2, etc. Simply applying these off the shelf may be competitive with the proposed method.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;borderline accept (6)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper solves its task (synthesis of histopathology images) well but also does not really offer a real novelty other than combining components that are already there. It would have also been interesting to see how this model performs on other medical data. The paper itself is exceptionally clear and delivers its message in a very direct way. In general, I would like to see more GANs this model is compared against.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-3&quot;&gt;Review #3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper presents a multi-attribute conditional generative model that is built on top of recent works on stable GANs, self-attention, and contrastive learning. The proposed model can synthesize high-resolution histopathology images with improved image quality and better-capturing input attributes.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;The model builds on recent works in GANs, efficient self-attention, and contrastive learning.&lt;/li&gt;
        &lt;li&gt;The use of contrastive learning within the discriminator to capture subtle changes in attribute levels of the image is an interesting idea.&lt;/li&gt;
        &lt;li&gt;Efficient self-attention reduces the computational overhead and allows for larger batch sizes needed for contrastive learning.&lt;/li&gt;
      &lt;/ul&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;As with all conditional generative models, this method require some level of supervision in defining the attributes to conditional image synthesis.&lt;/li&gt;
        &lt;li&gt;The proposed model can not leverage unlabeled data, e.g., in a semi-supervised fashion.&lt;/li&gt;
        &lt;li&gt;The performance of the proposed model is weakly baselined by a GAN variant that handle high resolution image. Other more relevant SOTA conditional models are not considered (e.g., Fader Networks and AttGAN,)&lt;/li&gt;
        &lt;li&gt;The qualitative way that attributes are defined adds in subjectivity and inter-/intra-rater variability.&lt;/li&gt;
        &lt;li&gt;Histopathology image generation is qualitatively evaluated by experts.&lt;/li&gt;
        &lt;li&gt;The impact of noisy labels/attributes is not considered.&lt;/li&gt;
      &lt;/ul&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper has enough details for reproducibility. It is not clear if code would be released.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;How can the model handle unlabeled data?&lt;/li&gt;
        &lt;li&gt;Authors should consider a more comprehensive comparisons with relevant SOTA models such as Fader Networks and AttGAN.&lt;/li&gt;
        &lt;li&gt;It is not clear how attributes were estimated from pathology reports.&lt;/li&gt;
        &lt;li&gt;Model evaluation can be further improved by considering a randomized/blind user study where pathologists provide pathology reports on images without prior knowledge of whether an image is a real or synthesized one.&lt;/li&gt;
        &lt;li&gt;In eq 1, how negative pairs contribute to the contrastive loss?&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Probably accept (7)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Results are weakly baselined and model evaluation could be improve. However, the paper presents some interesting adaptations to medical image synthesis. With its level of clarity and organization, the paper could be considered for a MICCAI publication.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;metareview-id&quot;&gt;Meta-Review(s)&lt;/h1&gt;

&lt;h2 id=&quot;primary-meta-review&quot;&gt;Primary Meta-Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper has received positive feedback from all reviewers, although some concerns were raised. The authors are encouraged to address the reviewer comments in the final version and improve the quality of images (&amp;amp;text within images) in the paper. A comprehensive comparison with the state of the art is also missing.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;authorFeedback-id&quot;&gt;Author Feedback&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;N/A&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;back to top&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Modalities - Histopathology" /><category term="Machine Learning - Attention models" /><category term="Machine Learning - Interpretability / Explainability" /><category term="Machine Learning - Self-supervised learning" /><category term="Ye, Jiarong" /><category term="Xue, Yuan" /><category term="Liu, Peter" /><category term="Zaino, Richard" /><category term="Cheng, Keith C." /><category term="Huang, Xiaolei" /><summary type="html">Paper Info Reviews Meta-review(s) Author Feedback Authors Jiarong Ye, Yuan Xue, Peter Liu, Richard Zaino, Keith C. Cheng, Xiaolei Huang Abstract Generative models have been applied in the medical imaging domain for various image recognition and synthesis tasks, however, a more controllable and interpretable image synthesis model is still lacking yet necessary towards important applications such as assisting in medical training. In this work, we leverage the efficient self-attention and contrastive learning modules and build upon state-of-the-art generative adversarial networks (GANs) to achieve an attribute-aware image synthesis model, termed AttributeGAN, which can generate high-quality histopathology images based on multi-attribute inputs. In comparison to existing single-attribute conditional generative models, our proposed model better reflects input attributes and enables smoother interpolation among attribute values. We conduct experiments on a histopathology dataset containing stained H&amp;amp;E images of urothelial carcinoma and demonstrate the effectiveness of our proposed model via comprehensive quantitative and qualitative comparisons with state-of-the-art models as well as different variants of our model. Link to paper https://doi.org/10.1007/978-3-030-87237-3_59 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The paper presents a deep conditional generative model (GAN based) which is able to generate H&amp;amp;E patches conditioned on different cellular attributes like cell crowding, cell polarity, mitosis, prominence of nucleoli and state of nuclear pleomorphism. The paper combines recent approaches like self supervised techniques and attention models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It is interesting how the authors get patch information from text based annotation. Additionally the use of contrastive loss terms to encourage similar features intra attributes seems to me as a good idea. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The idea of conditional generation with multiple attributes is not new at least in the computer vision community. On the other hand, the work seems to force the introduction of new techniques like self supervised and attention. The performance seems to increase by using them but no analysis was made on why is this the case. As an example, it could have been interesting to analyze the attention maps to see visually that is doing something relevant e.g. looking at individual cells. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The data used is publicly available which is a very good thing. In addition in the reproducibility checklist the authors engage themselves to share the code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The paper is concise and it is a new application of generative models to histopathology imaging. Using attributes extracted from text is also a positive point. On the other hand, it seems that the authors wanted to use the latest approaches in the computer vision technology (self supervised learning /attention) which is also interesting. However, the analysis of these techniques was quite limited and only focused on performance. A deeper analysis of these modules could significantly improve the paper. For example what is the attention model focusing at? are the self supervised learnt features relevant for other tasks such as classification? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The application of this type of techniques is relatively new to histopathology data. It is interesting to be able to control medical relevant attributes when generating synthetic images. My main objection of the paper is that it seems it just wants to use the latest approaches without providing a clear analysis on why it can be relevant for the task at hand other than just by performance. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The paper proposes AttributeGAN, a model that aims to synthesize high-resolution histopathology images whose appearance can be influenced by five different parameters (cell crowding, cell polarity, mitosis, prominence of nucleoli, state of nuclear plenomorphism). AttributeGAN combines concepts from three different areas: generative modelling, self-supervised learning and attention mechanisms. The synthesized images are compared by experts against the originals on a stained dataset, and empirically compared vs. BigGAN. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. +The paper is generally well-written and its language is easy to follow +The paragraph on efficient attention was interesting and highlights how the batch size (that needs to be sufficiently high for contrastive-learning) can be increased. +To my knowledge, this is the first attempt to conditionally generate realistic histopathology images. +The proposed technical contributions (contrastive loss, attention module) appear to improve performance. +The architecture makes use of modern components: SLE modules, GLU, attention modules, contrastive loss. +The graphics are good quality and help to understand the general aspects of the paper +The synthesized images are given to medical experts who found the quality to be “remarkably good” Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -It is good to see that AttributeGAN is compared against a third-party network (Big-GAN) and also that there are a couple of ablations (w/o attention module, contrastive learning) it is compared against. But a more appropriate comparison would be a model from the InfoGAN [1] cosmos, or something that comes close To Fader Networks [2] which tackle the same problem directly. -The number of provided images is a bit meager, I would have expected considerably more synthesized images, especially in the supplementary material -Frechet Inception Distance (FID) is not really suitable to medical imaging, as the benchmark network is trained on natural imagery. [1] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., &amp;amp; Abbeel, P. (2016). Infogan: Interpretable representation learning by information maximizing generative adversarial nets. arXiv preprint arXiv:1606.03657. [2] Lample, G., Zeghidour, N., Usunier, N., Bordes, A., Denoyer, L., &amp;amp; Ranzato, M. A. (2017). Fader networks: Manipulating images by sliding attributes. arXiv preprint arXiv:1706.00409. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have described their method in reasonable detail, although some details are missing. The code/data/models have not been made public. Medium/low reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html As mentioned above, the FID score is not an appropriate metric for histopathology images, but as there is no readily available alternative I can understand its use. The main shortcoming of the paper in my opinion is the missing comparison to well-known attribute-controlling architectures such as InfoGAN, Fader Networks, StyleGAN v2, etc. Simply applying these off the shelf may be competitive with the proposed method. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper solves its task (synthesis of histopathology images) well but also does not really offer a real novelty other than combining components that are already there. It would have also been interesting to see how this model performs on other medical data. The paper itself is exceptionally clear and delivers its message in a very direct way. In general, I would like to see more GANs this model is compared against. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper presents a multi-attribute conditional generative model that is built on top of recent works on stable GANs, self-attention, and contrastive learning. The proposed model can synthesize high-resolution histopathology images with improved image quality and better-capturing input attributes. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The model builds on recent works in GANs, efficient self-attention, and contrastive learning. The use of contrastive learning within the discriminator to capture subtle changes in attribute levels of the image is an interesting idea. Efficient self-attention reduces the computational overhead and allows for larger batch sizes needed for contrastive learning. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. As with all conditional generative models, this method require some level of supervision in defining the attributes to conditional image synthesis. The proposed model can not leverage unlabeled data, e.g., in a semi-supervised fashion. The performance of the proposed model is weakly baselined by a GAN variant that handle high resolution image. Other more relevant SOTA conditional models are not considered (e.g., Fader Networks and AttGAN,) The qualitative way that attributes are defined adds in subjectivity and inter-/intra-rater variability. Histopathology image generation is qualitatively evaluated by experts. The impact of noisy labels/attributes is not considered. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper has enough details for reproducibility. It is not clear if code would be released. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html How can the model handle unlabeled data? Authors should consider a more comprehensive comparisons with relevant SOTA models such as Fader Networks and AttGAN. It is not clear how attributes were estimated from pathology reports. Model evaluation can be further improved by considering a randomized/blind user study where pathologists provide pathology reports on images without prior knowledge of whether an image is a real or synthesized one. In eq 1, how negative pairs contribute to the contrastive loss? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Results are weakly baselined and model evaluation could be improve. However, the paper presents some interesting adaptations to medical image synthesis. With its level of clarity and organization, the paper could be considered for a MICCAI publication. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper has received positive feedback from all reviewers, although some concerns were raised. The authors are encouraged to address the reviewer comments in the final version and improve the quality of images (&amp;amp;text within images) in the paper. A comprehensive comparison with the state of the art is also missing. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Author Feedback N/A back to top</summary></entry><entry><title type="html">Adversarial learning of cancer tissue representations</title><link href="https://kittywong.github.io/kittywong/0857/12/31/Paper2230" rel="alternate" type="text/html" title="Adversarial learning of cancer tissue representations" /><published>0857-12-31T23:58:56-05:17</published><updated>0858-01-05T00:00:00-05:17</updated><id>https://kittywong.github.io/kittywong/0857/12/31/Paper2230</id><content type="html" xml:base="https://kittywong.github.io/kittywong/0857/12/31/Paper2230">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#author-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Paper Info&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#review-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Reviews&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#metareview-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Meta-review(s)&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#authorFeedback-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Author Feedback&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;author-id&quot;&gt;Authors&lt;/h1&gt;
&lt;p&gt;Adalberto Claudio Quiros, Nicolas Coudray, Anna Yeaton, Wisuwat Sunhem, Roderick Murray-Smith, Aristotelis Tsirigos, Ke Yuan
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-id&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Deep learning based analysis of histopathology images shows promise in advancing the understanding of tumor progression, tumor micro-environment,  and  their  underpinning  biological  processes.  So  far,  these approaches have focused on extracting information associated with annotations.  In  this  work,  we  ask  how  much  information  can  be  learned from the tissue architecture itself.&lt;/p&gt;

&lt;p&gt;We present an adversarial learning model to extract feature representations of cancer tissue, without the need for manual annotations. We show that these representations are able to identify a variety of morphological characteristics across three cancer types: Breast, colon, and lung. This is supported by 1) the separation of morphologic characteristics in the latent space; 2) the ability to classify tissue type with logistic regression using  latent  representations,  with  an  AUC  of  0.97  and  85%  accuracy, comparable to supervised deep models; 3) the ability to predict the presence  of  tumor  in  Whole  Slide  Images  (WSIs)  using  multiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.&lt;/p&gt;

&lt;p&gt;Our results show that our model captures distinct phenotypic characteristics of real tissue samples, paving the way for further understanding of tumor progression and tumor micro-environment, and ultimately refining histopathological classification for diagnosis and treatment
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;link-id&quot;&gt;Link to paper&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1007/978-3-030-87237-3_58&quot;&gt;https://doi.org/10.1007/978-3-030-87237-3_58&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;code-id&quot;&gt;Link to the code repository&lt;/h1&gt;
&lt;p&gt;https://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dataset-id&quot;&gt;Link to the dataset(s)&lt;/h1&gt;
&lt;p&gt;https://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;review-id&quot;&gt;Reviews&lt;/h1&gt;

&lt;h3 id=&quot;review-1&quot;&gt;Review #1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors proposed an unsupervised method, Generative Adversarial Network with an extra encoder, allowing to project real tissue onto the model’s latent space. The topic is interesting and this is an attempt to partially address the increasingly  ground truth demands in digital histopathology.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1). The method is semi-novel, introducing an encoder to map back generated tissue to the GAN’s latent space.
2). The authors have conducted thorough experiments on three different types of cancer to show the latent representation learned by the encoder is distinct and informative, and results are comparative with supervised methods in terms of AUC.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;My main concern about the proposed method is how much improvement can be brought by the encoder. The authors need to justify in the experiment part, e.g. to compare the latent representation before and after adding the encoder.&lt;/p&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The link to the code and pretrained models provided in the paper is not accessible. The following error appears: The repository is not found.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1). It would be better to show the details of the encoder architecture as the encoder is the main contribution/novelty in terms of methodology.
2). The authors need to compare the latent representation before and after adding the encoder, to provide more confidence/justification in the proposed method.
3). The breast cancer data is TMA, while in Figure 2 (left), the UMAP looks like a WSI, please explain this. In addition, in Figures 2 &amp;amp; 4, please clarify which column/row corresponds to real images. This is unclear. 
4). Figure 3 should have a quantitative evaluation to make the comparison more solid and meaningful, rather than just visualization.
5). The authors used ‘Figure’ in the main contents while ‘Fig.’ in the captions, please be consistent.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;borderline accept (6)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper has addressed an interesting topic and the application of unsupervised learning would be helpful in histopathology. The authors have conducted thorough experiments on different cancers, adding values to its generalization ability. However, the key experiment to show the improvements achieved by the encoder is lacking. Further solid justification based on results is needed.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;6&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Confident but not absolutely certain&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-2&quot;&gt;Review #2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors proposed a novel generative adversarial network with representation learning properties to effectively extract features from WSI patches of cancer tissues. The author also demonstrated that the learned representations of the tissue images captured meaningful information related to the tissues through a series of experiments.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Introducing the representation learning properties into the generative adversarial network is an interesting idea for learning effective latent representations of the tissue images. The author also demonstrated the effectiveness of the learned representations in different aspects through different experiments, including latent space visualization and tissue image reconstruction, tissue type classification, and tumor prediction.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;While the author demonstrates the usefulness of the proposed method through different applications, one major weakness of this paper is the lack of comparisons with baseline and existing methods. Based on the current results, it is hard to establish the effectiveness of the proposed method compared to some widely adopted approaches. For example, some existing works directly use ResNet pre-trained with ImageNet to extract features from WSI patches.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The reproducibility information provided in this paper is good. The authors provided a detailed description of the proposed method and the used datasets. It would be better if the authors could provide more implementation details of the proposed network.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;For the results shown in Fig. 2, besides visually inspecting the clustering structures, the authors could also use quantitative analysis to evaluate the clustering results. For example, the authors could use the silhouette score to evaluate whether different tissue types in the colorectal cancer tissue are well separated using the learned representation.&lt;/li&gt;
        &lt;li&gt;Comparisons with baseline and existing methods need to be added in the results section to fully demonstrate the effectiveness of the proposed method. One simple baseline comparison could be features extracted from a ResNet that is pre-trained on ImageNet. The authors could expand the results in Fig. 2 and Table 1 by adding evaluation results of the compared methods.&lt;/li&gt;
        &lt;li&gt;The proposed method focused on learning meaningful representations of the tissue images without the information from labels or annotations. However, applications in Section 3.2 and 3.3 are all supervised tasks. While these results in some way could reflect the effectiveness of the learned representations, there are other existing methods that could perform these tasks in a supervised setting, probably more effectively. Therefore, it would be better if the author could provide more applications in the unsupervised setting to fully demonstrate the effectiveness of the proposed method.&lt;/li&gt;
        &lt;li&gt;For the application in Section 3.3, both the attention-based deep MIL network and the learned representations from the proposed method could contribute to the high accuracy and AUC values reported by the authors. Therefore, an ablation study is needed to demonstrate the contribution of the learned representations to the tumor presence prediction task.&lt;/li&gt;
      &lt;/ol&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;probably reject (4)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;While the authors proposed a novel method based on GAN with representation learning properties for WSI patches, the lack of comparisons with baseline and existing methods in the results section made it hard to demonstrate the effectiveness of the learned representations.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;6&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Confident but not absolutely certain&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-3&quot;&gt;Review #3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;An adversarial learning model is designed for representative tissue extraction in an unsupervised manner. The representations are identified with high morphological characteristics.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The proposed adversarial method is implemented for three tasks, namely reconstruction, classification and MIL. The experinments large patient coherts reveals the effiency of the proposed unsupervised learning model. Good visualization. The reference are adequent.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;A few typos e.g., ‘representation’ in contribution 2) should be ‘representations’.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very convincing. The codes and pre-trained model is available. The used datasets and hyper-parameters are well explained.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;‘Total AUC’ in Table 1 represents macro-AUC or micro-AUC? Please clarify. In the same table, why AUC and Accuracy for Stroma are missing?&lt;/li&gt;
        &lt;li&gt;Confusion matrix may be a better way to illustrate the classification than Table 1.&lt;/li&gt;
        &lt;li&gt;$G(\omega)$ in Eq (1-2) is not a probability distribution. A better way should be $G\circ M (P_z)$.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;accept (8)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The model is novel. The paper is well written and organized. I firmly believe the paper should be accepted for publication.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;metareview-id&quot;&gt;Meta-Review(s)&lt;/h1&gt;

&lt;h2 id=&quot;primary-meta-review&quot;&gt;Primary Meta-Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper proposed a novel generative adversarial network with representation learning properties to effectively extract features from WSI patches of cancer tissues. The strengths of the paper include: 1) extensive experiments; 2) using  an encoder to map back generated tissue to the GAN’s latent space. The points should be addressed in the rebuttal:1) ablation study of improvement by encoder; 2) comparison with baseline and existing methods;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;6&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper proposed a generative adversarial network with representation learning properties to effectively extract features from WSI patches of cancer tissues. The rebuttal sufficiently addresses the major concern of  comparison with SOTA methods and ablation study for the encoder methods.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;9&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;meta-review-2&quot;&gt;Meta-Review #2&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;he authors propose a novel GAN based network to learn representations from 
histological images in a unsupervised manner. Apart from synthetisize images, the learnt representation is also useful for down-stream tasks such as classification or MIL classification. I think the paper is generally well-written, with well-justifed motivation and sufficient evaluations from multiple aspects. Reviewers concern (e.g. role of the encoder) are also addressed in the rebuttal and I therefore support paper acceptance.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;6&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;meta-review-3&quot;&gt;Meta-Review #3&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The presented idea of representation learning through an encoder to map back GAN outputs is indeed quite interesting.  However, with most evaluations being in the form of qualitative (Umap) illustrations, what one can truly achieve with learned representations in a clinical setting is not apparent. There is limited evaluation, and although the results added in the rebuttal improve this situation, the comparative improvements are not significant with even SVM doing quite well (although being supervised).&lt;/p&gt;

      &lt;p&gt;This paper is a narrow call for me, but it can perhaps foster discussions and ignite other ideas in histopathology circles in MICCAI.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Accept&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;9&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;br /&gt;&lt;br /&gt;&lt;/h2&gt;
&lt;h1 id=&quot;authorFeedback-id&quot;&gt;Author Feedback&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;We would like to thank the reviewers and AC for their effort in providing constructive and meaningful reviews. Please find our replies to the questions raised below.&lt;/p&gt;

  &lt;p&gt;1.‘Comparison baseline and existing methods needed’: 
We further provide a comparison baseline with existing methods in Section 3.2 ‘Tissue type classification over latent representations’. We already included in the text the results from the best state-of-art performance, a Bayesian DNN [1] with accuracy/AUC of 0.995/0.992, we will move these to Table 1 too. We will also add the performance of an RBF-SVM [2] (accuracy/AUC 0.968/0.874) to the table. These results reflect baseline performance of supervised deep learning and non-deep learning approaches. The high performance (accuracy/AUC 0.976/0.854) of our representations on logistic regression without any transformation or projection, and the fact that they are comparable with top supervised performance, demonstrate the effectiveness of our representations.
In addition, we provide another reference method as a comparison to ours in Section 3.3 ‘Multiple Instance Learning on latent representations. The Inception-V3 network from Coudray et al [3] was tested on the same dataset and achieves an accuracy/AUC of 0.975/0.993, comparable to our accuracy/AUC results of 0.980/0.940. Collectively, these results provide evidence of how our unsupervised representations can be as competitive as state-of-art supervised approaches in terms of discriminative signals they capture.&lt;/p&gt;

  &lt;p&gt;2.’Ablation study of the improvement of the encoder/how much improvement can be brought by the encoder’: 
We would like to take this opportunity to further clarify what are improvements/contributions made by our encoder. The main contribution of our encoder is allowing us to create representations of real tissue, without the encoder, the GAN cannot quickly and effectively produce any representation of real images to perform the three tasks studied in the manuscript.
The question raised about the contribution of the learned representations to the MIL classifier and the proposal of an ablation study is interesting. However, we argue that our tissue representations are already discriminative for the three separate kinds of labels. This is supported by the clear separations in the latent spaces across the three tasks. These separations are intrinsic properties of the representations, achieved without any label. 
The classifiers including logistic regression and MIL are selected to best demonstrate the discriminative signal in the representations. In the case of the attention MIL, the attention is set up to explain the prediction rather than improvement in performance, in our version we removed the CNN specified by the original model and replaced it with our presentations. This evidence shows performance mainly due to our novel unsupervised representations, therefore already addressing the need of an ablation study.&lt;/p&gt;

  &lt;p&gt;We provide details of the baseline and all network architectures of our model can be found in the anonymized GitHub link. They will also be included in the final version as an appendix: https://anonymous.4open.science/r/Adversarial-learning-of-cancer-tissue-representations-1C87&lt;/p&gt;

  &lt;p&gt;[1] Rączkowsk et al. ‘ARA: accurate, reliable and active histopathological image classification framework with Bayesian deep learning’ Scientific Reports 2019
[2] Kather et al. ‘Multi-class texture analysis in colorectal cancer histology’ Scientific Reports 2016.
[3] Coudray et al. ‘Classification and mutation prediction from non–small cell lung cancer histopathology images using deep learning’ Nature Medicine 2018.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;back to top&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Modalities - Histopathology" /><category term="Computational (Integrative) Pathology" /><category term="Machine Learning - Interpretability / Explainability" /><category term="Claudio Quiros, Adalberto" /><category term="Coudray, Nicolas" /><category term="Yeaton, Anna" /><category term="Sunhem, Wisuwat" /><category term="Murray-Smith, Roderick" /><category term="Tsirigos, Aristotelis" /><category term="Yuan, Ke" /><summary type="html">Paper Info Reviews Meta-review(s) Author Feedback Authors Adalberto Claudio Quiros, Nicolas Coudray, Anna Yeaton, Wisuwat Sunhem, Roderick Murray-Smith, Aristotelis Tsirigos, Ke Yuan Abstract Deep learning based analysis of histopathology images shows promise in advancing the understanding of tumor progression, tumor micro-environment, and their underpinning biological processes. So far, these approaches have focused on extracting information associated with annotations. In this work, we ask how much information can be learned from the tissue architecture itself. We present an adversarial learning model to extract feature representations of cancer tissue, without the need for manual annotations. We show that these representations are able to identify a variety of morphological characteristics across three cancer types: Breast, colon, and lung. This is supported by 1) the separation of morphologic characteristics in the latent space; 2) the ability to classify tissue type with logistic regression using latent representations, with an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3) the ability to predict the presence of tumor in Whole Slide Images (WSIs) using multiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy. Our results show that our model captures distinct phenotypic characteristics of real tissue samples, paving the way for further understanding of tumor progression and tumor micro-environment, and ultimately refining histopathological classification for diagnosis and treatment Link to paper https://doi.org/10.1007/978-3-030-87237-3_58 Link to the code repository https://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations Link to the dataset(s) https://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations Reviews Review #1 Please describe the contribution of the paper The authors proposed an unsupervised method, Generative Adversarial Network with an extra encoder, allowing to project real tissue onto the model’s latent space. The topic is interesting and this is an attempt to partially address the increasingly ground truth demands in digital histopathology. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. 1). The method is semi-novel, introducing an encoder to map back generated tissue to the GAN’s latent space. 2). The authors have conducted thorough experiments on three different types of cancer to show the latent representation learned by the encoder is distinct and informative, and results are comparative with supervised methods in terms of AUC. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. My main concern about the proposed method is how much improvement can be brought by the encoder. The authors need to justify in the experiment part, e.g. to compare the latent representation before and after adding the encoder. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The link to the code and pretrained models provided in the paper is not accessible. The following error appears: The repository is not found. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html 1). It would be better to show the details of the encoder architecture as the encoder is the main contribution/novelty in terms of methodology. 2). The authors need to compare the latent representation before and after adding the encoder, to provide more confidence/justification in the proposed method. 3). The breast cancer data is TMA, while in Figure 2 (left), the UMAP looks like a WSI, please explain this. In addition, in Figures 2 &amp;amp; 4, please clarify which column/row corresponds to real images. This is unclear. 4). Figure 3 should have a quantitative evaluation to make the comparison more solid and meaningful, rather than just visualization. 5). The authors used ‘Figure’ in the main contents while ‘Fig.’ in the captions, please be consistent. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper has addressed an interesting topic and the application of unsupervised learning would be helpful in histopathology. The authors have conducted thorough experiments on different cancers, adding values to its generalization ability. However, the key experiment to show the improvements achieved by the encoder is lacking. Further solid justification based on results is needed. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The authors proposed a novel generative adversarial network with representation learning properties to effectively extract features from WSI patches of cancer tissues. The author also demonstrated that the learned representations of the tissue images captured meaningful information related to the tissues through a series of experiments. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Introducing the representation learning properties into the generative adversarial network is an interesting idea for learning effective latent representations of the tissue images. The author also demonstrated the effectiveness of the learned representations in different aspects through different experiments, including latent space visualization and tissue image reconstruction, tissue type classification, and tumor prediction. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. While the author demonstrates the usefulness of the proposed method through different applications, one major weakness of this paper is the lack of comparisons with baseline and existing methods. Based on the current results, it is hard to establish the effectiveness of the proposed method compared to some widely adopted approaches. For example, some existing works directly use ResNet pre-trained with ImageNet to extract features from WSI patches. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility information provided in this paper is good. The authors provided a detailed description of the proposed method and the used datasets. It would be better if the authors could provide more implementation details of the proposed network. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html For the results shown in Fig. 2, besides visually inspecting the clustering structures, the authors could also use quantitative analysis to evaluate the clustering results. For example, the authors could use the silhouette score to evaluate whether different tissue types in the colorectal cancer tissue are well separated using the learned representation. Comparisons with baseline and existing methods need to be added in the results section to fully demonstrate the effectiveness of the proposed method. One simple baseline comparison could be features extracted from a ResNet that is pre-trained on ImageNet. The authors could expand the results in Fig. 2 and Table 1 by adding evaluation results of the compared methods. The proposed method focused on learning meaningful representations of the tissue images without the information from labels or annotations. However, applications in Section 3.2 and 3.3 are all supervised tasks. While these results in some way could reflect the effectiveness of the learned representations, there are other existing methods that could perform these tasks in a supervised setting, probably more effectively. Therefore, it would be better if the author could provide more applications in the unsupervised setting to fully demonstrate the effectiveness of the proposed method. For the application in Section 3.3, both the attention-based deep MIL network and the learned representations from the proposed method could contribute to the high accuracy and AUC values reported by the authors. Therefore, an ablation study is needed to demonstrate the contribution of the learned representations to the tumor presence prediction task. Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? While the authors proposed a novel method based on GAN with representation learning properties for WSI patches, the lack of comparisons with baseline and existing methods in the results section made it hard to demonstrate the effectiveness of the learned representations. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 6 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper An adversarial learning model is designed for representative tissue extraction in an unsupervised manner. The representations are identified with high morphological characteristics. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The proposed adversarial method is implemented for three tasks, namely reconstruction, classification and MIL. The experinments large patient coherts reveals the effiency of the proposed unsupervised learning model. Good visualization. The reference are adequent. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. A few typos e.g., ‘representation’ in contribution 2) should be ‘representations’. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Very convincing. The codes and pre-trained model is available. The used datasets and hyper-parameters are well explained. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html ‘Total AUC’ in Table 1 represents macro-AUC or micro-AUC? Please clarify. In the same table, why AUC and Accuracy for Stroma are missing? Confusion matrix may be a better way to illustrate the classification than Table 1. $G(\omega)$ in Eq (1-2) is not a probability distribution. A better way should be $G\circ M (P_z)$. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The model is novel. The paper is well written and organized. I firmly believe the paper should be accepted for publication. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper proposed a novel generative adversarial network with representation learning properties to effectively extract features from WSI patches of cancer tissues. The strengths of the paper include: 1) extensive experiments; 2) using an encoder to map back generated tissue to the GAN’s latent space. The points should be addressed in the rebuttal:1) ablation study of improvement by encoder; 2) comparison with baseline and existing methods; What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper proposed a generative adversarial network with representation learning properties to effectively extract features from WSI patches of cancer tissues. The rebuttal sufficiently addresses the major concern of comparison with SOTA methods and ablation study for the encoder methods. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 9 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. he authors propose a novel GAN based network to learn representations from histological images in a unsupervised manner. Apart from synthetisize images, the learnt representation is also useful for down-stream tasks such as classification or MIL classification. I think the paper is generally well-written, with well-justifed motivation and sufficient evaluations from multiple aspects. Reviewers concern (e.g. role of the encoder) are also addressed in the rebuttal and I therefore support paper acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The presented idea of representation learning through an encoder to map back GAN outputs is indeed quite interesting. However, with most evaluations being in the form of qualitative (Umap) illustrations, what one can truly achieve with learned representations in a clinical setting is not apparent. There is limited evaluation, and although the results added in the rebuttal improve this situation, the comparative improvements are not significant with even SVM doing quite well (although being supervised). This paper is a narrow call for me, but it can perhaps foster discussions and ignite other ideas in histopathology circles in MICCAI. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 9 Author Feedback We would like to thank the reviewers and AC for their effort in providing constructive and meaningful reviews. Please find our replies to the questions raised below. 1.‘Comparison baseline and existing methods needed’: We further provide a comparison baseline with existing methods in Section 3.2 ‘Tissue type classification over latent representations’. We already included in the text the results from the best state-of-art performance, a Bayesian DNN [1] with accuracy/AUC of 0.995/0.992, we will move these to Table 1 too. We will also add the performance of an RBF-SVM [2] (accuracy/AUC 0.968/0.874) to the table. These results reflect baseline performance of supervised deep learning and non-deep learning approaches. The high performance (accuracy/AUC 0.976/0.854) of our representations on logistic regression without any transformation or projection, and the fact that they are comparable with top supervised performance, demonstrate the effectiveness of our representations. In addition, we provide another reference method as a comparison to ours in Section 3.3 ‘Multiple Instance Learning on latent representations. The Inception-V3 network from Coudray et al [3] was tested on the same dataset and achieves an accuracy/AUC of 0.975/0.993, comparable to our accuracy/AUC results of 0.980/0.940. Collectively, these results provide evidence of how our unsupervised representations can be as competitive as state-of-art supervised approaches in terms of discriminative signals they capture. 2.’Ablation study of the improvement of the encoder/how much improvement can be brought by the encoder’: We would like to take this opportunity to further clarify what are improvements/contributions made by our encoder. The main contribution of our encoder is allowing us to create representations of real tissue, without the encoder, the GAN cannot quickly and effectively produce any representation of real images to perform the three tasks studied in the manuscript. The question raised about the contribution of the learned representations to the MIL classifier and the proposal of an ablation study is interesting. However, we argue that our tissue representations are already discriminative for the three separate kinds of labels. This is supported by the clear separations in the latent spaces across the three tasks. These separations are intrinsic properties of the representations, achieved without any label. The classifiers including logistic regression and MIL are selected to best demonstrate the discriminative signal in the representations. In the case of the attention MIL, the attention is set up to explain the prediction rather than improvement in performance, in our version we removed the CNN specified by the original model and replaced it with our presentations. This evidence shows performance mainly due to our novel unsupervised representations, therefore already addressing the need of an ablation study. We provide details of the baseline and all network architectures of our model can be found in the anonymized GitHub link. They will also be included in the final version as an appendix: https://anonymous.4open.science/r/Adversarial-learning-of-cancer-tissue-representations-1C87 [1] Rączkowsk et al. ‘ARA: accurate, reliable and active histopathological image classification framework with Bayesian deep learning’ Scientific Reports 2019 [2] Kather et al. ‘Multi-class texture analysis in colorectal cancer histology’ Scientific Reports 2016. [3] Coudray et al. ‘Classification and mutation prediction from non–small cell lung cancer histopathology images using deep learning’ Nature Medicine 2018. back to top</summary></entry><entry><title type="html">Learning Visual Features by Colorization for Slide-Consistent Survival Prediction from Whole Slide Images</title><link href="https://kittywong.github.io/kittywong/0856/12/31/Paper1670" rel="alternate" type="text/html" title="Learning Visual Features by Colorization for Slide-Consistent Survival Prediction from Whole Slide Images" /><published>0856-12-31T23:58:56-05:17</published><updated>0857-01-05T00:00:00-05:17</updated><id>https://kittywong.github.io/kittywong/0856/12/31/Paper1670</id><content type="html" xml:base="https://kittywong.github.io/kittywong/0856/12/31/Paper1670">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#author-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Paper Info&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#review-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Reviews&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#metareview-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Meta-review(s)&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;#authorFeedback-id&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;Author Feedback&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;author-id&quot;&gt;Authors&lt;/h1&gt;
&lt;p&gt;Lei Fan, Arcot Sowmya, Erik Meijering, Yang Song
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract-id&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Recent deep learning techniques have shown promising performance on survival prediction from Whole Slide Images (WSIs). These methods are often based on multiple-step frameworks including patch sampling, feature extraction and feature aggregation. However, feature extraction typically relies on handcrafted features or Convolutional Neural Networks (CNNs) pretrained on ImageNet without fine-tuning, thus leading to suboptimal performance. Besides, to aggregate features, previous studies focus on WSI-level survival prediction but ignore the heterogeneous information that is present in multiple WSIs acquired for the same patient. To address the above challenges, we propose a survival prediction model that exploits heterogeneous features at the patient-level. Specifically, we introduce colorization as the pretext task to train the CNNs which are tailored for extracting features from patches of WSIs. In addition, we develop a patient-level framework integrating multiple WSIs for survival prediction with consistency and ranking losses. Extensive experiments show that our model achieves state-of-the-art performance on two large-scale public datasets.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;link-id&quot;&gt;Link to paper&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1007/978-3-030-87237-3_57&quot;&gt;https://doi.org/10.1007/978-3-030-87237-3_57&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;code-id&quot;&gt;Link to the code repository&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dataset-id&quot;&gt;Link to the dataset(s)&lt;/h1&gt;
&lt;p&gt;N/A
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;review-id&quot;&gt;Reviews&lt;/h1&gt;

&lt;h3 id=&quot;review-1&quot;&gt;Review #1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The authors proposed a survival prediction based on features from the colorization model.&lt;/li&gt;
        &lt;li&gt;Additional consistency loss is introduced to regularize consistency among embedding features from multiple WSIs of the same patient.&lt;/li&gt;
        &lt;li&gt;Extensive experiments on GBM and LUSC dataset show performances of the proposed model comparing with other deep survival models.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;A colorization model as a pretext task is introduced in the framework which could make the model capture some semantic information and localize various objects in the images.&lt;/li&gt;
        &lt;li&gt;A two phase approach is used which provides better results than state-of-the-art deep survival models. The first phase trains the model using the widely used negative log-likelihood loss and the consistency loss. The second phase uses the ranking loss to fine-tune the model.&lt;/li&gt;
        &lt;li&gt;Extensive experiments are conducted to compare recent deep survival models. Ablation study is also presented to validate each component of the proposed framework.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The validation of using colorization-based feature extraction is not validated well in comprehensive ways. It is good to see Table 3 but experiment setting is missing. Also, visualization from Fig.3 is not clear enogh to show more meaningful results from colorization model.&lt;/li&gt;
        &lt;li&gt;The concern about consistency among different WSIs belong to one patient remains and the authors need to validate this. Some WSIs may not contain tumor sample and only have normal tissues. It seems not correct to require those normal WSIs to be consistent with same patient’s WSIs with tumor in patient-level.&lt;/li&gt;
        &lt;li&gt;The proposed work is quite similar with the work in [22]. They used the same C-MIL in their architectures. Only differences are ways to handle multiple WSIs and colorization-based feature extraction. It is necessary to compare [22] in the experiments and also the authors could use the design in [22] and change [22]’s CNN with the proposed colorization model. This could validate the effectiveness of handling multiple WSIs using consistency loss rather than phenotype clusters in [22].&lt;/li&gt;
        &lt;li&gt;It is not clear why fine-tune is needed and what is the motivation to use the proposed two phase training. The authors should report results from the first phase as most state-of-the-art survival models mainly use the negative log-likelihood loss for survival prediction.  The need of using ranking loss in the fine-tune step should be clearly justified.&lt;/li&gt;
        &lt;li&gt;Experimental settings are not very clear and more results could be mentioned. Results from each fold are not reported. Also, it is recommended to compare if significance could be found between the proposed model from the baseline.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;The network could be easily written with Pytorch as the framework used many open-sourced packages and codes, e.g. [2],[9],[22]&lt;/li&gt;
        &lt;li&gt;Implementation details are somewhat missing for readers to reproduce. The color palette construction is not presented with more details. Also, how to train the model in two phase is not very clear for reproduce purpose.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;It is not clear how authors handle various number of sampling patches from different patients. Do they set the fixed number using 1000 ?&lt;/li&gt;
        &lt;li&gt;Two phase training details are not very clear which seems very important. How to say the first phase is trained well and could be stop during traning ? Do you use fixed epochs or additional tunning set to perform early stop ? There are no details about fine-tune the model in the second phase.&lt;/li&gt;
        &lt;li&gt;Fig.3 is not very helpful to convince that colorization bring benefits as only one patch with such very small view could not provide meaningful visualization. It is necessary to show visualization from WSI or a much larger size of view.&lt;/li&gt;
        &lt;li&gt;The authors should test if the improvement of C-index in Table 4 is significant. Could use cindex.comp(cindex1, cindex2) in R survcomp package.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;borderline accept (6)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;It is an interesting paper and provide new findings about survival prediction using WSIs. However, validations are not sufficient to prove the effectiveness of each introduced component.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;3&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-2&quot;&gt;Review #2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Uses Colorization to train deep learning network to extract features for survival analysis.&lt;/li&gt;
        &lt;li&gt;Combines multiple slide images for a patient via a novel loss functions.&lt;/li&gt;
        &lt;li&gt;The experimental evaluation shows better performance compared to previous methods.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper trains a model by a colorization task to extract more relevant/meaningful features from whole slide images. It then combines these features and multiple images from a patient in a deep learning network to predict patient survival. The deep learning network uses an adaptation of ranking loss and consistency loss to train a survival prediction model.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Misses a relevant work: Wulczyn, E., Steiner, D.F., Xu, Z., Sadhwani, A., Wang, H., Flament-Auvigne, I., Mermel, C.H., Chen, P.H.C., Liu, Y. and Stumpe, M.C., 2020. Deep learning-based survival prediction for multiple cancer types using histopathology images. PLoS One, 15(6), p.e0233678.&lt;/li&gt;
        &lt;li&gt;Fails to experimentally compare with [22] (Yao, J et al. Whole slide images based cancer survival prediction…) which is a more recent work compared to WSISA, DeepCorrSurv, DeepGraphSurv, MILSurv.&lt;/li&gt;
        &lt;li&gt;While the proposed approach improve performance, the performance improvement is rather small. The new model is 0.4% and 2.5% better than RankSurv on the whole dataset.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Good&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The datasets used in the experiments are public. The methodology and experiments sections appear to have sufficient detail to reproduce the experiments with some help from the authors.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The authors use a novel approach of combining colorization, whole slide feature aggregation, and aggregation of multiple whole sidle images for a patient in a deep learning framework to predict survival. The experimental results using two different cancer types show performance improvements.&lt;/p&gt;

      &lt;p&gt;The authors should reference a recent relevant work:  Wulczyn, E., Steiner, D.F., Xu, Z., Sadhwani, A., Wang, H., Flament-Auvigne, I., Mermel, C.H., Chen, P.H.C., Liu, Y. and Stumpe, M.C., 2020. Deep learning-based survival prediction for multiple cancer types using histopathology images. PLoS One, 15(6), p.e0233678.&lt;/p&gt;

      &lt;p&gt;The paper could be improved by including the work by Wulczyn et al, and by Yao et al [22]. Given the proposed method has relatively small improvements over RankSurv, which is a more recent method than the other methods used in the experimental evaluation, it would be interesting to see how the proposed approach will do against the methods proposed by Wulczyn et al, and Yao et al.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;accept (8)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The paper presents an integrated deep learning framework that combines colorization and makes novel use of multiple loss functions for feature aggregation and aggregation of multiple whole slide images for a patient. The experimental evaluation is carried out using two different cancer types, rather than focusing on a single cancer type.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;5&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Somewhat confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;review-3&quot;&gt;Review #3&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please describe the contribution of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This apper develops a self-supervised learning based WSI embedding method for survival prediction. Experiments on two datasets show the effectiveness of the proposed method.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Applying self-supervised learning techniques to WSI representation learning improves the representation performance a lot.&lt;/p&gt;

      &lt;p&gt;Experimental results are comprehensive.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Other multiple-instance based methods can also generate a patient-level representation since there’s no difference between aggregating patches from single WSI and multiple WSIs.&lt;/p&gt;

      &lt;p&gt;To make Formula (1) more general, better to replace the numbers with variables.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please rate the clarity and organization of this paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Excellent&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Looks true.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: &lt;a href=&quot;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&quot;&gt;https://miccai2021.org/en/REVIEWER-GUIDELINES.html&lt;/a&gt;&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Solve the comments in weakness above.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please state your overall opinion of the paper&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;strong accept (9)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Please justify your recommendation. What were the major factors that led you to your overall score for this paper?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;novelty of the method; writing and experimental setting; clinical application.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your review stack?&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;1&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of papers in your stack&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;4&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviewer confidence&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Very confident&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;metareview-id&quot;&gt;Meta-Review(s)&lt;/h1&gt;

&lt;h2 id=&quot;primary-meta-review&quot;&gt;Primary Meta-Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;This paper develops a self-supervised learning based WSI embedding method for survival prediction, where colorization is used to train the network to extract features. Experiments on GBM and LUSC dataset show the effectiveness of the proposed method. All reviewers agree to accept this paper but raised many questions.  The revised manuscript should carefully address the questions raised by Reviewer 1 &amp;amp; 2.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;2&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;authorFeedback-id&quot;&gt;Author Feedback&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;We sincerely thank all reviewers and Area Chair for your time and comments. In the final version, we will (1) describe the experimental settings more clearly, improve Fig 3, include the statistical test results (log-rank p-values are smaller than 0.005 on both datasets), and better clarify the motivation of two-phase training (for R1); (2) include the additional references (for R2); (3) revise Eq 1 as suggested (for R3). We will also address other comments in our journal paper, including more ablation studies for the motivation of aggregating multiple WSIs, ways of patch sampling and different loss functions, better visualization of colorization output, and result comparison with [22] (for which we are trying to optimize the model proposed in [22] for the specific datasets to achieve optimal performance).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;a href=&quot;&quot;&gt;&lt;span style=&quot;color:#ff9900&quot;&gt;&lt;b&gt;back to top&lt;/b&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Modalities - Histopathology" /><category term="Computational (Integrative) Pathology" /><category term="Machine Learning - Self-supervised learning" /><category term="Fan, Lei" /><category term="Sowmya, Arcot" /><category term="Meijering, Erik" /><category term="Song, Yang" /><summary type="html">Paper Info Reviews Meta-review(s) Author Feedback Authors Lei Fan, Arcot Sowmya, Erik Meijering, Yang Song Abstract Recent deep learning techniques have shown promising performance on survival prediction from Whole Slide Images (WSIs). These methods are often based on multiple-step frameworks including patch sampling, feature extraction and feature aggregation. However, feature extraction typically relies on handcrafted features or Convolutional Neural Networks (CNNs) pretrained on ImageNet without fine-tuning, thus leading to suboptimal performance. Besides, to aggregate features, previous studies focus on WSI-level survival prediction but ignore the heterogeneous information that is present in multiple WSIs acquired for the same patient. To address the above challenges, we propose a survival prediction model that exploits heterogeneous features at the patient-level. Specifically, we introduce colorization as the pretext task to train the CNNs which are tailored for extracting features from patches of WSIs. In addition, we develop a patient-level framework integrating multiple WSIs for survival prediction with consistency and ranking losses. Extensive experiments show that our model achieves state-of-the-art performance on two large-scale public datasets. Link to paper https://doi.org/10.1007/978-3-030-87237-3_57 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors proposed a survival prediction based on features from the colorization model. Additional consistency loss is introduced to regularize consistency among embedding features from multiple WSIs of the same patient. Extensive experiments on GBM and LUSC dataset show performances of the proposed model comparing with other deep survival models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. A colorization model as a pretext task is introduced in the framework which could make the model capture some semantic information and localize various objects in the images. A two phase approach is used which provides better results than state-of-the-art deep survival models. The first phase trains the model using the widely used negative log-likelihood loss and the consistency loss. The second phase uses the ranking loss to fine-tune the model. Extensive experiments are conducted to compare recent deep survival models. Ablation study is also presented to validate each component of the proposed framework. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The validation of using colorization-based feature extraction is not validated well in comprehensive ways. It is good to see Table 3 but experiment setting is missing. Also, visualization from Fig.3 is not clear enogh to show more meaningful results from colorization model. The concern about consistency among different WSIs belong to one patient remains and the authors need to validate this. Some WSIs may not contain tumor sample and only have normal tissues. It seems not correct to require those normal WSIs to be consistent with same patient’s WSIs with tumor in patient-level. The proposed work is quite similar with the work in [22]. They used the same C-MIL in their architectures. Only differences are ways to handle multiple WSIs and colorization-based feature extraction. It is necessary to compare [22] in the experiments and also the authors could use the design in [22] and change [22]’s CNN with the proposed colorization model. This could validate the effectiveness of handling multiple WSIs using consistency loss rather than phenotype clusters in [22]. It is not clear why fine-tune is needed and what is the motivation to use the proposed two phase training. The authors should report results from the first phase as most state-of-the-art survival models mainly use the negative log-likelihood loss for survival prediction. The need of using ranking loss in the fine-tune step should be clearly justified. Experimental settings are not very clear and more results could be mentioned. Results from each fold are not reported. Also, it is recommended to compare if significance could be found between the proposed model from the baseline. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The network could be easily written with Pytorch as the framework used many open-sourced packages and codes, e.g. [2],[9],[22] Implementation details are somewhat missing for readers to reproduce. The color palette construction is not presented with more details. Also, how to train the model in two phase is not very clear for reproduce purpose. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It is not clear how authors handle various number of sampling patches from different patients. Do they set the fixed number using 1000 ? Two phase training details are not very clear which seems very important. How to say the first phase is trained well and could be stop during traning ? Do you use fixed epochs or additional tunning set to perform early stop ? There are no details about fine-tune the model in the second phase. Fig.3 is not very helpful to convince that colorization bring benefits as only one patch with such very small view could not provide meaningful visualization. It is necessary to show visualization from WSI or a much larger size of view. The authors should test if the improvement of C-index in Table 4 is significant. Could use cindex.comp(cindex1, cindex2) in R survcomp package. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It is an interesting paper and provide new findings about survival prediction using WSIs. However, validations are not sufficient to prove the effectiveness of each introduced component. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 4 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper Uses Colorization to train deep learning network to extract features for survival analysis. Combines multiple slide images for a patient via a novel loss functions. The experimental evaluation shows better performance compared to previous methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper trains a model by a colorization task to extract more relevant/meaningful features from whole slide images. It then combines these features and multiple images from a patient in a deep learning network to predict patient survival. The deep learning network uses an adaptation of ranking loss and consistency loss to train a survival prediction model. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Misses a relevant work: Wulczyn, E., Steiner, D.F., Xu, Z., Sadhwani, A., Wang, H., Flament-Auvigne, I., Mermel, C.H., Chen, P.H.C., Liu, Y. and Stumpe, M.C., 2020. Deep learning-based survival prediction for multiple cancer types using histopathology images. PLoS One, 15(6), p.e0233678. Fails to experimentally compare with [22] (Yao, J et al. Whole slide images based cancer survival prediction…) which is a more recent work compared to WSISA, DeepCorrSurv, DeepGraphSurv, MILSurv. While the proposed approach improve performance, the performance improvement is rather small. The new model is 0.4% and 2.5% better than RankSurv on the whole dataset. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The datasets used in the experiments are public. The methodology and experiments sections appear to have sufficient detail to reproduce the experiments with some help from the authors. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The authors use a novel approach of combining colorization, whole slide feature aggregation, and aggregation of multiple whole sidle images for a patient in a deep learning framework to predict survival. The experimental results using two different cancer types show performance improvements. The authors should reference a recent relevant work: Wulczyn, E., Steiner, D.F., Xu, Z., Sadhwani, A., Wang, H., Flament-Auvigne, I., Mermel, C.H., Chen, P.H.C., Liu, Y. and Stumpe, M.C., 2020. Deep learning-based survival prediction for multiple cancer types using histopathology images. PLoS One, 15(6), p.e0233678. The paper could be improved by including the work by Wulczyn et al, and by Yao et al [22]. Given the proposed method has relatively small improvements over RankSurv, which is a more recent method than the other methods used in the experimental evaluation, it would be interesting to see how the proposed approach will do against the methods proposed by Wulczyn et al, and Yao et al. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper presents an integrated deep learning framework that combines colorization and makes novel use of multiple loss functions for feature aggregation and aggregation of multiple whole slide images for a patient. The experimental evaluation is carried out using two different cancer types, rather than focusing on a single cancer type. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Somewhat confident Review #3 Please describe the contribution of the paper This apper develops a self-supervised learning based WSI embedding method for survival prediction. Experiments on two datasets show the effectiveness of the proposed method. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Applying self-supervised learning techniques to WSI representation learning improves the representation performance a lot. Experimental results are comprehensive. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Other multiple-instance based methods can also generate a patient-level representation since there’s no difference between aggregating patches from single WSI and multiple WSIs. To make Formula (1) more general, better to replace the numbers with variables. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Looks true. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Solve the comments in weakness above. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? novelty of the method; writing and experimental setting; clinical application. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper develops a self-supervised learning based WSI embedding method for survival prediction, where colorization is used to train the network to extract features. Experiments on GBM and LUSC dataset show the effectiveness of the proposed method. All reviewers agree to accept this paper but raised many questions. The revised manuscript should carefully address the questions raised by Reviewer 1 &amp;amp; 2. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback We sincerely thank all reviewers and Area Chair for your time and comments. In the final version, we will (1) describe the experimental settings more clearly, improve Fig 3, include the statistical test results (log-rank p-values are smaller than 0.005 on both datasets), and better clarify the motivation of two-phase training (for R1); (2) include the additional references (for R2); (3) revise Eq 1 as suggested (for R3). We will also address other comments in our journal paper, including more ablation studies for the motivation of aggregating multiple WSIs, ways of patch sampling and different loss functions, better visualization of colorization output, and result comparison with [22] (for which we are trying to optimize the model proposed in [22] for the specific datasets to achieve optimal performance). back to top</summary></entry></feed>