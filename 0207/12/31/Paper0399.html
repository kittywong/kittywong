<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Self-Supervised Longitudinal Neighbourhood Embedding | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Self-Supervised Longitudinal Neighbourhood Embedding" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Jiahong Ouyang, Qingyu Zhao, Ehsan Adeli, Edith V Sullivan, Adolf Pfefferbaum, Greg Zaharchuk, Kilian M. Pohl Abstract Longitudinal MRIs are often used to capture the gradual deterioration of brain structure and function caused by aging or neurological diseases. Analyzing this data via machine learning generally requires a large number of ground-truth labels, which are often missing or expensive to obtain. Reducing the need for labels, we propose a self-supervised strategy for representation learning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts in contrastive learning, LNE explicitly models the similarity between trajectory vectors across different subjects. We do so by building a graph in each training iteration defining neighborhoods in the latent space so that the progression direction of a subject follows the direction of its neighbors. This results in a smooth trajectory field that captures the global morphological change of the brain while maintaining the local continuity. We apply LNE to longitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274 healthy subjects, and Alzheimer’s Disease Neuroimaging Initiative (ADNI, N=632). The visualization of the smooth trajectory vector field and superior performance on downstream tasks demonstrate the strength of the proposed method over existing self-supervised methods in extracting information associated with normal aging and in revealing the impact of neurodegenerative disorders. The code is available at \url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding}. Link to paper https://doi.org/10.1007/978-3-030-87196-3_8 Link to the code repository https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding Link to the dataset(s) http://adni.loni.usc.edu/ Reviews Review #1 Please describe the contribution of the paper In this study, the author proposed a novel self-supervised feature embedding framework that captures consistent longitudinal variations from brain structure data. The well-designed constraint on the latent embedding space improves the performance of several downstream learning tasks and also provides a clear map of heterogeneous temporal changes in brain imaging data such as aging and disease progression. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is well organized and clearly written. The main contribution is explicitly addressed and seems novel. a self-supervised framework encodes longitudinal patterns in a pairwise manner controlled by a local linear constraint in the latent space. experiments on predictions of healthy aging and AD progression prove the effectiveness of the proposed framework. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Given the auto-encoder architecture, the reconstruction error term penalizes learning for optimal latent space. How does the author balance that in self-supervised learning while also improves the downstream supervised tasks, i.e. discussion of parameter \lambda. Also, it worth checking reconstruction errors visually in tasks of healthy aging and AD progression, when the encoder is frozen or fine-tuned. Parameters for the local neighborhood are defined empirically, e.g. N_nb, A_ij, dimension of Z, etc.. Discussion about these parameter choices w.r.t. model performance could better understand the proposed learning strategies. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No implementation code been shared. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please check my comments in the weaknesses section. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Solid method with convincing experimental results. Sufficient novelty. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper utilizes the temporal information embedded in longitudinal MRIs and proposed a new self-supervised learning approach (LNE) to captures the non-linear global trajectory field from local progression directions. The proposed approach is evaluated on two different downstream tasks (e.g., age prediction and Alzheimer’s Disease prediction) and outperforms many self-supervised learning baselines. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This manuscript is well written and illustrated, making it very easy to follow. The age distribution analysis of pretrained features clearly demonstrate that LNE can lean (non-linear) global trajectory field. The proposed approach is further justified by two downstream tasks and demonstrates superior performance compared with seral self-supervised baselines (e.g., SimCLR and LSSL). Especially, LNE with frozen features outperforms training from scratch and finetuning from AE/SimCLR on the ADNI dataset. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The baselines did not incorporate recent progress in self-supervised learning. For example, SwAV [1] and BYOL [2] show superior performance compared with SimCLR. It would be more comprehensive to include SwAV and BYOL as baselines. Besides, although LNE “achieves the best performance among all methods that were solely based on structural MRI,” its BACC is still 4.9 lower than state of the art. Since SimCLR is original proposed for natural images, applying it on 3D MRIs can not be straightforward. Therefore, the implementation details should be provided. [1] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P. and Joulin, A., 2020. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882. [2] Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P.H., Buchatskaya, E., Doersch, C., Pires, B.A., Guo, Z.D., Azar, M.G. and Piot, B., 2020. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance It may not be possible. All experiments may be conducted on private datasets. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html What is the purpose of combining cosine and reconstruction losses in Fig. 1? What is the performance of LNE without reconstruction? On page 3, what are \lambda_dir and \lambda_recon? They are not mentioned/described in the rest of this paper. An ablation study regarding the neighbour size (i.e., N_nb) should be conducted. Specifically, is the performance of LNE sensitive to the selection of N_nb? On page 4, the authors argue that LSSL “must define a globally linear direction in the latent space.” It will be more interesting to compare LSSL with LNE in Figs. 2 and 3, further supporting the augment. As mentioned on page 4, LNE can be regarded as a contrastive self-supervised method with only positive pairs. On the other hand, Chen et al. [1] demonstrated that contrastive learning with only positive pairs might suffer from the collapsing problem. I wonder whether LNE has the same problem. If not, it would be interesting to discuss why the proposed mechanism can prevent the collapsing problem. [1] Chen, X. and He, K., 2020. Exploring Simple Siamese Representation Learning. arXiv preprint arXiv:2011.10566. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The manuscript is well written and illustrated. The proposed approach is theoretically sound and experimentally justified. The performance is superior compared with serval existing self-supervised methods. The baselines in Table 1 should be updated and include more advanced studies. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The paper proposes Longitudinal Neighborhood Embedding (LNE), a self-supervised strategy to reduce the need for labels in longitudinal MRIs Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Well written Experimental results are well conducted Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There is only one lambda defined in loss equations. Where are \lambda_dir and \lambda_recon The regularization weights \lambda is big ( \lambda_dir = 1.0 and \lambda_recon = 2.0). It seems those regularization weights play an important role. However, there is no discussion about these Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The code was available on github. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html There is only one lambda defined in loss equations. Where are \lambda_dir and \lambda_recon The regularization weights \lambda is big ( \lambda_dir = 1.0 and \lambda_recon = 2.0). It seems those regularization weights play an important role. However, there is no discussion about these Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novelty Readability What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Reviewers in their detailed assessments agree on the clarity and the innovative and novel aspects of the paper and thus suitability for MICCAI. Whereas the code is available on github, authors are encouraged to also make test datasets available for others to reproduce their results. For a final submission, authors are strongly encouraged to follow the reviewer’s advice for improvements as given in the detailed comments and summary of weaknesses. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback N/A back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Jiahong Ouyang, Qingyu Zhao, Ehsan Adeli, Edith V Sullivan, Adolf Pfefferbaum, Greg Zaharchuk, Kilian M. Pohl Abstract Longitudinal MRIs are often used to capture the gradual deterioration of brain structure and function caused by aging or neurological diseases. Analyzing this data via machine learning generally requires a large number of ground-truth labels, which are often missing or expensive to obtain. Reducing the need for labels, we propose a self-supervised strategy for representation learning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts in contrastive learning, LNE explicitly models the similarity between trajectory vectors across different subjects. We do so by building a graph in each training iteration defining neighborhoods in the latent space so that the progression direction of a subject follows the direction of its neighbors. This results in a smooth trajectory field that captures the global morphological change of the brain while maintaining the local continuity. We apply LNE to longitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274 healthy subjects, and Alzheimer’s Disease Neuroimaging Initiative (ADNI, N=632). The visualization of the smooth trajectory vector field and superior performance on downstream tasks demonstrate the strength of the proposed method over existing self-supervised methods in extracting information associated with normal aging and in revealing the impact of neurodegenerative disorders. The code is available at \url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding}. Link to paper https://doi.org/10.1007/978-3-030-87196-3_8 Link to the code repository https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding Link to the dataset(s) http://adni.loni.usc.edu/ Reviews Review #1 Please describe the contribution of the paper In this study, the author proposed a novel self-supervised feature embedding framework that captures consistent longitudinal variations from brain structure data. The well-designed constraint on the latent embedding space improves the performance of several downstream learning tasks and also provides a clear map of heterogeneous temporal changes in brain imaging data such as aging and disease progression. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is well organized and clearly written. The main contribution is explicitly addressed and seems novel. a self-supervised framework encodes longitudinal patterns in a pairwise manner controlled by a local linear constraint in the latent space. experiments on predictions of healthy aging and AD progression prove the effectiveness of the proposed framework. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Given the auto-encoder architecture, the reconstruction error term penalizes learning for optimal latent space. How does the author balance that in self-supervised learning while also improves the downstream supervised tasks, i.e. discussion of parameter \lambda. Also, it worth checking reconstruction errors visually in tasks of healthy aging and AD progression, when the encoder is frozen or fine-tuned. Parameters for the local neighborhood are defined empirically, e.g. N_nb, A_ij, dimension of Z, etc.. Discussion about these parameter choices w.r.t. model performance could better understand the proposed learning strategies. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No implementation code been shared. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please check my comments in the weaknesses section. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Solid method with convincing experimental results. Sufficient novelty. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper utilizes the temporal information embedded in longitudinal MRIs and proposed a new self-supervised learning approach (LNE) to captures the non-linear global trajectory field from local progression directions. The proposed approach is evaluated on two different downstream tasks (e.g., age prediction and Alzheimer’s Disease prediction) and outperforms many self-supervised learning baselines. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This manuscript is well written and illustrated, making it very easy to follow. The age distribution analysis of pretrained features clearly demonstrate that LNE can lean (non-linear) global trajectory field. The proposed approach is further justified by two downstream tasks and demonstrates superior performance compared with seral self-supervised baselines (e.g., SimCLR and LSSL). Especially, LNE with frozen features outperforms training from scratch and finetuning from AE/SimCLR on the ADNI dataset. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The baselines did not incorporate recent progress in self-supervised learning. For example, SwAV [1] and BYOL [2] show superior performance compared with SimCLR. It would be more comprehensive to include SwAV and BYOL as baselines. Besides, although LNE “achieves the best performance among all methods that were solely based on structural MRI,” its BACC is still 4.9 lower than state of the art. Since SimCLR is original proposed for natural images, applying it on 3D MRIs can not be straightforward. Therefore, the implementation details should be provided. [1] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P. and Joulin, A., 2020. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882. [2] Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P.H., Buchatskaya, E., Doersch, C., Pires, B.A., Guo, Z.D., Azar, M.G. and Piot, B., 2020. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance It may not be possible. All experiments may be conducted on private datasets. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html What is the purpose of combining cosine and reconstruction losses in Fig. 1? What is the performance of LNE without reconstruction? On page 3, what are \lambda_dir and \lambda_recon? They are not mentioned/described in the rest of this paper. An ablation study regarding the neighbour size (i.e., N_nb) should be conducted. Specifically, is the performance of LNE sensitive to the selection of N_nb? On page 4, the authors argue that LSSL “must define a globally linear direction in the latent space.” It will be more interesting to compare LSSL with LNE in Figs. 2 and 3, further supporting the augment. As mentioned on page 4, LNE can be regarded as a contrastive self-supervised method with only positive pairs. On the other hand, Chen et al. [1] demonstrated that contrastive learning with only positive pairs might suffer from the collapsing problem. I wonder whether LNE has the same problem. If not, it would be interesting to discuss why the proposed mechanism can prevent the collapsing problem. [1] Chen, X. and He, K., 2020. Exploring Simple Siamese Representation Learning. arXiv preprint arXiv:2011.10566. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The manuscript is well written and illustrated. The proposed approach is theoretically sound and experimentally justified. The performance is superior compared with serval existing self-supervised methods. The baselines in Table 1 should be updated and include more advanced studies. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The paper proposes Longitudinal Neighborhood Embedding (LNE), a self-supervised strategy to reduce the need for labels in longitudinal MRIs Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Well written Experimental results are well conducted Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There is only one lambda defined in loss equations. Where are \lambda_dir and \lambda_recon The regularization weights \lambda is big ( \lambda_dir = 1.0 and \lambda_recon = 2.0). It seems those regularization weights play an important role. However, there is no discussion about these Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The code was available on github. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html There is only one lambda defined in loss equations. Where are \lambda_dir and \lambda_recon The regularization weights \lambda is big ( \lambda_dir = 1.0 and \lambda_recon = 2.0). It seems those regularization weights play an important role. However, there is no discussion about these Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novelty Readability What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Reviewers in their detailed assessments agree on the clarity and the innovative and novel aspects of the paper and thus suitability for MICCAI. Whereas the code is available on github, authors are encouraged to also make test datasets available for others to reproduce their results. For a final submission, authors are strongly encouraged to follow the reviewer’s advice for improvements as given in the detailed comments and summary of weaknesses. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback N/A back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0207/12/31/Paper0399" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0207/12/31/Paper0399" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0207-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Self-Supervised Longitudinal Neighbourhood Embedding" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0207/12/31/Paper0399"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0207/12/31/Paper0399","headline":"Self-Supervised Longitudinal Neighbourhood Embedding","dateModified":"0208-01-01T00:00:00-05:17","datePublished":"0207-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Jiahong Ouyang, Qingyu Zhao, Ehsan Adeli, Edith V Sullivan, Adolf Pfefferbaum, Greg Zaharchuk, Kilian M. Pohl Abstract Longitudinal MRIs are often used to capture the gradual deterioration of brain structure and function caused by aging or neurological diseases. Analyzing this data via machine learning generally requires a large number of ground-truth labels, which are often missing or expensive to obtain. Reducing the need for labels, we propose a self-supervised strategy for representation learning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts in contrastive learning, LNE explicitly models the similarity between trajectory vectors across different subjects. We do so by building a graph in each training iteration defining neighborhoods in the latent space so that the progression direction of a subject follows the direction of its neighbors. This results in a smooth trajectory field that captures the global morphological change of the brain while maintaining the local continuity. We apply LNE to longitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274 healthy subjects, and Alzheimer’s Disease Neuroimaging Initiative (ADNI, N=632). The visualization of the smooth trajectory vector field and superior performance on downstream tasks demonstrate the strength of the proposed method over existing self-supervised methods in extracting information associated with normal aging and in revealing the impact of neurodegenerative disorders. The code is available at \\url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding}. Link to paper https://doi.org/10.1007/978-3-030-87196-3_8 Link to the code repository https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding Link to the dataset(s) http://adni.loni.usc.edu/ Reviews Review #1 Please describe the contribution of the paper In this study, the author proposed a novel self-supervised feature embedding framework that captures consistent longitudinal variations from brain structure data. The well-designed constraint on the latent embedding space improves the performance of several downstream learning tasks and also provides a clear map of heterogeneous temporal changes in brain imaging data such as aging and disease progression. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is well organized and clearly written. The main contribution is explicitly addressed and seems novel. a self-supervised framework encodes longitudinal patterns in a pairwise manner controlled by a local linear constraint in the latent space. experiments on predictions of healthy aging and AD progression prove the effectiveness of the proposed framework. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Given the auto-encoder architecture, the reconstruction error term penalizes learning for optimal latent space. How does the author balance that in self-supervised learning while also improves the downstream supervised tasks, i.e. discussion of parameter \\lambda. Also, it worth checking reconstruction errors visually in tasks of healthy aging and AD progression, when the encoder is frozen or fine-tuned. Parameters for the local neighborhood are defined empirically, e.g. N_nb, A_ij, dimension of Z, etc.. Discussion about these parameter choices w.r.t. model performance could better understand the proposed learning strategies. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance No implementation code been shared. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please check my comments in the weaknesses section. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Solid method with convincing experimental results. Sufficient novelty. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper utilizes the temporal information embedded in longitudinal MRIs and proposed a new self-supervised learning approach (LNE) to captures the non-linear global trajectory field from local progression directions. The proposed approach is evaluated on two different downstream tasks (e.g., age prediction and Alzheimer’s Disease prediction) and outperforms many self-supervised learning baselines. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This manuscript is well written and illustrated, making it very easy to follow. The age distribution analysis of pretrained features clearly demonstrate that LNE can lean (non-linear) global trajectory field. The proposed approach is further justified by two downstream tasks and demonstrates superior performance compared with seral self-supervised baselines (e.g., SimCLR and LSSL). Especially, LNE with frozen features outperforms training from scratch and finetuning from AE/SimCLR on the ADNI dataset. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The baselines did not incorporate recent progress in self-supervised learning. For example, SwAV [1] and BYOL [2] show superior performance compared with SimCLR. It would be more comprehensive to include SwAV and BYOL as baselines. Besides, although LNE “achieves the best performance among all methods that were solely based on structural MRI,” its BACC is still 4.9 lower than state of the art. Since SimCLR is original proposed for natural images, applying it on 3D MRIs can not be straightforward. Therefore, the implementation details should be provided. [1] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P. and Joulin, A., 2020. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882. [2] Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P.H., Buchatskaya, E., Doersch, C., Pires, B.A., Guo, Z.D., Azar, M.G. and Piot, B., 2020. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance It may not be possible. All experiments may be conducted on private datasets. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html What is the purpose of combining cosine and reconstruction losses in Fig. 1? What is the performance of LNE without reconstruction? On page 3, what are \\lambda_dir and \\lambda_recon? They are not mentioned/described in the rest of this paper. An ablation study regarding the neighbour size (i.e., N_nb) should be conducted. Specifically, is the performance of LNE sensitive to the selection of N_nb? On page 4, the authors argue that LSSL “must define a globally linear direction in the latent space.” It will be more interesting to compare LSSL with LNE in Figs. 2 and 3, further supporting the augment. As mentioned on page 4, LNE can be regarded as a contrastive self-supervised method with only positive pairs. On the other hand, Chen et al. [1] demonstrated that contrastive learning with only positive pairs might suffer from the collapsing problem. I wonder whether LNE has the same problem. If not, it would be interesting to discuss why the proposed mechanism can prevent the collapsing problem. [1] Chen, X. and He, K., 2020. Exploring Simple Siamese Representation Learning. arXiv preprint arXiv:2011.10566. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The manuscript is well written and illustrated. The proposed approach is theoretically sound and experimentally justified. The performance is superior compared with serval existing self-supervised methods. The baselines in Table 1 should be updated and include more advanced studies. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The paper proposes Longitudinal Neighborhood Embedding (LNE), a self-supervised strategy to reduce the need for labels in longitudinal MRIs Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Well written Experimental results are well conducted Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. There is only one lambda defined in loss equations. Where are \\lambda_dir and \\lambda_recon The regularization weights \\lambda is big ( \\lambda_dir = 1.0 and \\lambda_recon = 2.0). It seems those regularization weights play an important role. However, there is no discussion about these Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The code was available on github. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html There is only one lambda defined in loss equations. Where are \\lambda_dir and \\lambda_recon The regularization weights \\lambda is big ( \\lambda_dir = 1.0 and \\lambda_recon = 2.0). It seems those regularization weights play an important role. However, there is no discussion about these Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Novelty Readability What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Reviewers in their detailed assessments agree on the clarity and the innovative and novel aspects of the paper and thus suitability for MICCAI. Whereas the code is available on github, authors are encouraged to also make test datasets available for others to reproduce their results. For a final submission, authors are strongly encouraged to follow the reviewer’s advice for improvements as given in the detailed comments and summary of weaknesses. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback N/A back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Ouyang, Jiahong,Zhao, Qingyu,Adeli, Ehsan,Sullivan, Edith V,Pfefferbaum, Adolf,Zaharchuk, Greg,Pohl, Kilian M." />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Self-Supervised Longitudinal Neighbourhood Embedding</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Neuroimaging - Others"
        class="post-category">
        Clinical applications - Neuroimaging - Others
      </a>
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Ouyang, Jiahong"
        class="post-tags">
        Ouyang, Jiahong
      </a> |  
      
      <a href="kittywong/tags#Zhao, Qingyu"
        class="post-tags">
        Zhao, Qingyu
      </a> |  
      
      <a href="kittywong/tags#Adeli, Ehsan"
        class="post-tags">
        Adeli, Ehsan
      </a> |  
      
      <a href="kittywong/tags#Sullivan, Edith V"
        class="post-tags">
        Sullivan, Edith V
      </a> |  
      
      <a href="kittywong/tags#Pfefferbaum, Adolf"
        class="post-tags">
        Pfefferbaum, Adolf
      </a> |  
      
      <a href="kittywong/tags#Zaharchuk, Greg"
        class="post-tags">
        Zaharchuk, Greg
      </a> |  
      
      <a href="kittywong/tags#Pohl, Kilian M."
        class="post-tags">
        Pohl, Kilian M.
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Jiahong Ouyang, Qingyu Zhao, Ehsan Adeli, Edith V Sullivan, Adolf Pfefferbaum, Greg Zaharchuk, Kilian M. Pohl
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Longitudinal MRIs are often used to capture the gradual deterioration of brain structure and function caused by aging or neurological diseases. Analyzing this data via machine learning generally requires a large number of ground-truth labels, which are often missing or expensive to obtain. Reducing the need for labels, we propose a self-supervised strategy for representation learning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts in contrastive learning, LNE explicitly models the similarity between trajectory vectors across different subjects. We do so by building a graph in each training iteration defining neighborhoods in the latent space so that the progression direction of a subject follows the direction of its neighbors. This results in a smooth trajectory field that captures the global morphological change of the brain while maintaining the local continuity. We apply LNE to longitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274 healthy subjects, and Alzheimer’s Disease Neuroimaging Initiative (ADNI, N=632). The visualization of the smooth trajectory vector field and superior performance on downstream tasks demonstrate the strength of the proposed method over existing self-supervised methods in extracting information associated with normal aging and in revealing the impact of neurodegenerative disorders. The code is available at \url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding}.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87196-3_8">https://doi.org/10.1007/978-3-030-87196-3_8</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>http://adni.loni.usc.edu/
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>In this study, the author proposed a novel self-supervised feature embedding framework that captures consistent longitudinal variations from brain structure data. The well-designed constraint on the latent embedding space improves the performance of several downstream learning tasks and also provides a clear map of heterogeneous temporal changes in brain imaging data such as aging and disease progression.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>The paper is well organized and clearly written. The main contribution is explicitly addressed and seems novel.</li>
        <li>a self-supervised framework encodes longitudinal patterns in a pairwise manner controlled by a local linear constraint in the latent space.</li>
        <li>experiments on predictions of healthy aging and AD progression prove the effectiveness of the proposed framework.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>Given the auto-encoder architecture, the reconstruction error term penalizes learning for optimal latent space. How does the author balance that in self-supervised learning while also improves the downstream supervised tasks, i.e. discussion of parameter \lambda. Also, it worth checking reconstruction errors visually in tasks of healthy aging and AD progression, when the encoder is frozen or fine-tuned.</li>
        <li>Parameters for the local neighborhood are defined empirically, e.g. N_nb, A_ij, dimension of Z, etc.. Discussion about these parameter choices w.r.t. model performance could better understand the proposed learning strategies.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>No implementation code been shared.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Please check my comments in the weaknesses section.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Solid method with convincing experimental results. Sufficient novelty.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper utilizes the temporal information embedded in longitudinal MRIs and proposed a new self-supervised learning approach (LNE) to captures the non-linear global trajectory field from local progression directions. The proposed approach is evaluated on two different downstream tasks (e.g., age prediction and Alzheimer’s Disease prediction) and outperforms many self-supervised learning baselines.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>This manuscript is well written and illustrated, making it very easy to follow.</li>
        <li>The age distribution analysis of pretrained features clearly demonstrate that LNE can lean (non-linear) global trajectory field.</li>
        <li>The proposed approach is further justified by two downstream tasks and demonstrates superior performance compared with seral self-supervised baselines (e.g., SimCLR and LSSL). Especially, LNE with frozen features outperforms training from scratch and finetuning from AE/SimCLR on the ADNI dataset.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>The baselines did not incorporate recent progress in self-supervised learning. For example, SwAV [1] and BYOL [2] show superior performance compared with SimCLR. It would be more comprehensive to include SwAV and BYOL as baselines. Besides, although LNE “achieves the best performance among all methods that were solely based on structural MRI,” its BACC is still 4.9 lower than state of the art.</li>
        <li>Since SimCLR is original proposed for natural images, applying it on 3D MRIs can not be straightforward. Therefore, the implementation details should be provided.</li>
      </ul>

      <p>[1] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P. and Joulin, A., 2020. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882.
[2] Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P.H., Buchatskaya, E., Doersch, C., Pires, B.A., Guo, Z.D., Azar, M.G. and Piot, B., 2020. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>It may not be possible. All experiments may be conducted on private datasets.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ol>
        <li>What is the purpose of combining cosine and reconstruction losses in Fig. 1? What is the performance of LNE without reconstruction?</li>
        <li>On page 3, what are \lambda_dir and \lambda_recon? They are not mentioned/described in the rest of this paper.</li>
        <li>An ablation study regarding the neighbour size (i.e., N_nb) should be conducted. Specifically, is the performance of LNE sensitive to the selection of N_nb?</li>
        <li>On page 4, the authors argue that LSSL “must define a globally linear direction in the latent space.” It will be more interesting to compare LSSL with LNE in Figs. 2 and 3, further supporting the augment.</li>
        <li>As mentioned on page 4, LNE can be regarded as a contrastive self-supervised method with only positive pairs. On the other hand, Chen et al. [1] demonstrated that contrastive learning with only positive pairs might suffer from the collapsing problem. I wonder whether LNE has the same problem. If not, it would be interesting to discuss why the proposed mechanism can prevent the collapsing problem.</li>
      </ol>

      <p>[1] Chen, X. and He, K., 2020. Exploring Simple Siamese Representation Learning. arXiv preprint arXiv:2011.10566.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <ul>
        <li>The manuscript is well written and illustrated.</li>
        <li>The proposed approach is theoretically sound and experimentally justified.</li>
        <li>The performance is superior compared with serval existing self-supervised methods.</li>
        <li>The baselines in Table 1 should be updated and include more advanced studies.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper proposes Longitudinal Neighborhood Embedding (LNE), a self-supervised
strategy to reduce the need for labels in longitudinal MRIs</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>Well written</li>
        <li>Experimental results are well conducted</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>There is only one lambda defined in loss equations. Where are \lambda_dir and \lambda_recon</li>
        <li>The regularization weights \lambda is big ( \lambda_dir = 1.0 and \lambda_recon = 2.0). It seems those regularization weights play an important role. However, there is no discussion about these</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The code was available on github.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ul>
        <li>There is only one lambda defined in loss equations. Where are \lambda_dir and \lambda_recon</li>
        <li>The regularization weights \lambda is big ( \lambda_dir = 1.0 and \lambda_recon = 2.0). It seems those regularization weights play an important role. However, there is no discussion about these</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Novelty
Readability</p>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>Reviewers in their detailed assessments agree on the clarity and the innovative and novel aspects of the paper and thus suitability for MICCAI. Whereas the code is available on github, authors are encouraged to also make test datasets available for others to reproduce their results. For a final submission, authors are strongly encouraged to follow the reviewer’s advice for improvements as given in the detailed comments and summary of weaknesses.</p>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>N/A</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0207-12-31
      -->
      <!--
      
        ,
        updated at 
        0208-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Neuroimaging - Others"
        class="post-category">
        Clinical applications - Neuroimaging - Others
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Ouyang, Jiahong"
        class="post-category">
        Ouyang, Jiahong
      </a> |  
      
      <a href="kittywong/tags#Zhao, Qingyu"
        class="post-category">
        Zhao, Qingyu
      </a> |  
      
      <a href="kittywong/tags#Adeli, Ehsan"
        class="post-category">
        Adeli, Ehsan
      </a> |  
      
      <a href="kittywong/tags#Sullivan, Edith V"
        class="post-category">
        Sullivan, Edith V
      </a> |  
      
      <a href="kittywong/tags#Pfefferbaum, Adolf"
        class="post-category">
        Pfefferbaum, Adolf
      </a> |  
      
      <a href="kittywong/tags#Zaharchuk, Greg"
        class="post-category">
        Zaharchuk, Greg
      </a> |  
      
      <a href="kittywong/tags#Pohl, Kilian M."
        class="post-category">
        Pohl, Kilian M.
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0208/12/31/Paper0538">
          Self-Supervised Multi-Modal Alignment For Whole Body Medical Imaging
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0206/12/31/Paper0395">
          Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
