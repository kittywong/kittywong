<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>One-Shot Medical Landmark Detection | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="One-Shot Medical Landmark Detection" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Qingsong Yao, Quan Quan, Li Xiao, S. Kevin Zhou Abstract The success of deep learning methods relies on the availability of a large number of datasets with annotations; however, curating such datasets is burdensome, especially for medical images. To relieve such a burden for a landmark detection task, we explore the feasibility of using only a single annotated image and propose a novel framework named Cascade Comparing to Detect (CC2D) for one-shot landmark detection. CC2D consists of two stages: 1) Self-supervised learning (CC2D-SSL) and 2) Training with pseudo-labels (CC2D-TPL). CC2D-SSL captures the consistent anatomical information in a coarse-to-fine fashion by comparing the cascade feature representations and generates predictions on the training set. CC2D-TPL further improves the performance by training a new landmark detector with those predictions. The effectiveness of CC2D is evaluated on a widely-used public dataset of cephalometric landmark detection, which achieves a competitive detection accuracy of 81.01% within 4.0mm, comparable to the state-of-the-art fully-supervised methods using a lot more than one training image. Link to paper https://doi.org/10.1007/978-3-030-87196-3_17 Link to the code repository https://github.com/ICT-MIRACLE-lab/Oneshot_landmark_detection Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper presents a deep learning framework for landmark detection, CC2D, which learns from a single annotated image and a number of unannotated images. The framework first uses a self-supervised learning step to learn to approximate the landmark positions for all training images and then trains a CNN-based landmark detector using the approximated landmark positions. CC2D was evaluated on a public cephalometric dataset and achieves good performance. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. CC2D is a novel framework to automatically generate landmark annotations using a single annotated training image. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors report the performance to be “comparable to the state-of-the-art fully-supervised methods”. However, looking at the results in Table 1 this does not seem to be the case. The best performing 4mm SDR is 89.85% compared to 81.01% for the proposed method. Looking at the clinically relevant 2mm SDR the best performing method achieves 73.33% compared to 49.81% for the proposed method. While the results are impressive for just using a single annotated image, they are not comparable to the state-of-the-art. The training/testing set-up is not clear. The authors state that they use a 150/250 image split (as in the related literature). However, from the wording it is not clear whether these are the very same images within the train/testing sets as previously reported, or whether the sets just contain the same numbers of images. It is also not mentioned how the single annotated image was chosen. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The dataset is publicly available. Parameter details are specified but no link to code is provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I would recommend for the authors to rephrase the statement about achieving comparable results to fully-supervised state-of-the-art methods. “On one hand” should be “On the one hand”. In Figure 2 it says “1th” instead of “1st” on one occasion. [6] is mentioned as having “satisfactory performance”. However, looking at the paper suggests that [6] achieves state-of-the-art performance. “testset” should be “test set”. [10] and [14] do not list performance values as per Table 1 - are these the wrong references? Should “most of the landmarks in Fig. 2(b)” be “most of the landmarks in Fig. 4(b)”? Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This work presents an interesting first step towards automatic landmark annotation from very limited labelled data (i.e. a single annotated image), which will be of interest to the MICCAI community. However, currently the results are not competitive to fully-supervised state-of-the-art methods. While it would definitely be beneficial to be able to learn from very few training samples, for clinical applications, performance would be the criterion of choice to decide on a method to be used. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The paper proposes a novel method for one-shot anatomical landmarks detection. The idea of the paper is to 1) create the pseudo-labels via template matching, where the matching is done in the feature space of a self-supervised model 2) training an end-to-end model for landmark detection based on the pseudolabels. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Novel method and excellent idea Clean approach Ablation study presented Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The paper lacks proper statistic reporting. The authors need to re-run the experiments multiple times, and report the standard errors over the runs. Evaluations in other domains could also strengthen the paper Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The method seems to be easily reproducible, but I cannot really guarantee that the results in the tables are easily replicable. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Dear authors. Please look at the weaknesses section and address the preporting issues in the camera ready. Otherwise, good work! Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? New general method, clean approach, and good empirical results. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper made a preliminary attempt to learn landmark detection from only one labeled image. A coarse-to-fine self-supervised learning strategy is proposed to train the feature extractor. Experiment results show that the proposed method achieves a competitive detection accuracy compared to fully-supervised methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The motivation of using very few labels to train a landmark detection network is good. The idea of using self-supervised learning to train the feature extractor is interesting. The manuscript is well-written. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Not compared with other few-shot landmark detection methods, such as [1]. The usefulness of the first stage CC2D-SSL is not validated. It can be validated by comparing with other one-shot methods, or unsupervised methods like kNN, or even the performance before training. It is important for one-shot methods to be able to generalize to the few-shot setting, but no few-shot experiment result is provided. It is unexpected that fine-tuning with pseudo labels could significantly improve the performance (from 68.38% to 81.01% in terms of 4mm SDR), but the reason is not clearly explained. [1] B. Browatzki, and C. Wallraven. “3FabRec: Fast Few-shot Face alignment by Reconstruction.” CVPR 2020. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Need the original code to reproduce the result. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please compare with at least 1 one-shot method and 1 unsupervised method. Please provide results of using 5 and 10 labels in training. Please provide more insightful analysis on the improvement of Stage 2, if possible. There are some typos of “1th” and “2th” in the manuscript. In stage 1, simply using randomly rotation and color jittering to simulate unseen test images is not realistic enough, which may limit the generalization ability of the feature extractor. The information of unlabeled images may help if it can be naturally incorporated into stage 1. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper makes an early attempt on few-shot landmark detection. Although there are some flaws in the experiment, it can be an important baseline in this field. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 3 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The work presents a one shot learning approach for medical landmark detection, evaluated on a cephalometric data set. One shot learning is a relevant idea in medical image analysis, since it allows costly and tedious expert annotation effort to be largely reduced. This work presents a step towards this goal. Reviewers agree on the relevance of the work and are all in favour of acceptance. Some weaknesses are mentioned, including mentioning the latest results on the benchmark dataset, and discussing the clinically relevant results of 2mm detection accuracy, where there still is quite a gap to fully supervised methods. In case of acceptance, the final paper should be revised to address at least these issues, and if possible other issues raised by the reviewers. Overall, very good work that is of interest to the MICCAI community. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback Thanks to all of the three reviewers for providing such valueble comments ! We will address the concerns in the following: Q1: While the results are impressive for just using a single annotated image, they are not comparable to the state-of-the-art. A1: Yes, we will correct this in the final version. Moreover, we have solved a bug in the training code and further improved the performance. Q2:The training/testing set-up is not clear. The authors state that they use a 150/250 image split (as in the related literature). However, from the wording it is not clear whether these are the very same images within the train/testing sets as previously reported, or whether the sets just contain the same numbers of images. It is also not mentioned how the single annotated image was chosen. A2: We chose 125# image in the training set. We use the same images provided by the official website. Also, we will report the mean and std performance when choosing 10 template images randomly in the footnote. Q3: The paper lacks proper statistic reporting. The authors need to re-run the experiments multiple times, and report the standard errors over the runs. A3: We will repeat our experiments and report std in the final version. Q4: Not compared with other few-shot landmark detection methods. The usefulness of the first stage CC2D-SSL is not validated. A4: We will compare our methods with those methods in the future work. Q5: It is unexpected that fine-tuning with pseudo labels could significantly improve the performance (from 68.38% to 81.01% in terms of 4mm SDR), but the reason is not clearly explained. A5: As shown in the first paragraph in page 3: “On theother hand, recent findings show that training an over-parameterized network from scratch tends to learn noiseless information firstly. In our case, as wecannot predict every training point as accurate as ground truth in the SSL stage,a newly trained landmark detector can improve the performance by capturingthe regular information hidden from the noisy labels produced by the CC2D-SSLstage” Q6: It is important for one-shot methods to be able to generalize to the few-shot setting, but no few-shot experiment result is provided. A6: We will support few-shot setting in the future work. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Qingsong Yao, Quan Quan, Li Xiao, S. Kevin Zhou Abstract The success of deep learning methods relies on the availability of a large number of datasets with annotations; however, curating such datasets is burdensome, especially for medical images. To relieve such a burden for a landmark detection task, we explore the feasibility of using only a single annotated image and propose a novel framework named Cascade Comparing to Detect (CC2D) for one-shot landmark detection. CC2D consists of two stages: 1) Self-supervised learning (CC2D-SSL) and 2) Training with pseudo-labels (CC2D-TPL). CC2D-SSL captures the consistent anatomical information in a coarse-to-fine fashion by comparing the cascade feature representations and generates predictions on the training set. CC2D-TPL further improves the performance by training a new landmark detector with those predictions. The effectiveness of CC2D is evaluated on a widely-used public dataset of cephalometric landmark detection, which achieves a competitive detection accuracy of 81.01% within 4.0mm, comparable to the state-of-the-art fully-supervised methods using a lot more than one training image. Link to paper https://doi.org/10.1007/978-3-030-87196-3_17 Link to the code repository https://github.com/ICT-MIRACLE-lab/Oneshot_landmark_detection Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper presents a deep learning framework for landmark detection, CC2D, which learns from a single annotated image and a number of unannotated images. The framework first uses a self-supervised learning step to learn to approximate the landmark positions for all training images and then trains a CNN-based landmark detector using the approximated landmark positions. CC2D was evaluated on a public cephalometric dataset and achieves good performance. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. CC2D is a novel framework to automatically generate landmark annotations using a single annotated training image. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors report the performance to be “comparable to the state-of-the-art fully-supervised methods”. However, looking at the results in Table 1 this does not seem to be the case. The best performing 4mm SDR is 89.85% compared to 81.01% for the proposed method. Looking at the clinically relevant 2mm SDR the best performing method achieves 73.33% compared to 49.81% for the proposed method. While the results are impressive for just using a single annotated image, they are not comparable to the state-of-the-art. The training/testing set-up is not clear. The authors state that they use a 150/250 image split (as in the related literature). However, from the wording it is not clear whether these are the very same images within the train/testing sets as previously reported, or whether the sets just contain the same numbers of images. It is also not mentioned how the single annotated image was chosen. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The dataset is publicly available. Parameter details are specified but no link to code is provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I would recommend for the authors to rephrase the statement about achieving comparable results to fully-supervised state-of-the-art methods. “On one hand” should be “On the one hand”. In Figure 2 it says “1th” instead of “1st” on one occasion. [6] is mentioned as having “satisfactory performance”. However, looking at the paper suggests that [6] achieves state-of-the-art performance. “testset” should be “test set”. [10] and [14] do not list performance values as per Table 1 - are these the wrong references? Should “most of the landmarks in Fig. 2(b)” be “most of the landmarks in Fig. 4(b)”? Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This work presents an interesting first step towards automatic landmark annotation from very limited labelled data (i.e. a single annotated image), which will be of interest to the MICCAI community. However, currently the results are not competitive to fully-supervised state-of-the-art methods. While it would definitely be beneficial to be able to learn from very few training samples, for clinical applications, performance would be the criterion of choice to decide on a method to be used. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The paper proposes a novel method for one-shot anatomical landmarks detection. The idea of the paper is to 1) create the pseudo-labels via template matching, where the matching is done in the feature space of a self-supervised model 2) training an end-to-end model for landmark detection based on the pseudolabels. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Novel method and excellent idea Clean approach Ablation study presented Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The paper lacks proper statistic reporting. The authors need to re-run the experiments multiple times, and report the standard errors over the runs. Evaluations in other domains could also strengthen the paper Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The method seems to be easily reproducible, but I cannot really guarantee that the results in the tables are easily replicable. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Dear authors. Please look at the weaknesses section and address the preporting issues in the camera ready. Otherwise, good work! Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? New general method, clean approach, and good empirical results. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper made a preliminary attempt to learn landmark detection from only one labeled image. A coarse-to-fine self-supervised learning strategy is proposed to train the feature extractor. Experiment results show that the proposed method achieves a competitive detection accuracy compared to fully-supervised methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The motivation of using very few labels to train a landmark detection network is good. The idea of using self-supervised learning to train the feature extractor is interesting. The manuscript is well-written. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Not compared with other few-shot landmark detection methods, such as [1]. The usefulness of the first stage CC2D-SSL is not validated. It can be validated by comparing with other one-shot methods, or unsupervised methods like kNN, or even the performance before training. It is important for one-shot methods to be able to generalize to the few-shot setting, but no few-shot experiment result is provided. It is unexpected that fine-tuning with pseudo labels could significantly improve the performance (from 68.38% to 81.01% in terms of 4mm SDR), but the reason is not clearly explained. [1] B. Browatzki, and C. Wallraven. “3FabRec: Fast Few-shot Face alignment by Reconstruction.” CVPR 2020. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Need the original code to reproduce the result. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please compare with at least 1 one-shot method and 1 unsupervised method. Please provide results of using 5 and 10 labels in training. Please provide more insightful analysis on the improvement of Stage 2, if possible. There are some typos of “1th” and “2th” in the manuscript. In stage 1, simply using randomly rotation and color jittering to simulate unseen test images is not realistic enough, which may limit the generalization ability of the feature extractor. The information of unlabeled images may help if it can be naturally incorporated into stage 1. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper makes an early attempt on few-shot landmark detection. Although there are some flaws in the experiment, it can be an important baseline in this field. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 3 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The work presents a one shot learning approach for medical landmark detection, evaluated on a cephalometric data set. One shot learning is a relevant idea in medical image analysis, since it allows costly and tedious expert annotation effort to be largely reduced. This work presents a step towards this goal. Reviewers agree on the relevance of the work and are all in favour of acceptance. Some weaknesses are mentioned, including mentioning the latest results on the benchmark dataset, and discussing the clinically relevant results of 2mm detection accuracy, where there still is quite a gap to fully supervised methods. In case of acceptance, the final paper should be revised to address at least these issues, and if possible other issues raised by the reviewers. Overall, very good work that is of interest to the MICCAI community. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback Thanks to all of the three reviewers for providing such valueble comments ! We will address the concerns in the following: Q1: While the results are impressive for just using a single annotated image, they are not comparable to the state-of-the-art. A1: Yes, we will correct this in the final version. Moreover, we have solved a bug in the training code and further improved the performance. Q2:The training/testing set-up is not clear. The authors state that they use a 150/250 image split (as in the related literature). However, from the wording it is not clear whether these are the very same images within the train/testing sets as previously reported, or whether the sets just contain the same numbers of images. It is also not mentioned how the single annotated image was chosen. A2: We chose 125# image in the training set. We use the same images provided by the official website. Also, we will report the mean and std performance when choosing 10 template images randomly in the footnote. Q3: The paper lacks proper statistic reporting. The authors need to re-run the experiments multiple times, and report the standard errors over the runs. A3: We will repeat our experiments and report std in the final version. Q4: Not compared with other few-shot landmark detection methods. The usefulness of the first stage CC2D-SSL is not validated. A4: We will compare our methods with those methods in the future work. Q5: It is unexpected that fine-tuning with pseudo labels could significantly improve the performance (from 68.38% to 81.01% in terms of 4mm SDR), but the reason is not clearly explained. A5: As shown in the first paragraph in page 3: “On theother hand, recent findings show that training an over-parameterized network from scratch tends to learn noiseless information firstly. In our case, as wecannot predict every training point as accurate as ground truth in the SSL stage,a newly trained landmark detector can improve the performance by capturingthe regular information hidden from the noisy labels produced by the CC2D-SSLstage” Q6: It is important for one-shot methods to be able to generalize to the few-shot setting, but no few-shot experiment result is provided. A6: We will support few-shot setting in the future work. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0216/12/31/Paper1110" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0216/12/31/Paper1110" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0216-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="One-Shot Medical Landmark Detection" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0216/12/31/Paper1110"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0216/12/31/Paper1110","headline":"One-Shot Medical Landmark Detection","dateModified":"0217-01-01T00:00:00-05:17","datePublished":"0216-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Qingsong Yao, Quan Quan, Li Xiao, S. Kevin Zhou Abstract The success of deep learning methods relies on the availability of a large number of datasets with annotations; however, curating such datasets is burdensome, especially for medical images. To relieve such a burden for a landmark detection task, we explore the feasibility of using only a single annotated image and propose a novel framework named Cascade Comparing to Detect (CC2D) for one-shot landmark detection. CC2D consists of two stages: 1) Self-supervised learning (CC2D-SSL) and 2) Training with pseudo-labels (CC2D-TPL). CC2D-SSL captures the consistent anatomical information in a coarse-to-fine fashion by comparing the cascade feature representations and generates predictions on the training set. CC2D-TPL further improves the performance by training a new landmark detector with those predictions. The effectiveness of CC2D is evaluated on a widely-used public dataset of cephalometric landmark detection, which achieves a competitive detection accuracy of 81.01% within 4.0mm, comparable to the state-of-the-art fully-supervised methods using a lot more than one training image. Link to paper https://doi.org/10.1007/978-3-030-87196-3_17 Link to the code repository https://github.com/ICT-MIRACLE-lab/Oneshot_landmark_detection Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper presents a deep learning framework for landmark detection, CC2D, which learns from a single annotated image and a number of unannotated images. The framework first uses a self-supervised learning step to learn to approximate the landmark positions for all training images and then trains a CNN-based landmark detector using the approximated landmark positions. CC2D was evaluated on a public cephalometric dataset and achieves good performance. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. CC2D is a novel framework to automatically generate landmark annotations using a single annotated training image. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors report the performance to be “comparable to the state-of-the-art fully-supervised methods”. However, looking at the results in Table 1 this does not seem to be the case. The best performing 4mm SDR is 89.85% compared to 81.01% for the proposed method. Looking at the clinically relevant 2mm SDR the best performing method achieves 73.33% compared to 49.81% for the proposed method. While the results are impressive for just using a single annotated image, they are not comparable to the state-of-the-art. The training/testing set-up is not clear. The authors state that they use a 150/250 image split (as in the related literature). However, from the wording it is not clear whether these are the very same images within the train/testing sets as previously reported, or whether the sets just contain the same numbers of images. It is also not mentioned how the single annotated image was chosen. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The dataset is publicly available. Parameter details are specified but no link to code is provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I would recommend for the authors to rephrase the statement about achieving comparable results to fully-supervised state-of-the-art methods. “On one hand” should be “On the one hand”. In Figure 2 it says “1th” instead of “1st” on one occasion. [6] is mentioned as having “satisfactory performance”. However, looking at the paper suggests that [6] achieves state-of-the-art performance. “testset” should be “test set”. [10] and [14] do not list performance values as per Table 1 - are these the wrong references? Should “most of the landmarks in Fig. 2(b)” be “most of the landmarks in Fig. 4(b)”? Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This work presents an interesting first step towards automatic landmark annotation from very limited labelled data (i.e. a single annotated image), which will be of interest to the MICCAI community. However, currently the results are not competitive to fully-supervised state-of-the-art methods. While it would definitely be beneficial to be able to learn from very few training samples, for clinical applications, performance would be the criterion of choice to decide on a method to be used. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The paper proposes a novel method for one-shot anatomical landmarks detection. The idea of the paper is to 1) create the pseudo-labels via template matching, where the matching is done in the feature space of a self-supervised model 2) training an end-to-end model for landmark detection based on the pseudolabels. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Novel method and excellent idea Clean approach Ablation study presented Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The paper lacks proper statistic reporting. The authors need to re-run the experiments multiple times, and report the standard errors over the runs. Evaluations in other domains could also strengthen the paper Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The method seems to be easily reproducible, but I cannot really guarantee that the results in the tables are easily replicable. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Dear authors. Please look at the weaknesses section and address the preporting issues in the camera ready. Otherwise, good work! Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? New general method, clean approach, and good empirical results. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper made a preliminary attempt to learn landmark detection from only one labeled image. A coarse-to-fine self-supervised learning strategy is proposed to train the feature extractor. Experiment results show that the proposed method achieves a competitive detection accuracy compared to fully-supervised methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The motivation of using very few labels to train a landmark detection network is good. The idea of using self-supervised learning to train the feature extractor is interesting. The manuscript is well-written. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Not compared with other few-shot landmark detection methods, such as [1]. The usefulness of the first stage CC2D-SSL is not validated. It can be validated by comparing with other one-shot methods, or unsupervised methods like kNN, or even the performance before training. It is important for one-shot methods to be able to generalize to the few-shot setting, but no few-shot experiment result is provided. It is unexpected that fine-tuning with pseudo labels could significantly improve the performance (from 68.38% to 81.01% in terms of 4mm SDR), but the reason is not clearly explained. [1] B. Browatzki, and C. Wallraven. “3FabRec: Fast Few-shot Face alignment by Reconstruction.” CVPR 2020. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Need the original code to reproduce the result. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please compare with at least 1 one-shot method and 1 unsupervised method. Please provide results of using 5 and 10 labels in training. Please provide more insightful analysis on the improvement of Stage 2, if possible. There are some typos of “1th” and “2th” in the manuscript. In stage 1, simply using randomly rotation and color jittering to simulate unseen test images is not realistic enough, which may limit the generalization ability of the feature extractor. The information of unlabeled images may help if it can be naturally incorporated into stage 1. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper makes an early attempt on few-shot landmark detection. Although there are some flaws in the experiment, it can be an important baseline in this field. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 3 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The work presents a one shot learning approach for medical landmark detection, evaluated on a cephalometric data set. One shot learning is a relevant idea in medical image analysis, since it allows costly and tedious expert annotation effort to be largely reduced. This work presents a step towards this goal. Reviewers agree on the relevance of the work and are all in favour of acceptance. Some weaknesses are mentioned, including mentioning the latest results on the benchmark dataset, and discussing the clinically relevant results of 2mm detection accuracy, where there still is quite a gap to fully supervised methods. In case of acceptance, the final paper should be revised to address at least these issues, and if possible other issues raised by the reviewers. Overall, very good work that is of interest to the MICCAI community. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback Thanks to all of the three reviewers for providing such valueble comments ! We will address the concerns in the following: Q1: While the results are impressive for just using a single annotated image, they are not comparable to the state-of-the-art. A1: Yes, we will correct this in the final version. Moreover, we have solved a bug in the training code and further improved the performance. Q2:The training/testing set-up is not clear. The authors state that they use a 150/250 image split (as in the related literature). However, from the wording it is not clear whether these are the very same images within the train/testing sets as previously reported, or whether the sets just contain the same numbers of images. It is also not mentioned how the single annotated image was chosen. A2: We chose 125# image in the training set. We use the same images provided by the official website. Also, we will report the mean and std performance when choosing 10 template images randomly in the footnote. Q3: The paper lacks proper statistic reporting. The authors need to re-run the experiments multiple times, and report the standard errors over the runs. A3: We will repeat our experiments and report std in the final version. Q4: Not compared with other few-shot landmark detection methods. The usefulness of the first stage CC2D-SSL is not validated. A4: We will compare our methods with those methods in the future work. Q5: It is unexpected that fine-tuning with pseudo labels could significantly improve the performance (from 68.38% to 81.01% in terms of 4mm SDR), but the reason is not clearly explained. A5: As shown in the first paragraph in page 3: “On theother hand, recent findings show that training an over-parameterized network from scratch tends to learn noiseless information firstly. In our case, as wecannot predict every training point as accurate as ground truth in the SSL stage,a newly trained landmark detector can improve the performance by capturingthe regular information hidden from the noisy labels produced by the CC2D-SSLstage” Q6: It is important for one-shot methods to be able to generalize to the few-shot setting, but no few-shot experiment result is provided. A6: We will support few-shot setting in the future work. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Yao, Qingsong,Quan, Quan,Xiao, Li,Zhou, S. Kevin" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>One-Shot Medical Landmark Detection</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Modalities - other"
        class="post-category">
        Modalities - other
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Semi-supervised learning"
        class="post-category">
        Machine Learning - Semi-supervised learning
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Yao, Qingsong"
        class="post-tags">
        Yao, Qingsong
      </a> |  
      
      <a href="kittywong/tags#Quan, Quan"
        class="post-tags">
        Quan, Quan
      </a> |  
      
      <a href="kittywong/tags#Xiao, Li"
        class="post-tags">
        Xiao, Li
      </a> |  
      
      <a href="kittywong/tags#Zhou, S. Kevin"
        class="post-tags">
        Zhou, S. Kevin
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Qingsong Yao, Quan Quan, Li Xiao, S. Kevin Zhou
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>The success of deep learning methods relies on the availability of a large number of datasets with annotations; however, curating such datasets is burdensome, especially for medical images. To relieve such a burden for a landmark detection task, we explore the feasibility of using only a single annotated image and propose a novel framework named Cascade Comparing to Detect (CC2D) for one-shot landmark detection. CC2D consists of two stages: 1)  Self-supervised learning (CC2D-SSL) and 2) Training with pseudo-labels (CC2D-TPL). CC2D-SSL captures the consistent anatomical information in a coarse-to-fine fashion by comparing the cascade feature representations and generates predictions on the training set. CC2D-TPL further improves the performance by training a new landmark detector with those predictions. The effectiveness of CC2D is evaluated on a  widely-used public dataset of cephalometric landmark detection, which achieves a competitive detection accuracy of 81.01% within 4.0mm, comparable to the state-of-the-art fully-supervised methods using a lot more than one training image.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87196-3_17">https://doi.org/10.1007/978-3-030-87196-3_17</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/ICT-MIRACLE-lab/Oneshot_landmark_detection
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper presents a deep learning framework for landmark detection, CC2D, which learns from a single annotated image and a number of unannotated images. The framework first uses a self-supervised learning step to learn to approximate the landmark positions for all training images and then trains a CNN-based landmark detector using the approximated landmark positions. CC2D was evaluated on a public cephalometric dataset and achieves good performance.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>CC2D is a novel framework to automatically generate landmark annotations using a single annotated training image.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>The authors report the performance to be “comparable to the state-of-the-art fully-supervised methods”. However, looking at the results in Table 1 this does not seem to be the case. The best performing 4mm SDR is 89.85% compared to 81.01% for the proposed method. Looking at the clinically relevant 2mm SDR the best performing method achieves 73.33% compared to 49.81% for the proposed method. While the results are impressive for just using a single annotated image, they are not comparable to the state-of-the-art.</li>
        <li>The training/testing set-up is not clear. The authors state that they use a 150/250 image split (as in the related literature). However, from the wording it is not clear whether these are the very same images within the train/testing sets as previously reported, or whether the sets just contain the same numbers of images. It is also not mentioned how the single annotated image was chosen.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <ul>
        <li>The dataset is publicly available.</li>
        <li>Parameter details are specified but no link to code is provided.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ul>
        <li>I would recommend for the authors to rephrase the statement about achieving comparable results to fully-supervised state-of-the-art methods.</li>
        <li>“On one hand” should be “On the one hand”.</li>
        <li>In Figure 2 it says “1th” instead of “1st” on one occasion.</li>
        <li>[6] is mentioned as having “satisfactory performance”. However, looking at the paper suggests that [6] achieves state-of-the-art performance.</li>
        <li>“testset” should be “test set”.</li>
        <li>[10] and [14] do not list performance values as per Table 1 - are these the wrong references?</li>
        <li>Should “most of the landmarks in Fig. 2(b)” be “most of the landmarks in Fig. 4(b)”?</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>This work presents an interesting first step towards automatic landmark annotation from very limited labelled data (i.e. a single annotated image), which will be of interest to the MICCAI community. However, currently the results are not competitive to fully-supervised state-of-the-art methods. While it would definitely be beneficial to be able to learn from very few training samples, for clinical applications, performance would be the criterion of choice to decide on a method to be used.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper proposes a novel method for one-shot anatomical landmarks detection. The idea of the paper is to 1) create the pseudo-labels via template matching, where the matching is done in the feature space of a self-supervised model 2) training an end-to-end model for landmark detection based on the pseudolabels.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>Novel method and excellent idea</li>
        <li>Clean approach</li>
        <li>Ablation study presented</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>The paper lacks proper statistic reporting. The authors need to re-run the experiments multiple times, and report the standard errors over the runs.</li>
        <li>Evaluations in other domains could also strengthen the paper</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The method seems to be easily reproducible, but I cannot really guarantee that the results in the tables are easily replicable.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Dear authors. Please look at the weaknesses section and address the preporting issues in the camera ready. Otherwise, good work!</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>New general method, clean approach, and good empirical results.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper made a preliminary attempt to learn landmark detection from only one labeled image. A coarse-to-fine self-supervised learning strategy is proposed to train the feature extractor. Experiment results show that the proposed method achieves a competitive detection accuracy compared to fully-supervised methods.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>The motivation of using very few labels to train a landmark detection network is good.</li>
        <li>The idea of using self-supervised learning to train the feature extractor is interesting.</li>
        <li>The manuscript is well-written.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>Not compared with other few-shot landmark detection methods, such as [1].</li>
        <li>The usefulness of the first stage CC2D-SSL is not validated. It can be validated by comparing with other one-shot methods, or unsupervised methods like kNN, or even the performance before training.</li>
        <li>It is important for one-shot methods to be able to generalize to the few-shot setting, but no few-shot experiment result is provided.</li>
        <li>It is unexpected that fine-tuning with pseudo labels could significantly improve the performance (from 68.38% to 81.01% in terms of 4mm SDR), but the reason is not clearly explained.
[1] B. Browatzki, and C. Wallraven. “3FabRec: Fast Few-shot Face alignment by Reconstruction.” CVPR 2020.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Need the original code to reproduce the result.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ol>
        <li>Please compare with at least 1 one-shot method and 1 unsupervised method.</li>
        <li>Please provide results of using 5 and 10 labels in training.</li>
        <li>Please provide more insightful analysis on the improvement of Stage 2, if possible.</li>
        <li>There are some typos of “1th” and “2th” in the manuscript.</li>
        <li>In stage 1, simply using randomly rotation and color jittering to simulate unseen test images is not realistic enough, which may limit the generalization ability of the feature extractor. The information of unlabeled images may help if it can be naturally incorporated into stage 1.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper makes an early attempt on few-shot landmark detection. Although there are some flaws in the experiment, it can be an important baseline in this field.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>The work presents a one shot learning approach for medical landmark detection, evaluated on a cephalometric data set. One shot learning is a relevant idea in medical image analysis, since it allows costly and tedious expert annotation effort to be largely reduced. This work presents a step towards this goal. Reviewers agree on the relevance of the work and are all in favour of acceptance. Some weaknesses are mentioned, including mentioning the latest results on the benchmark dataset, and discussing the clinically relevant results of 2mm detection accuracy, where there still is quite a gap to fully supervised methods. In case of acceptance, the final paper should be revised to address at least these issues, and if possible other issues raised by the reviewers. Overall, very good work that is of interest to the MICCAI community.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>Thanks to all of the three reviewers for providing such valueble comments ! We will address the concerns in the following:</p>

  <p>Q1: While the results are impressive for just using a single annotated image, they are not comparable to the state-of-the-art.
A1: Yes, we will correct this in the final version. Moreover, we have solved a bug in the training code and further improved the performance.</p>

  <p>Q2:The training/testing set-up is not clear. The authors state that they use a 150/250 image split (as in the related literature). However, from the wording it is not clear whether these are the very same images within the train/testing sets as previously reported, or whether the sets just contain the same numbers of images. It is also not mentioned how the single annotated image was chosen.
A2: We chose 125# image in the training set. We use the same images provided by the official website. Also, we will report the mean and std performance when choosing 10 template images randomly in the footnote.</p>

  <p>Q3: The paper lacks proper statistic reporting. The authors need to re-run the experiments multiple times, and report the standard errors over the runs.
A3: We will repeat our experiments and report std in the final version.</p>

  <p>Q4: Not compared with other few-shot landmark detection methods. The usefulness of the first stage CC2D-SSL is not validated. 
A4: We will compare our methods with those methods in the future work.</p>

  <p>Q5: It is unexpected that fine-tuning with pseudo labels could significantly improve the performance (from 68.38% to 81.01% in terms of 4mm SDR), but the reason is not clearly explained.
A5: As shown in the first paragraph in page 3: “On theother hand, recent findings show that training an over-parameterized network from scratch tends to learn noiseless information firstly. In our case, as wecannot predict every training point as accurate as ground truth in the SSL stage,a newly trained landmark detector can improve the performance by capturingthe regular information hidden from the noisy labels produced by the CC2D-SSLstage”</p>

  <p>Q6: It is important for one-shot methods to be able to generalize to the few-shot setting, but no few-shot experiment result is provided.
A6: We will support few-shot setting in the future work.</p>

</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0216-12-31
      -->
      <!--
      
        ,
        updated at 
        0217-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Modalities - other"
        class="post-category">
        Modalities - other
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Semi-supervised learning"
        class="post-category">
        Machine Learning - Semi-supervised learning
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Yao, Qingsong"
        class="post-category">
        Yao, Qingsong
      </a> |  
      
      <a href="kittywong/tags#Quan, Quan"
        class="post-category">
        Quan, Quan
      </a> |  
      
      <a href="kittywong/tags#Xiao, Li"
        class="post-category">
        Xiao, Li
      </a> |  
      
      <a href="kittywong/tags#Zhou, S. Kevin"
        class="post-category">
        Zhou, S. Kevin
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0217/12/31/Paper1188">
          Implicit field learning for unsupervised anomaly detection in medical images
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0215/12/31/Paper1088">
          Topological Learning and Its Application to Multimodal Brain Network Integration
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
