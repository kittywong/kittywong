<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Bo Liu, Li-Ming Zhan, Xiao-Ming Wu Abstract One of the primary challenges facing medical visual question answering (Med-VQA) is the lack of large-scale well-annotated datasets for training. To overcome this challenge, this paper proposes a two-stage pre-training framework by learning transferable feature representations of radiology images and distilling a lightweight visual feature extractor for Med-VQA. Specifically, we leverage large amounts of unlabeled radiology images to train three teacher models for the body regions of brain, chest, and abdomen respectively via contrastive learning. Then, we distill the teacher models to a lightweight student model that can be used as a universal visual feature extractor for any Med-VQA system. The lightweight feature extractor can be readily fine-tuned on the training radiology images of any Med-VQA dataset, saving the annotation effort while preventing overfitting to small-scale training data. The effectiveness and advantages of the pre-trained model are demonstrated by extensive experiments with state-of-the-art Med-VQA methods on existing benchmarks. The source code and the pre-training dataset can be downloaded from https://github.com/awenbocc/cprd. Link to paper https://doi.org/10.1007/978-3-030-87196-3_20 Link to the code repository https://github.com/awenbocc/cprd Link to the dataset(s) https://github.com/awenbocc/cprd Reviews Review #1 Please describe the contribution of the paper -The authors proposed a novel pre-training method which uses easily-acquired un-annotated radiology images to pre-train a lightweight visual feature extractor from multiple feature extractors that were trained on the larger dataset. -To retain the structural information from the larger model the KLD loss is combined with Costrastive loss. -This lightweight pre-trained model can adapt to smaller dataset to perform VQA task without over-fitting. This claim is justified by performing experiments on Med-VQA and VQA-RAD datasets. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. the paper is very clearly written. there are comparisons with the state of the art work the strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. What are the datasets used while training the teacher model? are they publicly available? How is the preprocessing of images done while training the teacher visual models? Are there any transformations done during data augmentation for contrast, brightness and saturation with normalization so that the input is uniform across multiple datasets? How are the values of the temperature parameter (T) and number of images (K) decided while training the student model? Are there any ablation studies for deciding these parameters? Why is LSTM being used as a text encoder and why are the latest encoders like BERT/ClinicalBert not considered? In the tables 1 and 2. for obtaining the results of Open Ended (Open) column, where the output text is a form of free flowing text, what was the structure of the network used? Equation 7. looks like it is used for predicting “Close Ended” (Close) questions, what is the loss function for predicting the answers for “Open Ended” (Open) questions? Is it still Equation 7? If yes how? For Open ended question, is Accuracy a correct measure of evaluation? Are there other scores like BLUE, ROUGE, CIDEr, etc which could be calculated? From figure 1c, what is the definition/equation of Lvqa? Is it Equation 7? In my opinion, to correctly evaluate the efficacy of the representations from the student model one more experiment like Image to Text/Text to Image retrieval involving both text and image representations should have been performed and the recall results reported. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance the pre-train datasets used for teacher training are not mentioned. lack to clarity for modelling the loss function for free flowing text apart from the above two points, the paper is mostly reproducible Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html If possible please provide the datasets used for teacher training. report the results using state of the art text encoders mention the augmentation and normalization techniques as it will help reproducibility other multimodal experiments than classification needed to prove the efficiency Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? the approach is novel classification accuracy is much better than related work accuracy is not sufficient as in the Close Ended answers the data bias is not mentioned, need more metrics like precision, recall and more tasks like image to text, text to image retrieval. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper addresses a medical visual question answering (Med-VQA) task using self-supervised learning coupled with a knowledge distillation framework. The proposed method was evaluated using 2 public datasets. The results show that the proposed method had higher accuracies compared to other existing self-supervised learning methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of incorporating contrastive learning into a knowledge distillation framework was interesting. The collection of different medical imaging modalities including X-ray, computed tomography (CT) and magnetic resonance imaging (MRI) was good. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors’ idea of collecting only 3 human body regions: brain, chest and abdomen was not clearly justified. I agree that many existing studies focus on these 3 regions but this does not mean that the proposed method will be beneficial for all radiology images. The authors collected different types of medical images including x-ray, CT and MRI. However, these modalities have their unique characteristics that can affect the learning process or subsequent Med-VQA task. Did the authors consider this? Some of the Method/experiment setup descriptions are not clear or are missing important details. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Authors provided most of implementation details of their proposed method. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html 1. The authors did not properly define the acronyms throughout the paper. Acronyms need to be defined when they are mentioned for the first time. For example, what is MEVF? 2. The technical motivation of incorporating contrastive learning into a knowledge distillation framework was not clearly explained. It would be great to include how the proposed method differ from existing knowledge distillation methods. 3. The performance of the proposed method seems to heavily depend on the use of parameter α in equation 6. What value did you use in your experiments and how does use of different parameter affect the results? 4. The description of how the 2 public datasets were split into training, validation and testing sets. 5. The descriptions of other VQA methods such as SAN and BAN are missing. What are they? Why does your proposed method perform better? 6. There are a few typos and grammatical mistakes throughout the paper. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed idea was interesting, but the benefits or justifications of idea were presented poorly. There are some missing descriptions of the proposed method. The were no clear discussions why the proposed method performed better than other state-of-the-art self-supervised learning methods. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper presents a two-stage pre-training framework to tackle the challenge of data scarcity in the Med-VQA domain. More specifically, large-scale unlabeled data is used for pretraining for each domain (brain, chest, and abdomen). The three pretrained networks are then used to distill the final network for feature extraction. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This work is well-motivated. Leveraging large-scale unlabeled dataset is a hot and important topic for medical image analysis This work is well-written. The experiments demonstrate the effectiveness of the proposed method. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Unfair comparison with the existing methods The unlabeled setting is fundamentally flawed Lack of novelty Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance As the authors is going to release the code to the public, the work is reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html While the proposed method used large-scale domain-specific datasets to pretrain the network, the baseline methods use only the Imagenet-pretrained weights. While the visualization in the experiments clearly demonstrate that the network can distinguish different human parts, I am concerned that this is because the authors use the labels to train the network. According to Eq. 5, the authors know the label of each subdataset and used it to distill the final ensemble network. Why not use a pretrained ResNet and finetune on this classification task, and use the representation for VQA? This work is essentially a combination of existing methods, which lacks of novelty. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Unfair comparison with the existing methods The unlabeled setting is fundamentally flawed Lack of novelty What is the ranking of this paper in your review stack? 4 Number of papers in your stack 8 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Summary: A novel pre-training method using easily-acquired un-annotated radiology images to pre-train a lightweight visual feature extractor from multiple feature extractors that were trained on the larger dataset. Structural information is retained from the larger model by combining the KLD loss with a contrastive loss. Experiments on two public datasets show this lightweight pre-trained model can adapt to smaller dataset to perform VQA tasks without over-fitting. Positives: Clearly written, but with some mistakes. Strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel, with an interesting idea of incorporating contrastive learning into a knowledge distillation framework . Comparisons against the state of the art work, using a variety of image modalities, demonstrate the effectiveness of the proposed method. Code to be made available. Negatives: Some important details are missing. These could be added at a rebuttal stage. Might need further experiments, such as assessing on a wider variety of organs - although not for a rebuttal. Unfair comparison because baseline methods only used ImageNet training, whereas the proposed method was pre-trained in a domain-specific way. Try to justify why this should be a fair comparison. May lack novelty because it is a combination of existing methods. Explain whether the combination of existing methods is integrated in a way that is significantly novel. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. I’m satisfied with the rebuttal to R4’s concern about unfair comparisons, given that the authors compared against MEVF, which had been pretrained using medical image data. One aim of the work was to assess whether pre-training on medical imaging data was better than pre-training on ImageNet, hence the other comparisons. In answering concerns about missing details, I would rather the authors had said something about fixing this in the manuscript, rather than simply provide the missing information for the reviewers. R4 considered the work not to be sufficiently novel, but other reviewers commented on the novelty. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This is a novel method. The idea for pretraining is convincing, and the method of using the visual “question answer” in a contrastive setting is a nice application to clinical data. The experimental results are sufficient and convincing. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. After reading the paper, the reviews and the rebutall, the main critiscim of the paper is in the baselines used. Indeed R2 / R3 bring up an important point which strongly limits the claims of the paper and method. While I appreaciate the authors responses to these points in teh rebutall, the it is insufficient to to make the claims formulated. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 16 Author Feedback We thank all the reviewers and clarify some confusion/misunderstanding below. Fair Comparison (R4) Our experiments (Table 1&amp;2) are intended to compare our pretraining and distillation method to MEVF ([17], MICCAI 2019), which is the only baseline that uses a small model and pretrains with medical images, and to other baselines (BAN fw., etc.) that use large models (e.g., ResNet) and pretrain on ImageNet. The results clearly demonstrate the superiority of our method over MEVF in both accuracy and model size, and that other baselines will severely overfit, indicating the necessity of pretraining with domain knowledge and representation distillation, which is the point of our paper. Important details Due to space limitation, we cannot explain every detail in the paper. We try to provide more information below. We ensure that all experiments are reproducible and will release all codes and datasets (collected from public resources e.g., medicaldecathlon.com) after acceptance. [R1] Preprocessing: We use the same transformation group as in MoCoV2 [6]. [R1] Open-ended questions: In current research, both open- and closed-ended questions are formulated as classification tasks with the same loss as in Eq 7. The answers cannot be generated, and they must appear in the training set. [R1] Evaluation metric: We follow current practice to use classification accuracy as evaluation metric, but we agree more reasonable metrics should be devised/used. [R1] BERT as text encoder: Large models severely overfit on the small-scale training data. [R1] Lvqa is the loss in Eq 7. [R1, R2] Parameter analysis: All hyper-parameters are chosen by cross validation based on observing the loss in Eq 2&amp;6. We want to include these studies but couldn’t due to limited space. [R2] Body regions: The observation is that current Med-VQA datasets mainly contain radiology images of the three body regions, and hence our method is beneficial for most of Med-VQA tasks. [R2] Acronyms: Thank you. We will define them properly in the final version. [R2] Data split: We mention in Sec. 5.1 that we follow the settings in previous papers. VQA-RAD: 85% training, 15% test. SLAKE: 70% training, 15% validation, 15% test. [R2] α in Eq (6) is 0.9. [R2] SAN and BAN are two different reasoning modules for VQA. BAN fw. and SAN fw. are two VQA models with the corresponding reasoning modules and ResNet as visual extractor and LSTM as text encoder, and they often overfit on Med-VQA datasets. Novelty (R4) Our contrastive pre-training and representation distillation (CPRD) method is proposed to address two critical issues in Med-VQA: the lack of training images and the diversity of images in terms of different modalities and organs. Existing self-supervised methods cannot be straightforwardly applied to address these problems. Our CPRD is designed based on the statistics of the Med-VQA domain: the datasets mainly consist of radiology images of three body regions. For each region, we collect publicly available unlabeled data and use contrastive learning to learn intra-region prior knowledge. Then, we use contrastive learning again to distill intra-region knowledge and learn inter-region knowledge, which yields a lightweight model. As pointed out by R1 and R2, the strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel and can benefit other medical applications such as medical image-text retrieval and medical image captions. Unlabeled setting (R4) The unlabeled setting is valid. We collect radiology images for each body region (Head, Chest, or Abdomen) from public resources (the images are already grouped by regions). The collected images are unlabeled without any annotations of organs or modalities. Also see Fig 2 (left), where Brain CT and Brain MRI are well separated. This shows even with unlabeled data, the distilled student model can capture the differences between different image modalities. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Bo Liu, Li-Ming Zhan, Xiao-Ming Wu Abstract One of the primary challenges facing medical visual question answering (Med-VQA) is the lack of large-scale well-annotated datasets for training. To overcome this challenge, this paper proposes a two-stage pre-training framework by learning transferable feature representations of radiology images and distilling a lightweight visual feature extractor for Med-VQA. Specifically, we leverage large amounts of unlabeled radiology images to train three teacher models for the body regions of brain, chest, and abdomen respectively via contrastive learning. Then, we distill the teacher models to a lightweight student model that can be used as a universal visual feature extractor for any Med-VQA system. The lightweight feature extractor can be readily fine-tuned on the training radiology images of any Med-VQA dataset, saving the annotation effort while preventing overfitting to small-scale training data. The effectiveness and advantages of the pre-trained model are demonstrated by extensive experiments with state-of-the-art Med-VQA methods on existing benchmarks. The source code and the pre-training dataset can be downloaded from https://github.com/awenbocc/cprd. Link to paper https://doi.org/10.1007/978-3-030-87196-3_20 Link to the code repository https://github.com/awenbocc/cprd Link to the dataset(s) https://github.com/awenbocc/cprd Reviews Review #1 Please describe the contribution of the paper -The authors proposed a novel pre-training method which uses easily-acquired un-annotated radiology images to pre-train a lightweight visual feature extractor from multiple feature extractors that were trained on the larger dataset. -To retain the structural information from the larger model the KLD loss is combined with Costrastive loss. -This lightweight pre-trained model can adapt to smaller dataset to perform VQA task without over-fitting. This claim is justified by performing experiments on Med-VQA and VQA-RAD datasets. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. the paper is very clearly written. there are comparisons with the state of the art work the strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. What are the datasets used while training the teacher model? are they publicly available? How is the preprocessing of images done while training the teacher visual models? Are there any transformations done during data augmentation for contrast, brightness and saturation with normalization so that the input is uniform across multiple datasets? How are the values of the temperature parameter (T) and number of images (K) decided while training the student model? Are there any ablation studies for deciding these parameters? Why is LSTM being used as a text encoder and why are the latest encoders like BERT/ClinicalBert not considered? In the tables 1 and 2. for obtaining the results of Open Ended (Open) column, where the output text is a form of free flowing text, what was the structure of the network used? Equation 7. looks like it is used for predicting “Close Ended” (Close) questions, what is the loss function for predicting the answers for “Open Ended” (Open) questions? Is it still Equation 7? If yes how? For Open ended question, is Accuracy a correct measure of evaluation? Are there other scores like BLUE, ROUGE, CIDEr, etc which could be calculated? From figure 1c, what is the definition/equation of Lvqa? Is it Equation 7? In my opinion, to correctly evaluate the efficacy of the representations from the student model one more experiment like Image to Text/Text to Image retrieval involving both text and image representations should have been performed and the recall results reported. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance the pre-train datasets used for teacher training are not mentioned. lack to clarity for modelling the loss function for free flowing text apart from the above two points, the paper is mostly reproducible Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html If possible please provide the datasets used for teacher training. report the results using state of the art text encoders mention the augmentation and normalization techniques as it will help reproducibility other multimodal experiments than classification needed to prove the efficiency Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? the approach is novel classification accuracy is much better than related work accuracy is not sufficient as in the Close Ended answers the data bias is not mentioned, need more metrics like precision, recall and more tasks like image to text, text to image retrieval. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper addresses a medical visual question answering (Med-VQA) task using self-supervised learning coupled with a knowledge distillation framework. The proposed method was evaluated using 2 public datasets. The results show that the proposed method had higher accuracies compared to other existing self-supervised learning methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of incorporating contrastive learning into a knowledge distillation framework was interesting. The collection of different medical imaging modalities including X-ray, computed tomography (CT) and magnetic resonance imaging (MRI) was good. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors’ idea of collecting only 3 human body regions: brain, chest and abdomen was not clearly justified. I agree that many existing studies focus on these 3 regions but this does not mean that the proposed method will be beneficial for all radiology images. The authors collected different types of medical images including x-ray, CT and MRI. However, these modalities have their unique characteristics that can affect the learning process or subsequent Med-VQA task. Did the authors consider this? Some of the Method/experiment setup descriptions are not clear or are missing important details. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Authors provided most of implementation details of their proposed method. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html 1. The authors did not properly define the acronyms throughout the paper. Acronyms need to be defined when they are mentioned for the first time. For example, what is MEVF? 2. The technical motivation of incorporating contrastive learning into a knowledge distillation framework was not clearly explained. It would be great to include how the proposed method differ from existing knowledge distillation methods. 3. The performance of the proposed method seems to heavily depend on the use of parameter α in equation 6. What value did you use in your experiments and how does use of different parameter affect the results? 4. The description of how the 2 public datasets were split into training, validation and testing sets. 5. The descriptions of other VQA methods such as SAN and BAN are missing. What are they? Why does your proposed method perform better? 6. There are a few typos and grammatical mistakes throughout the paper. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed idea was interesting, but the benefits or justifications of idea were presented poorly. There are some missing descriptions of the proposed method. The were no clear discussions why the proposed method performed better than other state-of-the-art self-supervised learning methods. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper presents a two-stage pre-training framework to tackle the challenge of data scarcity in the Med-VQA domain. More specifically, large-scale unlabeled data is used for pretraining for each domain (brain, chest, and abdomen). The three pretrained networks are then used to distill the final network for feature extraction. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This work is well-motivated. Leveraging large-scale unlabeled dataset is a hot and important topic for medical image analysis This work is well-written. The experiments demonstrate the effectiveness of the proposed method. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Unfair comparison with the existing methods The unlabeled setting is fundamentally flawed Lack of novelty Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance As the authors is going to release the code to the public, the work is reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html While the proposed method used large-scale domain-specific datasets to pretrain the network, the baseline methods use only the Imagenet-pretrained weights. While the visualization in the experiments clearly demonstrate that the network can distinguish different human parts, I am concerned that this is because the authors use the labels to train the network. According to Eq. 5, the authors know the label of each subdataset and used it to distill the final ensemble network. Why not use a pretrained ResNet and finetune on this classification task, and use the representation for VQA? This work is essentially a combination of existing methods, which lacks of novelty. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Unfair comparison with the existing methods The unlabeled setting is fundamentally flawed Lack of novelty What is the ranking of this paper in your review stack? 4 Number of papers in your stack 8 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Summary: A novel pre-training method using easily-acquired un-annotated radiology images to pre-train a lightweight visual feature extractor from multiple feature extractors that were trained on the larger dataset. Structural information is retained from the larger model by combining the KLD loss with a contrastive loss. Experiments on two public datasets show this lightweight pre-trained model can adapt to smaller dataset to perform VQA tasks without over-fitting. Positives: Clearly written, but with some mistakes. Strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel, with an interesting idea of incorporating contrastive learning into a knowledge distillation framework . Comparisons against the state of the art work, using a variety of image modalities, demonstrate the effectiveness of the proposed method. Code to be made available. Negatives: Some important details are missing. These could be added at a rebuttal stage. Might need further experiments, such as assessing on a wider variety of organs - although not for a rebuttal. Unfair comparison because baseline methods only used ImageNet training, whereas the proposed method was pre-trained in a domain-specific way. Try to justify why this should be a fair comparison. May lack novelty because it is a combination of existing methods. Explain whether the combination of existing methods is integrated in a way that is significantly novel. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. I’m satisfied with the rebuttal to R4’s concern about unfair comparisons, given that the authors compared against MEVF, which had been pretrained using medical image data. One aim of the work was to assess whether pre-training on medical imaging data was better than pre-training on ImageNet, hence the other comparisons. In answering concerns about missing details, I would rather the authors had said something about fixing this in the manuscript, rather than simply provide the missing information for the reviewers. R4 considered the work not to be sufficiently novel, but other reviewers commented on the novelty. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This is a novel method. The idea for pretraining is convincing, and the method of using the visual “question answer” in a contrastive setting is a nice application to clinical data. The experimental results are sufficient and convincing. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. After reading the paper, the reviews and the rebutall, the main critiscim of the paper is in the baselines used. Indeed R2 / R3 bring up an important point which strongly limits the claims of the paper and method. While I appreaciate the authors responses to these points in teh rebutall, the it is insufficient to to make the claims formulated. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 16 Author Feedback We thank all the reviewers and clarify some confusion/misunderstanding below. Fair Comparison (R4) Our experiments (Table 1&amp;2) are intended to compare our pretraining and distillation method to MEVF ([17], MICCAI 2019), which is the only baseline that uses a small model and pretrains with medical images, and to other baselines (BAN fw., etc.) that use large models (e.g., ResNet) and pretrain on ImageNet. The results clearly demonstrate the superiority of our method over MEVF in both accuracy and model size, and that other baselines will severely overfit, indicating the necessity of pretraining with domain knowledge and representation distillation, which is the point of our paper. Important details Due to space limitation, we cannot explain every detail in the paper. We try to provide more information below. We ensure that all experiments are reproducible and will release all codes and datasets (collected from public resources e.g., medicaldecathlon.com) after acceptance. [R1] Preprocessing: We use the same transformation group as in MoCoV2 [6]. [R1] Open-ended questions: In current research, both open- and closed-ended questions are formulated as classification tasks with the same loss as in Eq 7. The answers cannot be generated, and they must appear in the training set. [R1] Evaluation metric: We follow current practice to use classification accuracy as evaluation metric, but we agree more reasonable metrics should be devised/used. [R1] BERT as text encoder: Large models severely overfit on the small-scale training data. [R1] Lvqa is the loss in Eq 7. [R1, R2] Parameter analysis: All hyper-parameters are chosen by cross validation based on observing the loss in Eq 2&amp;6. We want to include these studies but couldn’t due to limited space. [R2] Body regions: The observation is that current Med-VQA datasets mainly contain radiology images of the three body regions, and hence our method is beneficial for most of Med-VQA tasks. [R2] Acronyms: Thank you. We will define them properly in the final version. [R2] Data split: We mention in Sec. 5.1 that we follow the settings in previous papers. VQA-RAD: 85% training, 15% test. SLAKE: 70% training, 15% validation, 15% test. [R2] α in Eq (6) is 0.9. [R2] SAN and BAN are two different reasoning modules for VQA. BAN fw. and SAN fw. are two VQA models with the corresponding reasoning modules and ResNet as visual extractor and LSTM as text encoder, and they often overfit on Med-VQA datasets. Novelty (R4) Our contrastive pre-training and representation distillation (CPRD) method is proposed to address two critical issues in Med-VQA: the lack of training images and the diversity of images in terms of different modalities and organs. Existing self-supervised methods cannot be straightforwardly applied to address these problems. Our CPRD is designed based on the statistics of the Med-VQA domain: the datasets mainly consist of radiology images of three body regions. For each region, we collect publicly available unlabeled data and use contrastive learning to learn intra-region prior knowledge. Then, we use contrastive learning again to distill intra-region knowledge and learn inter-region knowledge, which yields a lightweight model. As pointed out by R1 and R2, the strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel and can benefit other medical applications such as medical image-text retrieval and medical image captions. Unlabeled setting (R4) The unlabeled setting is valid. We collect radiology images for each body region (Head, Chest, or Abdomen) from public resources (the images are already grouped by regions). The collected images are unlabeled without any annotations of organs or modalities. Also see Fig 2 (left), where Brain CT and Brain MRI are well separated. This shows even with unlabeled data, the distilled student model can capture the differences between different image modalities. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0219/12/31/Paper1235" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0219/12/31/Paper1235" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0219-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0219/12/31/Paper1235"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0219/12/31/Paper1235","headline":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images","dateModified":"0220-01-01T00:00:00-05:17","datePublished":"0219-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Bo Liu, Li-Ming Zhan, Xiao-Ming Wu Abstract One of the primary challenges facing medical visual question answering (Med-VQA) is the lack of large-scale well-annotated datasets for training. To overcome this challenge, this paper proposes a two-stage pre-training framework by learning transferable feature representations of radiology images and distilling a lightweight visual feature extractor for Med-VQA. Specifically, we leverage large amounts of unlabeled radiology images to train three teacher models for the body regions of brain, chest, and abdomen respectively via contrastive learning. Then, we distill the teacher models to a lightweight student model that can be used as a universal visual feature extractor for any Med-VQA system. The lightweight feature extractor can be readily fine-tuned on the training radiology images of any Med-VQA dataset, saving the annotation effort while preventing overfitting to small-scale training data. The effectiveness and advantages of the pre-trained model are demonstrated by extensive experiments with state-of-the-art Med-VQA methods on existing benchmarks. The source code and the pre-training dataset can be downloaded from https://github.com/awenbocc/cprd. Link to paper https://doi.org/10.1007/978-3-030-87196-3_20 Link to the code repository https://github.com/awenbocc/cprd Link to the dataset(s) https://github.com/awenbocc/cprd Reviews Review #1 Please describe the contribution of the paper -The authors proposed a novel pre-training method which uses easily-acquired un-annotated radiology images to pre-train a lightweight visual feature extractor from multiple feature extractors that were trained on the larger dataset. -To retain the structural information from the larger model the KLD loss is combined with Costrastive loss. -This lightweight pre-trained model can adapt to smaller dataset to perform VQA task without over-fitting. This claim is justified by performing experiments on Med-VQA and VQA-RAD datasets. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. the paper is very clearly written. there are comparisons with the state of the art work the strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. What are the datasets used while training the teacher model? are they publicly available? How is the preprocessing of images done while training the teacher visual models? Are there any transformations done during data augmentation for contrast, brightness and saturation with normalization so that the input is uniform across multiple datasets? How are the values of the temperature parameter (T) and number of images (K) decided while training the student model? Are there any ablation studies for deciding these parameters? Why is LSTM being used as a text encoder and why are the latest encoders like BERT/ClinicalBert not considered? In the tables 1 and 2. for obtaining the results of Open Ended (Open) column, where the output text is a form of free flowing text, what was the structure of the network used? Equation 7. looks like it is used for predicting “Close Ended” (Close) questions, what is the loss function for predicting the answers for “Open Ended” (Open) questions? Is it still Equation 7? If yes how? For Open ended question, is Accuracy a correct measure of evaluation? Are there other scores like BLUE, ROUGE, CIDEr, etc which could be calculated? From figure 1c, what is the definition/equation of Lvqa? Is it Equation 7? In my opinion, to correctly evaluate the efficacy of the representations from the student model one more experiment like Image to Text/Text to Image retrieval involving both text and image representations should have been performed and the recall results reported. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance the pre-train datasets used for teacher training are not mentioned. lack to clarity for modelling the loss function for free flowing text apart from the above two points, the paper is mostly reproducible Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html If possible please provide the datasets used for teacher training. report the results using state of the art text encoders mention the augmentation and normalization techniques as it will help reproducibility other multimodal experiments than classification needed to prove the efficiency Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? the approach is novel classification accuracy is much better than related work accuracy is not sufficient as in the Close Ended answers the data bias is not mentioned, need more metrics like precision, recall and more tasks like image to text, text to image retrieval. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper addresses a medical visual question answering (Med-VQA) task using self-supervised learning coupled with a knowledge distillation framework. The proposed method was evaluated using 2 public datasets. The results show that the proposed method had higher accuracies compared to other existing self-supervised learning methods. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of incorporating contrastive learning into a knowledge distillation framework was interesting. The collection of different medical imaging modalities including X-ray, computed tomography (CT) and magnetic resonance imaging (MRI) was good. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors’ idea of collecting only 3 human body regions: brain, chest and abdomen was not clearly justified. I agree that many existing studies focus on these 3 regions but this does not mean that the proposed method will be beneficial for all radiology images. The authors collected different types of medical images including x-ray, CT and MRI. However, these modalities have their unique characteristics that can affect the learning process or subsequent Med-VQA task. Did the authors consider this? Some of the Method/experiment setup descriptions are not clear or are missing important details. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Authors provided most of implementation details of their proposed method. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html 1. The authors did not properly define the acronyms throughout the paper. Acronyms need to be defined when they are mentioned for the first time. For example, what is MEVF? 2. The technical motivation of incorporating contrastive learning into a knowledge distillation framework was not clearly explained. It would be great to include how the proposed method differ from existing knowledge distillation methods. 3. The performance of the proposed method seems to heavily depend on the use of parameter α in equation 6. What value did you use in your experiments and how does use of different parameter affect the results? 4. The description of how the 2 public datasets were split into training, validation and testing sets. 5. The descriptions of other VQA methods such as SAN and BAN are missing. What are they? Why does your proposed method perform better? 6. There are a few typos and grammatical mistakes throughout the paper. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed idea was interesting, but the benefits or justifications of idea were presented poorly. There are some missing descriptions of the proposed method. The were no clear discussions why the proposed method performed better than other state-of-the-art self-supervised learning methods. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper presents a two-stage pre-training framework to tackle the challenge of data scarcity in the Med-VQA domain. More specifically, large-scale unlabeled data is used for pretraining for each domain (brain, chest, and abdomen). The three pretrained networks are then used to distill the final network for feature extraction. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This work is well-motivated. Leveraging large-scale unlabeled dataset is a hot and important topic for medical image analysis This work is well-written. The experiments demonstrate the effectiveness of the proposed method. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Unfair comparison with the existing methods The unlabeled setting is fundamentally flawed Lack of novelty Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance As the authors is going to release the code to the public, the work is reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html While the proposed method used large-scale domain-specific datasets to pretrain the network, the baseline methods use only the Imagenet-pretrained weights. While the visualization in the experiments clearly demonstrate that the network can distinguish different human parts, I am concerned that this is because the authors use the labels to train the network. According to Eq. 5, the authors know the label of each subdataset and used it to distill the final ensemble network. Why not use a pretrained ResNet and finetune on this classification task, and use the representation for VQA? This work is essentially a combination of existing methods, which lacks of novelty. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Unfair comparison with the existing methods The unlabeled setting is fundamentally flawed Lack of novelty What is the ranking of this paper in your review stack? 4 Number of papers in your stack 8 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Summary: A novel pre-training method using easily-acquired un-annotated radiology images to pre-train a lightweight visual feature extractor from multiple feature extractors that were trained on the larger dataset. Structural information is retained from the larger model by combining the KLD loss with a contrastive loss. Experiments on two public datasets show this lightweight pre-trained model can adapt to smaller dataset to perform VQA tasks without over-fitting. Positives: Clearly written, but with some mistakes. Strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel, with an interesting idea of incorporating contrastive learning into a knowledge distillation framework . Comparisons against the state of the art work, using a variety of image modalities, demonstrate the effectiveness of the proposed method. Code to be made available. Negatives: Some important details are missing. These could be added at a rebuttal stage. Might need further experiments, such as assessing on a wider variety of organs - although not for a rebuttal. Unfair comparison because baseline methods only used ImageNet training, whereas the proposed method was pre-trained in a domain-specific way. Try to justify why this should be a fair comparison. May lack novelty because it is a combination of existing methods. Explain whether the combination of existing methods is integrated in a way that is significantly novel. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. I’m satisfied with the rebuttal to R4’s concern about unfair comparisons, given that the authors compared against MEVF, which had been pretrained using medical image data. One aim of the work was to assess whether pre-training on medical imaging data was better than pre-training on ImageNet, hence the other comparisons. In answering concerns about missing details, I would rather the authors had said something about fixing this in the manuscript, rather than simply provide the missing information for the reviewers. R4 considered the work not to be sufficiently novel, but other reviewers commented on the novelty. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This is a novel method. The idea for pretraining is convincing, and the method of using the visual “question answer” in a contrastive setting is a nice application to clinical data. The experimental results are sufficient and convincing. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. After reading the paper, the reviews and the rebutall, the main critiscim of the paper is in the baselines used. Indeed R2 / R3 bring up an important point which strongly limits the claims of the paper and method. While I appreaciate the authors responses to these points in teh rebutall, the it is insufficient to to make the claims formulated. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 16 Author Feedback We thank all the reviewers and clarify some confusion/misunderstanding below. Fair Comparison (R4) Our experiments (Table 1&amp;2) are intended to compare our pretraining and distillation method to MEVF ([17], MICCAI 2019), which is the only baseline that uses a small model and pretrains with medical images, and to other baselines (BAN fw., etc.) that use large models (e.g., ResNet) and pretrain on ImageNet. The results clearly demonstrate the superiority of our method over MEVF in both accuracy and model size, and that other baselines will severely overfit, indicating the necessity of pretraining with domain knowledge and representation distillation, which is the point of our paper. Important details Due to space limitation, we cannot explain every detail in the paper. We try to provide more information below. We ensure that all experiments are reproducible and will release all codes and datasets (collected from public resources e.g., medicaldecathlon.com) after acceptance. [R1] Preprocessing: We use the same transformation group as in MoCoV2 [6]. [R1] Open-ended questions: In current research, both open- and closed-ended questions are formulated as classification tasks with the same loss as in Eq 7. The answers cannot be generated, and they must appear in the training set. [R1] Evaluation metric: We follow current practice to use classification accuracy as evaluation metric, but we agree more reasonable metrics should be devised/used. [R1] BERT as text encoder: Large models severely overfit on the small-scale training data. [R1] Lvqa is the loss in Eq 7. [R1, R2] Parameter analysis: All hyper-parameters are chosen by cross validation based on observing the loss in Eq 2&amp;6. We want to include these studies but couldn’t due to limited space. [R2] Body regions: The observation is that current Med-VQA datasets mainly contain radiology images of the three body regions, and hence our method is beneficial for most of Med-VQA tasks. [R2] Acronyms: Thank you. We will define them properly in the final version. [R2] Data split: We mention in Sec. 5.1 that we follow the settings in previous papers. VQA-RAD: 85% training, 15% test. SLAKE: 70% training, 15% validation, 15% test. [R2] α in Eq (6) is 0.9. [R2] SAN and BAN are two different reasoning modules for VQA. BAN fw. and SAN fw. are two VQA models with the corresponding reasoning modules and ResNet as visual extractor and LSTM as text encoder, and they often overfit on Med-VQA datasets. Novelty (R4) Our contrastive pre-training and representation distillation (CPRD) method is proposed to address two critical issues in Med-VQA: the lack of training images and the diversity of images in terms of different modalities and organs. Existing self-supervised methods cannot be straightforwardly applied to address these problems. Our CPRD is designed based on the statistics of the Med-VQA domain: the datasets mainly consist of radiology images of three body regions. For each region, we collect publicly available unlabeled data and use contrastive learning to learn intra-region prior knowledge. Then, we use contrastive learning again to distill intra-region knowledge and learn inter-region knowledge, which yields a lightweight model. As pointed out by R1 and R2, the strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel and can benefit other medical applications such as medical image-text retrieval and medical image captions. Unlabeled setting (R4) The unlabeled setting is valid. We collect radiology images for each body region (Head, Chest, or Abdomen) from public resources (the images are already grouped by regions). The collected images are unlabeled without any annotations of organs or modalities. Also see Fig 2 (left), where Brain CT and Brain MRI are well separated. This shows even with unlabeled data, the distilled student model can capture the differences between different image modalities. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Liu, Bo,Zhan, Li-Ming,Wu, Xiao-Ming" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a>
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a>
      
      <a 
        href="kittywong/categories#Modalities - CT"
        class="post-category">
        Modalities - CT
      </a>
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a>
      
      <a 
        href="kittywong/categories#Modalities - Text (clinical/radiology reports)"
        class="post-category">
        Modalities - Text (clinical/radiology reports)
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Liu, Bo"
        class="post-tags">
        Liu, Bo
      </a> |  
      
      <a href="kittywong/tags#Zhan, Li-Ming"
        class="post-tags">
        Zhan, Li-Ming
      </a> |  
      
      <a href="kittywong/tags#Wu, Xiao-Ming"
        class="post-tags">
        Wu, Xiao-Ming
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Bo Liu, Li-Ming Zhan, Xiao-Ming Wu
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>One of the primary challenges facing medical visual question answering (Med-VQA) is the lack of large-scale well-annotated datasets for training. To overcome this challenge, this paper proposes a two-stage pre-training framework by learning transferable feature representations of radiology images and distilling a lightweight visual feature extractor for Med-VQA. Specifically, we leverage large amounts of unlabeled radiology images to train three teacher models for the body regions of brain, chest, and abdomen respectively via contrastive learning. Then, we distill the teacher models to a lightweight student model that can be used as a universal visual feature extractor for any Med-VQA system. The lightweight feature extractor can be readily fine-tuned on the training radiology images of any Med-VQA dataset, saving the annotation effort while preventing overfitting to small-scale training data. The effectiveness and advantages of the pre-trained model are demonstrated by extensive experiments with state-of-the-art Med-VQA methods on existing benchmarks. The source code and the pre-training dataset can be downloaded from https://github.com/awenbocc/cprd.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87196-3_20">https://doi.org/10.1007/978-3-030-87196-3_20</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/awenbocc/cprd
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>https://github.com/awenbocc/cprd
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>-The authors proposed a novel pre-training method which uses easily-acquired un-annotated radiology images to pre-train a lightweight visual feature extractor from multiple feature extractors that were trained on the larger dataset.
-To retain the structural information from the larger model the KLD loss is combined with Costrastive loss.
-This lightweight pre-trained model can adapt to smaller dataset  to perform VQA task without over-fitting. This claim is justified by performing experiments on Med-VQA and VQA-RAD datasets.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>the paper is very clearly written.</li>
        <li>there are comparisons with the state of the art work</li>
        <li>the strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>What are the datasets used while training the teacher model? are they publicly available?</li>
        <li>How is the preprocessing of images done while training the teacher visual models? Are there any transformations done during data augmentation for contrast, brightness and saturation with normalization so that the input is uniform across multiple datasets?</li>
        <li>How are the values of the temperature parameter (T) and number of images (K) decided while training the student model? Are there any ablation studies for deciding these parameters?</li>
        <li>Why is LSTM being used as a text encoder and why are the latest encoders like BERT/ClinicalBert not considered?</li>
        <li>In the tables 1 and 2. for obtaining the results of Open Ended (Open) column, where the output text is a form of free flowing text, what was the structure of the network used?</li>
        <li>Equation 7. looks like it is used for predicting “Close Ended” (Close) questions, what is the loss function for predicting the answers for “Open Ended” (Open) questions? Is it still Equation 7? If yes how?</li>
        <li>For Open ended question, is Accuracy a correct measure of evaluation? Are there other scores like BLUE, ROUGE, CIDEr, etc which could be calculated?</li>
        <li>From figure 1c, what is the definition/equation of Lvqa? Is it Equation 7?</li>
        <li>In my opinion, to correctly evaluate the efficacy of the representations from the student model one more experiment like Image to Text/Text to Image retrieval involving both text and image representations should have been performed and the recall results reported.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <ul>
        <li>the pre-train datasets used for teacher training are not mentioned.</li>
        <li>lack to clarity for modelling the loss function for free flowing text
apart from the above two points, the paper is mostly reproducible</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ul>
        <li>If possible please provide the datasets used for teacher training.</li>
        <li>report the results using state of the art text encoders</li>
        <li>mention the augmentation and normalization techniques as it will help reproducibility</li>
        <li>other multimodal experiments than classification needed to prove the efficiency</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <ul>
        <li>the approach is novel</li>
        <li>classification accuracy is much better than related work</li>
        <li>accuracy is not sufficient as in the Close Ended answers the data bias is not mentioned, need more metrics like precision, recall and more tasks like image to text, text to image retrieval.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper addresses a medical visual question answering (Med-VQA) task using self-supervised learning coupled with a knowledge distillation framework. The proposed method was evaluated using 2 public datasets. The results show that the proposed method had higher accuracies compared to other existing self-supervised learning methods.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>The idea of incorporating contrastive learning into a knowledge distillation framework was interesting.</li>
        <li>The collection of different medical imaging modalities including X-ray, computed tomography (CT) and magnetic resonance imaging (MRI) was good.</li>
      </ol>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>The authors’ idea of collecting only 3 human body regions: brain, chest and abdomen was not clearly justified. I agree that many existing studies focus on these 3 regions but this does not mean that the proposed method will be beneficial for all radiology images.</li>
        <li>The authors collected different types of medical images including x-ray, CT and MRI. However, these modalities have their unique characteristics that can affect the learning process or subsequent Med-VQA task. Did the authors consider this?</li>
        <li>Some of the Method/experiment setup descriptions are not clear or are missing important details.</li>
      </ol>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Authors provided most of implementation details of their proposed method.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. The authors did not properly define the acronyms throughout the paper. Acronyms need to be defined when they are mentioned for the first time. For example, what is MEVF? 
2. The technical motivation of incorporating contrastive learning into a knowledge distillation framework was not clearly explained. It would be great to include how the proposed method differ from existing knowledge distillation methods.
3. The performance of the proposed method seems to heavily depend on the use of parameter α in equation 6. What value did you use in your experiments and how does use of different parameter affect the results?
4. The description of how the 2 public datasets were split into training, validation and testing sets.
5. The descriptions of other VQA methods such as SAN and BAN are missing. What are they? Why does your proposed method perform better?
6. There are a few typos and grammatical mistakes throughout the paper. 
</code></pre></div>      </div>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline reject (5)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The proposed idea was interesting, but the benefits or justifications of idea were presented poorly. There are some missing descriptions of the proposed method. The were no clear discussions why the proposed method performed better than other state-of-the-art self-supervised learning methods.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper presents a two-stage pre-training framework to tackle the challenge of data scarcity in the Med-VQA domain. More specifically, large-scale unlabeled data is used for pretraining for each domain (brain, chest, and abdomen). The three pretrained networks are then used to distill the final network for feature extraction.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>This work is well-motivated. Leveraging large-scale unlabeled dataset is a hot and important topic for medical image analysis</li>
        <li>This work is well-written.</li>
        <li>The experiments demonstrate the effectiveness of the proposed method.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>Unfair comparison with the existing methods</li>
        <li>The unlabeled setting is fundamentally flawed</li>
        <li>Lack of novelty</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>As the authors is going to release the code to the public, the work is reproducible.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ul>
        <li>
          <p>While the proposed method used large-scale domain-specific datasets to pretrain the network, the baseline methods use only the Imagenet-pretrained weights.</p>
        </li>
        <li>
          <p>While the visualization in the experiments clearly demonstrate that the network can distinguish different human parts, I am concerned that this is because the authors use the labels to train the network. According to Eq. 5, the authors know the label of each subdataset and used it to distill the final ensemble network. Why not use a pretrained ResNet and finetune on this classification task, and use the representation for VQA?</p>
        </li>
        <li>
          <p>This work is essentially a combination of existing methods, which lacks of novelty.</p>
        </li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline reject (5)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <ul>
        <li>Unfair comparison with the existing methods</li>
        <li>The unlabeled setting is fundamentally flawed</li>
        <li>Lack of novelty</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>8</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>Summary:
A novel pre-training method using easily-acquired un-annotated radiology images to pre-train a lightweight visual feature extractor from multiple feature extractors that were trained on the larger dataset. Structural information is retained from the larger model by combining the KLD loss with a contrastive loss. Experiments on two public datasets show this lightweight pre-trained model can adapt to smaller dataset to perform VQA tasks without over-fitting.</p>

      <p>Positives:</p>
      <ul>
        <li>Clearly written, but with some mistakes.</li>
        <li>Strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel, with an interesting idea of incorporating contrastive learning into a knowledge distillation framework .</li>
        <li>Comparisons against the state of the art work, using a variety of image modalities, demonstrate the effectiveness of the proposed method.</li>
        <li>Code to be made available.</li>
      </ul>

      <p>Negatives:</p>
      <ul>
        <li>Some important details are missing. These could be added at a rebuttal stage.</li>
        <li>Might need further experiments, such as assessing on a wider variety of organs - although not for a rebuttal.</li>
        <li>Unfair comparison because baseline methods only used ImageNet training, whereas the proposed method was pre-trained in a domain-specific way.  Try to justify why this should be a fair comparison.</li>
        <li>May lack novelty because it is a combination of existing methods. Explain whether the combination of existing methods is integrated in a way that is significantly novel.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>I’m satisfied with the rebuttal to R4’s concern about unfair comparisons, given that the authors compared against MEVF, which had been pretrained using medical image data.  One aim of the work was to assess whether pre-training on medical imaging data was better than pre-training on ImageNet, hence the other comparisons.  In answering concerns about missing details, I would rather the authors had said something about fixing this in the manuscript, rather than simply provide the missing information for the reviewers. R4 considered the work not to be sufficiently novel, but other reviewers commented on the novelty.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>10</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="meta-review-2">Meta-Review #2</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>This is a novel method. The idea for pretraining is convincing, and the method of using the visual “question answer” in a contrastive setting is a nice application to clinical data. The experimental results are sufficient and convincing.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>
<h2 id="meta-review-3">Meta-Review #3</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>After reading the paper, the reviews and the rebutall, the main critiscim of the paper is in the baselines used. Indeed R2 / R3 bring up an important point which strongly limits the claims of the paper and method. While I appreaciate the authors responses to these points in teh rebutall, the it is insufficient to to make the claims formulated.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Reject</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>16</p>
    </blockquote>
  </li>
</ul>

<h2><br /><br /></h2>
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>

  <p>We thank all the reviewers and clarify some confusion/misunderstanding below.</p>

  <ul>
    <li>Fair Comparison (R4)</li>
  </ul>

  <p>Our experiments (Table 1&amp;2) are intended to compare our pretraining and distillation method to MEVF ([17], MICCAI 2019), which is the only baseline that uses a small model and pretrains with medical images, and to other baselines (BAN fw., etc.) that use large models (e.g., ResNet) and pretrain on ImageNet.</p>

  <p>The results clearly demonstrate the superiority of our method over MEVF in both accuracy and model size, and that other baselines will severely overfit, indicating the necessity of pretraining with domain knowledge and representation distillation, which is the point of our paper.</p>

  <ul>
    <li>Important details</li>
  </ul>

  <p>Due to space limitation, we cannot explain every detail in the paper. We try to provide more information below. We ensure that all experiments are reproducible and will release all codes and datasets (collected from public resources e.g., medicaldecathlon.com) after acceptance.</p>

  <p>[R1] Preprocessing: We use the same transformation group as in MoCoV2 [6]. 
[R1] Open-ended questions: In current research, both open- and closed-ended questions are formulated as classification tasks with the same loss as in Eq 7. The answers cannot be generated, and they must appear in the training set.
[R1] Evaluation metric: We follow current practice to use classification accuracy as evaluation metric, but we agree more reasonable metrics should be devised/used.
[R1] BERT as text encoder: Large models severely overfit on the small-scale training data.
[R1] Lvqa is the loss in Eq 7.</p>

  <p>[R1, R2] Parameter analysis: All hyper-parameters are chosen by cross validation based on observing the loss in Eq 2&amp;6. We want to include these studies but couldn’t due to limited space.</p>

  <p>[R2] Body regions: The observation is that current Med-VQA datasets mainly contain radiology images of the three body regions, and hence our method is beneficial for most of Med-VQA tasks. 
[R2] Acronyms: Thank you. We will define them properly in the final version.
[R2] Data split: We mention in Sec. 5.1 that we follow the settings in previous papers. VQA-RAD: 85% training, 15% test. SLAKE: 70% training, 15% validation, 15% test.<br />
[R2] α in Eq (6) is 0.9.
[R2] SAN and BAN are two different reasoning modules for VQA. BAN fw. and SAN fw. are two VQA models with the corresponding reasoning modules and ResNet as visual extractor and LSTM as text encoder, and they often overfit on Med-VQA datasets.</p>

  <ul>
    <li>Novelty (R4)</li>
  </ul>

  <p>Our contrastive pre-training and representation distillation (CPRD) method is proposed to address two critical issues in Med-VQA: the lack of training images and the diversity of images in terms of different modalities and organs. Existing self-supervised methods cannot be straightforwardly applied to address these problems. Our CPRD is designed based on the statistics of the Med-VQA domain: the datasets mainly consist of radiology images of three body regions. For each region, we collect publicly available unlabeled data and use contrastive learning to learn intra-region prior knowledge. Then, we use contrastive learning again to distill intra-region knowledge and learn inter-region knowledge, which yields a lightweight model.</p>

  <p>As pointed out by R1 and R2, the strategy of using knowledge distillation for pretraining the visual feature extractor in medical domain is novel and can benefit other medical applications such as medical image-text retrieval and medical image captions.</p>

  <ul>
    <li>Unlabeled setting (R4)</li>
  </ul>

  <p>The unlabeled setting is valid. We collect radiology images for each body region (Head, Chest, or Abdomen) from public resources (the images are already grouped by regions). The collected images are unlabeled without any annotations of organs or modalities.</p>

  <p>Also see Fig 2 (left), where Brain CT and Brain MRI are well separated. This shows even with unlabeled data, the distilled student model can capture the differences between different image modalities.</p>

</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0219-12-31
      -->
      <!--
      
        ,
        updated at 
        0220-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a> |
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - CT"
        class="post-category">
        Modalities - CT
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - Text (clinical/radiology reports)"
        class="post-category">
        Modalities - Text (clinical/radiology reports)
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Liu, Bo"
        class="post-category">
        Liu, Bo
      </a> |  
      
      <a href="kittywong/tags#Zhan, Li-Ming"
        class="post-category">
        Zhan, Li-Ming
      </a> |  
      
      <a href="kittywong/tags#Wu, Xiao-Ming"
        class="post-category">
        Wu, Xiao-Ming
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0220/12/31/Paper1432">
          Positional Contrastive Learning for Volumetric Medical Image Segmentation
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0218/12/31/Paper1210">
          Dual-Consistency Semi-Supervised Learning with Uncertainty Quantification for COVID-19 Lesion Segmentation from CT Images
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
