<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Surgical Workflow Anticipation using Instrument Interaction | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Surgical Workflow Anticipation using Instrument Interaction" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Kun Yuan, Matthew Holden, Shijian Gao, Won-Sook Lee Abstract Surgical workflow anticipation, including surgical instrument and phase anticipation, is essential for an intra-operative decision-support system. It deciphers the surgeon’s behaviors and the patient’s status to forecast surgical instrument and phase occurrence before they appear, providing support for instrument preparation and computer-assisted intervention (CAI) systems. We investigate an unexplored surgical workflow anticipation problem by proposing an Instrument Interaction Aware Anticipation Network (IIA-Net). Spatially, it utilizes rich visual features about the context information around the instrument, i.e., instrument interaction with their surroundings. Temporally, it allows for a large receptive field to capture the long-term dependency in the long and untrimmed surgical videos through a causal dilated multi-stage temporal convolutional network. Our model enforces an online inference with reliable predictions even with severe noise and artifacts in the recorded videos. Extensive experiments on Cholec80 dataset demonstrate the performance of our proposed method exceeds the state-of-the-art method by a large margin (1.40 v.s. 1.75 for inMAE and 2.14 v.s. 2.68 for eMAE). The code is published on https://github.com/Flaick/Surgical-Workflow-Anticipation. Link to paper https://doi.org/10.1007/978-3-030-87202-1_59 Link to the code repository https://github.com/Flaick/Surgical-Workflow-Anticipation Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper A surgical workflow anticipation method is proposed, which uses a combination of an instrument interaction module and a temporal model with a multi-stage temporal convolutional network (MSTCN). The method is evaluated on the Cholec80 dataset in terms of mean absolute error (MAE). Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The method is evaluated on a public dataset. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The motivation of the work is not entirely clear. Why is the addressed problem important for clinicians and what is the difference between the works on remaining surgery time estimation? This work seems to be an incremental improvement of remaining time estimation with a slightly different target. The description of the method is confusing, since it is not clear from the beginning, what type of neural network is used for instrument interaction detection. The evaluation method is not complete. Instead of evaluating the method only with MAE, it should be evaluated also with the actual users, i.e., the surgeons, for the achieved benefit. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The presented approach should be largely reproducible, since a public dataset is used for evaluation. I am not fully sure about the technical details of the method though - it would need further investigation to see whether it is fully reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html One of the problems with this work is the motivation: why is surgical workflow anticipation important for clinicians and how does it differ from surgical workflow analysis? To me the presented work is kind of incremental and I do not see a big difference to the works in references [20] and [26] (and other similar works). This is also confirmed by Fig. 1 which shows that the anticipation task is a real-time remaining time prediction task. Section 2.1 seems to be incorrect: in my opinion it should not read x_1 but rather i_1, since the symbol i is representing a frame. Also, tau and alpha is not clearly defined. Another issue is the fact that the description of the network architecture does not clearly express whether an object detection network (or an R-CNN) is used. It is only mentioned that CNNs are used for the instrument interaction module. This is confusing, since YOLOv3 is then mentioned in Section 3.1 (Experiment Setup) but not mentioned in Section 4.1 again, which only states ResNet50. It is also unclear why an old version of YOLO has been used, while v4 is already available since about one year - and why it has been favored over Faster R-CNN or Mask R-CNN, which provide better performance. Finally, although the evaluations show promising performance in terms of MAE, it remains unclear how fast the method works (no evaluation about run-time performance) and what benefit it really brings to surgeons. Please state your overall opinion of the paper strong reject (2) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The major factors for my overall rating are: unclear motivation, unclear description of the method, and unclear clinical benefit. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 3 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The authors propose a deep learning method, called IIA-Net, to predict phases and instrument utilization. IIA-Net toke 5 different outputs for each frame, video frame, semantic map, bounding boxes, instrument presence, and phase. First, they extract features thanks to different networks: visual features by RestNET 50, instrument-instrument, and instrument-surrounding interactions by a new model called IIM. On this paper instrument presence and phase are added from the ground truth. The authors consider that this information can be provided by other types of models. These features are used to the anticipated phase thanks to an MSTCN model. Finally, the feature and the anticipated phase are used to make the instrument prediction. IIA-Net has compared to the state-of-the-art model in Cholech 80 dataset. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strength of the paper is the introduction of IMM a novel instrument interaction module using a semantic map and bounding box as input. Secondly is the extraction from different models of feature as input to performed phase and instrument prediction. Finally, the proposed method is compared with state-of-the-art methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness is the need to compute semantic map, bounding box, presence of the instrument, and phase for each frame. So, it is difficult to ensure that the complete method is suitable for clinical use, especially if the computation time to create these inputs is too expensive. The second weakness is, for the validation, the mix between computed (semantic map and bounding box) and ground truth (instrument and phase inputs). It is difficult to know how the quality of the computed inputs influences the prediction, and if computed instrument presence and recognition of phase will impact the results. However, this is taking into account by authors and note as future works. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance According to information provide on the paper and the checklist, this paper seems to be reproducible especially with the release of the code with acceptance. Data are publicly available . Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Except for the main weakness, which could only be addressed on an additional journal paper, I only have minor comments. The authors used 3 metrics inMAE, eMAE, and pMAE. The last one is defined in 22, to help to the readability, it could be good to remind the definition of pMAE. Tab1 caption indicates that the metrics are the inMAE/eMAE, but the corresponding text mentions inMAE and pMAE. Which one is the correct metric for this tab, eMAE or pMAE? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Despite main weaknesses, which one could not be addressed for a conference paper, the paper is clear, the method innovative, the validation complete, the limitations highlighted. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 7 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The authors propose a MS-TCN model for anticipating both surgical phases as well as instrument usage before they occur based on laparoscopic video. Their main contribution lies in extracting information about the observed surgical scene (such as the current phase, instrument interactions and anatomical structures) and using it as additional features for the temporal model. Also, phase anticipation is newly introduced as prior work only anticipated instruments. Extensive experiments on the Cholec80 dataset show that they achieve superior results on both anticipation tasks compared to prior work which only uses visual features. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The experimental results are very strong as they clearly outperform prior work. Although the model relies on rich annotation on the dataset, it shows the potential of anticipating surgical events when the model is provided with adequate information. Since the performance ceiling of anticipation is not known, this model is also very informative as an upper bound for less annotation-intensive approaches. Even without non-visual features, the 2-step ResNet+TCN might outperform the end-to-end AlexNet+LSTM model which is very interesting for future work. However, this would be more conclusive if the pMAE was also reported in the ablation study, since inMAE and eMAE do not capture “false positives”. Evaluation is done on a large dataset with several insightful metrics and an extensive ablation study. The authors introduce inMAE and eMAE, which are insightful new metrics for anticipation. Section 4.3 discusses the limitations of the method, which is very valuable. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. By relying on the availability of phase and instrument information as well as trained networks for instrument detection and scene segmentation, the applicability of the model in strongly limited to academic datasets like Cholec80 (for now). This stands in contrast to the motivation of the baseline method of Rivoir et al., which only requires image-level labels for the events of interest. Nevertheless, for settings with rich annotation, this is a very strong model and can serve as an upper bound for further research (see ‘Strengths’). In the ablation study (tbl. 1), only inMAE and eMAE are reported. Due to the analogy of pMAE and inMAE to the classification metrics precision and recall, they might provide a more complete evaluation of the ablated models. These metrics were also chosen in table 2 and would make the overall evaluation more consistent. Although the authors promise to publish the code, a few more details regarding the training setup would be helpful. There is no information on loss functions or hyperparameters of the two-stage training setup. The simulated images used to train the segmentation model were generated by training a GAN on the Cholec80 dataset (including the test set of this work). This could cause a minor data leak, as the segmentation network was trained to recognize anatomical textures which themselves were trained to resemble textures in the test data. However, the leaked data is only indirectly “visible” to the model and doesn’t leak ground truth for anticipation. I believe it is not a major flaw. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors promise to publish their code, which makes the method reproducible. However, the training process could be described in more detail in the paper. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html A few more details on the training process would benefit the clarity of the paper. Using metrics inMAE and pMAE for the ablation study would make the evaluation more consistent. Alternatively, it could be elaborated why eMAE was chosen here. The possible data leak regarding the segmentation network could be mentioned in the limitations sections as it is basically a weaker form of the ground truth used for phase and instrument presence. In section 3.1, is the segmentation model really from [5] (YOLOv3) or is that an incorrect citation? If I understand correctly, this model can only do boundingbox detection. The eMAE improves drastically when adding the IIM module. Is there an explanation/hypothesis for this? In the instrument-surrounding module in Fig. 3: Why are the instrument bounding boxes drawn on the feature map learned from the segmentation map? If I understand correctly, boudning boxes and distinction between different instrument types are not ulitized here. Why is the name “instrument-surrounding” chosen here? I find it a little bit misleading since the there is no mechanism which explicitely analyzes structures that are close to instruments (if I understand correctly) and most spatial information is probably lost in the subsequent pooling operation anyway. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The authors provide a strong new model for anticipating events during surgery. The task is very relevant for CAS and the model clearly outperforms previous work. Although the model requires more supervision / pretrained models, the method is still interesting as it shows that events can be anticipated more accurately when more explicit information (e.g. current phase, visible instruments/organs) is known. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposes a novel DL method for anticipating events during the surgery. Overall, the reviews are very positive and the strengths of the paper are the novelty, experimental setup and the results. However there are some issues raised related to the motivation, the description of the method and the clinical benefit. I would like to encourage the authors to clarify these as well as state the run-time performance (is it actually applicable in a real-world setup?). What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper proposes a DL-based method to predict phases and instrument utilization during the surgery. The reviews are very positive and the authors convincingly addressed the concerns that were raised, in particular the motivation and the relation to existing methods. I would like to encourage the authors to double-check the real-time performance comparison to [22] as this can depend on many factors (e.g. Hardware, Framework version,…), but mention their inference performance in the final manuscript as this is a major factor for this application. I do agree with the authors that for a feasibility study a real-world setup, as recommended by R1, may be out of scope. The proposed method has been evaluated on a public dataset and compared to other state of the art. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper proposes a new architecture (IIA-Net) for anticipating surgical phase and instrument usage from video data of the Cholec80 dataset, taking instrument interactions into account. Surgical workflow anticipation is a topic of interest in the CAI community, the paper is well-written, the methodology is innovative with strong validation and experimental results. Concerns from reviewers (including regarding run-time performance and motivation of the work) have been addressed by the rebuttal. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors have addressed all major concerns highlighted by the meta reviewer in the rebuttal and have provided the run-time performance. The motivation, method description and clinical benefits have been clarified. These justifications must be included in the camera-ready. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback Review1: Running Time: Our experiments show that the running time per frame is Max(YOLO, UNet, ResNet50)+MSTCN = Max(0.0090, 0.0087 ,0.0142)+0.0151= 0.0293s, 10% faster than [22] taking 0.0328s. The Max operation is because several models can run in parallel. The speed shows our model is applicable in a real-world setup. Motivation: Our work benefits in three areas (Maier-Hein et al., Nat Biomed Eng, 2017; https://doi.org/10.1038/s41551-017-0132-7). First, tool anticipation offers a useful reference for decision making in a robotic assistance system. It helps to identify tool usage triggering so that a robotic system can decide when to intervene. Second, for context-aware assistance, anticipating tools such as irrigator can help early detection and prevention of potential complications,e.g., massive haemorrhage. Third, phase anticipation allows real-time instruction for automated surgical coaching. Difference from prior work: Our work is one of papers for surgery time estimation. However, one prominent distinction compared to previous papers is that our work deals with the tool/phase-wise remaining time estimation while the others only predict the terminating instant. Therefore, our work enjoys finer time granularity. [20] aims to indicate the current tool/phase but ours can forecast which tool/phase will come into play. [26] merely cares about the surgery’s completion but ours can indicate each of the tool/phase individually throughout the surgery. Also, our work outperforms prior works by inferring surgeon’s action through IIM. Method Description: The instrument interaction detection does not involve any network. Instead, we represent the instrument interaction feature as a length-4 feature vector in Eq.1 and calculate it by combining two tool-detection-bounding boxes’ coordinates . Network Description: Thank you for pointing out. The unclarity comes from our mistake in the citation. We use UNet rather than YOLO to extract segmentation. In Fig.3, the detection &amp; segmentation from YOLO &amp; UNet are used in IIM. We favor YOLO over Faster R-CNN because the former can meet real-time pursuit better. YOLOv4 was not explored as YOLOv3’s performance was good enough for our use. We will correct the citation and update the experiment with YOLOv5. Evaluation: We agree that the real-world test is needed and consider it as a next step. The current study shows feasibility of the proposed method, which is a necessary step prior to real-world testing. Real-world testing involves many substeps, including the hospital’s ethics approval, patients’ consents, surgeon’s collaboration and availability of extra funding, etc. For now, we have added the qualitative results in the supplementary material. Alongside MAE, we hope these promising results will encourage surgeons’ collaboration. Review2: Running Time: Please refer to reviewer 1 for details. Phase &amp; Tool Signals: Thanks for raising the weakness. Prior works on phase/tool detection on Cholec 80 is reasonably accurate, meaning that the real-world signal will not degrade the result a lot. We plan the relevant analysis in future work and consider realistic datasets where these signals might not be available. Review3: Limit on Dataset: Please refer to reviewer 2 for details. eMAE Hypothesis: Our hypothesis is that the interaction feature from IIM is better at short-horizon anticipation. The feature bears a form of [tool, action, anatomy]. We used the Surgical Action Triplet Recognition 2021 challenge in MICCAI and computed a histogram to confirm that action is an essential clue to forecast next 1 minute’s future. Instrument-Surrounding: Sorry for ambiguity in our figure. The extra bounding boxes are removed in the new version. As you said, we do not model the instrument-surrounding feature explicitly. Instead, the semantic map contains the tool location (blue part in Fig.3), through which the surrounding features are modeled implicitly. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Kun Yuan, Matthew Holden, Shijian Gao, Won-Sook Lee Abstract Surgical workflow anticipation, including surgical instrument and phase anticipation, is essential for an intra-operative decision-support system. It deciphers the surgeon’s behaviors and the patient’s status to forecast surgical instrument and phase occurrence before they appear, providing support for instrument preparation and computer-assisted intervention (CAI) systems. We investigate an unexplored surgical workflow anticipation problem by proposing an Instrument Interaction Aware Anticipation Network (IIA-Net). Spatially, it utilizes rich visual features about the context information around the instrument, i.e., instrument interaction with their surroundings. Temporally, it allows for a large receptive field to capture the long-term dependency in the long and untrimmed surgical videos through a causal dilated multi-stage temporal convolutional network. Our model enforces an online inference with reliable predictions even with severe noise and artifacts in the recorded videos. Extensive experiments on Cholec80 dataset demonstrate the performance of our proposed method exceeds the state-of-the-art method by a large margin (1.40 v.s. 1.75 for inMAE and 2.14 v.s. 2.68 for eMAE). The code is published on https://github.com/Flaick/Surgical-Workflow-Anticipation. Link to paper https://doi.org/10.1007/978-3-030-87202-1_59 Link to the code repository https://github.com/Flaick/Surgical-Workflow-Anticipation Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper A surgical workflow anticipation method is proposed, which uses a combination of an instrument interaction module and a temporal model with a multi-stage temporal convolutional network (MSTCN). The method is evaluated on the Cholec80 dataset in terms of mean absolute error (MAE). Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The method is evaluated on a public dataset. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The motivation of the work is not entirely clear. Why is the addressed problem important for clinicians and what is the difference between the works on remaining surgery time estimation? This work seems to be an incremental improvement of remaining time estimation with a slightly different target. The description of the method is confusing, since it is not clear from the beginning, what type of neural network is used for instrument interaction detection. The evaluation method is not complete. Instead of evaluating the method only with MAE, it should be evaluated also with the actual users, i.e., the surgeons, for the achieved benefit. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The presented approach should be largely reproducible, since a public dataset is used for evaluation. I am not fully sure about the technical details of the method though - it would need further investigation to see whether it is fully reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html One of the problems with this work is the motivation: why is surgical workflow anticipation important for clinicians and how does it differ from surgical workflow analysis? To me the presented work is kind of incremental and I do not see a big difference to the works in references [20] and [26] (and other similar works). This is also confirmed by Fig. 1 which shows that the anticipation task is a real-time remaining time prediction task. Section 2.1 seems to be incorrect: in my opinion it should not read x_1 but rather i_1, since the symbol i is representing a frame. Also, tau and alpha is not clearly defined. Another issue is the fact that the description of the network architecture does not clearly express whether an object detection network (or an R-CNN) is used. It is only mentioned that CNNs are used for the instrument interaction module. This is confusing, since YOLOv3 is then mentioned in Section 3.1 (Experiment Setup) but not mentioned in Section 4.1 again, which only states ResNet50. It is also unclear why an old version of YOLO has been used, while v4 is already available since about one year - and why it has been favored over Faster R-CNN or Mask R-CNN, which provide better performance. Finally, although the evaluations show promising performance in terms of MAE, it remains unclear how fast the method works (no evaluation about run-time performance) and what benefit it really brings to surgeons. Please state your overall opinion of the paper strong reject (2) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The major factors for my overall rating are: unclear motivation, unclear description of the method, and unclear clinical benefit. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 3 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The authors propose a deep learning method, called IIA-Net, to predict phases and instrument utilization. IIA-Net toke 5 different outputs for each frame, video frame, semantic map, bounding boxes, instrument presence, and phase. First, they extract features thanks to different networks: visual features by RestNET 50, instrument-instrument, and instrument-surrounding interactions by a new model called IIM. On this paper instrument presence and phase are added from the ground truth. The authors consider that this information can be provided by other types of models. These features are used to the anticipated phase thanks to an MSTCN model. Finally, the feature and the anticipated phase are used to make the instrument prediction. IIA-Net has compared to the state-of-the-art model in Cholech 80 dataset. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strength of the paper is the introduction of IMM a novel instrument interaction module using a semantic map and bounding box as input. Secondly is the extraction from different models of feature as input to performed phase and instrument prediction. Finally, the proposed method is compared with state-of-the-art methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness is the need to compute semantic map, bounding box, presence of the instrument, and phase for each frame. So, it is difficult to ensure that the complete method is suitable for clinical use, especially if the computation time to create these inputs is too expensive. The second weakness is, for the validation, the mix between computed (semantic map and bounding box) and ground truth (instrument and phase inputs). It is difficult to know how the quality of the computed inputs influences the prediction, and if computed instrument presence and recognition of phase will impact the results. However, this is taking into account by authors and note as future works. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance According to information provide on the paper and the checklist, this paper seems to be reproducible especially with the release of the code with acceptance. Data are publicly available . Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Except for the main weakness, which could only be addressed on an additional journal paper, I only have minor comments. The authors used 3 metrics inMAE, eMAE, and pMAE. The last one is defined in 22, to help to the readability, it could be good to remind the definition of pMAE. Tab1 caption indicates that the metrics are the inMAE/eMAE, but the corresponding text mentions inMAE and pMAE. Which one is the correct metric for this tab, eMAE or pMAE? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Despite main weaknesses, which one could not be addressed for a conference paper, the paper is clear, the method innovative, the validation complete, the limitations highlighted. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 7 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The authors propose a MS-TCN model for anticipating both surgical phases as well as instrument usage before they occur based on laparoscopic video. Their main contribution lies in extracting information about the observed surgical scene (such as the current phase, instrument interactions and anatomical structures) and using it as additional features for the temporal model. Also, phase anticipation is newly introduced as prior work only anticipated instruments. Extensive experiments on the Cholec80 dataset show that they achieve superior results on both anticipation tasks compared to prior work which only uses visual features. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The experimental results are very strong as they clearly outperform prior work. Although the model relies on rich annotation on the dataset, it shows the potential of anticipating surgical events when the model is provided with adequate information. Since the performance ceiling of anticipation is not known, this model is also very informative as an upper bound for less annotation-intensive approaches. Even without non-visual features, the 2-step ResNet+TCN might outperform the end-to-end AlexNet+LSTM model which is very interesting for future work. However, this would be more conclusive if the pMAE was also reported in the ablation study, since inMAE and eMAE do not capture “false positives”. Evaluation is done on a large dataset with several insightful metrics and an extensive ablation study. The authors introduce inMAE and eMAE, which are insightful new metrics for anticipation. Section 4.3 discusses the limitations of the method, which is very valuable. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. By relying on the availability of phase and instrument information as well as trained networks for instrument detection and scene segmentation, the applicability of the model in strongly limited to academic datasets like Cholec80 (for now). This stands in contrast to the motivation of the baseline method of Rivoir et al., which only requires image-level labels for the events of interest. Nevertheless, for settings with rich annotation, this is a very strong model and can serve as an upper bound for further research (see ‘Strengths’). In the ablation study (tbl. 1), only inMAE and eMAE are reported. Due to the analogy of pMAE and inMAE to the classification metrics precision and recall, they might provide a more complete evaluation of the ablated models. These metrics were also chosen in table 2 and would make the overall evaluation more consistent. Although the authors promise to publish the code, a few more details regarding the training setup would be helpful. There is no information on loss functions or hyperparameters of the two-stage training setup. The simulated images used to train the segmentation model were generated by training a GAN on the Cholec80 dataset (including the test set of this work). This could cause a minor data leak, as the segmentation network was trained to recognize anatomical textures which themselves were trained to resemble textures in the test data. However, the leaked data is only indirectly “visible” to the model and doesn’t leak ground truth for anticipation. I believe it is not a major flaw. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors promise to publish their code, which makes the method reproducible. However, the training process could be described in more detail in the paper. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html A few more details on the training process would benefit the clarity of the paper. Using metrics inMAE and pMAE for the ablation study would make the evaluation more consistent. Alternatively, it could be elaborated why eMAE was chosen here. The possible data leak regarding the segmentation network could be mentioned in the limitations sections as it is basically a weaker form of the ground truth used for phase and instrument presence. In section 3.1, is the segmentation model really from [5] (YOLOv3) or is that an incorrect citation? If I understand correctly, this model can only do boundingbox detection. The eMAE improves drastically when adding the IIM module. Is there an explanation/hypothesis for this? In the instrument-surrounding module in Fig. 3: Why are the instrument bounding boxes drawn on the feature map learned from the segmentation map? If I understand correctly, boudning boxes and distinction between different instrument types are not ulitized here. Why is the name “instrument-surrounding” chosen here? I find it a little bit misleading since the there is no mechanism which explicitely analyzes structures that are close to instruments (if I understand correctly) and most spatial information is probably lost in the subsequent pooling operation anyway. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The authors provide a strong new model for anticipating events during surgery. The task is very relevant for CAS and the model clearly outperforms previous work. Although the model requires more supervision / pretrained models, the method is still interesting as it shows that events can be anticipated more accurately when more explicit information (e.g. current phase, visible instruments/organs) is known. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposes a novel DL method for anticipating events during the surgery. Overall, the reviews are very positive and the strengths of the paper are the novelty, experimental setup and the results. However there are some issues raised related to the motivation, the description of the method and the clinical benefit. I would like to encourage the authors to clarify these as well as state the run-time performance (is it actually applicable in a real-world setup?). What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper proposes a DL-based method to predict phases and instrument utilization during the surgery. The reviews are very positive and the authors convincingly addressed the concerns that were raised, in particular the motivation and the relation to existing methods. I would like to encourage the authors to double-check the real-time performance comparison to [22] as this can depend on many factors (e.g. Hardware, Framework version,…), but mention their inference performance in the final manuscript as this is a major factor for this application. I do agree with the authors that for a feasibility study a real-world setup, as recommended by R1, may be out of scope. The proposed method has been evaluated on a public dataset and compared to other state of the art. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper proposes a new architecture (IIA-Net) for anticipating surgical phase and instrument usage from video data of the Cholec80 dataset, taking instrument interactions into account. Surgical workflow anticipation is a topic of interest in the CAI community, the paper is well-written, the methodology is innovative with strong validation and experimental results. Concerns from reviewers (including regarding run-time performance and motivation of the work) have been addressed by the rebuttal. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors have addressed all major concerns highlighted by the meta reviewer in the rebuttal and have provided the run-time performance. The motivation, method description and clinical benefits have been clarified. These justifications must be included in the camera-ready. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback Review1: Running Time: Our experiments show that the running time per frame is Max(YOLO, UNet, ResNet50)+MSTCN = Max(0.0090, 0.0087 ,0.0142)+0.0151= 0.0293s, 10% faster than [22] taking 0.0328s. The Max operation is because several models can run in parallel. The speed shows our model is applicable in a real-world setup. Motivation: Our work benefits in three areas (Maier-Hein et al., Nat Biomed Eng, 2017; https://doi.org/10.1038/s41551-017-0132-7). First, tool anticipation offers a useful reference for decision making in a robotic assistance system. It helps to identify tool usage triggering so that a robotic system can decide when to intervene. Second, for context-aware assistance, anticipating tools such as irrigator can help early detection and prevention of potential complications,e.g., massive haemorrhage. Third, phase anticipation allows real-time instruction for automated surgical coaching. Difference from prior work: Our work is one of papers for surgery time estimation. However, one prominent distinction compared to previous papers is that our work deals with the tool/phase-wise remaining time estimation while the others only predict the terminating instant. Therefore, our work enjoys finer time granularity. [20] aims to indicate the current tool/phase but ours can forecast which tool/phase will come into play. [26] merely cares about the surgery’s completion but ours can indicate each of the tool/phase individually throughout the surgery. Also, our work outperforms prior works by inferring surgeon’s action through IIM. Method Description: The instrument interaction detection does not involve any network. Instead, we represent the instrument interaction feature as a length-4 feature vector in Eq.1 and calculate it by combining two tool-detection-bounding boxes’ coordinates . Network Description: Thank you for pointing out. The unclarity comes from our mistake in the citation. We use UNet rather than YOLO to extract segmentation. In Fig.3, the detection &amp; segmentation from YOLO &amp; UNet are used in IIM. We favor YOLO over Faster R-CNN because the former can meet real-time pursuit better. YOLOv4 was not explored as YOLOv3’s performance was good enough for our use. We will correct the citation and update the experiment with YOLOv5. Evaluation: We agree that the real-world test is needed and consider it as a next step. The current study shows feasibility of the proposed method, which is a necessary step prior to real-world testing. Real-world testing involves many substeps, including the hospital’s ethics approval, patients’ consents, surgeon’s collaboration and availability of extra funding, etc. For now, we have added the qualitative results in the supplementary material. Alongside MAE, we hope these promising results will encourage surgeons’ collaboration. Review2: Running Time: Please refer to reviewer 1 for details. Phase &amp; Tool Signals: Thanks for raising the weakness. Prior works on phase/tool detection on Cholec 80 is reasonably accurate, meaning that the real-world signal will not degrade the result a lot. We plan the relevant analysis in future work and consider realistic datasets where these signals might not be available. Review3: Limit on Dataset: Please refer to reviewer 2 for details. eMAE Hypothesis: Our hypothesis is that the interaction feature from IIM is better at short-horizon anticipation. The feature bears a form of [tool, action, anatomy]. We used the Surgical Action Triplet Recognition 2021 challenge in MICCAI and computed a histogram to confirm that action is an essential clue to forecast next 1 minute’s future. Instrument-Surrounding: Sorry for ambiguity in our figure. The extra bounding boxes are removed in the new version. As you said, we do not model the instrument-surrounding feature explicitly. Instead, the semantic map contains the tool location (blue part in Fig.3), through which the surrounding features are modeled implicitly. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0458/12/31/Paper0849" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0458/12/31/Paper0849" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0458-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Surgical Workflow Anticipation using Instrument Interaction" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0458/12/31/Paper0849"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0458/12/31/Paper0849","headline":"Surgical Workflow Anticipation using Instrument Interaction","dateModified":"0459-01-02T00:00:00-05:17","datePublished":"0458-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Kun Yuan, Matthew Holden, Shijian Gao, Won-Sook Lee Abstract Surgical workflow anticipation, including surgical instrument and phase anticipation, is essential for an intra-operative decision-support system. It deciphers the surgeon’s behaviors and the patient’s status to forecast surgical instrument and phase occurrence before they appear, providing support for instrument preparation and computer-assisted intervention (CAI) systems. We investigate an unexplored surgical workflow anticipation problem by proposing an Instrument Interaction Aware Anticipation Network (IIA-Net). Spatially, it utilizes rich visual features about the context information around the instrument, i.e., instrument interaction with their surroundings. Temporally, it allows for a large receptive field to capture the long-term dependency in the long and untrimmed surgical videos through a causal dilated multi-stage temporal convolutional network. Our model enforces an online inference with reliable predictions even with severe noise and artifacts in the recorded videos. Extensive experiments on Cholec80 dataset demonstrate the performance of our proposed method exceeds the state-of-the-art method by a large margin (1.40 v.s. 1.75 for inMAE and 2.14 v.s. 2.68 for eMAE). The code is published on https://github.com/Flaick/Surgical-Workflow-Anticipation. Link to paper https://doi.org/10.1007/978-3-030-87202-1_59 Link to the code repository https://github.com/Flaick/Surgical-Workflow-Anticipation Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper A surgical workflow anticipation method is proposed, which uses a combination of an instrument interaction module and a temporal model with a multi-stage temporal convolutional network (MSTCN). The method is evaluated on the Cholec80 dataset in terms of mean absolute error (MAE). Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The method is evaluated on a public dataset. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The motivation of the work is not entirely clear. Why is the addressed problem important for clinicians and what is the difference between the works on remaining surgery time estimation? This work seems to be an incremental improvement of remaining time estimation with a slightly different target. The description of the method is confusing, since it is not clear from the beginning, what type of neural network is used for instrument interaction detection. The evaluation method is not complete. Instead of evaluating the method only with MAE, it should be evaluated also with the actual users, i.e., the surgeons, for the achieved benefit. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The presented approach should be largely reproducible, since a public dataset is used for evaluation. I am not fully sure about the technical details of the method though - it would need further investigation to see whether it is fully reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html One of the problems with this work is the motivation: why is surgical workflow anticipation important for clinicians and how does it differ from surgical workflow analysis? To me the presented work is kind of incremental and I do not see a big difference to the works in references [20] and [26] (and other similar works). This is also confirmed by Fig. 1 which shows that the anticipation task is a real-time remaining time prediction task. Section 2.1 seems to be incorrect: in my opinion it should not read x_1 but rather i_1, since the symbol i is representing a frame. Also, tau and alpha is not clearly defined. Another issue is the fact that the description of the network architecture does not clearly express whether an object detection network (or an R-CNN) is used. It is only mentioned that CNNs are used for the instrument interaction module. This is confusing, since YOLOv3 is then mentioned in Section 3.1 (Experiment Setup) but not mentioned in Section 4.1 again, which only states ResNet50. It is also unclear why an old version of YOLO has been used, while v4 is already available since about one year - and why it has been favored over Faster R-CNN or Mask R-CNN, which provide better performance. Finally, although the evaluations show promising performance in terms of MAE, it remains unclear how fast the method works (no evaluation about run-time performance) and what benefit it really brings to surgeons. Please state your overall opinion of the paper strong reject (2) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The major factors for my overall rating are: unclear motivation, unclear description of the method, and unclear clinical benefit. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 3 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The authors propose a deep learning method, called IIA-Net, to predict phases and instrument utilization. IIA-Net toke 5 different outputs for each frame, video frame, semantic map, bounding boxes, instrument presence, and phase. First, they extract features thanks to different networks: visual features by RestNET 50, instrument-instrument, and instrument-surrounding interactions by a new model called IIM. On this paper instrument presence and phase are added from the ground truth. The authors consider that this information can be provided by other types of models. These features are used to the anticipated phase thanks to an MSTCN model. Finally, the feature and the anticipated phase are used to make the instrument prediction. IIA-Net has compared to the state-of-the-art model in Cholech 80 dataset. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strength of the paper is the introduction of IMM a novel instrument interaction module using a semantic map and bounding box as input. Secondly is the extraction from different models of feature as input to performed phase and instrument prediction. Finally, the proposed method is compared with state-of-the-art methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness is the need to compute semantic map, bounding box, presence of the instrument, and phase for each frame. So, it is difficult to ensure that the complete method is suitable for clinical use, especially if the computation time to create these inputs is too expensive. The second weakness is, for the validation, the mix between computed (semantic map and bounding box) and ground truth (instrument and phase inputs). It is difficult to know how the quality of the computed inputs influences the prediction, and if computed instrument presence and recognition of phase will impact the results. However, this is taking into account by authors and note as future works. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance According to information provide on the paper and the checklist, this paper seems to be reproducible especially with the release of the code with acceptance. Data are publicly available . Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Except for the main weakness, which could only be addressed on an additional journal paper, I only have minor comments. The authors used 3 metrics inMAE, eMAE, and pMAE. The last one is defined in 22, to help to the readability, it could be good to remind the definition of pMAE. Tab1 caption indicates that the metrics are the inMAE/eMAE, but the corresponding text mentions inMAE and pMAE. Which one is the correct metric for this tab, eMAE or pMAE? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Despite main weaknesses, which one could not be addressed for a conference paper, the paper is clear, the method innovative, the validation complete, the limitations highlighted. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 7 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The authors propose a MS-TCN model for anticipating both surgical phases as well as instrument usage before they occur based on laparoscopic video. Their main contribution lies in extracting information about the observed surgical scene (such as the current phase, instrument interactions and anatomical structures) and using it as additional features for the temporal model. Also, phase anticipation is newly introduced as prior work only anticipated instruments. Extensive experiments on the Cholec80 dataset show that they achieve superior results on both anticipation tasks compared to prior work which only uses visual features. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The experimental results are very strong as they clearly outperform prior work. Although the model relies on rich annotation on the dataset, it shows the potential of anticipating surgical events when the model is provided with adequate information. Since the performance ceiling of anticipation is not known, this model is also very informative as an upper bound for less annotation-intensive approaches. Even without non-visual features, the 2-step ResNet+TCN might outperform the end-to-end AlexNet+LSTM model which is very interesting for future work. However, this would be more conclusive if the pMAE was also reported in the ablation study, since inMAE and eMAE do not capture “false positives”. Evaluation is done on a large dataset with several insightful metrics and an extensive ablation study. The authors introduce inMAE and eMAE, which are insightful new metrics for anticipation. Section 4.3 discusses the limitations of the method, which is very valuable. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. By relying on the availability of phase and instrument information as well as trained networks for instrument detection and scene segmentation, the applicability of the model in strongly limited to academic datasets like Cholec80 (for now). This stands in contrast to the motivation of the baseline method of Rivoir et al., which only requires image-level labels for the events of interest. Nevertheless, for settings with rich annotation, this is a very strong model and can serve as an upper bound for further research (see ‘Strengths’). In the ablation study (tbl. 1), only inMAE and eMAE are reported. Due to the analogy of pMAE and inMAE to the classification metrics precision and recall, they might provide a more complete evaluation of the ablated models. These metrics were also chosen in table 2 and would make the overall evaluation more consistent. Although the authors promise to publish the code, a few more details regarding the training setup would be helpful. There is no information on loss functions or hyperparameters of the two-stage training setup. The simulated images used to train the segmentation model were generated by training a GAN on the Cholec80 dataset (including the test set of this work). This could cause a minor data leak, as the segmentation network was trained to recognize anatomical textures which themselves were trained to resemble textures in the test data. However, the leaked data is only indirectly “visible” to the model and doesn’t leak ground truth for anticipation. I believe it is not a major flaw. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors promise to publish their code, which makes the method reproducible. However, the training process could be described in more detail in the paper. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html A few more details on the training process would benefit the clarity of the paper. Using metrics inMAE and pMAE for the ablation study would make the evaluation more consistent. Alternatively, it could be elaborated why eMAE was chosen here. The possible data leak regarding the segmentation network could be mentioned in the limitations sections as it is basically a weaker form of the ground truth used for phase and instrument presence. In section 3.1, is the segmentation model really from [5] (YOLOv3) or is that an incorrect citation? If I understand correctly, this model can only do boundingbox detection. The eMAE improves drastically when adding the IIM module. Is there an explanation/hypothesis for this? In the instrument-surrounding module in Fig. 3: Why are the instrument bounding boxes drawn on the feature map learned from the segmentation map? If I understand correctly, boudning boxes and distinction between different instrument types are not ulitized here. Why is the name “instrument-surrounding” chosen here? I find it a little bit misleading since the there is no mechanism which explicitely analyzes structures that are close to instruments (if I understand correctly) and most spatial information is probably lost in the subsequent pooling operation anyway. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The authors provide a strong new model for anticipating events during surgery. The task is very relevant for CAS and the model clearly outperforms previous work. Although the model requires more supervision / pretrained models, the method is still interesting as it shows that events can be anticipated more accurately when more explicit information (e.g. current phase, visible instruments/organs) is known. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposes a novel DL method for anticipating events during the surgery. Overall, the reviews are very positive and the strengths of the paper are the novelty, experimental setup and the results. However there are some issues raised related to the motivation, the description of the method and the clinical benefit. I would like to encourage the authors to clarify these as well as state the run-time performance (is it actually applicable in a real-world setup?). What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper proposes a DL-based method to predict phases and instrument utilization during the surgery. The reviews are very positive and the authors convincingly addressed the concerns that were raised, in particular the motivation and the relation to existing methods. I would like to encourage the authors to double-check the real-time performance comparison to [22] as this can depend on many factors (e.g. Hardware, Framework version,…), but mention their inference performance in the final manuscript as this is a major factor for this application. I do agree with the authors that for a feasibility study a real-world setup, as recommended by R1, may be out of scope. The proposed method has been evaluated on a public dataset and compared to other state of the art. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper proposes a new architecture (IIA-Net) for anticipating surgical phase and instrument usage from video data of the Cholec80 dataset, taking instrument interactions into account. Surgical workflow anticipation is a topic of interest in the CAI community, the paper is well-written, the methodology is innovative with strong validation and experimental results. Concerns from reviewers (including regarding run-time performance and motivation of the work) have been addressed by the rebuttal. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors have addressed all major concerns highlighted by the meta reviewer in the rebuttal and have provided the run-time performance. The motivation, method description and clinical benefits have been clarified. These justifications must be included in the camera-ready. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback Review1: Running Time: Our experiments show that the running time per frame is Max(YOLO, UNet, ResNet50)+MSTCN = Max(0.0090, 0.0087 ,0.0142)+0.0151= 0.0293s, 10% faster than [22] taking 0.0328s. The Max operation is because several models can run in parallel. The speed shows our model is applicable in a real-world setup. Motivation: Our work benefits in three areas (Maier-Hein et al., Nat Biomed Eng, 2017; https://doi.org/10.1038/s41551-017-0132-7). First, tool anticipation offers a useful reference for decision making in a robotic assistance system. It helps to identify tool usage triggering so that a robotic system can decide when to intervene. Second, for context-aware assistance, anticipating tools such as irrigator can help early detection and prevention of potential complications,e.g., massive haemorrhage. Third, phase anticipation allows real-time instruction for automated surgical coaching. Difference from prior work: Our work is one of papers for surgery time estimation. However, one prominent distinction compared to previous papers is that our work deals with the tool/phase-wise remaining time estimation while the others only predict the terminating instant. Therefore, our work enjoys finer time granularity. [20] aims to indicate the current tool/phase but ours can forecast which tool/phase will come into play. [26] merely cares about the surgery’s completion but ours can indicate each of the tool/phase individually throughout the surgery. Also, our work outperforms prior works by inferring surgeon’s action through IIM. Method Description: The instrument interaction detection does not involve any network. Instead, we represent the instrument interaction feature as a length-4 feature vector in Eq.1 and calculate it by combining two tool-detection-bounding boxes’ coordinates . Network Description: Thank you for pointing out. The unclarity comes from our mistake in the citation. We use UNet rather than YOLO to extract segmentation. In Fig.3, the detection &amp; segmentation from YOLO &amp; UNet are used in IIM. We favor YOLO over Faster R-CNN because the former can meet real-time pursuit better. YOLOv4 was not explored as YOLOv3’s performance was good enough for our use. We will correct the citation and update the experiment with YOLOv5. Evaluation: We agree that the real-world test is needed and consider it as a next step. The current study shows feasibility of the proposed method, which is a necessary step prior to real-world testing. Real-world testing involves many substeps, including the hospital’s ethics approval, patients’ consents, surgeon’s collaboration and availability of extra funding, etc. For now, we have added the qualitative results in the supplementary material. Alongside MAE, we hope these promising results will encourage surgeons’ collaboration. Review2: Running Time: Please refer to reviewer 1 for details. Phase &amp; Tool Signals: Thanks for raising the weakness. Prior works on phase/tool detection on Cholec 80 is reasonably accurate, meaning that the real-world signal will not degrade the result a lot. We plan the relevant analysis in future work and consider realistic datasets where these signals might not be available. Review3: Limit on Dataset: Please refer to reviewer 2 for details. eMAE Hypothesis: Our hypothesis is that the interaction feature from IIM is better at short-horizon anticipation. The feature bears a form of [tool, action, anatomy]. We used the Surgical Action Triplet Recognition 2021 challenge in MICCAI and computed a histogram to confirm that action is an essential clue to forecast next 1 minute’s future. Instrument-Surrounding: Sorry for ambiguity in our figure. The extra bounding boxes are removed in the new version. As you said, we do not model the instrument-surrounding feature explicitly. Instead, the semantic map contains the tool location (blue part in Fig.3), through which the surrounding features are modeled implicitly. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Yuan, Kun,Holden, Matthew,Gao, Shijian,Lee, Won-Sook" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Surgical Workflow Anticipation using Instrument Interaction</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Surgical Skill and Work Flow Analysis"
        class="post-category">
        Surgical Skill and Work Flow Analysis
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Abdomen"
        class="post-category">
        Clinical applications - Abdomen
      </a>
      
      <a 
        href="kittywong/categories#Modalities - Video"
        class="post-category">
        Modalities - Video
      </a>
      
      <a 
        href="kittywong/categories#Surgical Data Science"
        class="post-category">
        Surgical Data Science
      </a>
      
      <a 
        href="kittywong/categories#Surgical Planning and Simulation"
        class="post-category">
        Surgical Planning and Simulation
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Yuan, Kun"
        class="post-tags">
        Yuan, Kun
      </a> |  
      
      <a href="kittywong/tags#Holden, Matthew"
        class="post-tags">
        Holden, Matthew
      </a> |  
      
      <a href="kittywong/tags#Gao, Shijian"
        class="post-tags">
        Gao, Shijian
      </a> |  
      
      <a href="kittywong/tags#Lee, Won-Sook"
        class="post-tags">
        Lee, Won-Sook
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Kun Yuan, Matthew Holden, Shijian Gao, Won-Sook Lee
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Surgical workflow anticipation, including surgical instrument and phase anticipation, is essential for an intra-operative decision-support system. It deciphers the surgeon’s behaviors and the patient’s status to forecast surgical instrument and phase occurrence before they appear, providing support for instrument preparation and computer-assisted intervention (CAI) systems. We investigate an unexplored surgical workflow anticipation problem by proposing an Instrument Interaction Aware Anticipation Network (IIA-Net). Spatially, it utilizes rich visual features about the context information around the instrument, i.e., instrument interaction with their surroundings. Temporally, it allows for a large receptive field to capture the long-term dependency in the long and untrimmed surgical videos through a causal dilated multi-stage temporal convolutional network. Our model enforces an online inference with reliable predictions even with severe noise and artifacts in the recorded videos. Extensive experiments on Cholec80 dataset demonstrate the performance of our proposed method exceeds the state-of-the-art method by a large margin (1.40 v.s. 1.75 for inMAE and 2.14 v.s. 2.68 for eMAE). The code is published on https://github.com/Flaick/Surgical-Workflow-Anticipation.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87202-1_59">https://doi.org/10.1007/978-3-030-87202-1_59</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/Flaick/Surgical-Workflow-Anticipation
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>A surgical workflow anticipation method is proposed, which uses a combination of an instrument interaction module and a temporal model with a multi-stage temporal convolutional network (MSTCN). The method is evaluated on the Cholec80 dataset in terms of mean absolute error (MAE).</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>The method is evaluated on a public dataset.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>The motivation of the work is not entirely clear. Why is the addressed problem important for clinicians and what is the difference between the works on remaining surgery time estimation? This work seems to be an incremental improvement of remaining time estimation with a slightly different target.</li>
        <li>The description of the method is confusing, since it is not clear from the beginning, what type of neural network is used for instrument interaction detection.</li>
        <li>The evaluation method is not complete. Instead of evaluating the method only with MAE, it should be evaluated also with the actual users, i.e., the surgeons, for the achieved benefit.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Satisfactory</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The presented approach should be largely reproducible, since a public dataset is used for evaluation. I am not fully sure about the technical details of the method though - it would need further investigation to see whether it is fully reproducible.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>One of the problems with this work is the motivation: why is surgical workflow anticipation important for clinicians and how does it differ from surgical workflow analysis? To me the presented work is kind of incremental and I do not see a big difference to the works in references [20] and [26] (and other similar works). This is also confirmed by Fig. 1 which shows that the anticipation task is a real-time remaining time prediction task.</p>

      <p>Section 2.1 seems to be incorrect: in my opinion it should not read x_1 but rather i_1, since the symbol i is representing a frame. Also, tau and alpha is not clearly defined.</p>

      <p>Another issue is the fact that the description of the network architecture does not clearly express whether an object detection network (or an R-CNN) is used. It is only mentioned that CNNs are used for the instrument interaction module. This is confusing, since YOLOv3 is then mentioned in Section 3.1 (Experiment Setup) but not mentioned in Section 4.1 again, which only states ResNet50. It is also unclear why an old version of YOLO has been used, while v4 is already available since about one year - and why it has been favored over Faster R-CNN or Mask R-CNN, which provide better performance.</p>

      <p>Finally, although the evaluations show promising performance in terms of MAE, it remains unclear how fast the method works (no evaluation about run-time performance) and what benefit it really brings to surgeons.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong reject (2)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The major factors for my overall rating are: unclear motivation, unclear description of the method, and unclear clinical benefit.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors propose a deep learning method, called IIA-Net, to predict phases and instrument utilization. IIA-Net toke 5 different outputs for each frame, video frame, semantic map, bounding boxes, instrument presence, and phase. First, they extract features thanks to different networks: visual features by RestNET 50, instrument-instrument, and instrument-surrounding interactions by a new model called IIM. On this paper instrument presence and phase are added from the ground truth. The authors consider that this information can be provided by other types of models. These features are used to the anticipated phase thanks to an MSTCN model. Finally, the feature and the anticipated phase are used to make the instrument prediction. IIA-Net has compared to the state-of-the-art model in Cholech 80 dataset.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The main strength of the paper is the introduction of IMM a novel instrument interaction module using a semantic map and bounding box as input. Secondly is the extraction from different models of feature as input to performed phase and instrument prediction. Finally, the proposed method is compared with state-of-the-art methods.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The main weakness is the need to compute semantic map, bounding box, presence of the instrument, and phase for each frame. So, it is difficult to ensure that the complete method is suitable for clinical use, especially if the computation time to create these inputs is too expensive.</p>

      <p>The second weakness is, for the validation, the mix between computed (semantic map and bounding box) and ground truth (instrument and phase inputs). It is difficult to know how the quality of the computed inputs influences the prediction, and if computed instrument presence and recognition of phase will impact the results. However, this is taking into account by authors and note as future works.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>According to information provide on the paper and the checklist, this paper seems to be reproducible especially with the release of the code with acceptance. Data are publicly available .</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Except for the main weakness, which could only be addressed on an additional journal paper, I only have minor comments.
The authors used 3 metrics inMAE, eMAE, and pMAE. The last one is defined in 22, to help to the readability, it could be good to remind the definition of pMAE.
Tab1 caption indicates that the metrics are the inMAE/eMAE, but the corresponding text mentions inMAE and pMAE. Which one is the correct metric for this tab, eMAE or pMAE?</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Despite main weaknesses, which one could not be addressed for a conference paper, the paper is clear, the method innovative, the validation complete, the limitations highlighted.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>7</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors propose a MS-TCN model for anticipating both surgical phases as well as instrument usage before they occur based on laparoscopic video. Their main contribution lies in extracting information about the observed surgical scene (such as the current phase, instrument interactions and anatomical structures) and using it as additional features for the temporal model. Also, phase anticipation is newly introduced as prior work only anticipated instruments. Extensive experiments on the Cholec80 dataset show that they achieve superior results on both anticipation tasks compared to prior work which only uses visual features.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>The experimental results are very strong as they clearly outperform prior work.</li>
        <li>Although the model relies on rich annotation on the dataset, it shows the potential of anticipating surgical events when the model is provided with adequate information. Since the performance ceiling of anticipation is not known, this model is also very informative as an upper bound for less annotation-intensive approaches.</li>
        <li>Even without non-visual features, the 2-step ResNet+TCN might outperform the end-to-end AlexNet+LSTM model which is very interesting for future work. However, this would be more conclusive if the pMAE was also reported in the ablation study, since inMAE and eMAE do not capture “false positives”.</li>
        <li>Evaluation is done on a large dataset with several insightful metrics and an extensive ablation study.</li>
        <li>The authors introduce inMAE and eMAE, which are insightful new metrics for anticipation.</li>
        <li>Section 4.3 discusses the limitations of the method, which is very valuable.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>By relying on the availability of phase and instrument information as well as trained networks for instrument detection and scene segmentation, the applicability of the model in strongly limited to academic datasets like Cholec80 (for now). This stands in contrast to the motivation of the baseline method of Rivoir et al., which only requires image-level labels for the events of interest. Nevertheless, for settings with rich annotation, this is a very strong model and can serve as an upper bound for further research (see ‘Strengths’).</li>
        <li>In the ablation study (tbl. 1), only inMAE and eMAE are reported. Due to the analogy of pMAE and inMAE to the classification metrics precision and recall, they might provide a more complete evaluation of the ablated models. These metrics were also chosen in table 2 and would make the overall evaluation more consistent.</li>
        <li>Although the authors promise to publish the code, a few more details regarding the training setup would be helpful. There is no information on loss functions or hyperparameters of the two-stage training setup.</li>
        <li>The simulated images used to train the segmentation model were generated by training a GAN on the Cholec80 dataset (including the test set of this work). This could cause a minor data leak, as the segmentation network was trained to recognize anatomical textures which themselves were trained to resemble textures in the test data. However, the leaked data is only indirectly “visible” to the model and doesn’t leak ground truth for anticipation. I believe it is not a major flaw.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The authors promise to publish their code, which makes the method reproducible. 
However, the training process could be described in more detail in the paper.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ul>
        <li>A few more details on the training process would benefit the clarity of the paper.</li>
        <li>Using metrics inMAE and pMAE for the ablation study would make the evaluation more consistent. Alternatively, it could be elaborated why eMAE was chosen here.</li>
        <li>The possible data leak regarding the segmentation network could be mentioned in the limitations sections as it is basically a weaker form of the ground truth used for phase and instrument presence.</li>
        <li>In section 3.1, is the segmentation model really from [5] (YOLOv3) or is that an incorrect citation? If I understand correctly, this model can only do boundingbox detection.</li>
        <li>The eMAE improves drastically when adding the IIM module. Is there an explanation/hypothesis for this?</li>
        <li>In the instrument-surrounding module in Fig. 3: Why are the instrument bounding boxes drawn on the feature map learned from the segmentation map? If I understand correctly, boudning boxes and distinction between different instrument types are not ulitized here.</li>
        <li>Why is the name “instrument-surrounding” chosen here? I find it a little bit misleading since the there is no mechanism which explicitely analyzes structures that are close to instruments (if I understand correctly) and most spatial information is probably lost in the subsequent pooling operation anyway.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The authors provide a strong new model for anticipating events during surgery. The task is very relevant for CAS and the model clearly outperforms previous work. Although the model requires more supervision / pretrained models, the method is still interesting as it shows that events can be anticipated more accurately when more explicit information (e.g. current phase, visible instruments/organs) is known.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>This paper proposes a novel DL method for anticipating events during the surgery. Overall, the reviews are very positive and the strengths of the paper are the novelty, experimental setup and the results. However there are some issues raised related to the motivation, the description of the method and the clinical benefit. I would like to encourage the authors to clarify these as well as state the run-time performance (is it actually applicable in a real-world setup?).</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>10</p>
    </blockquote>
  </li>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>This paper proposes a DL-based method to predict phases and instrument utilization during the surgery. The reviews are very positive and the authors convincingly addressed the concerns that were raised, in particular the motivation and the relation to existing methods. I would like to encourage the authors to double-check the real-time performance comparison to [22] as this can depend on many factors (e.g. Hardware, Framework version,…), but mention their inference performance in the final manuscript as this is a major factor for this application. I do agree with the authors that for a feasibility study a real-world setup, as recommended by R1, may be out of scope. The proposed method has been evaluated on a public dataset and compared to other state of the art.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="meta-review-2">Meta-Review #2</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The paper proposes a new architecture (IIA-Net) for anticipating surgical phase and instrument usage from video data of the Cholec80 dataset, taking instrument interactions into account. Surgical workflow anticipation is a topic of interest in the CAI community, the paper is well-written, the methodology is innovative with strong validation and experimental results. Concerns from reviewers (including regarding run-time performance and motivation of the work) have been addressed by the rebuttal.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>
<h2 id="meta-review-3">Meta-Review #3</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The authors have addressed all major concerns highlighted by the meta reviewer in the rebuttal and have provided the run-time performance. The motivation, method description and clinical benefits have been clarified. These justifications must be included in the camera-ready.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
</ul>

<h2><br /><br /></h2>
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>Review1:
Running Time:
Our experiments show that the running time per frame is Max(YOLO, UNet, ResNet50)+MSTCN = Max(0.0090, 0.0087 ,0.0142)+0.0151= 0.0293s, 10% faster than [22] taking 0.0328s. The Max operation is because several models can run in parallel. The speed shows our model is applicable in a real-world setup. 
Motivation:
Our work benefits in three areas (Maier-Hein et al., Nat Biomed Eng, 2017; https://doi.org/10.1038/s41551-017-0132-7). First, tool anticipation offers a useful reference for decision making in a robotic assistance system. It helps to identify tool usage triggering so that a robotic system can decide when to intervene. Second, for context-aware assistance, anticipating tools such as irrigator can help early detection and prevention of potential complications,e.g., massive haemorrhage. Third, phase anticipation allows real-time instruction for automated surgical coaching.
Difference from prior work:
Our work is one of papers for surgery time estimation. However, one prominent distinction compared to previous papers is that our work deals with the tool/phase-wise remaining time estimation while the others only predict the terminating instant. Therefore, our work enjoys finer time granularity. [20] aims to indicate the current tool/phase but ours can forecast which tool/phase will come into play. [26] merely cares about the surgery’s completion but ours can indicate each of the tool/phase individually throughout the surgery. Also, our work outperforms prior works by inferring surgeon’s action through IIM.
Method Description:
The instrument interaction detection does not involve any network. Instead, we represent the instrument interaction feature as a length-4 feature vector in Eq.1 and calculate it by combining two tool-detection-bounding boxes’ coordinates .
Network Description:
Thank you for pointing out. The unclarity comes from our mistake in the citation. We use UNet rather than YOLO to extract segmentation. In Fig.3, the detection &amp; segmentation from YOLO &amp; UNet are used in IIM. We favor YOLO over Faster R-CNN because the former can meet real-time pursuit better. YOLOv4 was not explored as YOLOv3’s performance was good enough for our use. We will correct the citation and update the experiment with YOLOv5.
Evaluation:
We agree that the real-world test is needed and consider it as a next step. The current study shows feasibility of the proposed method, which is a necessary step prior to real-world testing. Real-world testing involves many substeps, including the hospital’s ethics approval, patients’ consents, surgeon’s collaboration and availability of extra funding, etc. For now, we have added the qualitative results in the supplementary material. Alongside MAE, we hope these promising results will encourage surgeons’ collaboration.
Review2:
Running Time:
Please refer to reviewer 1 for details. 
Phase &amp; Tool Signals:
Thanks for raising the weakness. Prior works on phase/tool detection on Cholec 80 is reasonably accurate, meaning that the real-world signal will not degrade the result a lot. We plan the relevant analysis in future work and consider realistic datasets where these signals might not be available.
Review3:
Limit on Dataset:
Please refer to reviewer 2 for details.
eMAE Hypothesis:
Our hypothesis is that the interaction feature from IIM is better at short-horizon anticipation. The feature bears a form of [tool, action, anatomy]. We used the Surgical Action Triplet Recognition 2021 challenge in MICCAI and computed a histogram to confirm that action is an essential clue to forecast next 1 minute’s future. 
Instrument-Surrounding:
Sorry for ambiguity in our figure. The extra bounding boxes are removed in the new version. As you said, we do not model the instrument-surrounding feature explicitly. Instead, the semantic map contains the tool location (blue part in Fig.3), through which the surrounding features are modeled implicitly.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0458-12-31
      -->
      <!--
      
        ,
        updated at 
        0459-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Surgical Skill and Work Flow Analysis"
        class="post-category">
        Surgical Skill and Work Flow Analysis
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Abdomen"
        class="post-category">
        Clinical applications - Abdomen
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - Video"
        class="post-category">
        Modalities - Video
      </a> |
      
      <a 
        href="kittywong/categories#Surgical Data Science"
        class="post-category">
        Surgical Data Science
      </a> |
      
      <a 
        href="kittywong/categories#Surgical Planning and Simulation"
        class="post-category">
        Surgical Planning and Simulation
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Yuan, Kun"
        class="post-category">
        Yuan, Kun
      </a> |  
      
      <a href="kittywong/tags#Holden, Matthew"
        class="post-category">
        Holden, Matthew
      </a> |  
      
      <a href="kittywong/tags#Gao, Shijian"
        class="post-category">
        Gao, Shijian
      </a> |  
      
      <a href="kittywong/tags#Lee, Won-Sook"
        class="post-category">
        Lee, Won-Sook
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0459/12/31/Paper1533">
          Multi-View Surgical Video Action Detection via Mixed Global View Attention
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0457/12/31/Paper0235">
          OperA: Attention-Regularized Transformers for Surgical Phase Recognition
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
