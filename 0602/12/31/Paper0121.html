<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>TarGAN: Target-Aware Generative Adversarial Networks for Multi-modality Medical Image Translation | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="TarGAN: Target-Aware Generative Adversarial Networks for Multi-modality Medical Image Translation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Junxiao Chen, Jia Wei, Rui Li Abstract Paired multi-modality medical images, can provide complementary information to help physicians make more reasonable decisions than single modality medical images. But they are difficult to generate due to multiple factors in practice (e.g., time, cost, radiation dose). To address these problems, multi-modality medical image translation has aroused increasing research interest recently. However, the existing works mainly focus on translation effect of a whole image instead of a critical target area or Region of Interest (ROI), e.g., organ and so on. This leads to poor-quality translation of the localized target area which becomes blurry, deformed or even with extra unreasonable textures. In this paper, we propose a novel target-aware generative adversarial network called TarGAN, which is a generic multi-modality medical image translation model capable of (1) learning multi-modality medical image translation without relying on paired data, (2) enhancing quality of target area generation with the help of target area labels. The generator of TarGAN jointly learns mapping at two levels simultaneously — whole image translation mapping and target area translation mapping. These two mappings are interrelated through a proposed crossing loss. The experiments on both quantitative measures and qualitative evaluations demonstrate that TarGAN outperforms the state-of-the-art methods in all cases. Subsequent segmentation task is conducted to demonstrate effectiveness of synthetic images generated by TarGAN in a real-world application. Our code is available at https://github.com/cs-xiao/TarGAN. Link to paper https://doi.org/10.1007/978-3-030-87231-1_3 Link to the code repository https://github.com/cs-xiao/TarGAN Link to the dataset(s) https://chaos.grand-challenge.org/ Reviews Review #1 Please describe the contribution of the paper The paper presents a modality translation (CT to MRI) framework which is trained without any paired data using the now commonly used cycle consistency loss. The paper not only generates a translation at the whole image level but also generates a sub region e.g. a specific anatomical region; in this case the liver. It is argued that the focus on specific regions is able to generate higher quality images in these specific areas. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The combination of different losses like cycle consistency, modality classification and shape consistency seem appropriate for the task at hand. It is also interesting to make use a dataset whose original purpose was segmentation for a rather different task such as synthetic image generation. I appreciated the ablation studies showing the impact of the different types of loss fiunctions. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Some details are not very clear; for example what exactly are the inputs to the G network? it is written in Fig 2 that is a depthwise concatenation of the source image plus the target modality. Fomr the figure it seems to be a one hot encoding, but it is not very clear if it is just a full binary image encoding or some other way. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I really appreciated the authors went ahead and shared the code in an anonymous fashion using (anonymous.4open.science). However, the weights of the model are not available so it is not possible to reproduce exactly the experiments. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Fig. 2 it is not clear how is ‘t’ encoded. is this a one hot encoding e.g. a 3XHxW tensor? I would appreciate if the tensor shapes were added to the figures and to the networks descriptions in the suppl. material. It is very good to evaluate the generated images on a downstream task such as segmentation. However, it could good to train an MRI liver segmentation model only with real data, then using the CT as input generate a synthetic MRI. Now we have the segmentation of this MRI since it is the same as the one of the CT, we could then use the synthetic MRI and segment it with the MRI segmentation network (which has never seen synthetic data) and evaluate the performance this way. Like this we could see if a network trained only with real data would perform well segmenting synthetic data. I appreciated a lot the share of the source code; however, without it seems that only the training part is there, and without sharing the weights of the networks it is not possible to reproduce the results. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper presents an interesting combination of loss functions and to me it seems as a good idea to focus on specific anatomical regions. The fact os using the generated images on a downstream task such as segmentation is also very appropiate to evaluate the usefulness of the generated data. I would have given a higher score if the shared code had also included the weights of the models in order to reproduce the results better. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The manuscript proposes a method for multi-modal medical image translation based on Generative Adversarial Networks. While there is a plethora of architectures very similar to the proposed one since around 2018 for medical imaging, the authors modify the networks to explicitly enforce shape consistency via the use of binary segmentation masks from the source domain and improve translation quality by the use of the untraceable constraint. Experiments are performed on 2D slices of 3D data gathered from a relatively small public dataset composed of CTs and MRIs. An ablation study and comparison with baselines and reported and discussed in the experiments. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is overall well written with only very minor grammar issues. The methodology is technically sound and the insertion of multiple loss components and modules are both properly motivated in a theoretical sense and validated during ablation. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. TarGAN requires previous segmentation of the organs of interest in the source domain to enforce shape consistency, thus severely limiting the application of the method in real-world scenarios. The experimental setup only focuses on liver segmentation, while there are multiple other labeled CT datasets in the literature (i.e. https://zenodo.org/record/1169361#.X7IOsZ1KiV4, https://www.kaggle.com/user123454321/liversegtestimages/version/1). The manuscript focuses on volumetric data translations and only during the experimental setup (Sec. 3.1) the reader is informed that the proposed method only works in 2D slices of 3D images. While there are indeed applications for this methodology, the authors should clarify this much earlier in the text. Additionally, while 2D image-translation is a very well explored area in the literature of visual pattern recognition, including multi-modality image translation in medical imaging, 3D data presents a whole new set of difficulties, including the sharp increase in computational requirements due to 3D convolutions and the lack of pretrained models commonly used to enforce perceptual losses in modern 2D image translation architectures. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Authors propose an architecture of image-translation GAN composed of multiple modules, loss components, two discriminators and combinations of existing architectures (namely StarGAN and CycleGAN). In the manuscript (including supplementary material) there are not enough details regarding architectural choices, hyperparameters and implementation details to allow for replication of this study, as all of these aspects of the method would require extensive tuning. However, the authors marked that pretrained models, training and evaluation code would be made available, with an anonymized link for code in the supplementary material. If these codes are inserted in the camera ready version of the main text, there should be no major reproducibility concerns regarding this manuscript. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html A more thorough analysis in multiple organ segmentations using additional public datasets would be my main suggestion to the authors. While the method indeed presented promising results compared to other baselines in the literature, a proper validation of the methods effectiveness is not possible if only liver translation is conducted in the experiments. Inserting multiple datasets instead of focusing only on CHAOS (a rather small data scenario) would help to further validate the method in comparison with other baselines. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The manuscript does seem to provide an incremental gain in the literature of medical image translation using GANs. However, the experiments are way too narrow due to the choice of liver segmentation in a small dataset only. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper proposed a target-aware GAN for unpaired multi-modality medical image translation. The generator translate the global image and interested ROI jointly with the help of ROI labels. The method is based on starGAN with additional untraceable constraint loss, shape constraint loss and crossing loss to help the generator to learn. The generated results were found to be both of high quality in terms of FID score and beneficial to the down stream segmentation task. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Experiments were through. The paper not only provided image quality based metrics but also showed the usefulness of the generated image in downstream segmentation tasks. The two streams generator that handles both the global and local transfer and the use of crossing loss for regularization are interesting. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. It is a little bit unfair for the setup of the baselines in the liver segmentation experiment. The CSGAN and ReMIC should be compared in the “image enrichment” setting also using nnU-net. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Using open access challenge data in the experiment Code is publicly available Training hyper-parameters were well documented Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html What is the training data for the shape controller? Please clarify what were the foreground and background for both the global image and the region ROI image. What makes the S-score on the T2w modality better than the other 2 modalities but segmentation performance significantly worse? Please discuss Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Interesting idea with through experiments What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors design a sophisticated loss combination, for shape consistency constraint in CT to MRI generation. All review comments are positive, considering the technical soundness and the validation of each claimed contribution. The authors are expected to share their code/model, which is exactly in line with the comments from all reviewers. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback We sincerely thank all the reviewers and ACs for the insightful comments and useful suggestions. We will share our code and model soon. Q1. Some details are not very clear; for example what exactly are the inputs to the G network? In Fig. 2 it is not clear how is ‘t’ encoded. is this a one hot encoding e.g. a 3XHxW tensor? (R1) A1: Because TarGAN is based on StarGAN, the forms of inputs to the G are the same as StarGAN. Specifically, in Fig 2, a target modality t is a one-hot code, it will be extent to HxW shape and concatenate with the image in second dimension. So, in Fig 2, the tensor shape of input to G is 3xHxW. Thanks your advice, we will consider adding the tensor shape to the figures and to the networks descriptions in the supplementary. Q2. The manuscript focuses on volumetric data translations and only during the experimental setup (Sec. 3.1) the reader is informed that the proposed method only works in 2D slices of 3D images. While there are indeed applications for this methodology, the authors should clarify this much earlier in the text. (R2) A2: We thank the reviewer for pointing out this issue. We will clarify this at the head of paper. Q3. What is the training data for the shape controller? Please clarify what were the foreground and background for both the global image and the region ROI image. (R3) A3: Thanks. The foreground of the global image or the region ROI image means the content area of image while the rest area is background. That is, an image inputs the shape controller, we want the shape controller to segment the content area. Q4. What makes the S-score on the T2w modality better than the other 2 modalities but segmentation performance significantly worse? Please discuss(R3) A4: Thank you for the detailed comments. You have raised an interesting point; however, we would like to clarify that there is no relationship between the S-score in Table 1 and the segmentation performance in Table 2 due to the different calculation ways. For example, T2w in S-score, we first train a segmentation network using real T2w images, then test it using the synthetic T2w images translated from CT or T1w. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Junxiao Chen, Jia Wei, Rui Li Abstract Paired multi-modality medical images, can provide complementary information to help physicians make more reasonable decisions than single modality medical images. But they are difficult to generate due to multiple factors in practice (e.g., time, cost, radiation dose). To address these problems, multi-modality medical image translation has aroused increasing research interest recently. However, the existing works mainly focus on translation effect of a whole image instead of a critical target area or Region of Interest (ROI), e.g., organ and so on. This leads to poor-quality translation of the localized target area which becomes blurry, deformed or even with extra unreasonable textures. In this paper, we propose a novel target-aware generative adversarial network called TarGAN, which is a generic multi-modality medical image translation model capable of (1) learning multi-modality medical image translation without relying on paired data, (2) enhancing quality of target area generation with the help of target area labels. The generator of TarGAN jointly learns mapping at two levels simultaneously — whole image translation mapping and target area translation mapping. These two mappings are interrelated through a proposed crossing loss. The experiments on both quantitative measures and qualitative evaluations demonstrate that TarGAN outperforms the state-of-the-art methods in all cases. Subsequent segmentation task is conducted to demonstrate effectiveness of synthetic images generated by TarGAN in a real-world application. Our code is available at https://github.com/cs-xiao/TarGAN. Link to paper https://doi.org/10.1007/978-3-030-87231-1_3 Link to the code repository https://github.com/cs-xiao/TarGAN Link to the dataset(s) https://chaos.grand-challenge.org/ Reviews Review #1 Please describe the contribution of the paper The paper presents a modality translation (CT to MRI) framework which is trained without any paired data using the now commonly used cycle consistency loss. The paper not only generates a translation at the whole image level but also generates a sub region e.g. a specific anatomical region; in this case the liver. It is argued that the focus on specific regions is able to generate higher quality images in these specific areas. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The combination of different losses like cycle consistency, modality classification and shape consistency seem appropriate for the task at hand. It is also interesting to make use a dataset whose original purpose was segmentation for a rather different task such as synthetic image generation. I appreciated the ablation studies showing the impact of the different types of loss fiunctions. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Some details are not very clear; for example what exactly are the inputs to the G network? it is written in Fig 2 that is a depthwise concatenation of the source image plus the target modality. Fomr the figure it seems to be a one hot encoding, but it is not very clear if it is just a full binary image encoding or some other way. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I really appreciated the authors went ahead and shared the code in an anonymous fashion using (anonymous.4open.science). However, the weights of the model are not available so it is not possible to reproduce exactly the experiments. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Fig. 2 it is not clear how is ‘t’ encoded. is this a one hot encoding e.g. a 3XHxW tensor? I would appreciate if the tensor shapes were added to the figures and to the networks descriptions in the suppl. material. It is very good to evaluate the generated images on a downstream task such as segmentation. However, it could good to train an MRI liver segmentation model only with real data, then using the CT as input generate a synthetic MRI. Now we have the segmentation of this MRI since it is the same as the one of the CT, we could then use the synthetic MRI and segment it with the MRI segmentation network (which has never seen synthetic data) and evaluate the performance this way. Like this we could see if a network trained only with real data would perform well segmenting synthetic data. I appreciated a lot the share of the source code; however, without it seems that only the training part is there, and without sharing the weights of the networks it is not possible to reproduce the results. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper presents an interesting combination of loss functions and to me it seems as a good idea to focus on specific anatomical regions. The fact os using the generated images on a downstream task such as segmentation is also very appropiate to evaluate the usefulness of the generated data. I would have given a higher score if the shared code had also included the weights of the models in order to reproduce the results better. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The manuscript proposes a method for multi-modal medical image translation based on Generative Adversarial Networks. While there is a plethora of architectures very similar to the proposed one since around 2018 for medical imaging, the authors modify the networks to explicitly enforce shape consistency via the use of binary segmentation masks from the source domain and improve translation quality by the use of the untraceable constraint. Experiments are performed on 2D slices of 3D data gathered from a relatively small public dataset composed of CTs and MRIs. An ablation study and comparison with baselines and reported and discussed in the experiments. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is overall well written with only very minor grammar issues. The methodology is technically sound and the insertion of multiple loss components and modules are both properly motivated in a theoretical sense and validated during ablation. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. TarGAN requires previous segmentation of the organs of interest in the source domain to enforce shape consistency, thus severely limiting the application of the method in real-world scenarios. The experimental setup only focuses on liver segmentation, while there are multiple other labeled CT datasets in the literature (i.e. https://zenodo.org/record/1169361#.X7IOsZ1KiV4, https://www.kaggle.com/user123454321/liversegtestimages/version/1). The manuscript focuses on volumetric data translations and only during the experimental setup (Sec. 3.1) the reader is informed that the proposed method only works in 2D slices of 3D images. While there are indeed applications for this methodology, the authors should clarify this much earlier in the text. Additionally, while 2D image-translation is a very well explored area in the literature of visual pattern recognition, including multi-modality image translation in medical imaging, 3D data presents a whole new set of difficulties, including the sharp increase in computational requirements due to 3D convolutions and the lack of pretrained models commonly used to enforce perceptual losses in modern 2D image translation architectures. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Authors propose an architecture of image-translation GAN composed of multiple modules, loss components, two discriminators and combinations of existing architectures (namely StarGAN and CycleGAN). In the manuscript (including supplementary material) there are not enough details regarding architectural choices, hyperparameters and implementation details to allow for replication of this study, as all of these aspects of the method would require extensive tuning. However, the authors marked that pretrained models, training and evaluation code would be made available, with an anonymized link for code in the supplementary material. If these codes are inserted in the camera ready version of the main text, there should be no major reproducibility concerns regarding this manuscript. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html A more thorough analysis in multiple organ segmentations using additional public datasets would be my main suggestion to the authors. While the method indeed presented promising results compared to other baselines in the literature, a proper validation of the methods effectiveness is not possible if only liver translation is conducted in the experiments. Inserting multiple datasets instead of focusing only on CHAOS (a rather small data scenario) would help to further validate the method in comparison with other baselines. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The manuscript does seem to provide an incremental gain in the literature of medical image translation using GANs. However, the experiments are way too narrow due to the choice of liver segmentation in a small dataset only. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper proposed a target-aware GAN for unpaired multi-modality medical image translation. The generator translate the global image and interested ROI jointly with the help of ROI labels. The method is based on starGAN with additional untraceable constraint loss, shape constraint loss and crossing loss to help the generator to learn. The generated results were found to be both of high quality in terms of FID score and beneficial to the down stream segmentation task. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Experiments were through. The paper not only provided image quality based metrics but also showed the usefulness of the generated image in downstream segmentation tasks. The two streams generator that handles both the global and local transfer and the use of crossing loss for regularization are interesting. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. It is a little bit unfair for the setup of the baselines in the liver segmentation experiment. The CSGAN and ReMIC should be compared in the “image enrichment” setting also using nnU-net. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Using open access challenge data in the experiment Code is publicly available Training hyper-parameters were well documented Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html What is the training data for the shape controller? Please clarify what were the foreground and background for both the global image and the region ROI image. What makes the S-score on the T2w modality better than the other 2 modalities but segmentation performance significantly worse? Please discuss Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Interesting idea with through experiments What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors design a sophisticated loss combination, for shape consistency constraint in CT to MRI generation. All review comments are positive, considering the technical soundness and the validation of each claimed contribution. The authors are expected to share their code/model, which is exactly in line with the comments from all reviewers. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback We sincerely thank all the reviewers and ACs for the insightful comments and useful suggestions. We will share our code and model soon. Q1. Some details are not very clear; for example what exactly are the inputs to the G network? In Fig. 2 it is not clear how is ‘t’ encoded. is this a one hot encoding e.g. a 3XHxW tensor? (R1) A1: Because TarGAN is based on StarGAN, the forms of inputs to the G are the same as StarGAN. Specifically, in Fig 2, a target modality t is a one-hot code, it will be extent to HxW shape and concatenate with the image in second dimension. So, in Fig 2, the tensor shape of input to G is 3xHxW. Thanks your advice, we will consider adding the tensor shape to the figures and to the networks descriptions in the supplementary. Q2. The manuscript focuses on volumetric data translations and only during the experimental setup (Sec. 3.1) the reader is informed that the proposed method only works in 2D slices of 3D images. While there are indeed applications for this methodology, the authors should clarify this much earlier in the text. (R2) A2: We thank the reviewer for pointing out this issue. We will clarify this at the head of paper. Q3. What is the training data for the shape controller? Please clarify what were the foreground and background for both the global image and the region ROI image. (R3) A3: Thanks. The foreground of the global image or the region ROI image means the content area of image while the rest area is background. That is, an image inputs the shape controller, we want the shape controller to segment the content area. Q4. What makes the S-score on the T2w modality better than the other 2 modalities but segmentation performance significantly worse? Please discuss(R3) A4: Thank you for the detailed comments. You have raised an interesting point; however, we would like to clarify that there is no relationship between the S-score in Table 1 and the segmentation performance in Table 2 due to the different calculation ways. For example, T2w in S-score, we first train a segmentation network using real T2w images, then test it using the synthetic T2w images translated from CT or T1w. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0602/12/31/Paper0121" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0602/12/31/Paper0121" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0602-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="TarGAN: Target-Aware Generative Adversarial Networks for Multi-modality Medical Image Translation" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0602/12/31/Paper0121"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0602/12/31/Paper0121","headline":"TarGAN: Target-Aware Generative Adversarial Networks for Multi-modality Medical Image Translation","dateModified":"0603-01-04T00:00:00-05:17","datePublished":"0602-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Junxiao Chen, Jia Wei, Rui Li Abstract Paired multi-modality medical images, can provide complementary information to help physicians make more reasonable decisions than single modality medical images. But they are difficult to generate due to multiple factors in practice (e.g., time, cost, radiation dose). To address these problems, multi-modality medical image translation has aroused increasing research interest recently. However, the existing works mainly focus on translation effect of a whole image instead of a critical target area or Region of Interest (ROI), e.g., organ and so on. This leads to poor-quality translation of the localized target area which becomes blurry, deformed or even with extra unreasonable textures. In this paper, we propose a novel target-aware generative adversarial network called TarGAN, which is a generic multi-modality medical image translation model capable of (1) learning multi-modality medical image translation without relying on paired data, (2) enhancing quality of target area generation with the help of target area labels. The generator of TarGAN jointly learns mapping at two levels simultaneously — whole image translation mapping and target area translation mapping. These two mappings are interrelated through a proposed crossing loss. The experiments on both quantitative measures and qualitative evaluations demonstrate that TarGAN outperforms the state-of-the-art methods in all cases. Subsequent segmentation task is conducted to demonstrate effectiveness of synthetic images generated by TarGAN in a real-world application. Our code is available at https://github.com/cs-xiao/TarGAN. Link to paper https://doi.org/10.1007/978-3-030-87231-1_3 Link to the code repository https://github.com/cs-xiao/TarGAN Link to the dataset(s) https://chaos.grand-challenge.org/ Reviews Review #1 Please describe the contribution of the paper The paper presents a modality translation (CT to MRI) framework which is trained without any paired data using the now commonly used cycle consistency loss. The paper not only generates a translation at the whole image level but also generates a sub region e.g. a specific anatomical region; in this case the liver. It is argued that the focus on specific regions is able to generate higher quality images in these specific areas. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The combination of different losses like cycle consistency, modality classification and shape consistency seem appropriate for the task at hand. It is also interesting to make use a dataset whose original purpose was segmentation for a rather different task such as synthetic image generation. I appreciated the ablation studies showing the impact of the different types of loss fiunctions. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Some details are not very clear; for example what exactly are the inputs to the G network? it is written in Fig 2 that is a depthwise concatenation of the source image plus the target modality. Fomr the figure it seems to be a one hot encoding, but it is not very clear if it is just a full binary image encoding or some other way. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I really appreciated the authors went ahead and shared the code in an anonymous fashion using (anonymous.4open.science). However, the weights of the model are not available so it is not possible to reproduce exactly the experiments. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Fig. 2 it is not clear how is ‘t’ encoded. is this a one hot encoding e.g. a 3XHxW tensor? I would appreciate if the tensor shapes were added to the figures and to the networks descriptions in the suppl. material. It is very good to evaluate the generated images on a downstream task such as segmentation. However, it could good to train an MRI liver segmentation model only with real data, then using the CT as input generate a synthetic MRI. Now we have the segmentation of this MRI since it is the same as the one of the CT, we could then use the synthetic MRI and segment it with the MRI segmentation network (which has never seen synthetic data) and evaluate the performance this way. Like this we could see if a network trained only with real data would perform well segmenting synthetic data. I appreciated a lot the share of the source code; however, without it seems that only the training part is there, and without sharing the weights of the networks it is not possible to reproduce the results. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper presents an interesting combination of loss functions and to me it seems as a good idea to focus on specific anatomical regions. The fact os using the generated images on a downstream task such as segmentation is also very appropiate to evaluate the usefulness of the generated data. I would have given a higher score if the shared code had also included the weights of the models in order to reproduce the results better. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The manuscript proposes a method for multi-modal medical image translation based on Generative Adversarial Networks. While there is a plethora of architectures very similar to the proposed one since around 2018 for medical imaging, the authors modify the networks to explicitly enforce shape consistency via the use of binary segmentation masks from the source domain and improve translation quality by the use of the untraceable constraint. Experiments are performed on 2D slices of 3D data gathered from a relatively small public dataset composed of CTs and MRIs. An ablation study and comparison with baselines and reported and discussed in the experiments. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is overall well written with only very minor grammar issues. The methodology is technically sound and the insertion of multiple loss components and modules are both properly motivated in a theoretical sense and validated during ablation. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. TarGAN requires previous segmentation of the organs of interest in the source domain to enforce shape consistency, thus severely limiting the application of the method in real-world scenarios. The experimental setup only focuses on liver segmentation, while there are multiple other labeled CT datasets in the literature (i.e. https://zenodo.org/record/1169361#.X7IOsZ1KiV4, https://www.kaggle.com/user123454321/liversegtestimages/version/1). The manuscript focuses on volumetric data translations and only during the experimental setup (Sec. 3.1) the reader is informed that the proposed method only works in 2D slices of 3D images. While there are indeed applications for this methodology, the authors should clarify this much earlier in the text. Additionally, while 2D image-translation is a very well explored area in the literature of visual pattern recognition, including multi-modality image translation in medical imaging, 3D data presents a whole new set of difficulties, including the sharp increase in computational requirements due to 3D convolutions and the lack of pretrained models commonly used to enforce perceptual losses in modern 2D image translation architectures. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Authors propose an architecture of image-translation GAN composed of multiple modules, loss components, two discriminators and combinations of existing architectures (namely StarGAN and CycleGAN). In the manuscript (including supplementary material) there are not enough details regarding architectural choices, hyperparameters and implementation details to allow for replication of this study, as all of these aspects of the method would require extensive tuning. However, the authors marked that pretrained models, training and evaluation code would be made available, with an anonymized link for code in the supplementary material. If these codes are inserted in the camera ready version of the main text, there should be no major reproducibility concerns regarding this manuscript. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html A more thorough analysis in multiple organ segmentations using additional public datasets would be my main suggestion to the authors. While the method indeed presented promising results compared to other baselines in the literature, a proper validation of the methods effectiveness is not possible if only liver translation is conducted in the experiments. Inserting multiple datasets instead of focusing only on CHAOS (a rather small data scenario) would help to further validate the method in comparison with other baselines. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The manuscript does seem to provide an incremental gain in the literature of medical image translation using GANs. However, the experiments are way too narrow due to the choice of liver segmentation in a small dataset only. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper proposed a target-aware GAN for unpaired multi-modality medical image translation. The generator translate the global image and interested ROI jointly with the help of ROI labels. The method is based on starGAN with additional untraceable constraint loss, shape constraint loss and crossing loss to help the generator to learn. The generated results were found to be both of high quality in terms of FID score and beneficial to the down stream segmentation task. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Experiments were through. The paper not only provided image quality based metrics but also showed the usefulness of the generated image in downstream segmentation tasks. The two streams generator that handles both the global and local transfer and the use of crossing loss for regularization are interesting. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. It is a little bit unfair for the setup of the baselines in the liver segmentation experiment. The CSGAN and ReMIC should be compared in the “image enrichment” setting also using nnU-net. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Using open access challenge data in the experiment Code is publicly available Training hyper-parameters were well documented Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html What is the training data for the shape controller? Please clarify what were the foreground and background for both the global image and the region ROI image. What makes the S-score on the T2w modality better than the other 2 modalities but segmentation performance significantly worse? Please discuss Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Interesting idea with through experiments What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors design a sophisticated loss combination, for shape consistency constraint in CT to MRI generation. All review comments are positive, considering the technical soundness and the validation of each claimed contribution. The authors are expected to share their code/model, which is exactly in line with the comments from all reviewers. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback We sincerely thank all the reviewers and ACs for the insightful comments and useful suggestions. We will share our code and model soon. Q1. Some details are not very clear; for example what exactly are the inputs to the G network? In Fig. 2 it is not clear how is ‘t’ encoded. is this a one hot encoding e.g. a 3XHxW tensor? (R1) A1: Because TarGAN is based on StarGAN, the forms of inputs to the G are the same as StarGAN. Specifically, in Fig 2, a target modality t is a one-hot code, it will be extent to HxW shape and concatenate with the image in second dimension. So, in Fig 2, the tensor shape of input to G is 3xHxW. Thanks your advice, we will consider adding the tensor shape to the figures and to the networks descriptions in the supplementary. Q2. The manuscript focuses on volumetric data translations and only during the experimental setup (Sec. 3.1) the reader is informed that the proposed method only works in 2D slices of 3D images. While there are indeed applications for this methodology, the authors should clarify this much earlier in the text. (R2) A2: We thank the reviewer for pointing out this issue. We will clarify this at the head of paper. Q3. What is the training data for the shape controller? Please clarify what were the foreground and background for both the global image and the region ROI image. (R3) A3: Thanks. The foreground of the global image or the region ROI image means the content area of image while the rest area is background. That is, an image inputs the shape controller, we want the shape controller to segment the content area. Q4. What makes the S-score on the T2w modality better than the other 2 modalities but segmentation performance significantly worse? Please discuss(R3) A4: Thank you for the detailed comments. You have raised an interesting point; however, we would like to clarify that there is no relationship between the S-score in Table 1 and the segmentation performance in Table 2 due to the different calculation ways. For example, T2w in S-score, we first train a segmentation network using real T2w images, then test it using the synthetic T2w images translated from CT or T1w. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Chen, Junxiao,Wei, Jia,Li, Rui" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>TarGAN: Target-Aware Generative Adversarial Networks for Multi-modality Medical Image Translation</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Image Reconstruction"
        class="post-category">
        Image Reconstruction
      </a>
      
      <a 
        href="kittywong/categories#Image Segmentation"
        class="post-category">
        Image Segmentation
      </a>
      
      <a 
        href="kittywong/categories#Modalities - CT"
        class="post-category">
        Modalities - CT
      </a>
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Chen, Junxiao"
        class="post-tags">
        Chen, Junxiao
      </a> |  
      
      <a href="kittywong/tags#Wei, Jia"
        class="post-tags">
        Wei, Jia
      </a> |  
      
      <a href="kittywong/tags#Li, Rui"
        class="post-tags">
        Li, Rui
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Junxiao Chen, Jia Wei, Rui Li
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Paired multi-modality medical images, can provide complementary information to help physicians make more reasonable decisions than single modality medical images. But they are difficult to generate due to multiple factors in practice (e.g., time, cost, radiation dose). To address these problems, multi-modality medical image translation has aroused increasing research interest recently. However, the existing works mainly focus on translation effect of a whole image instead of a
critical target area or Region of Interest (ROI), e.g., organ and so on. This leads to poor-quality translation of the localized target area which becomes blurry, deformed or even with extra unreasonable textures. In this paper, we propose a novel target-aware generative adversarial network called TarGAN, which is a generic multi-modality medical image translation model capable of (1) learning multi-modality medical image translation without relying on paired data, (2) enhancing quality of target area generation with the help of target area labels. The generator of TarGAN jointly learns mapping at two levels simultaneously — whole
image translation mapping and target area translation mapping. These two mappings are interrelated through a proposed crossing loss. The experiments on both quantitative measures and qualitative evaluations demonstrate that TarGAN outperforms the state-of-the-art methods in all cases. Subsequent segmentation task is conducted to demonstrate effectiveness of synthetic images generated by TarGAN in a real-world application. Our code is available at https://github.com/cs-xiao/TarGAN.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87231-1_3">https://doi.org/10.1007/978-3-030-87231-1_3</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/cs-xiao/TarGAN
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>https://chaos.grand-challenge.org/
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper presents a modality translation (CT to MRI) framework which is trained without any paired data using the now commonly used cycle consistency loss. The paper not only generates a translation at the whole image level but also generates a sub region e.g. a specific anatomical region; in this case the liver. It is argued that the focus on specific regions is able to generate higher quality images in these specific areas.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The combination of different losses like cycle consistency, modality classification and shape consistency seem appropriate for the task at hand. It is also interesting to make use a dataset whose original purpose was segmentation for a rather different task such as synthetic image generation. I appreciated the ablation studies showing the impact of the different types of loss fiunctions.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>Some details are not very clear; for example what exactly are the inputs to the G network? it is written in Fig 2 that is a depthwise concatenation of the source image plus the target modality. Fomr the figure it seems to be a one hot encoding, but it is not very clear if it is just a full binary image encoding or some other way.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>I really appreciated the authors went ahead and shared the code in an anonymous fashion using (anonymous.4open.science). However, the weights of the model are not available so it is not possible to reproduce exactly the experiments.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>In Fig. 2 it is not clear how is ‘t’ encoded. is this a one hot encoding e.g. a 3XHxW tensor? I would appreciate if the tensor shapes were added to the figures and to the networks descriptions in the suppl. material.</p>

      <p>It is very good to evaluate the generated images on a downstream task such as segmentation. However, it could good to train an MRI liver segmentation model only with real data, then using the CT as input generate a synthetic MRI. Now we have the segmentation of this MRI since it is the same as the one of the CT, we could then use the synthetic MRI and segment it with the MRI segmentation network (which has never seen synthetic data) and evaluate the performance this way. Like this we could see if a network trained only with real data would perform well segmenting synthetic data.</p>

      <p>I appreciated a lot the share of the source code; however, without it seems that only the training part is there, and without sharing the weights of the networks it is not possible to reproduce the results.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper presents an interesting combination of loss functions and to me it seems as a good idea to focus on specific anatomical regions. The fact os using the generated images on a downstream task such as segmentation is also very appropiate to evaluate the usefulness of the generated data. 
I would have given a higher score if the shared code had also included the weights of the models in order to reproduce the results better.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The manuscript proposes a method for multi-modal medical image translation based on Generative Adversarial Networks. While there is a plethora of architectures very similar to the proposed one since around 2018 for medical imaging, the authors modify the networks to explicitly enforce shape consistency via the use of binary segmentation masks from the source domain and improve translation quality by the use of the untraceable constraint. Experiments are performed on 2D slices of 3D data gathered from a relatively small public dataset composed of CTs and MRIs. An ablation study and comparison with baselines and reported and discussed in the experiments.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The paper is overall well written with only very minor grammar issues. The methodology is technically sound and the insertion of multiple loss components and modules are both properly motivated in a theoretical sense and validated during ablation.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>TarGAN requires previous segmentation of the organs of interest in the source domain to enforce shape consistency, thus severely limiting the application of the method in real-world scenarios.</p>

      <p>The experimental setup only focuses on liver segmentation, while there are multiple other labeled CT datasets in the literature (i.e. https://zenodo.org/record/1169361#.X7IOsZ1KiV4, https://www.kaggle.com/user123454321/liversegtestimages/version/1).</p>

      <p>The manuscript focuses on volumetric data translations and only during the experimental setup (Sec. 3.1) the reader is informed that the proposed method only works in 2D slices of 3D images. While there are indeed applications for this methodology, the authors should clarify this <em>much</em> earlier in the text. Additionally, while 2D image-translation is a very well explored area in the literature of visual pattern recognition, including multi-modality image translation in medical imaging, 3D data presents a whole new set of difficulties, including the sharp increase in computational requirements due to 3D convolutions and the lack of pretrained models commonly used to enforce perceptual losses in modern 2D image translation architectures.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Authors propose an architecture of image-translation GAN composed of multiple modules, loss components, two discriminators and combinations of existing architectures (namely StarGAN and CycleGAN). In the manuscript (including supplementary material) there are not enough details regarding architectural choices, hyperparameters and implementation details to allow for replication of this study, as all of these aspects of the method would require extensive tuning. However, the authors marked that pretrained models, training and evaluation code would be made available, with an anonymized link for code in the supplementary material. If these codes are inserted in the camera ready version of the main text, there should be no major reproducibility concerns regarding this manuscript.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>A more thorough analysis in multiple organ segmentations using additional public datasets would be my main suggestion to the authors. While the method indeed presented promising results compared to other baselines in the literature, a proper validation of the methods effectiveness is not possible if only liver translation is conducted in the experiments. Inserting multiple datasets instead of focusing only on CHAOS (a rather small data scenario) would help to further validate the method in comparison with other baselines.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The manuscript does seem to provide an incremental gain in the literature of medical image translation using GANs. However, the experiments are way too narrow due to the choice of liver segmentation in a small dataset only.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper proposed a target-aware GAN for unpaired multi-modality medical image translation. The generator translate the global image and interested ROI jointly with the help of ROI labels. The method is based on starGAN with additional untraceable constraint loss, shape constraint loss and crossing loss to help the generator to learn. The generated results were found to be both of high quality in terms of FID score and beneficial to the down stream  segmentation task.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>
          <p>Experiments were through. The paper not only provided image quality based metrics but also showed the usefulness of the generated image in downstream segmentation tasks.</p>
        </li>
        <li>
          <p>The two streams generator that handles both the global and local transfer and the use of crossing loss for regularization are interesting.</p>
        </li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>It is a little bit unfair for the setup of the baselines in the liver segmentation experiment. The CSGAN and ReMIC should  be compared in the “image enrichment” setting also using nnU-net.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <ul>
        <li>Using open access challenge data in the experiment</li>
        <li>
          <p>Code is publicly available</p>
        </li>
        <li>Training hyper-parameters were well documented</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ul>
        <li>
          <p>What is the training data for the shape controller? Please clarify what were the foreground and background for both the global image and the region ROI image.</p>
        </li>
        <li>
          <p>What makes the S-score on the T2w modality better than the other  2 modalities but segmentation performance significantly worse? Please discuss</p>
        </li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Interesting idea with through experiments</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>The authors design a sophisticated loss combination, for shape consistency constraint in CT to MRI generation. All review comments are positive, considering the technical soundness and the validation of each claimed contribution. The authors are expected to share their code/model, which is exactly in line with the comments from all reviewers.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We sincerely thank all the reviewers and ACs for the insightful comments and useful suggestions. We will share our code and model soon.
Q1. Some details are not very clear; for example what exactly are the inputs to the G network? In Fig. 2 it is not clear how is ‘t’ encoded. is this a one hot encoding e.g. a 3XHxW tensor? (R1)
A1: Because TarGAN is based on StarGAN, the forms of inputs to the G are the same as  StarGAN. Specifically, in Fig 2, a target modality t is a one-hot code, it will be extent to HxW shape and concatenate with the image in second dimension. So, in Fig 2, the tensor shape of input to G is 3xHxW. Thanks your advice, we will consider adding the tensor shape to the figures and to the networks descriptions in the supplementary.
Q2. The manuscript focuses on volumetric data translations and only during the experimental setup (Sec. 3.1) the reader is informed that the proposed method only works in 2D slices of 3D images. While there are indeed applications for this methodology, the authors should clarify this <em>much</em> earlier in the text. (R2)
A2: We thank the reviewer for pointing out this issue. We will clarify this at the head of paper.
Q3. What is the training data for the shape controller? Please clarify what were the foreground and background for both the global image and the region ROI image. (R3)
A3: Thanks. The foreground of the global image or the region ROI image means the content area of image while the rest area is background. That is, an image inputs the shape controller, we want the shape controller to segment the content area. 
Q4. What makes the S-score on the T2w modality better than the other 2 modalities but segmentation performance significantly worse? Please discuss(R3)
A4: Thank you for the detailed comments. You have raised an interesting point; however, we would like to clarify that there is no relationship between the S-score in Table 1 and the segmentation performance in Table 2 due to the different calculation ways. For example, T2w in S-score, we first train a segmentation network using real T2w images, then test it using the synthetic T2w images translated from CT or T1w.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0602-12-31
      -->
      <!--
      
        ,
        updated at 
        0603-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Image Reconstruction"
        class="post-category">
        Image Reconstruction
      </a> |
      
      <a 
        href="kittywong/categories#Image Segmentation"
        class="post-category">
        Image Segmentation
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - CT"
        class="post-category">
        Modalities - CT
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Chen, Junxiao"
        class="post-category">
        Chen, Junxiao
      </a> |  
      
      <a href="kittywong/tags#Wei, Jia"
        class="post-category">
        Wei, Jia
      </a> |  
      
      <a href="kittywong/tags#Li, Rui"
        class="post-category">
        Li, Rui
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0603/12/31/Paper0181">
          Synthesizing Multi-Tracer PET Images for Alzheimer's Disease Patients using a 3D Unified Anatomy-aware Cyclic Adversarial Network
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0601/12/31/Paper0096">
          Over-and-Under Complete Convolutional RNN for MRI Reconstruction
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
