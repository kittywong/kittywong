<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Improving the Explainability of Skin Cancer Diagnosis Using CBIR | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Improving the Explainability of Skin Cancer Diagnosis Using CBIR" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Catarina Barata, Carlos Santiago Abstract Explainability is a key feature for computer-aided diagnosis systems. This property not only helps doctors understand their decisions, but also allows less experienced practitioners to improve their knowledge. Skin cancer diagnosis is a field where explainability is of critical importance, as lesions of different classes often exhibit confounding characteristics. This work proposes a deep neural network (DNN) for skin cancer diagnosis that provides explainability through content-based image retrieval. We explore several state-of-the-art approaches to improve the feature space learned by the DNN, namely contrastive, distillation, and triplet losses. We demonstrate that the combination of these regularization losses with the categorical cross-entropy leads to the best performances on melanoma classification, and results in a hybrid DNN that simultaneously: i) classifies the images; and ii) retrieves similar images justifying the diagnosis. The code is available at \url{https://github.com/catarina-barata/CBIR_Explainability_Skin_Cancer}. Link to paper https://doi.org/10.1007/978-3-030-87199-4_52 Link to the code repository https://github.com/catarina-barata/CBIR_Explainability_Skin_Cancer Link to the dataset(s) https://challenge.isic-archive.com/data Reviews Review #1 Please describe the contribution of the paper This paper explores explainability of image-based skin cancer diagnosis by incorporating a content-based image retrieval (CBIR) module that provides the user with similar images and their diagnoses. As motivation, the paper notes that latent spaces trained for classification often do not perform well for image retrieval, and apply additional losses optimizing to strengthen separability in the learned latent space, via a triplet loss, a contrastive loss and a distillation loss. Results are validated on ISIC, which is a good size dataset. Improvements are moderate. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea is logical and easy to follow. Seems to have some effect on output, but not huge. There is a visible improvement in the latent space visualizations. Paper is relatively easy to read. I like the comparison with multiple architectures Empirical performance looks good Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The methodological novelty is limited The idea of using CBIR to provide explainability is not new, as this was done in [1]. It does not seem that the authors compare to [1], which differs from the current approach in that the triplet loss for CBIR was trained separately, not jointly with the diagnostic predictor Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Reproducibility is fine. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see above. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is nice but has limited novelty. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper tackles the problem of explainability of modern deep learning techniques by using a content based image retrieval (CBIR) approach. This solution allows the authors to account for two of the main design needs for methodological translation to the clinic among other listed applications: (i) reliability and (ii) visualization interface. The approach presented here incorporates regularization losses to the standard cross-entropy loss for prediction: triplet loss, contrastive loss and distillation loss. They are able to show that these three losses forces the latent space learned by the neural network to have a better structure (e.g., examples from the same class lie closer to each other and further to other classes). This helps the CBIR step and, in a positive feedback loop, the result from the CBIR step can be used to generate an aggregate score (e.g., majority voting) between the selected images that improves upon the original classification loop. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strenghts of the paper can be summarized as: Very well written manuscript and pleasant to read. The problem is properly introduced and motivated, the goals and contributions are clearly stated and the methodology well explained. Training and evaluation details are also detailed. A novel formulation for explainability in skin cancer: they incorporate to the standard classification scheme a CBIR module that will retrieve the most similar images to the input. This is specially interesting for helping doctors in the clinic and train less experienced practitioners. Extensive testing of different configuration of framework: hyperparameters, losses. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness of this work may be the presentation of the results: In Table ,1 I believe @GAP means that just one-head of the network is used but clarification is needed. It’s not entirely clear why the combination of the three losses is not shown in Table 1. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have committed to make the code publicly available on acceptance. They also provide a link to the training data. Training strategy is explained and hyperparameter selection technique is also detailed. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I have just some minor comments: If space allows, a simple phrase defining what is a positive and a negative examples for the triplet loss would make the reading a bit clearer. 2.- Fig.3 can be improved as legend and camptions are hardly readable. Some suggestions are: increasing font size, placing all shared legends (per row) outside the plot and using seaborn package, include a grid 3.-. Fig.4 caption can be placed outside the plots as it is shared for all 3 subfigures. 4.- In the caption of Fig.5 a sentence indicating what each row shows (no regularization/regularzation losses) may be needed. Suggestion on future work: after going through the images provided in the results section, it would be very interesting to see how this methods performs in tasks such as outlier detection or detecting noise in labels. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? In my opinion, this is a strong accept. The paper is very well written and pleasant to read. The methodological approach is properly presented and design decisions are justified or analysed in the experiments section. Finally, I believe that the benefis of the method as well as the potential clinical impact is of interest to the medical imaging community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper proposes to use a deep learning method to help on skin cancer diagnosis. The method simultaneously performed a classification of images and retrieve similar images justifying the diagnosis. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strength of the paper is the capability of the model to perform simultaneously the classification and the retrieval. The second strength is the combination of the several loss functions including some who are generally used for self-supervised methods. Each of these functions is very well explained. Moreover, the result part provided a comparison of the different loss functions. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness of the paper is the lack of clarity of the results. It is unclear if only one type of loss function is used or it is a combination of multiple loss functions. Table 1 presents results for loss distillation loss, we could suppose it is results with only this loss function, but on the text, it is indicated “when the regularization losses are incorporated…”. Moreover, what is the significance of “@GAP” on this Table? Finally, the results seem not provided the performance of the use of the four losses. The title and keywords of the paper suggest a focus on the explainability of the network. The content-based image retrieval module provides an interesting tool to understand which factors lead to the classification and on which parameters the images are considered similar. However, the paper does not exploit this sufficiently to talk about the explainability of the network. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper provided all information to reproduce the works, including the range of hyperparameter used to compute the global loss function. However, the exact values used to compute the results are not specified. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Fig 3 is difficult to read for several reasons. First, the legend and the axis values are not readable. Increase the size of the axis would help to the clarity. If the legend can be having a higher size, it would be possible to provide it in the figure caption. Secondly, currently, each subfigure has is how range, which does not allow easy comparison between each network. In the rest of the paper, the authors only focused on DenseNET-121, one other solution could be to only present the result for this network and provided the other one as supplementary material. The range of hyperparameters test is very complete, however, it is not specified for with values the results are given, except for tau_d in table 1. One of the evaluation metrics is precision@K, it is not clear what it refers. It is the mean precision on the K images provides by the CBIR? Fig4 presents the embedding space obtained with each loss with DenseNet-121. The authors compared it to the left part of Fig1. However, it is not clear from with model the embedding space is obtained on FIG1. This needs to be clarified when Fig1 is mentioned. Due to the unavailability of the test set ground truth, the authors are not able to compute the recall and the precision@K. But it is not clear which metric is used in Table 1 for the test set. Moreover, to allow comparison with the result on validation it could be interesting to compute the same metric on the validation set. The authors present a global loss function, but this one is not used in the result section. In Fig 5, it is not clear with the row is corresponds with or without regularization. The authors make reference to Fig4 for color legend. On this figure the legend is small. It would be better to refer to Fig1 for these booth figures. In section 3.1, there is an empty reference after de data set splitting. Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This paper needs a lot of improvements to be accepted especially in the results section and on the exploitation of explainability. The strengths do not compensate the weakness. What is the ranking of this paper in your review stack? 6 Number of papers in your stack 7 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper explores the explainability of image-based skin cancer diagnosis by incorporating a content-based image retrieval (CBIR) module, which is logical and easy to follow. As the main concern, the methodological novelty is limited and the results of this paper are not described clearly. For example, It is unclear if only one type of loss function is used or it is a combination of multiple loss functions. Also, the results seem not provided the performance of the use of the four losses. As the used four losses, this article does not introduce their specific functions in detail to explain which problem is to be solved in the classification task. According to the comments of the reviewers, the author is requested to revise the reviewers’ comments point by point. Therfore, a rebuttal is appropriate. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper simultaneously performs the classification of images and retrieves similar images for skin cancer diagnosis. By this way, it can help doctors understand their decisions, and allows less experienced practitioners to improve their knowledge. Thus, this method has important clinical application value. In addition, the author has clarified the main issues raised by the reviewers such as adding the results for the missing combinations of Eq. (2) in supplementary material and improving the readability of Fig. 3 and the captions of Table 1. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. One of the concerns for this paper is that the methodological novelty is limited. The rebuttal does not address this point. However it addresses all other concerns. Especially those of the loss function. This paper presents an important clinical application and is thus appropriate for MICCAI. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The problem of improving the explainability of CNN is an important topic for skin cancer diagnosis. Unfortunatley, the motivation of using 4 losses is still unclear; the authors did not clearly explain nor discuss how / why such losses (e.g., contrastive and distillation loss) improves the skin cancer diagnosis. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 14 Author Feedback We would like to thank the reviewers for their assessment of the paper and constructive suggestions. Below we address the major concerns and clarify misunderstandings. We acknowledge that the manuscript was not clear regarding the results and the loss function used to train the model, mainly due to Table 1 and Fig. 3. First, we would like to clarify the meaning of “@GAP” in the results. As described in the last paragraph of Section 2.1, the proposed model simultaneously performs classification and CBIR. The former is provided by the “FC Classifier” head in Fig. 2, while for the latter we explored two alternatives: a) using the “FC Embedding” head (which is only used when the Triplet and/or Contrastive loss are considered); or b) using the embeddings given by the GAP layer. In the results section, we referred to option “b)” as @GAP, and we found this to lead to the best performing and more stable CBIR, when compared to the “FC Embedding”, as shown in Fig. 3. Nevertheless, we will add a more detailed quantitative comparison (as in Table 1) between these two options in supplementary material. Second, the global loss specified in eq (2) comprises a term (L_CE) which is always used to train the classifier, and three regularization terms whose contribution is controlled by the hyperparameters (α,β, γ). In Fig. 3, the first row shows the results of combining L_CE with each regularization term (e.g., =1,β=0, γ=0). The second row shows the results of combining multiple regularization terms, including using all four (in brown and pink), illustrating their synergistic effect. Table 1 shows the following results: classification loss alone (L_CE), the best combination of loss terms (L_CE + L_Cont + L_Dist) and their individual counterparts (L_CE + L_Dist and L_CE + L_Cont). In all these cases, the CBIR was obtained using the embeddings of the GAP layer (“@GAP” was mistakenly missing in the first two columns). We will address these issues by following the reviewers’ suggestions, namely: 1) adding the results for the missing combinations of terms in eq (2) – in supplementary material, due to the lack of space in the main document; 2) clarifying the definition of eq (2) in Section 2.1; and 3) improving readability of Fig. 3 and the captions of Table 1. We agree with the reviewer that exploring CBIR for explainability is not new. In fact, Section 1 identifies previous works that use CBIR on CNN features learned for classification tasks. However, they led to conflicting results about the utility of CBIR. In our paper, we argue that this arises from the lack of structure of the feature space learned for a diagnostic task, which does not account for similarities between lesions, as exemplified in Fig. 1. To the best of our knowledge, ours is the first work to address this issue by augmenting the standard cross-entropy loss function with regularization terms. This encourages the model to also learn similarities between dermoscopy images. Fig. 3 and Table 1 demonstrate the validity of our formulation over the standard CBIR performed on features learned for the classification task (L_CE column vs. the remaining). We show that using regularization terms improves both the class recall and precision@K metrics. Additionally, two of the regularization losses explored in our work (contrastive and distillation) have never been explored as mechanisms to improve the CBIR in skin cancer. The third loss (triplet) has only been used in [1], which is conceptually different from our work. Their work aims to develop a model for retrieval only, while our model is capable of performing retrieval and classification simultaneously, as mentioned in the last paragraph of page 2 (Section 1). Our results also showed that triplet loss is the least suitable strategy to improve CBIR when using small batch sizes. We will make the differences between our work and [1] clearer in the paper. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Catarina Barata, Carlos Santiago Abstract Explainability is a key feature for computer-aided diagnosis systems. This property not only helps doctors understand their decisions, but also allows less experienced practitioners to improve their knowledge. Skin cancer diagnosis is a field where explainability is of critical importance, as lesions of different classes often exhibit confounding characteristics. This work proposes a deep neural network (DNN) for skin cancer diagnosis that provides explainability through content-based image retrieval. We explore several state-of-the-art approaches to improve the feature space learned by the DNN, namely contrastive, distillation, and triplet losses. We demonstrate that the combination of these regularization losses with the categorical cross-entropy leads to the best performances on melanoma classification, and results in a hybrid DNN that simultaneously: i) classifies the images; and ii) retrieves similar images justifying the diagnosis. The code is available at \url{https://github.com/catarina-barata/CBIR_Explainability_Skin_Cancer}. Link to paper https://doi.org/10.1007/978-3-030-87199-4_52 Link to the code repository https://github.com/catarina-barata/CBIR_Explainability_Skin_Cancer Link to the dataset(s) https://challenge.isic-archive.com/data Reviews Review #1 Please describe the contribution of the paper This paper explores explainability of image-based skin cancer diagnosis by incorporating a content-based image retrieval (CBIR) module that provides the user with similar images and their diagnoses. As motivation, the paper notes that latent spaces trained for classification often do not perform well for image retrieval, and apply additional losses optimizing to strengthen separability in the learned latent space, via a triplet loss, a contrastive loss and a distillation loss. Results are validated on ISIC, which is a good size dataset. Improvements are moderate. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea is logical and easy to follow. Seems to have some effect on output, but not huge. There is a visible improvement in the latent space visualizations. Paper is relatively easy to read. I like the comparison with multiple architectures Empirical performance looks good Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The methodological novelty is limited The idea of using CBIR to provide explainability is not new, as this was done in [1]. It does not seem that the authors compare to [1], which differs from the current approach in that the triplet loss for CBIR was trained separately, not jointly with the diagnostic predictor Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Reproducibility is fine. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see above. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is nice but has limited novelty. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper tackles the problem of explainability of modern deep learning techniques by using a content based image retrieval (CBIR) approach. This solution allows the authors to account for two of the main design needs for methodological translation to the clinic among other listed applications: (i) reliability and (ii) visualization interface. The approach presented here incorporates regularization losses to the standard cross-entropy loss for prediction: triplet loss, contrastive loss and distillation loss. They are able to show that these three losses forces the latent space learned by the neural network to have a better structure (e.g., examples from the same class lie closer to each other and further to other classes). This helps the CBIR step and, in a positive feedback loop, the result from the CBIR step can be used to generate an aggregate score (e.g., majority voting) between the selected images that improves upon the original classification loop. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strenghts of the paper can be summarized as: Very well written manuscript and pleasant to read. The problem is properly introduced and motivated, the goals and contributions are clearly stated and the methodology well explained. Training and evaluation details are also detailed. A novel formulation for explainability in skin cancer: they incorporate to the standard classification scheme a CBIR module that will retrieve the most similar images to the input. This is specially interesting for helping doctors in the clinic and train less experienced practitioners. Extensive testing of different configuration of framework: hyperparameters, losses. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness of this work may be the presentation of the results: In Table ,1 I believe @GAP means that just one-head of the network is used but clarification is needed. It’s not entirely clear why the combination of the three losses is not shown in Table 1. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have committed to make the code publicly available on acceptance. They also provide a link to the training data. Training strategy is explained and hyperparameter selection technique is also detailed. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I have just some minor comments: If space allows, a simple phrase defining what is a positive and a negative examples for the triplet loss would make the reading a bit clearer. 2.- Fig.3 can be improved as legend and camptions are hardly readable. Some suggestions are: increasing font size, placing all shared legends (per row) outside the plot and using seaborn package, include a grid 3.-. Fig.4 caption can be placed outside the plots as it is shared for all 3 subfigures. 4.- In the caption of Fig.5 a sentence indicating what each row shows (no regularization/regularzation losses) may be needed. Suggestion on future work: after going through the images provided in the results section, it would be very interesting to see how this methods performs in tasks such as outlier detection or detecting noise in labels. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? In my opinion, this is a strong accept. The paper is very well written and pleasant to read. The methodological approach is properly presented and design decisions are justified or analysed in the experiments section. Finally, I believe that the benefis of the method as well as the potential clinical impact is of interest to the medical imaging community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper proposes to use a deep learning method to help on skin cancer diagnosis. The method simultaneously performed a classification of images and retrieve similar images justifying the diagnosis. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strength of the paper is the capability of the model to perform simultaneously the classification and the retrieval. The second strength is the combination of the several loss functions including some who are generally used for self-supervised methods. Each of these functions is very well explained. Moreover, the result part provided a comparison of the different loss functions. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness of the paper is the lack of clarity of the results. It is unclear if only one type of loss function is used or it is a combination of multiple loss functions. Table 1 presents results for loss distillation loss, we could suppose it is results with only this loss function, but on the text, it is indicated “when the regularization losses are incorporated…”. Moreover, what is the significance of “@GAP” on this Table? Finally, the results seem not provided the performance of the use of the four losses. The title and keywords of the paper suggest a focus on the explainability of the network. The content-based image retrieval module provides an interesting tool to understand which factors lead to the classification and on which parameters the images are considered similar. However, the paper does not exploit this sufficiently to talk about the explainability of the network. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper provided all information to reproduce the works, including the range of hyperparameter used to compute the global loss function. However, the exact values used to compute the results are not specified. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Fig 3 is difficult to read for several reasons. First, the legend and the axis values are not readable. Increase the size of the axis would help to the clarity. If the legend can be having a higher size, it would be possible to provide it in the figure caption. Secondly, currently, each subfigure has is how range, which does not allow easy comparison between each network. In the rest of the paper, the authors only focused on DenseNET-121, one other solution could be to only present the result for this network and provided the other one as supplementary material. The range of hyperparameters test is very complete, however, it is not specified for with values the results are given, except for tau_d in table 1. One of the evaluation metrics is precision@K, it is not clear what it refers. It is the mean precision on the K images provides by the CBIR? Fig4 presents the embedding space obtained with each loss with DenseNet-121. The authors compared it to the left part of Fig1. However, it is not clear from with model the embedding space is obtained on FIG1. This needs to be clarified when Fig1 is mentioned. Due to the unavailability of the test set ground truth, the authors are not able to compute the recall and the precision@K. But it is not clear which metric is used in Table 1 for the test set. Moreover, to allow comparison with the result on validation it could be interesting to compute the same metric on the validation set. The authors present a global loss function, but this one is not used in the result section. In Fig 5, it is not clear with the row is corresponds with or without regularization. The authors make reference to Fig4 for color legend. On this figure the legend is small. It would be better to refer to Fig1 for these booth figures. In section 3.1, there is an empty reference after de data set splitting. Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This paper needs a lot of improvements to be accepted especially in the results section and on the exploitation of explainability. The strengths do not compensate the weakness. What is the ranking of this paper in your review stack? 6 Number of papers in your stack 7 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper explores the explainability of image-based skin cancer diagnosis by incorporating a content-based image retrieval (CBIR) module, which is logical and easy to follow. As the main concern, the methodological novelty is limited and the results of this paper are not described clearly. For example, It is unclear if only one type of loss function is used or it is a combination of multiple loss functions. Also, the results seem not provided the performance of the use of the four losses. As the used four losses, this article does not introduce their specific functions in detail to explain which problem is to be solved in the classification task. According to the comments of the reviewers, the author is requested to revise the reviewers’ comments point by point. Therfore, a rebuttal is appropriate. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper simultaneously performs the classification of images and retrieves similar images for skin cancer diagnosis. By this way, it can help doctors understand their decisions, and allows less experienced practitioners to improve their knowledge. Thus, this method has important clinical application value. In addition, the author has clarified the main issues raised by the reviewers such as adding the results for the missing combinations of Eq. (2) in supplementary material and improving the readability of Fig. 3 and the captions of Table 1. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. One of the concerns for this paper is that the methodological novelty is limited. The rebuttal does not address this point. However it addresses all other concerns. Especially those of the loss function. This paper presents an important clinical application and is thus appropriate for MICCAI. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The problem of improving the explainability of CNN is an important topic for skin cancer diagnosis. Unfortunatley, the motivation of using 4 losses is still unclear; the authors did not clearly explain nor discuss how / why such losses (e.g., contrastive and distillation loss) improves the skin cancer diagnosis. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 14 Author Feedback We would like to thank the reviewers for their assessment of the paper and constructive suggestions. Below we address the major concerns and clarify misunderstandings. We acknowledge that the manuscript was not clear regarding the results and the loss function used to train the model, mainly due to Table 1 and Fig. 3. First, we would like to clarify the meaning of “@GAP” in the results. As described in the last paragraph of Section 2.1, the proposed model simultaneously performs classification and CBIR. The former is provided by the “FC Classifier” head in Fig. 2, while for the latter we explored two alternatives: a) using the “FC Embedding” head (which is only used when the Triplet and/or Contrastive loss are considered); or b) using the embeddings given by the GAP layer. In the results section, we referred to option “b)” as @GAP, and we found this to lead to the best performing and more stable CBIR, when compared to the “FC Embedding”, as shown in Fig. 3. Nevertheless, we will add a more detailed quantitative comparison (as in Table 1) between these two options in supplementary material. Second, the global loss specified in eq (2) comprises a term (L_CE) which is always used to train the classifier, and three regularization terms whose contribution is controlled by the hyperparameters (α,β, γ). In Fig. 3, the first row shows the results of combining L_CE with each regularization term (e.g., =1,β=0, γ=0). The second row shows the results of combining multiple regularization terms, including using all four (in brown and pink), illustrating their synergistic effect. Table 1 shows the following results: classification loss alone (L_CE), the best combination of loss terms (L_CE + L_Cont + L_Dist) and their individual counterparts (L_CE + L_Dist and L_CE + L_Cont). In all these cases, the CBIR was obtained using the embeddings of the GAP layer (“@GAP” was mistakenly missing in the first two columns). We will address these issues by following the reviewers’ suggestions, namely: 1) adding the results for the missing combinations of terms in eq (2) – in supplementary material, due to the lack of space in the main document; 2) clarifying the definition of eq (2) in Section 2.1; and 3) improving readability of Fig. 3 and the captions of Table 1. We agree with the reviewer that exploring CBIR for explainability is not new. In fact, Section 1 identifies previous works that use CBIR on CNN features learned for classification tasks. However, they led to conflicting results about the utility of CBIR. In our paper, we argue that this arises from the lack of structure of the feature space learned for a diagnostic task, which does not account for similarities between lesions, as exemplified in Fig. 1. To the best of our knowledge, ours is the first work to address this issue by augmenting the standard cross-entropy loss function with regularization terms. This encourages the model to also learn similarities between dermoscopy images. Fig. 3 and Table 1 demonstrate the validity of our formulation over the standard CBIR performed on features learned for the classification task (L_CE column vs. the remaining). We show that using regularization terms improves both the class recall and precision@K metrics. Additionally, two of the regularization losses explored in our work (contrastive and distillation) have never been explored as mechanisms to improve the CBIR in skin cancer. The third loss (triplet) has only been used in [1], which is conceptually different from our work. Their work aims to develop a model for retrieval only, while our model is capable of performing retrieval and classification simultaneously, as mentioned in the last paragraph of page 2 (Section 1). Our results also showed that triplet loss is the least suitable strategy to improve CBIR when using small batch sizes. We will make the differences between our work and [1] clearer in the paper. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0351/12/31/Paper2437" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0351/12/31/Paper2437" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0351-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Improving the Explainability of Skin Cancer Diagnosis Using CBIR" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0351/12/31/Paper2437"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0351/12/31/Paper2437","headline":"Improving the Explainability of Skin Cancer Diagnosis Using CBIR","dateModified":"0352-01-02T00:00:00-05:17","datePublished":"0351-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Catarina Barata, Carlos Santiago Abstract Explainability is a key feature for computer-aided diagnosis systems. This property not only helps doctors understand their decisions, but also allows less experienced practitioners to improve their knowledge. Skin cancer diagnosis is a field where explainability is of critical importance, as lesions of different classes often exhibit confounding characteristics. This work proposes a deep neural network (DNN) for skin cancer diagnosis that provides explainability through content-based image retrieval. We explore several state-of-the-art approaches to improve the feature space learned by the DNN, namely contrastive, distillation, and triplet losses. We demonstrate that the combination of these regularization losses with the categorical cross-entropy leads to the best performances on melanoma classification, and results in a hybrid DNN that simultaneously: i) classifies the images; and ii) retrieves similar images justifying the diagnosis. The code is available at \\url{https://github.com/catarina-barata/CBIR_Explainability_Skin_Cancer}. Link to paper https://doi.org/10.1007/978-3-030-87199-4_52 Link to the code repository https://github.com/catarina-barata/CBIR_Explainability_Skin_Cancer Link to the dataset(s) https://challenge.isic-archive.com/data Reviews Review #1 Please describe the contribution of the paper This paper explores explainability of image-based skin cancer diagnosis by incorporating a content-based image retrieval (CBIR) module that provides the user with similar images and their diagnoses. As motivation, the paper notes that latent spaces trained for classification often do not perform well for image retrieval, and apply additional losses optimizing to strengthen separability in the learned latent space, via a triplet loss, a contrastive loss and a distillation loss. Results are validated on ISIC, which is a good size dataset. Improvements are moderate. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea is logical and easy to follow. Seems to have some effect on output, but not huge. There is a visible improvement in the latent space visualizations. Paper is relatively easy to read. I like the comparison with multiple architectures Empirical performance looks good Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The methodological novelty is limited The idea of using CBIR to provide explainability is not new, as this was done in [1]. It does not seem that the authors compare to [1], which differs from the current approach in that the triplet loss for CBIR was trained separately, not jointly with the diagnostic predictor Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Reproducibility is fine. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see above. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is nice but has limited novelty. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper tackles the problem of explainability of modern deep learning techniques by using a content based image retrieval (CBIR) approach. This solution allows the authors to account for two of the main design needs for methodological translation to the clinic among other listed applications: (i) reliability and (ii) visualization interface. The approach presented here incorporates regularization losses to the standard cross-entropy loss for prediction: triplet loss, contrastive loss and distillation loss. They are able to show that these three losses forces the latent space learned by the neural network to have a better structure (e.g., examples from the same class lie closer to each other and further to other classes). This helps the CBIR step and, in a positive feedback loop, the result from the CBIR step can be used to generate an aggregate score (e.g., majority voting) between the selected images that improves upon the original classification loop. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strenghts of the paper can be summarized as: Very well written manuscript and pleasant to read. The problem is properly introduced and motivated, the goals and contributions are clearly stated and the methodology well explained. Training and evaluation details are also detailed. A novel formulation for explainability in skin cancer: they incorporate to the standard classification scheme a CBIR module that will retrieve the most similar images to the input. This is specially interesting for helping doctors in the clinic and train less experienced practitioners. Extensive testing of different configuration of framework: hyperparameters, losses. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness of this work may be the presentation of the results: In Table ,1 I believe @GAP means that just one-head of the network is used but clarification is needed. It’s not entirely clear why the combination of the three losses is not shown in Table 1. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have committed to make the code publicly available on acceptance. They also provide a link to the training data. Training strategy is explained and hyperparameter selection technique is also detailed. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I have just some minor comments: If space allows, a simple phrase defining what is a positive and a negative examples for the triplet loss would make the reading a bit clearer. 2.- Fig.3 can be improved as legend and camptions are hardly readable. Some suggestions are: increasing font size, placing all shared legends (per row) outside the plot and using seaborn package, include a grid 3.-. Fig.4 caption can be placed outside the plots as it is shared for all 3 subfigures. 4.- In the caption of Fig.5 a sentence indicating what each row shows (no regularization/regularzation losses) may be needed. Suggestion on future work: after going through the images provided in the results section, it would be very interesting to see how this methods performs in tasks such as outlier detection or detecting noise in labels. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? In my opinion, this is a strong accept. The paper is very well written and pleasant to read. The methodological approach is properly presented and design decisions are justified or analysed in the experiments section. Finally, I believe that the benefis of the method as well as the potential clinical impact is of interest to the medical imaging community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper proposes to use a deep learning method to help on skin cancer diagnosis. The method simultaneously performed a classification of images and retrieve similar images justifying the diagnosis. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The main strength of the paper is the capability of the model to perform simultaneously the classification and the retrieval. The second strength is the combination of the several loss functions including some who are generally used for self-supervised methods. Each of these functions is very well explained. Moreover, the result part provided a comparison of the different loss functions. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The main weakness of the paper is the lack of clarity of the results. It is unclear if only one type of loss function is used or it is a combination of multiple loss functions. Table 1 presents results for loss distillation loss, we could suppose it is results with only this loss function, but on the text, it is indicated “when the regularization losses are incorporated…”. Moreover, what is the significance of “@GAP” on this Table? Finally, the results seem not provided the performance of the use of the four losses. The title and keywords of the paper suggest a focus on the explainability of the network. The content-based image retrieval module provides an interesting tool to understand which factors lead to the classification and on which parameters the images are considered similar. However, the paper does not exploit this sufficiently to talk about the explainability of the network. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper provided all information to reproduce the works, including the range of hyperparameter used to compute the global loss function. However, the exact values used to compute the results are not specified. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Fig 3 is difficult to read for several reasons. First, the legend and the axis values are not readable. Increase the size of the axis would help to the clarity. If the legend can be having a higher size, it would be possible to provide it in the figure caption. Secondly, currently, each subfigure has is how range, which does not allow easy comparison between each network. In the rest of the paper, the authors only focused on DenseNET-121, one other solution could be to only present the result for this network and provided the other one as supplementary material. The range of hyperparameters test is very complete, however, it is not specified for with values the results are given, except for tau_d in table 1. One of the evaluation metrics is precision@K, it is not clear what it refers. It is the mean precision on the K images provides by the CBIR? Fig4 presents the embedding space obtained with each loss with DenseNet-121. The authors compared it to the left part of Fig1. However, it is not clear from with model the embedding space is obtained on FIG1. This needs to be clarified when Fig1 is mentioned. Due to the unavailability of the test set ground truth, the authors are not able to compute the recall and the precision@K. But it is not clear which metric is used in Table 1 for the test set. Moreover, to allow comparison with the result on validation it could be interesting to compute the same metric on the validation set. The authors present a global loss function, but this one is not used in the result section. In Fig 5, it is not clear with the row is corresponds with or without regularization. The authors make reference to Fig4 for color legend. On this figure the legend is small. It would be better to refer to Fig1 for these booth figures. In section 3.1, there is an empty reference after de data set splitting. Please state your overall opinion of the paper probably reject (4) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This paper needs a lot of improvements to be accepted especially in the results section and on the exploitation of explainability. The strengths do not compensate the weakness. What is the ranking of this paper in your review stack? 6 Number of papers in your stack 7 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper explores the explainability of image-based skin cancer diagnosis by incorporating a content-based image retrieval (CBIR) module, which is logical and easy to follow. As the main concern, the methodological novelty is limited and the results of this paper are not described clearly. For example, It is unclear if only one type of loss function is used or it is a combination of multiple loss functions. Also, the results seem not provided the performance of the use of the four losses. As the used four losses, this article does not introduce their specific functions in detail to explain which problem is to be solved in the classification task. According to the comments of the reviewers, the author is requested to revise the reviewers’ comments point by point. Therfore, a rebuttal is appropriate. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper simultaneously performs the classification of images and retrieves similar images for skin cancer diagnosis. By this way, it can help doctors understand their decisions, and allows less experienced practitioners to improve their knowledge. Thus, this method has important clinical application value. In addition, the author has clarified the main issues raised by the reviewers such as adding the results for the missing combinations of Eq. (2) in supplementary material and improving the readability of Fig. 3 and the captions of Table 1. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. One of the concerns for this paper is that the methodological novelty is limited. The rebuttal does not address this point. However it addresses all other concerns. Especially those of the loss function. This paper presents an important clinical application and is thus appropriate for MICCAI. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The problem of improving the explainability of CNN is an important topic for skin cancer diagnosis. Unfortunatley, the motivation of using 4 losses is still unclear; the authors did not clearly explain nor discuss how / why such losses (e.g., contrastive and distillation loss) improves the skin cancer diagnosis. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 14 Author Feedback We would like to thank the reviewers for their assessment of the paper and constructive suggestions. Below we address the major concerns and clarify misunderstandings. We acknowledge that the manuscript was not clear regarding the results and the loss function used to train the model, mainly due to Table 1 and Fig. 3. First, we would like to clarify the meaning of “@GAP” in the results. As described in the last paragraph of Section 2.1, the proposed model simultaneously performs classification and CBIR. The former is provided by the “FC Classifier” head in Fig. 2, while for the latter we explored two alternatives: a) using the “FC Embedding” head (which is only used when the Triplet and/or Contrastive loss are considered); or b) using the embeddings given by the GAP layer. In the results section, we referred to option “b)” as @GAP, and we found this to lead to the best performing and more stable CBIR, when compared to the “FC Embedding”, as shown in Fig. 3. Nevertheless, we will add a more detailed quantitative comparison (as in Table 1) between these two options in supplementary material. Second, the global loss specified in eq (2) comprises a term (L_CE) which is always used to train the classifier, and three regularization terms whose contribution is controlled by the hyperparameters (α,β, γ). In Fig. 3, the first row shows the results of combining L_CE with each regularization term (e.g., =1,β=0, γ=0). The second row shows the results of combining multiple regularization terms, including using all four (in brown and pink), illustrating their synergistic effect. Table 1 shows the following results: classification loss alone (L_CE), the best combination of loss terms (L_CE + L_Cont + L_Dist) and their individual counterparts (L_CE + L_Dist and L_CE + L_Cont). In all these cases, the CBIR was obtained using the embeddings of the GAP layer (“@GAP” was mistakenly missing in the first two columns). We will address these issues by following the reviewers’ suggestions, namely: 1) adding the results for the missing combinations of terms in eq (2) – in supplementary material, due to the lack of space in the main document; 2) clarifying the definition of eq (2) in Section 2.1; and 3) improving readability of Fig. 3 and the captions of Table 1. We agree with the reviewer that exploring CBIR for explainability is not new. In fact, Section 1 identifies previous works that use CBIR on CNN features learned for classification tasks. However, they led to conflicting results about the utility of CBIR. In our paper, we argue that this arises from the lack of structure of the feature space learned for a diagnostic task, which does not account for similarities between lesions, as exemplified in Fig. 1. To the best of our knowledge, ours is the first work to address this issue by augmenting the standard cross-entropy loss function with regularization terms. This encourages the model to also learn similarities between dermoscopy images. Fig. 3 and Table 1 demonstrate the validity of our formulation over the standard CBIR performed on features learned for the classification task (L_CE column vs. the remaining). We show that using regularization terms improves both the class recall and precision@K metrics. Additionally, two of the regularization losses explored in our work (contrastive and distillation) have never been explored as mechanisms to improve the CBIR in skin cancer. The third loss (triplet) has only been used in [1], which is conceptually different from our work. Their work aims to develop a model for retrieval only, while our model is capable of performing retrieval and classification simultaneously, as mentioned in the last paragraph of page 2 (Section 1). Our results also showed that triplet loss is the least suitable strategy to improve CBIR when using small batch sizes. We will make the differences between our work and [1] clearer in the paper. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Barata, Catarina,Santiago, Carlos" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Improving the Explainability of Skin Cancer Diagnosis Using CBIR</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Machine Learning - Interpretability / Explainability"
        class="post-category">
        Machine Learning - Interpretability / Explainability
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Dermatology"
        class="post-category">
        Clinical applications - Dermatology
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Barata, Catarina"
        class="post-tags">
        Barata, Catarina
      </a> |  
      
      <a href="kittywong/tags#Santiago, Carlos"
        class="post-tags">
        Santiago, Carlos
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Catarina Barata, Carlos Santiago
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Explainability is a key feature for computer-aided diagnosis systems. This property not only helps doctors understand their decisions, but also allows less experienced practitioners to improve their knowledge. Skin cancer diagnosis is a field where explainability is of critical importance, as lesions of different classes often exhibit confounding characteristics. This work proposes a deep neural network (DNN) for skin cancer diagnosis that provides explainability through content-based image retrieval. We explore several state-of-the-art approaches to improve the feature space learned by the DNN, namely contrastive, distillation, and triplet losses. We demonstrate that the combination of these regularization losses with the categorical cross-entropy leads to the best performances on melanoma classification, and results in a hybrid DNN that simultaneously: i) classifies the images; and ii) retrieves similar images justifying the diagnosis. The code is available at \url{https://github.com/catarina-barata/CBIR_Explainability_Skin_Cancer}.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87199-4_52">https://doi.org/10.1007/978-3-030-87199-4_52</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/catarina-barata/CBIR_Explainability_Skin_Cancer
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>https://challenge.isic-archive.com/data
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper explores explainability of image-based skin cancer diagnosis by incorporating a content-based image retrieval (CBIR) module that provides the user with similar images and their diagnoses. As motivation, the paper notes that latent spaces trained for classification often do not perform well for image retrieval, and apply additional losses optimizing to strengthen separability in the learned latent space, via a triplet loss, a contrastive loss and a distillation loss.</p>
      <ul>
        <li>Results are validated on ISIC, which is a good size dataset. Improvements are moderate.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>The idea is logical and easy to follow. Seems to have some effect on output, but not huge.</li>
        <li>There is a visible improvement in the latent space visualizations.</li>
        <li>Paper is relatively easy to read.</li>
        <li>I like the comparison with multiple architectures</li>
        <li>Empirical performance looks good</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>The methodological novelty is limited</li>
        <li>The idea of using CBIR to provide explainability is not new, as this was done in [1].</li>
        <li>It does not seem that the authors compare to [1], which differs from the current approach in that the triplet loss for CBIR was trained separately, not jointly with the diagnostic predictor</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Reproducibility is fine.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Please see above.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper is nice but has limited novelty.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper tackles the problem of explainability of modern deep learning techniques by using a content based image retrieval (CBIR) approach.
This solution allows the authors to account for two of the main design needs for methodological translation to the clinic among other listed applications: (i) reliability and (ii) visualization interface.</p>

      <p>The approach presented here incorporates regularization losses to the standard cross-entropy loss for prediction: triplet loss, contrastive loss and distillation loss. They are able to show that these three losses forces the latent space learned by the neural network to have a better structure (e.g., examples from the same class lie closer to each other and further to other classes). This helps the CBIR step and, in a positive feedback loop, the result from the CBIR step can be used to generate an aggregate score (e.g., majority voting) between the selected images that improves upon the original classification loop.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The main strenghts of the paper can be summarized as:</p>

      <ul>
        <li>
          <p>Very well written manuscript and pleasant to read. The problem is properly introduced and motivated, the goals and contributions are clearly stated and the methodology well explained. Training and evaluation details are also detailed.</p>
        </li>
        <li>
          <p>A novel formulation for explainability in skin cancer: they incorporate to the standard classification scheme a CBIR module that will retrieve the most similar images to the input. This is specially interesting for helping doctors in the clinic and train less experienced practitioners.</p>
        </li>
        <li>
          <p>Extensive testing of different configuration of framework: hyperparameters, losses.</p>
        </li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The main weakness of this work may be the presentation of the results:</p>
      <ul>
        <li>In Table ,1 I believe @GAP means that just one-head of the network is used but clarification is needed.</li>
        <li>It’s not entirely clear why the combination of the three losses is not shown in Table 1.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The authors have committed to make the code publicly available on acceptance. They also provide a link to the training data. 
Training strategy is explained and hyperparameter selection technique is also detailed.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>I have just some minor comments:</p>
      <ul>
        <li>If space allows, a simple phrase defining what is a positive and a negative examples for the triplet loss would make the reading a bit clearer. 
2.- Fig.3 can be improved as legend and camptions are hardly readable. Some suggestions are: increasing font size, placing all shared legends (per row) outside the plot and using seaborn package, include a grid
3.-. Fig.4 caption can be placed outside the plots as it is shared for all 3 subfigures.
4.- In the caption of Fig.5 a sentence indicating what each row shows (no regularization/regularzation losses) may be needed.</li>
      </ul>

      <p>Suggestion on future work: after going through the images provided in the results section, it would be very interesting to see how this methods performs in tasks such as outlier detection or detecting noise in labels.</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>In my opinion, this is a strong accept. The paper is very well written and pleasant to read. The methodological approach is properly presented and design decisions are justified or analysed in the experiments section. 
Finally, I believe that the benefis of the method as well as the potential clinical impact is of interest to the medical imaging community.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper proposes to use a deep learning method to help on skin cancer diagnosis. The method simultaneously performed a classification of images and retrieve similar images justifying the diagnosis.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The main strength of the paper is the capability of the model to perform simultaneously the classification and the retrieval. The second strength is the combination of the several loss functions including some who are generally used for self-supervised methods. Each of these functions is very well explained. Moreover, the result part provided a comparison of the different loss functions.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The main weakness of the paper is the lack of clarity of the results. It is unclear if only one type of loss function is used or it is a combination of multiple loss functions. Table 1 presents results for loss distillation loss, we could suppose it is results with only this loss function, but on the text, it is indicated “when the regularization losses are incorporated…”. Moreover, what is the significance of “@GAP” on this Table? Finally, the results seem not provided the performance of the use of the four losses.</p>

      <p>The title and keywords of the paper suggest a focus on the explainability of the network. The content-based image retrieval module provides an interesting tool to understand which factors lead to the classification and on which parameters the images are considered similar. However, the paper does not exploit this sufficiently to talk about the explainability of the network.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Satisfactory</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The paper provided all information to reproduce the works, including the range of hyperparameter used to compute the global loss function. However, the exact values used to compute the results are not specified.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Fig 3 is difficult to read for several reasons. First, the legend and the axis values are not readable. Increase the size of the axis would help to the clarity. If the legend can be having a higher size, it would be possible to provide it in the figure caption. Secondly, currently, each subfigure has is how range, which does not allow easy comparison between each network. In the rest of the paper, the authors only focused on DenseNET-121, one other solution could be to only present the result for this network and provided the other one as supplementary material.</p>

      <p>The range of hyperparameters test is very complete, however, it is not specified for with values the results are given, except for tau_d in table 1.
One of the evaluation metrics is precision@K, it is not clear what it refers. It is the mean precision on the K images provides by the CBIR?</p>

      <p>Fig4 presents the embedding space obtained with each loss with DenseNet-121. The authors compared it to the left part of Fig1. However, it is not clear from with model the embedding space is obtained on FIG1. This needs to be clarified when Fig1 is mentioned.</p>

      <p>Due to the unavailability of the test set ground truth, the authors are not able to compute the recall and the precision@K. But it is not clear which metric is used in Table 1 for the test set. Moreover, to allow comparison with the result on validation it could be interesting to compute the same metric on the validation set.</p>

      <p>The authors present a global loss function, but this one is not used in the result section.</p>

      <p>In Fig 5, it is not clear with the row is corresponds with or without regularization. The authors make reference to Fig4 for color legend. On this figure the legend is small. It would be better to refer to Fig1 for these booth figures.</p>

      <p>In section 3.1, there is an empty reference after de data set splitting.</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>probably reject (4)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>This paper needs a lot of improvements to be accepted especially in the results section and on the exploitation of explainability. The strengths do not compensate the weakness.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>7</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>This paper explores the explainability of image-based skin cancer diagnosis by incorporating a content-based image retrieval (CBIR) module, which is logical and easy to follow. As the main concern, the methodological novelty is limited and the results of this paper are not described clearly. For example,  It is unclear if only one type of loss function is used or it is a combination of multiple loss functions. Also,  the results seem not provided the performance of the use of the four losses. 
As the used four losses, this article does not introduce their specific functions in detail to explain which problem is to be solved in the classification task. According to the comments of the reviewers, the author is requested to revise the reviewers’ comments point by point. Therfore, a rebuttal is appropriate.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The paper simultaneously performs the classification of images and retrieves similar images for skin cancer diagnosis. By this way, it  can help doctors understand their decisions, and allows less experienced practitioners to improve their knowledge. Thus, this method has important clinical application value.
In addition, the author has clarified the main issues raised by the reviewers such as adding the results for the missing combinations of  Eq. (2) in supplementary material and improving the readability of Fig. 3 and the captions of Table 1.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>10</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="meta-review-2">Meta-Review #2</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>One of the concerns for this paper is that the methodological novelty is limited. The rebuttal does not address this point. However it addresses all other concerns. Especially those of the loss function. This paper presents an important clinical application and is thus appropriate for MICCAI.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>
<h2 id="meta-review-3">Meta-Review #3</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The problem of improving the explainability of CNN is an important topic for skin cancer diagnosis. Unfortunatley, the motivation of using 4 losses is still unclear; the authors did not clearly explain nor discuss how / why such losses (e.g., contrastive and distillation loss) improves the skin cancer diagnosis.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Reject</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>14</p>
    </blockquote>
  </li>
</ul>

<h2><br /><br /></h2>
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We would like to thank the reviewers for their assessment of the paper and constructive suggestions. Below we address the major concerns and clarify misunderstandings.</p>

  <p>We acknowledge that the manuscript was not clear regarding the results and the loss function used to train the model, mainly due to Table 1 and Fig. 3. 
First, we would like to clarify the meaning of “@GAP” in the results. As described in the last paragraph of Section 2.1, the proposed model simultaneously performs classification and CBIR. The former is provided by the “FC Classifier” head in Fig. 2, while for the latter we explored two alternatives: a) using the “FC Embedding” head (which is only used when the Triplet and/or Contrastive loss are considered); or b) using the embeddings given by the GAP layer. In the results section, we referred to option “b)” as @GAP, and we found this to lead to the best performing and more stable CBIR, when compared to the “FC Embedding”, as shown in Fig. 3. Nevertheless, we will add a more detailed quantitative comparison (as in Table 1) between these two options in supplementary material.</p>

  <p>Second, the global loss specified in eq (2) comprises a term (L_CE) which is always used to train the classifier, and three regularization terms whose contribution is controlled by the hyperparameters (α,β, γ). In Fig. 3, the first row shows the results of combining L_CE with each regularization term (e.g., =1,β=0, γ=0). The second row shows the results of combining multiple regularization terms, including using all four (in brown and pink), illustrating their synergistic effect. Table 1 shows the following results: classification loss alone (L_CE), the best combination of loss terms (L_CE + L_Cont + L_Dist) and their individual counterparts (L_CE + L_Dist and L_CE + L_Cont). In all these cases, the CBIR was obtained using the embeddings of the GAP layer (“@GAP” was mistakenly missing in the first two columns).
We will address these issues by following the reviewers’ suggestions, namely: 1) adding the results for the missing combinations of terms in eq (2) – in supplementary material, due to the lack of space in the main document; 2) clarifying the definition of eq (2) in Section 2.1; and 3) improving readability of Fig. 3  and the captions of Table 1.</p>

  <p>We agree with the reviewer that exploring CBIR for explainability is not new. In fact, Section 1 identifies previous works that use CBIR on CNN features learned for classification tasks. However, they led to conflicting results about the utility of CBIR. In our paper, we argue that this arises from the lack of structure of the feature space learned for a diagnostic task, which does not account for similarities between lesions, as exemplified in Fig. 1.</p>

  <p>To the best of our knowledge, ours is the first work to address this issue by augmenting the standard cross-entropy loss function with regularization terms. This encourages the model to also learn similarities between dermoscopy images. Fig. 3 and Table 1 demonstrate the validity of our formulation over the standard CBIR performed on features learned for the classification task (L_CE column vs. the remaining). We show that using regularization terms improves both the class recall and precision@K metrics. Additionally, two of the regularization losses explored in our work (contrastive and distillation) have never been explored as mechanisms to improve the CBIR in skin cancer. The third loss (triplet) has only been used in [1], which is conceptually different from our work. Their work aims to develop a model for retrieval only, while our model is capable of performing retrieval and classification simultaneously, as mentioned in the last paragraph of page 2 (Section 1). Our results also showed that triplet loss is the least suitable strategy to improve CBIR when using small batch sizes. We will make the differences between our work and [1] clearer in the paper.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0351-12-31
      -->
      <!--
      
        ,
        updated at 
        0352-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Machine Learning - Interpretability / Explainability"
        class="post-category">
        Machine Learning - Interpretability / Explainability
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Dermatology"
        class="post-category">
        Clinical applications - Dermatology
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Barata, Catarina"
        class="post-category">
        Barata, Catarina
      </a> |  
      
      <a href="kittywong/tags#Santiago, Carlos"
        class="post-category">
        Santiago, Carlos
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0352/12/31/Paper2481">
          PAC Bayesian Performance Guarantees for (Stochastic) Deep Networks in Medical Imaging
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0350/12/31/Paper2380">
          Sharpening Local Interpretable Model-agnostic Explanations for Histopathology: Improved Understandability and Reliability
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
