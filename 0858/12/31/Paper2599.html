<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>A Multi-attribute Controllable Generative Model for Histopathology Image Synthesis | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="A Multi-attribute Controllable Generative Model for Histopathology Image Synthesis" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Jiarong Ye, Yuan Xue, Peter Liu, Richard Zaino, Keith C. Cheng, Xiaolei Huang Abstract Generative models have been applied in the medical imaging domain for various image recognition and synthesis tasks, however, a more controllable and interpretable image synthesis model is still lacking yet necessary towards important applications such as assisting in medical training. In this work, we leverage the efficient self-attention and contrastive learning modules and build upon state-of-the-art generative adversarial networks (GANs) to achieve an attribute-aware image synthesis model, termed AttributeGAN, which can generate high-quality histopathology images based on multi-attribute inputs. In comparison to existing single-attribute conditional generative models, our proposed model better reflects input attributes and enables smoother interpolation among attribute values. We conduct experiments on a histopathology dataset containing stained H&amp;E images of urothelial carcinoma and demonstrate the effectiveness of our proposed model via comprehensive quantitative and qualitative comparisons with state-of-the-art models as well as different variants of our model. Link to paper https://doi.org/10.1007/978-3-030-87237-3_59 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The paper presents a deep conditional generative model (GAN based) which is able to generate H&amp;E patches conditioned on different cellular attributes like cell crowding, cell polarity, mitosis, prominence of nucleoli and state of nuclear pleomorphism. The paper combines recent approaches like self supervised techniques and attention models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It is interesting how the authors get patch information from text based annotation. Additionally the use of contrastive loss terms to encourage similar features intra attributes seems to me as a good idea. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The idea of conditional generation with multiple attributes is not new at least in the computer vision community. On the other hand, the work seems to force the introduction of new techniques like self supervised and attention. The performance seems to increase by using them but no analysis was made on why is this the case. As an example, it could have been interesting to analyze the attention maps to see visually that is doing something relevant e.g. looking at individual cells. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The data used is publicly available which is a very good thing. In addition in the reproducibility checklist the authors engage themselves to share the code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The paper is concise and it is a new application of generative models to histopathology imaging. Using attributes extracted from text is also a positive point. On the other hand, it seems that the authors wanted to use the latest approaches in the computer vision technology (self supervised learning /attention) which is also interesting. However, the analysis of these techniques was quite limited and only focused on performance. A deeper analysis of these modules could significantly improve the paper. For example what is the attention model focusing at? are the self supervised learnt features relevant for other tasks such as classification? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The application of this type of techniques is relatively new to histopathology data. It is interesting to be able to control medical relevant attributes when generating synthetic images. My main objection of the paper is that it seems it just wants to use the latest approaches without providing a clear analysis on why it can be relevant for the task at hand other than just by performance. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The paper proposes AttributeGAN, a model that aims to synthesize high-resolution histopathology images whose appearance can be influenced by five different parameters (cell crowding, cell polarity, mitosis, prominence of nucleoli, state of nuclear plenomorphism). AttributeGAN combines concepts from three different areas: generative modelling, self-supervised learning and attention mechanisms. The synthesized images are compared by experts against the originals on a stained dataset, and empirically compared vs. BigGAN. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. +The paper is generally well-written and its language is easy to follow +The paragraph on efficient attention was interesting and highlights how the batch size (that needs to be sufficiently high for contrastive-learning) can be increased. +To my knowledge, this is the first attempt to conditionally generate realistic histopathology images. +The proposed technical contributions (contrastive loss, attention module) appear to improve performance. +The architecture makes use of modern components: SLE modules, GLU, attention modules, contrastive loss. +The graphics are good quality and help to understand the general aspects of the paper +The synthesized images are given to medical experts who found the quality to be “remarkably good” Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -It is good to see that AttributeGAN is compared against a third-party network (Big-GAN) and also that there are a couple of ablations (w/o attention module, contrastive learning) it is compared against. But a more appropriate comparison would be a model from the InfoGAN [1] cosmos, or something that comes close To Fader Networks [2] which tackle the same problem directly. -The number of provided images is a bit meager, I would have expected considerably more synthesized images, especially in the supplementary material -Frechet Inception Distance (FID) is not really suitable to medical imaging, as the benchmark network is trained on natural imagery. [1] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., &amp; Abbeel, P. (2016). Infogan: Interpretable representation learning by information maximizing generative adversarial nets. arXiv preprint arXiv:1606.03657. [2] Lample, G., Zeghidour, N., Usunier, N., Bordes, A., Denoyer, L., &amp; Ranzato, M. A. (2017). Fader networks: Manipulating images by sliding attributes. arXiv preprint arXiv:1706.00409. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have described their method in reasonable detail, although some details are missing. The code/data/models have not been made public. Medium/low reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html As mentioned above, the FID score is not an appropriate metric for histopathology images, but as there is no readily available alternative I can understand its use. The main shortcoming of the paper in my opinion is the missing comparison to well-known attribute-controlling architectures such as InfoGAN, Fader Networks, StyleGAN v2, etc. Simply applying these off the shelf may be competitive with the proposed method. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper solves its task (synthesis of histopathology images) well but also does not really offer a real novelty other than combining components that are already there. It would have also been interesting to see how this model performs on other medical data. The paper itself is exceptionally clear and delivers its message in a very direct way. In general, I would like to see more GANs this model is compared against. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper presents a multi-attribute conditional generative model that is built on top of recent works on stable GANs, self-attention, and contrastive learning. The proposed model can synthesize high-resolution histopathology images with improved image quality and better-capturing input attributes. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The model builds on recent works in GANs, efficient self-attention, and contrastive learning. The use of contrastive learning within the discriminator to capture subtle changes in attribute levels of the image is an interesting idea. Efficient self-attention reduces the computational overhead and allows for larger batch sizes needed for contrastive learning. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. As with all conditional generative models, this method require some level of supervision in defining the attributes to conditional image synthesis. The proposed model can not leverage unlabeled data, e.g., in a semi-supervised fashion. The performance of the proposed model is weakly baselined by a GAN variant that handle high resolution image. Other more relevant SOTA conditional models are not considered (e.g., Fader Networks and AttGAN,) The qualitative way that attributes are defined adds in subjectivity and inter-/intra-rater variability. Histopathology image generation is qualitatively evaluated by experts. The impact of noisy labels/attributes is not considered. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper has enough details for reproducibility. It is not clear if code would be released. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html How can the model handle unlabeled data? Authors should consider a more comprehensive comparisons with relevant SOTA models such as Fader Networks and AttGAN. It is not clear how attributes were estimated from pathology reports. Model evaluation can be further improved by considering a randomized/blind user study where pathologists provide pathology reports on images without prior knowledge of whether an image is a real or synthesized one. In eq 1, how negative pairs contribute to the contrastive loss? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Results are weakly baselined and model evaluation could be improve. However, the paper presents some interesting adaptations to medical image synthesis. With its level of clarity and organization, the paper could be considered for a MICCAI publication. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper has received positive feedback from all reviewers, although some concerns were raised. The authors are encouraged to address the reviewer comments in the final version and improve the quality of images (&amp;text within images) in the paper. A comprehensive comparison with the state of the art is also missing. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Author Feedback N/A back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Jiarong Ye, Yuan Xue, Peter Liu, Richard Zaino, Keith C. Cheng, Xiaolei Huang Abstract Generative models have been applied in the medical imaging domain for various image recognition and synthesis tasks, however, a more controllable and interpretable image synthesis model is still lacking yet necessary towards important applications such as assisting in medical training. In this work, we leverage the efficient self-attention and contrastive learning modules and build upon state-of-the-art generative adversarial networks (GANs) to achieve an attribute-aware image synthesis model, termed AttributeGAN, which can generate high-quality histopathology images based on multi-attribute inputs. In comparison to existing single-attribute conditional generative models, our proposed model better reflects input attributes and enables smoother interpolation among attribute values. We conduct experiments on a histopathology dataset containing stained H&amp;E images of urothelial carcinoma and demonstrate the effectiveness of our proposed model via comprehensive quantitative and qualitative comparisons with state-of-the-art models as well as different variants of our model. Link to paper https://doi.org/10.1007/978-3-030-87237-3_59 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The paper presents a deep conditional generative model (GAN based) which is able to generate H&amp;E patches conditioned on different cellular attributes like cell crowding, cell polarity, mitosis, prominence of nucleoli and state of nuclear pleomorphism. The paper combines recent approaches like self supervised techniques and attention models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It is interesting how the authors get patch information from text based annotation. Additionally the use of contrastive loss terms to encourage similar features intra attributes seems to me as a good idea. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The idea of conditional generation with multiple attributes is not new at least in the computer vision community. On the other hand, the work seems to force the introduction of new techniques like self supervised and attention. The performance seems to increase by using them but no analysis was made on why is this the case. As an example, it could have been interesting to analyze the attention maps to see visually that is doing something relevant e.g. looking at individual cells. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The data used is publicly available which is a very good thing. In addition in the reproducibility checklist the authors engage themselves to share the code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The paper is concise and it is a new application of generative models to histopathology imaging. Using attributes extracted from text is also a positive point. On the other hand, it seems that the authors wanted to use the latest approaches in the computer vision technology (self supervised learning /attention) which is also interesting. However, the analysis of these techniques was quite limited and only focused on performance. A deeper analysis of these modules could significantly improve the paper. For example what is the attention model focusing at? are the self supervised learnt features relevant for other tasks such as classification? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The application of this type of techniques is relatively new to histopathology data. It is interesting to be able to control medical relevant attributes when generating synthetic images. My main objection of the paper is that it seems it just wants to use the latest approaches without providing a clear analysis on why it can be relevant for the task at hand other than just by performance. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The paper proposes AttributeGAN, a model that aims to synthesize high-resolution histopathology images whose appearance can be influenced by five different parameters (cell crowding, cell polarity, mitosis, prominence of nucleoli, state of nuclear plenomorphism). AttributeGAN combines concepts from three different areas: generative modelling, self-supervised learning and attention mechanisms. The synthesized images are compared by experts against the originals on a stained dataset, and empirically compared vs. BigGAN. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. +The paper is generally well-written and its language is easy to follow +The paragraph on efficient attention was interesting and highlights how the batch size (that needs to be sufficiently high for contrastive-learning) can be increased. +To my knowledge, this is the first attempt to conditionally generate realistic histopathology images. +The proposed technical contributions (contrastive loss, attention module) appear to improve performance. +The architecture makes use of modern components: SLE modules, GLU, attention modules, contrastive loss. +The graphics are good quality and help to understand the general aspects of the paper +The synthesized images are given to medical experts who found the quality to be “remarkably good” Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -It is good to see that AttributeGAN is compared against a third-party network (Big-GAN) and also that there are a couple of ablations (w/o attention module, contrastive learning) it is compared against. But a more appropriate comparison would be a model from the InfoGAN [1] cosmos, or something that comes close To Fader Networks [2] which tackle the same problem directly. -The number of provided images is a bit meager, I would have expected considerably more synthesized images, especially in the supplementary material -Frechet Inception Distance (FID) is not really suitable to medical imaging, as the benchmark network is trained on natural imagery. [1] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., &amp; Abbeel, P. (2016). Infogan: Interpretable representation learning by information maximizing generative adversarial nets. arXiv preprint arXiv:1606.03657. [2] Lample, G., Zeghidour, N., Usunier, N., Bordes, A., Denoyer, L., &amp; Ranzato, M. A. (2017). Fader networks: Manipulating images by sliding attributes. arXiv preprint arXiv:1706.00409. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have described their method in reasonable detail, although some details are missing. The code/data/models have not been made public. Medium/low reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html As mentioned above, the FID score is not an appropriate metric for histopathology images, but as there is no readily available alternative I can understand its use. The main shortcoming of the paper in my opinion is the missing comparison to well-known attribute-controlling architectures such as InfoGAN, Fader Networks, StyleGAN v2, etc. Simply applying these off the shelf may be competitive with the proposed method. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper solves its task (synthesis of histopathology images) well but also does not really offer a real novelty other than combining components that are already there. It would have also been interesting to see how this model performs on other medical data. The paper itself is exceptionally clear and delivers its message in a very direct way. In general, I would like to see more GANs this model is compared against. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper presents a multi-attribute conditional generative model that is built on top of recent works on stable GANs, self-attention, and contrastive learning. The proposed model can synthesize high-resolution histopathology images with improved image quality and better-capturing input attributes. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The model builds on recent works in GANs, efficient self-attention, and contrastive learning. The use of contrastive learning within the discriminator to capture subtle changes in attribute levels of the image is an interesting idea. Efficient self-attention reduces the computational overhead and allows for larger batch sizes needed for contrastive learning. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. As with all conditional generative models, this method require some level of supervision in defining the attributes to conditional image synthesis. The proposed model can not leverage unlabeled data, e.g., in a semi-supervised fashion. The performance of the proposed model is weakly baselined by a GAN variant that handle high resolution image. Other more relevant SOTA conditional models are not considered (e.g., Fader Networks and AttGAN,) The qualitative way that attributes are defined adds in subjectivity and inter-/intra-rater variability. Histopathology image generation is qualitatively evaluated by experts. The impact of noisy labels/attributes is not considered. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper has enough details for reproducibility. It is not clear if code would be released. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html How can the model handle unlabeled data? Authors should consider a more comprehensive comparisons with relevant SOTA models such as Fader Networks and AttGAN. It is not clear how attributes were estimated from pathology reports. Model evaluation can be further improved by considering a randomized/blind user study where pathologists provide pathology reports on images without prior knowledge of whether an image is a real or synthesized one. In eq 1, how negative pairs contribute to the contrastive loss? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Results are weakly baselined and model evaluation could be improve. However, the paper presents some interesting adaptations to medical image synthesis. With its level of clarity and organization, the paper could be considered for a MICCAI publication. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper has received positive feedback from all reviewers, although some concerns were raised. The authors are encouraged to address the reviewer comments in the final version and improve the quality of images (&amp;text within images) in the paper. A comprehensive comparison with the state of the art is also missing. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Author Feedback N/A back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0858/12/31/Paper2599" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0858/12/31/Paper2599" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0858-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="A Multi-attribute Controllable Generative Model for Histopathology Image Synthesis" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0858/12/31/Paper2599"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0858/12/31/Paper2599","headline":"A Multi-attribute Controllable Generative Model for Histopathology Image Synthesis","dateModified":"0859-01-05T00:00:00-05:17","datePublished":"0858-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Jiarong Ye, Yuan Xue, Peter Liu, Richard Zaino, Keith C. Cheng, Xiaolei Huang Abstract Generative models have been applied in the medical imaging domain for various image recognition and synthesis tasks, however, a more controllable and interpretable image synthesis model is still lacking yet necessary towards important applications such as assisting in medical training. In this work, we leverage the efficient self-attention and contrastive learning modules and build upon state-of-the-art generative adversarial networks (GANs) to achieve an attribute-aware image synthesis model, termed AttributeGAN, which can generate high-quality histopathology images based on multi-attribute inputs. In comparison to existing single-attribute conditional generative models, our proposed model better reflects input attributes and enables smoother interpolation among attribute values. We conduct experiments on a histopathology dataset containing stained H&amp;E images of urothelial carcinoma and demonstrate the effectiveness of our proposed model via comprehensive quantitative and qualitative comparisons with state-of-the-art models as well as different variants of our model. Link to paper https://doi.org/10.1007/978-3-030-87237-3_59 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The paper presents a deep conditional generative model (GAN based) which is able to generate H&amp;E patches conditioned on different cellular attributes like cell crowding, cell polarity, mitosis, prominence of nucleoli and state of nuclear pleomorphism. The paper combines recent approaches like self supervised techniques and attention models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It is interesting how the authors get patch information from text based annotation. Additionally the use of contrastive loss terms to encourage similar features intra attributes seems to me as a good idea. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The idea of conditional generation with multiple attributes is not new at least in the computer vision community. On the other hand, the work seems to force the introduction of new techniques like self supervised and attention. The performance seems to increase by using them but no analysis was made on why is this the case. As an example, it could have been interesting to analyze the attention maps to see visually that is doing something relevant e.g. looking at individual cells. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The data used is publicly available which is a very good thing. In addition in the reproducibility checklist the authors engage themselves to share the code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The paper is concise and it is a new application of generative models to histopathology imaging. Using attributes extracted from text is also a positive point. On the other hand, it seems that the authors wanted to use the latest approaches in the computer vision technology (self supervised learning /attention) which is also interesting. However, the analysis of these techniques was quite limited and only focused on performance. A deeper analysis of these modules could significantly improve the paper. For example what is the attention model focusing at? are the self supervised learnt features relevant for other tasks such as classification? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The application of this type of techniques is relatively new to histopathology data. It is interesting to be able to control medical relevant attributes when generating synthetic images. My main objection of the paper is that it seems it just wants to use the latest approaches without providing a clear analysis on why it can be relevant for the task at hand other than just by performance. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper The paper proposes AttributeGAN, a model that aims to synthesize high-resolution histopathology images whose appearance can be influenced by five different parameters (cell crowding, cell polarity, mitosis, prominence of nucleoli, state of nuclear plenomorphism). AttributeGAN combines concepts from three different areas: generative modelling, self-supervised learning and attention mechanisms. The synthesized images are compared by experts against the originals on a stained dataset, and empirically compared vs. BigGAN. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. +The paper is generally well-written and its language is easy to follow +The paragraph on efficient attention was interesting and highlights how the batch size (that needs to be sufficiently high for contrastive-learning) can be increased. +To my knowledge, this is the first attempt to conditionally generate realistic histopathology images. +The proposed technical contributions (contrastive loss, attention module) appear to improve performance. +The architecture makes use of modern components: SLE modules, GLU, attention modules, contrastive loss. +The graphics are good quality and help to understand the general aspects of the paper +The synthesized images are given to medical experts who found the quality to be “remarkably good” Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -It is good to see that AttributeGAN is compared against a third-party network (Big-GAN) and also that there are a couple of ablations (w/o attention module, contrastive learning) it is compared against. But a more appropriate comparison would be a model from the InfoGAN [1] cosmos, or something that comes close To Fader Networks [2] which tackle the same problem directly. -The number of provided images is a bit meager, I would have expected considerably more synthesized images, especially in the supplementary material -Frechet Inception Distance (FID) is not really suitable to medical imaging, as the benchmark network is trained on natural imagery. [1] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., &amp; Abbeel, P. (2016). Infogan: Interpretable representation learning by information maximizing generative adversarial nets. arXiv preprint arXiv:1606.03657. [2] Lample, G., Zeghidour, N., Usunier, N., Bordes, A., Denoyer, L., &amp; Ranzato, M. A. (2017). Fader networks: Manipulating images by sliding attributes. arXiv preprint arXiv:1706.00409. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors have described their method in reasonable detail, although some details are missing. The code/data/models have not been made public. Medium/low reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html As mentioned above, the FID score is not an appropriate metric for histopathology images, but as there is no readily available alternative I can understand its use. The main shortcoming of the paper in my opinion is the missing comparison to well-known attribute-controlling architectures such as InfoGAN, Fader Networks, StyleGAN v2, etc. Simply applying these off the shelf may be competitive with the proposed method. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper solves its task (synthesis of histopathology images) well but also does not really offer a real novelty other than combining components that are already there. It would have also been interesting to see how this model performs on other medical data. The paper itself is exceptionally clear and delivers its message in a very direct way. In general, I would like to see more GANs this model is compared against. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper presents a multi-attribute conditional generative model that is built on top of recent works on stable GANs, self-attention, and contrastive learning. The proposed model can synthesize high-resolution histopathology images with improved image quality and better-capturing input attributes. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The model builds on recent works in GANs, efficient self-attention, and contrastive learning. The use of contrastive learning within the discriminator to capture subtle changes in attribute levels of the image is an interesting idea. Efficient self-attention reduces the computational overhead and allows for larger batch sizes needed for contrastive learning. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. As with all conditional generative models, this method require some level of supervision in defining the attributes to conditional image synthesis. The proposed model can not leverage unlabeled data, e.g., in a semi-supervised fashion. The performance of the proposed model is weakly baselined by a GAN variant that handle high resolution image. Other more relevant SOTA conditional models are not considered (e.g., Fader Networks and AttGAN,) The qualitative way that attributes are defined adds in subjectivity and inter-/intra-rater variability. Histopathology image generation is qualitatively evaluated by experts. The impact of noisy labels/attributes is not considered. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper has enough details for reproducibility. It is not clear if code would be released. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html How can the model handle unlabeled data? Authors should consider a more comprehensive comparisons with relevant SOTA models such as Fader Networks and AttGAN. It is not clear how attributes were estimated from pathology reports. Model evaluation can be further improved by considering a randomized/blind user study where pathologists provide pathology reports on images without prior knowledge of whether an image is a real or synthesized one. In eq 1, how negative pairs contribute to the contrastive loss? Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Results are weakly baselined and model evaluation could be improve. However, the paper presents some interesting adaptations to medical image synthesis. With its level of clarity and organization, the paper could be considered for a MICCAI publication. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper has received positive feedback from all reviewers, although some concerns were raised. The authors are encouraged to address the reviewer comments in the final version and improve the quality of images (&amp;text within images) in the paper. A comprehensive comparison with the state of the art is also missing. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Author Feedback N/A back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Ye, Jiarong,Xue, Yuan,Liu, Peter,Zaino, Richard,Cheng, Keith C.,Huang, Xiaolei" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>A Multi-attribute Controllable Generative Model for Histopathology Image Synthesis</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Modalities - Histopathology"
        class="post-category">
        Modalities - Histopathology
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Interpretability / Explainability"
        class="post-category">
        Machine Learning - Interpretability / Explainability
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Ye, Jiarong"
        class="post-tags">
        Ye, Jiarong
      </a> |  
      
      <a href="kittywong/tags#Xue, Yuan"
        class="post-tags">
        Xue, Yuan
      </a> |  
      
      <a href="kittywong/tags#Liu, Peter"
        class="post-tags">
        Liu, Peter
      </a> |  
      
      <a href="kittywong/tags#Zaino, Richard"
        class="post-tags">
        Zaino, Richard
      </a> |  
      
      <a href="kittywong/tags#Cheng, Keith C."
        class="post-tags">
        Cheng, Keith C.
      </a> |  
      
      <a href="kittywong/tags#Huang, Xiaolei"
        class="post-tags">
        Huang, Xiaolei
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Jiarong Ye, Yuan Xue, Peter Liu, Richard Zaino, Keith C. Cheng, Xiaolei Huang
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Generative models have been applied in the medical imaging domain for various image recognition and synthesis tasks, however, a more controllable and interpretable image synthesis model is still lacking yet necessary towards important applications such as assisting in medical training. In this work, we leverage the efficient self-attention and contrastive learning modules and build upon state-of-the-art generative adversarial networks (GANs) to achieve an attribute-aware image synthesis model, termed AttributeGAN, which can generate high-quality histopathology images based on multi-attribute inputs. In comparison to existing single-attribute conditional generative models, our proposed model better reflects input attributes and enables smoother interpolation among attribute values. We conduct experiments on a histopathology dataset containing stained H&amp;E images of urothelial carcinoma and demonstrate the effectiveness of our proposed model via comprehensive quantitative and qualitative comparisons with state-of-the-art models as well as different variants of our model.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87237-3_59">https://doi.org/10.1007/978-3-030-87237-3_59</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper presents a deep conditional generative model (GAN based) which is able to generate H&amp;E patches conditioned on different cellular attributes like cell crowding,
cell polarity, mitosis, prominence of nucleoli and state of nuclear pleomorphism. The paper combines recent approaches like self supervised techniques and attention models.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>It is interesting how the authors get patch information from text based annotation.
Additionally the use of contrastive loss terms to encourage similar features intra attributes seems to me as a good idea.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The idea of conditional generation with multiple attributes is not new at least in the computer vision community. 
On the other hand, the work seems to force the introduction of new techniques like self supervised and attention. The performance seems to increase by using them but no analysis was made on why is this the case. As an example, it could have been interesting to analyze the  attention maps to see visually that is doing something relevant e.g. looking at individual cells.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The data used is publicly available which is a very good thing. In addition in the reproducibility checklist the authors engage themselves to share the code.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>The paper is concise and it is a new application of generative models to histopathology imaging. Using attributes extracted from text is also a positive point. On the other hand, it seems that the authors wanted to use the latest approaches in the computer vision technology (self supervised learning /attention) which is also interesting. However, the analysis of these techniques was quite limited and only focused on performance. A deeper analysis of these modules could significantly improve the paper. For example what is the attention model focusing at? are the self supervised learnt features relevant for other tasks such as classification?</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The application of this type of techniques is relatively new to histopathology data. It is interesting to be able to control medical relevant attributes when generating synthetic images. My main objection of the paper is that it seems it just wants to use the latest approaches without providing a clear analysis on why it can be relevant for the task at hand other than just by performance.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper proposes AttributeGAN, a model that aims to synthesize high-resolution histopathology images whose appearance can be influenced by five different parameters (cell crowding, cell polarity, mitosis, prominence of nucleoli, state of nuclear plenomorphism). AttributeGAN combines concepts from three different areas: generative modelling, self-supervised learning and attention mechanisms. The synthesized images are compared by experts against the originals on a stained dataset, and empirically compared vs. BigGAN.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>+The paper is generally well-written and its language is easy to follow
+The paragraph on efficient attention was interesting and highlights how the batch size (that needs to be sufficiently high for contrastive-learning) can be increased.
+To my knowledge, this is the first attempt to conditionally generate realistic histopathology images.
+The proposed technical contributions (contrastive loss, attention module) appear to improve performance.
+The architecture makes use of modern components: SLE modules, GLU, attention modules, contrastive loss.
+The graphics are good quality and help to understand the general aspects of the paper
+The synthesized images are given to medical experts who found the quality to be “remarkably good”</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>-It is good to see that AttributeGAN is compared against a third-party network (Big-GAN) and also that there are a couple of ablations (w/o attention module, contrastive learning) it is compared against. But a more appropriate comparison would be a model from the InfoGAN [1] cosmos, or something that comes close To Fader Networks [2] which tackle the same problem directly.</p>

      <p>-The number of provided images is a bit meager, I would have expected considerably more synthesized images, especially in the supplementary material</p>

      <p>-Frechet Inception Distance (FID) is not really suitable to medical imaging, as the benchmark network is trained on natural imagery.</p>

      <p>[1] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., &amp; Abbeel, P. (2016). Infogan: Interpretable representation learning by information maximizing generative adversarial nets. arXiv preprint arXiv:1606.03657.
[2] Lample, G., Zeghidour, N., Usunier, N., Bordes, A., Denoyer, L., &amp; Ranzato, M. A. (2017). Fader networks: Manipulating images by sliding attributes. arXiv preprint arXiv:1706.00409.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The authors have described their method in reasonable detail, although some details are missing. The code/data/models have not been made public. Medium/low reproducibility.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>As mentioned above, the FID score is not an appropriate metric for histopathology images, but as there is no readily available alternative I can understand its use.</p>

      <p>The main shortcoming of the paper in my opinion is the missing comparison to well-known attribute-controlling architectures such as InfoGAN, Fader Networks, StyleGAN v2, etc. Simply applying these off the shelf may be competitive with the proposed method.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper solves its task (synthesis of histopathology images) well but also does not really offer a real novelty other than combining components that are already there. It would have also been interesting to see how this model performs on other medical data. The paper itself is exceptionally clear and delivers its message in a very direct way. In general, I would like to see more GANs this model is compared against.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper presents a multi-attribute conditional generative model that is built on top of recent works on stable GANs, self-attention, and contrastive learning. The proposed model can synthesize high-resolution histopathology images with improved image quality and better-capturing input attributes.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>The model builds on recent works in GANs, efficient self-attention, and contrastive learning.</li>
        <li>The use of contrastive learning within the discriminator to capture subtle changes in attribute levels of the image is an interesting idea.</li>
        <li>Efficient self-attention reduces the computational overhead and allows for larger batch sizes needed for contrastive learning.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>As with all conditional generative models, this method require some level of supervision in defining the attributes to conditional image synthesis.</li>
        <li>The proposed model can not leverage unlabeled data, e.g., in a semi-supervised fashion.</li>
        <li>The performance of the proposed model is weakly baselined by a GAN variant that handle high resolution image. Other more relevant SOTA conditional models are not considered (e.g., Fader Networks and AttGAN,)</li>
        <li>The qualitative way that attributes are defined adds in subjectivity and inter-/intra-rater variability.</li>
        <li>Histopathology image generation is qualitatively evaluated by experts.</li>
        <li>The impact of noisy labels/attributes is not considered.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The paper has enough details for reproducibility. It is not clear if code would be released.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ul>
        <li>How can the model handle unlabeled data?</li>
        <li>Authors should consider a more comprehensive comparisons with relevant SOTA models such as Fader Networks and AttGAN.</li>
        <li>It is not clear how attributes were estimated from pathology reports.</li>
        <li>Model evaluation can be further improved by considering a randomized/blind user study where pathologists provide pathology reports on images without prior knowledge of whether an image is a real or synthesized one.</li>
        <li>In eq 1, how negative pairs contribute to the contrastive loss?</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Results are weakly baselined and model evaluation could be improve. However, the paper presents some interesting adaptations to medical image synthesis. With its level of clarity and organization, the paper could be considered for a MICCAI publication.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>The paper has received positive feedback from all reviewers, although some concerns were raised. The authors are encouraged to address the reviewer comments in the final version and improve the quality of images (&amp;text within images) in the paper. A comprehensive comparison with the state of the art is also missing.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>N/A</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0858-12-31
      -->
      <!--
      
        ,
        updated at 
        0859-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Modalities - Histopathology"
        class="post-category">
        Modalities - Histopathology
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Interpretability / Explainability"
        class="post-category">
        Machine Learning - Interpretability / Explainability
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Ye, Jiarong"
        class="post-category">
        Ye, Jiarong
      </a> |  
      
      <a href="kittywong/tags#Xue, Yuan"
        class="post-category">
        Xue, Yuan
      </a> |  
      
      <a href="kittywong/tags#Liu, Peter"
        class="post-category">
        Liu, Peter
      </a> |  
      
      <a href="kittywong/tags#Zaino, Richard"
        class="post-category">
        Zaino, Richard
      </a> |  
      
      <a href="kittywong/tags#Cheng, Keith C."
        class="post-category">
        Cheng, Keith C.
      </a> |  
      
      <a href="kittywong/tags#Huang, Xiaolei"
        class="post-category">
        Huang, Xiaolei
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0859/12/31/Paper0040">
          USCL: Pretraining Deep Ultrasound Image Diagnosis Model through Video Contrastive Representation Learning
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0857/12/31/Paper2230">
          Adversarial learning of cancer tissue representations
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
