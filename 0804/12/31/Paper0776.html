<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>MIL-VT: Multiple Instance Learning Enhanced Vision Transformer for Fundus Image Classification | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="MIL-VT: Multiple Instance Learning Enhanced Vision Transformer for Fundus Image Classification" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Shuang Yu, Kai Ma, Qi Bi, Cheng Bian, Munan Ning, Nanjun He, Yuexiang Li, Hanruo Liu, Yefeng Zheng Abstract With the advancement and prevailing success of Transformer models in the natural language processing (NLP) field, an increasing number of research works have explored the applicability of Transformer for various vision tasks and reported superior performance compared with convolutional neural networks (CNNs). However, as the proper training of Transformer generally requires an extremely large quantity of data, it has rarely been explored for the medical imaging tasks. In this paper, we attempt to adopt the Vision Transformer for the retinal disease classification tasks, by pre-training the Transformer model on a large fundus image database and then fine-tuning on downstream retinal disease classification tasks. In addition, to fully exploit the feature representations extracted by individual image patches, we propose a multiple instance learning (MIL) based `MIL head’, which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks. The proposed MIL-VT framework achieves superior performance over CNN models on two publicly available datasets when being trained and tested under the same setup. We will release the implementation code and pre-trained weights for public access. Link to paper https://doi.org/10.1007/978-3-030-87237-3_5 Link to the code repository https://github.com/greentreeys/MIL-VT Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The main idea of the paper is to propose a multiple instance learning (MIL) based ‘MIL head’, which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of MIL head is interesting and novel. The use of MIL head is effective and may be helpful to improve the classification performance. The experiments are presented with an ablation study and comparison. Details analysis of the results is given. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. What is the difference between the MIL Embedding and Patch Embedding? There are similar operations in these two modules. More details about why MIL head could be helpful for improving the classification performance. Low-dimensional embedding (formulation 1) is similar to the FFN method in the transformer, could it be understood as a 1x1 convolution operation here to extract nonlinear features? The author claims that individual patches may contain important complementary feature information, but the Attention aggregation function only included two linear layers, layer normalization, a ReLU layer, a dropout layer, and a softmax layer. I am not convinced that such an operation can get more complementary feature information than patch embedding. Vision transformer contains the self-attention, but MIL head still adopts the attention way. Is the attention way redundant? Transformer in Transformer(https://arxiv.org/pdf/2103.00112.pdfarxiv.org) is used to handle the same problem that patch embedding loses the complementary feature information. Therefore, what is the difference between TIT and MIL, and which is better? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance no code Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html see 4 Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed MIL head for vision transformer is novelty for fundus image classification, and experimental results demonstrate its effectivness. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper applies the vision transformer on the retinal disease classification task with pretraining on a large fundus image database. This paper also develops a multiple instance learning head, which can be combined with vision transformer to enhance the model performance. Superior performance is demonstrated on two datasets. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. By using the large fundus database rather than the ImageNet dataset, the finetuning on retinal disease classification can achieve better performance. The feature representation of individual patches are utilized by the multiple instance learning head to help improving the classification performance. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The pretraining on a large fundus image classification database uses the pretrained model on ImageNet. It means the model pretrained on the large fundus database use extra data to train the model compared to the pure ImageNet based pretrained one. Since a weighted average of two classification predictions is used as the final prediction result. The contribution of each part is not clear by only reporting the results of a empirically set value of 0.5. The motivation to utilize the feature representation is easy to be understood. But the motivation use MIL is not clear. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Code not provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html How does the value of lambda influence the performance? Different aggregation strategies, for example avg, mean pooling can be considered as baselines to study the effect of the proposed MIL head. ‘only they can it perform well on downstream classification’ in page 2 Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? In addition to apply ViT to the fundus image classification, the paper originally proposes an MIL head to further take advantage of the feature representations. Experimental results on two datasets demonstrates the effectiveness of the proposed methods. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper In this paper, the authors adopt the Vision Transformer for the retinal disease classification tasks. The Transformer model is pre-trained on a large fundus image database and then fine-tuned on retinal disease classification tasks. The authors propose a multiple instance learning (MIL) module to fully exploit the feature representations extracted by individual image patches. The proposed method has been tested on two fundus image classification datasets. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It firstly adopt the Vision Transformer for the retinal disease classification tasks, by pre-training on a large fundus image database. The authors propose a multiple instance learning (MIL) module which exploit the features extracted from individual patches. The proposed method has been evaluated on two fundus image classification datasets, APTOS2019 and RFMiD2020, and achieved good performances compared with existing methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The proposed method is supposed to be compared with state-of-the-art methods. The compared methods are mainly based on ResNet34, which is not an advanced baseline nowadays. More strong baseline models should be considered for comparison. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The methodology is clearly demonstrated. It seems that the authors use private large dataset for training, therefore, it may be hard to reproduce the experiments without this dataset. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Regarding table 1 and 2, it will be more convincing if you can show the results of VT(ImageNet) with MIL. A typo: in the last sentence in Section 2.2, ‘The proposed MIL head… take full utilization of…’, ‘take’ should be ‘takes’. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The introduction for the method is good. The experiment designs and results are satisfactory. But the novelty is limited. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposes a ‘MIL head’ that can be conveniently attached to the Vision Transformer in a plug-and-play manner, to improve the classification performance on fundus images. Given three consistent positive reviews, I recommend accepting this submission. The authors should address the detailed comments from the reviewers in the camera-ready manuscript. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback N/A back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Shuang Yu, Kai Ma, Qi Bi, Cheng Bian, Munan Ning, Nanjun He, Yuexiang Li, Hanruo Liu, Yefeng Zheng Abstract With the advancement and prevailing success of Transformer models in the natural language processing (NLP) field, an increasing number of research works have explored the applicability of Transformer for various vision tasks and reported superior performance compared with convolutional neural networks (CNNs). However, as the proper training of Transformer generally requires an extremely large quantity of data, it has rarely been explored for the medical imaging tasks. In this paper, we attempt to adopt the Vision Transformer for the retinal disease classification tasks, by pre-training the Transformer model on a large fundus image database and then fine-tuning on downstream retinal disease classification tasks. In addition, to fully exploit the feature representations extracted by individual image patches, we propose a multiple instance learning (MIL) based `MIL head’, which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks. The proposed MIL-VT framework achieves superior performance over CNN models on two publicly available datasets when being trained and tested under the same setup. We will release the implementation code and pre-trained weights for public access. Link to paper https://doi.org/10.1007/978-3-030-87237-3_5 Link to the code repository https://github.com/greentreeys/MIL-VT Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The main idea of the paper is to propose a multiple instance learning (MIL) based ‘MIL head’, which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of MIL head is interesting and novel. The use of MIL head is effective and may be helpful to improve the classification performance. The experiments are presented with an ablation study and comparison. Details analysis of the results is given. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. What is the difference between the MIL Embedding and Patch Embedding? There are similar operations in these two modules. More details about why MIL head could be helpful for improving the classification performance. Low-dimensional embedding (formulation 1) is similar to the FFN method in the transformer, could it be understood as a 1x1 convolution operation here to extract nonlinear features? The author claims that individual patches may contain important complementary feature information, but the Attention aggregation function only included two linear layers, layer normalization, a ReLU layer, a dropout layer, and a softmax layer. I am not convinced that such an operation can get more complementary feature information than patch embedding. Vision transformer contains the self-attention, but MIL head still adopts the attention way. Is the attention way redundant? Transformer in Transformer(https://arxiv.org/pdf/2103.00112.pdfarxiv.org) is used to handle the same problem that patch embedding loses the complementary feature information. Therefore, what is the difference between TIT and MIL, and which is better? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance no code Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html see 4 Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed MIL head for vision transformer is novelty for fundus image classification, and experimental results demonstrate its effectivness. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper applies the vision transformer on the retinal disease classification task with pretraining on a large fundus image database. This paper also develops a multiple instance learning head, which can be combined with vision transformer to enhance the model performance. Superior performance is demonstrated on two datasets. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. By using the large fundus database rather than the ImageNet dataset, the finetuning on retinal disease classification can achieve better performance. The feature representation of individual patches are utilized by the multiple instance learning head to help improving the classification performance. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The pretraining on a large fundus image classification database uses the pretrained model on ImageNet. It means the model pretrained on the large fundus database use extra data to train the model compared to the pure ImageNet based pretrained one. Since a weighted average of two classification predictions is used as the final prediction result. The contribution of each part is not clear by only reporting the results of a empirically set value of 0.5. The motivation to utilize the feature representation is easy to be understood. But the motivation use MIL is not clear. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Code not provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html How does the value of lambda influence the performance? Different aggregation strategies, for example avg, mean pooling can be considered as baselines to study the effect of the proposed MIL head. ‘only they can it perform well on downstream classification’ in page 2 Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? In addition to apply ViT to the fundus image classification, the paper originally proposes an MIL head to further take advantage of the feature representations. Experimental results on two datasets demonstrates the effectiveness of the proposed methods. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper In this paper, the authors adopt the Vision Transformer for the retinal disease classification tasks. The Transformer model is pre-trained on a large fundus image database and then fine-tuned on retinal disease classification tasks. The authors propose a multiple instance learning (MIL) module to fully exploit the feature representations extracted by individual image patches. The proposed method has been tested on two fundus image classification datasets. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It firstly adopt the Vision Transformer for the retinal disease classification tasks, by pre-training on a large fundus image database. The authors propose a multiple instance learning (MIL) module which exploit the features extracted from individual patches. The proposed method has been evaluated on two fundus image classification datasets, APTOS2019 and RFMiD2020, and achieved good performances compared with existing methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The proposed method is supposed to be compared with state-of-the-art methods. The compared methods are mainly based on ResNet34, which is not an advanced baseline nowadays. More strong baseline models should be considered for comparison. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The methodology is clearly demonstrated. It seems that the authors use private large dataset for training, therefore, it may be hard to reproduce the experiments without this dataset. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Regarding table 1 and 2, it will be more convincing if you can show the results of VT(ImageNet) with MIL. A typo: in the last sentence in Section 2.2, ‘The proposed MIL head… take full utilization of…’, ‘take’ should be ‘takes’. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The introduction for the method is good. The experiment designs and results are satisfactory. But the novelty is limited. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposes a ‘MIL head’ that can be conveniently attached to the Vision Transformer in a plug-and-play manner, to improve the classification performance on fundus images. Given three consistent positive reviews, I recommend accepting this submission. The authors should address the detailed comments from the reviewers in the camera-ready manuscript. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback N/A back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0804/12/31/Paper0776" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0804/12/31/Paper0776" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0804-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="MIL-VT: Multiple Instance Learning Enhanced Vision Transformer for Fundus Image Classification" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0804/12/31/Paper0776"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0804/12/31/Paper0776","headline":"MIL-VT: Multiple Instance Learning Enhanced Vision Transformer for Fundus Image Classification","dateModified":"0805-01-05T00:00:00-05:17","datePublished":"0804-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Shuang Yu, Kai Ma, Qi Bi, Cheng Bian, Munan Ning, Nanjun He, Yuexiang Li, Hanruo Liu, Yefeng Zheng Abstract With the advancement and prevailing success of Transformer models in the natural language processing (NLP) field, an increasing number of research works have explored the applicability of Transformer for various vision tasks and reported superior performance compared with convolutional neural networks (CNNs). However, as the proper training of Transformer generally requires an extremely large quantity of data, it has rarely been explored for the medical imaging tasks. In this paper, we attempt to adopt the Vision Transformer for the retinal disease classification tasks, by pre-training the Transformer model on a large fundus image database and then fine-tuning on downstream retinal disease classification tasks. In addition, to fully exploit the feature representations extracted by individual image patches, we propose a multiple instance learning (MIL) based `MIL head’, which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks. The proposed MIL-VT framework achieves superior performance over CNN models on two publicly available datasets when being trained and tested under the same setup. We will release the implementation code and pre-trained weights for public access. Link to paper https://doi.org/10.1007/978-3-030-87237-3_5 Link to the code repository https://github.com/greentreeys/MIL-VT Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The main idea of the paper is to propose a multiple instance learning (MIL) based ‘MIL head’, which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The idea of MIL head is interesting and novel. The use of MIL head is effective and may be helpful to improve the classification performance. The experiments are presented with an ablation study and comparison. Details analysis of the results is given. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. What is the difference between the MIL Embedding and Patch Embedding? There are similar operations in these two modules. More details about why MIL head could be helpful for improving the classification performance. Low-dimensional embedding (formulation 1) is similar to the FFN method in the transformer, could it be understood as a 1x1 convolution operation here to extract nonlinear features? The author claims that individual patches may contain important complementary feature information, but the Attention aggregation function only included two linear layers, layer normalization, a ReLU layer, a dropout layer, and a softmax layer. I am not convinced that such an operation can get more complementary feature information than patch embedding. Vision transformer contains the self-attention, but MIL head still adopts the attention way. Is the attention way redundant? Transformer in Transformer(https://arxiv.org/pdf/2103.00112.pdfarxiv.org) is used to handle the same problem that patch embedding loses the complementary feature information. Therefore, what is the difference between TIT and MIL, and which is better? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance no code Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html see 4 Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The proposed MIL head for vision transformer is novelty for fundus image classification, and experimental results demonstrate its effectivness. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper applies the vision transformer on the retinal disease classification task with pretraining on a large fundus image database. This paper also develops a multiple instance learning head, which can be combined with vision transformer to enhance the model performance. Superior performance is demonstrated on two datasets. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. By using the large fundus database rather than the ImageNet dataset, the finetuning on retinal disease classification can achieve better performance. The feature representation of individual patches are utilized by the multiple instance learning head to help improving the classification performance. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The pretraining on a large fundus image classification database uses the pretrained model on ImageNet. It means the model pretrained on the large fundus database use extra data to train the model compared to the pure ImageNet based pretrained one. Since a weighted average of two classification predictions is used as the final prediction result. The contribution of each part is not clear by only reporting the results of a empirically set value of 0.5. The motivation to utilize the feature representation is easy to be understood. But the motivation use MIL is not clear. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Code not provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html How does the value of lambda influence the performance? Different aggregation strategies, for example avg, mean pooling can be considered as baselines to study the effect of the proposed MIL head. ‘only they can it perform well on downstream classification’ in page 2 Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? In addition to apply ViT to the fundus image classification, the paper originally proposes an MIL head to further take advantage of the feature representations. Experimental results on two datasets demonstrates the effectiveness of the proposed methods. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper In this paper, the authors adopt the Vision Transformer for the retinal disease classification tasks. The Transformer model is pre-trained on a large fundus image database and then fine-tuned on retinal disease classification tasks. The authors propose a multiple instance learning (MIL) module to fully exploit the feature representations extracted by individual image patches. The proposed method has been tested on two fundus image classification datasets. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. It firstly adopt the Vision Transformer for the retinal disease classification tasks, by pre-training on a large fundus image database. The authors propose a multiple instance learning (MIL) module which exploit the features extracted from individual patches. The proposed method has been evaluated on two fundus image classification datasets, APTOS2019 and RFMiD2020, and achieved good performances compared with existing methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The proposed method is supposed to be compared with state-of-the-art methods. The compared methods are mainly based on ResNet34, which is not an advanced baseline nowadays. More strong baseline models should be considered for comparison. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The methodology is clearly demonstrated. It seems that the authors use private large dataset for training, therefore, it may be hard to reproduce the experiments without this dataset. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Regarding table 1 and 2, it will be more convincing if you can show the results of VT(ImageNet) with MIL. A typo: in the last sentence in Section 2.2, ‘The proposed MIL head… take full utilization of…’, ‘take’ should be ‘takes’. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The introduction for the method is good. The experiment designs and results are satisfactory. But the novelty is limited. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposes a ‘MIL head’ that can be conveniently attached to the Vision Transformer in a plug-and-play manner, to improve the classification performance on fundus images. Given three consistent positive reviews, I recommend accepting this submission. The authors should address the detailed comments from the reviewers in the camera-ready manuscript. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback N/A back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Yu, Shuang,Ma, Kai,Bi, Qi,Bian, Cheng,Ning, Munan,He, Nanjun,Li, Yuexiang,Liu, Hanruo,Zheng, Yefeng" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>MIL-VT: Multiple Instance Learning Enhanced Vision Transformer for Fundus Image Classification</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Clinical applications - Ophthalmology"
        class="post-category">
        Clinical applications - Ophthalmology
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Yu, Shuang"
        class="post-tags">
        Yu, Shuang
      </a> |  
      
      <a href="kittywong/tags#Ma, Kai"
        class="post-tags">
        Ma, Kai
      </a> |  
      
      <a href="kittywong/tags#Bi, Qi"
        class="post-tags">
        Bi, Qi
      </a> |  
      
      <a href="kittywong/tags#Bian, Cheng"
        class="post-tags">
        Bian, Cheng
      </a> |  
      
      <a href="kittywong/tags#Ning, Munan"
        class="post-tags">
        Ning, Munan
      </a> |  
      
      <a href="kittywong/tags#He, Nanjun"
        class="post-tags">
        He, Nanjun
      </a> |  
      
      <a href="kittywong/tags#Li, Yuexiang"
        class="post-tags">
        Li, Yuexiang
      </a> |  
      
      <a href="kittywong/tags#Liu, Hanruo"
        class="post-tags">
        Liu, Hanruo
      </a> |  
      
      <a href="kittywong/tags#Zheng, Yefeng"
        class="post-tags">
        Zheng, Yefeng
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Shuang Yu, Kai Ma, Qi Bi, Cheng Bian, Munan Ning, Nanjun He, Yuexiang Li, Hanruo Liu, Yefeng Zheng
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>With the advancement and prevailing success of Transformer models in the natural language processing (NLP) field, an increasing number of research works have explored the applicability of Transformer for various vision tasks and reported superior performance compared with convolutional neural networks (CNNs). However, as the proper training of Transformer generally requires an extremely large quantity of data, it has rarely been explored for the medical imaging tasks. In this paper, we attempt to adopt the Vision Transformer for the retinal disease classification tasks, by pre-training the Transformer model on a large fundus image database and then fine-tuning on downstream retinal disease classification tasks. In addition, to fully exploit the feature representations extracted by individual image patches, we propose a multiple instance learning (MIL) based `MIL head’, which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks. The proposed MIL-VT framework achieves superior performance over CNN models on two publicly available datasets when being trained and tested under the same setup. We will release the implementation code and pre-trained weights for public access.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87237-3_5">https://doi.org/10.1007/978-3-030-87237-3_5</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/greentreeys/MIL-VT
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The main idea of the paper is to propose a multiple instance learning (MIL) based ‘MIL head’, which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>The idea of MIL head is interesting and novel.</li>
        <li>The use of MIL head is effective and may be helpful to improve the classification performance.</li>
        <li>The experiments are presented with an ablation study and comparison. Details analysis of the results is given.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>What is the difference between the MIL Embedding and Patch Embedding? There are similar operations in these two modules.</li>
        <li>More details about why MIL head could be helpful for improving the classification performance.</li>
        <li>Low-dimensional embedding (formulation 1) is similar to the FFN method in the transformer, could it be understood as a 1x1 convolution operation here to extract nonlinear features?</li>
        <li>The author claims that individual patches may contain important complementary feature information, but the Attention aggregation function only included two linear layers, layer normalization, a ReLU layer, a dropout layer, and a softmax layer. I am not convinced that such an operation can get more complementary feature information than patch embedding.</li>
        <li>Vision transformer contains the self-attention, but MIL head still adopts the attention way. Is the attention way redundant?</li>
        <li>Transformer in Transformer(https://arxiv.org/pdf/2103.00112.pdfarxiv.org) is used to handle the same problem that patch embedding loses the complementary feature information. Therefore, what is the difference between TIT and MIL, and which is better?</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>no code</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>see 4</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The proposed MIL head for vision transformer is novelty for fundus image classification, and experimental results demonstrate its effectivness.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper applies the vision transformer on the retinal disease classification task with pretraining on a large fundus image database. This paper also develops a multiple instance learning head, which can be combined with vision transformer to enhance the model performance. Superior performance is demonstrated on two datasets.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>By using the large fundus database rather than the ImageNet dataset, the finetuning on retinal disease classification can achieve better performance.
The feature representation of individual patches are utilized by the multiple instance learning head to help improving the classification performance.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The pretraining on a large fundus image classification database uses the pretrained model on ImageNet. It means the model pretrained on the large fundus database use extra data to train the model compared to the pure ImageNet based pretrained one.
Since a weighted average of two classification predictions is used as the final prediction result. The contribution of each part is not clear by only reporting the results of a empirically set value of 0.5.
The motivation to utilize the feature representation is easy to be understood. But the motivation use MIL is not clear.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Code not provided.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>How does the value of lambda influence the performance?
Different aggregation strategies, for example avg, mean pooling can be considered as baselines to study the effect of the proposed MIL head.
‘only they can it perform well on downstream classification’ in page 2</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>In addition to apply ViT to the fundus image classification, the paper originally proposes an MIL head to further take advantage of the feature representations. Experimental results on two datasets demonstrates the effectiveness of the proposed methods.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>In this paper, the authors adopt the Vision Transformer for the retinal disease classification tasks. The Transformer model is pre-trained on a large fundus image database and then fine-tuned on retinal disease classification tasks. The authors propose a multiple instance learning (MIL) module to fully exploit the feature representations extracted by individual image patches. The proposed method has been tested on two fundus image classification datasets.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>It firstly adopt the Vision Transformer for the retinal disease classification tasks, by pre-training on a large fundus image database.</li>
        <li>The authors propose a multiple instance learning (MIL) module which exploit the features extracted from individual patches.</li>
        <li>The proposed method has been evaluated on two fundus image classification datasets, APTOS2019 and RFMiD2020, and achieved good performances compared with existing methods.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>The proposed method is supposed to be compared with state-of-the-art methods. The compared methods are mainly based on ResNet34, which is not an advanced baseline nowadays. More strong baseline models should be considered for comparison.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The methodology is clearly demonstrated.
It seems that the authors use private large dataset for training, therefore, it may be hard to reproduce the experiments without this dataset.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Regarding table 1 and 2, it will be more convincing if you can show the results of VT(ImageNet) with MIL. 
A typo: in the last sentence in Section 2.2, ‘The proposed MIL head… take full utilization of…’, ‘take’ should be ‘takes’.</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The introduction for the method is good. The experiment designs and results are satisfactory. But the novelty is limited.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>This paper proposes a ‘MIL head’ that can be conveniently attached to the Vision Transformer in a plug-and-play manner, to improve the classification performance on fundus images. Given three consistent positive reviews, I recommend accepting this submission. The authors should address the detailed comments from the reviewers in the camera-ready manuscript.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>N/A</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0804-12-31
      -->
      <!--
      
        ,
        updated at 
        0805-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Clinical applications - Ophthalmology"
        class="post-category">
        Clinical applications - Ophthalmology
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Yu, Shuang"
        class="post-category">
        Yu, Shuang
      </a> |  
      
      <a href="kittywong/tags#Ma, Kai"
        class="post-category">
        Ma, Kai
      </a> |  
      
      <a href="kittywong/tags#Bi, Qi"
        class="post-category">
        Bi, Qi
      </a> |  
      
      <a href="kittywong/tags#Bian, Cheng"
        class="post-category">
        Bian, Cheng
      </a> |  
      
      <a href="kittywong/tags#Ning, Munan"
        class="post-category">
        Ning, Munan
      </a> |  
      
      <a href="kittywong/tags#He, Nanjun"
        class="post-category">
        He, Nanjun
      </a> |  
      
      <a href="kittywong/tags#Li, Yuexiang"
        class="post-category">
        Li, Yuexiang
      </a> |  
      
      <a href="kittywong/tags#Liu, Hanruo"
        class="post-category">
        Liu, Hanruo
      </a> |  
      
      <a href="kittywong/tags#Zheng, Yefeng"
        class="post-category">
        Zheng, Yefeng
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0805/12/31/Paper0778">
          Local-global Dual Perception based Deep Multiple Instance Learning for Retinal Disease Classification
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0803/12/31/Paper0464">
          RV-GAN: Segmenting Retinal Vascular Structure in Fundus Photographs using a Novel Multi-scale Generative Adversarial Network
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
