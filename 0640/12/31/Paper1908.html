<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Interpretable deep learning for multimodal super-resolution of medical images | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Interpretable deep learning for multimodal super-resolution of medical images" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Evaggelia Tsiligianni, Matina Zerva, Iman Marivani, Nikos Deligiannis, Lisimachos Kondi Abstract In medical image acquisition, hardware limitations and scanning time constraints result in degraded images. Super-resolution (SR) is a post-processing approach aiming to reconstruct a high-resolution image from its low-resolution counterpart. Recent advances in medical image SR include the application of deep neural networks, which can improve image quality at a low computational cost. When dealing with medical data, accuracy is important for discovery and diagnosis, therefore, interpretable neural network models are of significant interest as they enable a theoretical study and increase trustworthiness needed in clinical practice. While several interpretable deep learning designs have been proposed to treat unimodal images, to the best of our knowledge, there is no multimodal SR approach applied for medical images. In this paper, we present an interpretable neural network model that exploits information from multiple modalities to super-resolve an image of a target modality. Experiments with simulated and real MRI data show the performance of the proposed approach in terms of numerical and visual results. Link to paper https://doi.org/10.1007/978-3-030-87231-1_41 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This work proposed a multimodel convolutional deep unfolding framework for medical image super-resolution. Specifically, they try to utilize multimodel convolution spare coding model to restore high-resolution T2W image using low-resolution T2W and high-resolution T1W image. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper is clearly written and easy to follow. The proposed framework is technically sound. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Some citations are missing, e.g., [1] and [2] The comparison with other state-of-the-art methods is not sufficient, only one method is compared. There is no detailed analysis and discussion about the results quantitatively and qualitatively [1]. Zhou, Bo, and S. Kevin Zhou. “DuDoRNet: Learning a dual-domain recurrent network for fast MRI reconstruction with deep T1 prior.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. [2]. Xiang, Lei, et al. “Deep-learning-based multi-modal fusion for fast MR reconstruction.” IEEE Transactions on Biomedical Engineering 66.7 (2018): 2105-2114. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance it’s possible to reproduce this work. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Section 2 and 3 should be brief introduction, the authors spends too much context on these two parts. More comparisons with other related work as listed above are needed. There is no visualization comparison between coISTA and proposed method. The figures in Fig. 2 should be rotated by 90 degrees. Ablation study for adding high-resolution T1W or FLAIR images as complementary information to help recover better T2W image should be conducted. The authors claim the proposed approach has fast inference, however there is no experiments to support this. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I think the paper has merit, it is well written and technically sound. Result section is problematic because the comparison between the proposed approach and other state-of-the-art methods is not sufficient. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper An interpretable image Super-Resolution neural network model exploiting information from diverse modalities has been proposed. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Interpretable Deep Learning modeling for multimodal medical image reconstruction is a novel research topic. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Information about computational cost of the proposed method is missing. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I think that reproducibility checklist for this paper has been properly fulfilled except for the average runtime for each result which has not been given. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html A learned multimodal convolutional sparse coding (LMCSC) model has been proposed. It is an interpretable neural network model. The model has been applied to multi-constrast Medical Resonance Imaging obtaining very good results for simulated and real images. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? An interesting approach to a very hard problem has been described. The proposed model seems to have useful clinical application. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The authors proposed an interpretable neural network model, which exploits information from multiple modalities to super-resolve an image of a target modality. Experiments show the good performance in terms of numerical and visual results. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -This paper is well organized. -The authors solved a coupled sparse representation problem using an iterative thresholding algorithm. And then unfolded this algorithm into a neural network form. -The authors incorporated this learned multimodal convolutional sparse coding model into a network to reconstruct high-resolution images. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -The experiment is not sufficient and the details are missing. -The number of article pages needs to be expanded, and the workload is not enough. -Vague discussion about future work. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The details of the paper are not clear enough, and the reproducibility is insufficient. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -The details of experiment are missing, and less experimental comparison results. -The number of article pages needs to be expanded. -A more clear outline of the next steps in research would be appropriate, some discussion but vague. There are already some good work about interpretable deep learning for medical images. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? -An interpretable neural network model was presented to exploit information from multiple modalities to super-resolve an image of a target modality. -The experimental results are few, and the comparative experiments are not convincing, and there is no ablation study. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposed an interpretable deep network by fusing cross-modal medical images for super-resolution. The reviewers mostly feel positive on this work, and scored 6, 9, 6. The reviewers also have some comments on missing references, clarity on details, insufficient comparisons. The authors are suggested to revise the paper according to these comments in the final version. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback We would like to thank the reviewers for the positive evaluation of our work and their constructive comments helping tο improve our paper. Next, we will try to reply to the major concerns raised by the reviewers. We will use the notation R#:Q$ to refer to comments included as a reply to Question $ by Reviewer #. We group the major reviewers’ comments as follows: All reviewers (R1:Q7, R2:Q4, R3:Q4) assert that more reference methods should be included in our experiments for comparison. A major obstacle while designing our experiments is that neither the code of related methods nor the details of the datasets employed in other works are published. Therefore, due to time limitations, we have chosen to implement an interpretable deep learning model and include it as a reference method, since we believe that this work is mostly related with our approach. However, we agree that the comparison is limited and we will do our best to include more reference methods in our final submission. Some comments suggest that an ablation study should also be included (R3:Q9). We agree with the suggestion, and we will provide a study concerning the performance of our model for a varying number of stages in our final submission. However, we think that R1 has misunderstood our experimental setting in R1:Q7, where it is asked to include high-resolution (HR) T1W or HR-FLAIR images as complementary information to super-resolve low-resolution (LR) T2W; this is exactly what we have presented. An alternative experiment would be to test our method with LR-T1W or LR-FLAIR images as complementary information. As long as page limitations allow including such an experiment, it will be added in our final submission. In R1:Q7, the reviewer suggests adding visual results of the reference methods. We will include such results in our final submission exploiting the additional page allowance. In R3:Q6, the reviewer raises concerns regarding the reproducibility of our work. We believe that we have provided all the information concerning the parameter setting and the initialization details of the proposed neural network so as the interested researcher can reproduce the code. Nevertheless, we plan to make our code publicly available after paper publication. Some references suggested in R1:Q4 will be definitely included in our final submission. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Evaggelia Tsiligianni, Matina Zerva, Iman Marivani, Nikos Deligiannis, Lisimachos Kondi Abstract In medical image acquisition, hardware limitations and scanning time constraints result in degraded images. Super-resolution (SR) is a post-processing approach aiming to reconstruct a high-resolution image from its low-resolution counterpart. Recent advances in medical image SR include the application of deep neural networks, which can improve image quality at a low computational cost. When dealing with medical data, accuracy is important for discovery and diagnosis, therefore, interpretable neural network models are of significant interest as they enable a theoretical study and increase trustworthiness needed in clinical practice. While several interpretable deep learning designs have been proposed to treat unimodal images, to the best of our knowledge, there is no multimodal SR approach applied for medical images. In this paper, we present an interpretable neural network model that exploits information from multiple modalities to super-resolve an image of a target modality. Experiments with simulated and real MRI data show the performance of the proposed approach in terms of numerical and visual results. Link to paper https://doi.org/10.1007/978-3-030-87231-1_41 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This work proposed a multimodel convolutional deep unfolding framework for medical image super-resolution. Specifically, they try to utilize multimodel convolution spare coding model to restore high-resolution T2W image using low-resolution T2W and high-resolution T1W image. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper is clearly written and easy to follow. The proposed framework is technically sound. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Some citations are missing, e.g., [1] and [2] The comparison with other state-of-the-art methods is not sufficient, only one method is compared. There is no detailed analysis and discussion about the results quantitatively and qualitatively [1]. Zhou, Bo, and S. Kevin Zhou. “DuDoRNet: Learning a dual-domain recurrent network for fast MRI reconstruction with deep T1 prior.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. [2]. Xiang, Lei, et al. “Deep-learning-based multi-modal fusion for fast MR reconstruction.” IEEE Transactions on Biomedical Engineering 66.7 (2018): 2105-2114. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance it’s possible to reproduce this work. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Section 2 and 3 should be brief introduction, the authors spends too much context on these two parts. More comparisons with other related work as listed above are needed. There is no visualization comparison between coISTA and proposed method. The figures in Fig. 2 should be rotated by 90 degrees. Ablation study for adding high-resolution T1W or FLAIR images as complementary information to help recover better T2W image should be conducted. The authors claim the proposed approach has fast inference, however there is no experiments to support this. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I think the paper has merit, it is well written and technically sound. Result section is problematic because the comparison between the proposed approach and other state-of-the-art methods is not sufficient. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper An interpretable image Super-Resolution neural network model exploiting information from diverse modalities has been proposed. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Interpretable Deep Learning modeling for multimodal medical image reconstruction is a novel research topic. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Information about computational cost of the proposed method is missing. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I think that reproducibility checklist for this paper has been properly fulfilled except for the average runtime for each result which has not been given. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html A learned multimodal convolutional sparse coding (LMCSC) model has been proposed. It is an interpretable neural network model. The model has been applied to multi-constrast Medical Resonance Imaging obtaining very good results for simulated and real images. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? An interesting approach to a very hard problem has been described. The proposed model seems to have useful clinical application. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The authors proposed an interpretable neural network model, which exploits information from multiple modalities to super-resolve an image of a target modality. Experiments show the good performance in terms of numerical and visual results. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -This paper is well organized. -The authors solved a coupled sparse representation problem using an iterative thresholding algorithm. And then unfolded this algorithm into a neural network form. -The authors incorporated this learned multimodal convolutional sparse coding model into a network to reconstruct high-resolution images. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -The experiment is not sufficient and the details are missing. -The number of article pages needs to be expanded, and the workload is not enough. -Vague discussion about future work. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The details of the paper are not clear enough, and the reproducibility is insufficient. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -The details of experiment are missing, and less experimental comparison results. -The number of article pages needs to be expanded. -A more clear outline of the next steps in research would be appropriate, some discussion but vague. There are already some good work about interpretable deep learning for medical images. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? -An interpretable neural network model was presented to exploit information from multiple modalities to super-resolve an image of a target modality. -The experimental results are few, and the comparative experiments are not convincing, and there is no ablation study. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposed an interpretable deep network by fusing cross-modal medical images for super-resolution. The reviewers mostly feel positive on this work, and scored 6, 9, 6. The reviewers also have some comments on missing references, clarity on details, insufficient comparisons. The authors are suggested to revise the paper according to these comments in the final version. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback We would like to thank the reviewers for the positive evaluation of our work and their constructive comments helping tο improve our paper. Next, we will try to reply to the major concerns raised by the reviewers. We will use the notation R#:Q$ to refer to comments included as a reply to Question $ by Reviewer #. We group the major reviewers’ comments as follows: All reviewers (R1:Q7, R2:Q4, R3:Q4) assert that more reference methods should be included in our experiments for comparison. A major obstacle while designing our experiments is that neither the code of related methods nor the details of the datasets employed in other works are published. Therefore, due to time limitations, we have chosen to implement an interpretable deep learning model and include it as a reference method, since we believe that this work is mostly related with our approach. However, we agree that the comparison is limited and we will do our best to include more reference methods in our final submission. Some comments suggest that an ablation study should also be included (R3:Q9). We agree with the suggestion, and we will provide a study concerning the performance of our model for a varying number of stages in our final submission. However, we think that R1 has misunderstood our experimental setting in R1:Q7, where it is asked to include high-resolution (HR) T1W or HR-FLAIR images as complementary information to super-resolve low-resolution (LR) T2W; this is exactly what we have presented. An alternative experiment would be to test our method with LR-T1W or LR-FLAIR images as complementary information. As long as page limitations allow including such an experiment, it will be added in our final submission. In R1:Q7, the reviewer suggests adding visual results of the reference methods. We will include such results in our final submission exploiting the additional page allowance. In R3:Q6, the reviewer raises concerns regarding the reproducibility of our work. We believe that we have provided all the information concerning the parameter setting and the initialization details of the proposed neural network so as the interested researcher can reproduce the code. Nevertheless, we plan to make our code publicly available after paper publication. Some references suggested in R1:Q4 will be definitely included in our final submission. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0640/12/31/Paper1908" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0640/12/31/Paper1908" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0640-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Interpretable deep learning for multimodal super-resolution of medical images" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0640/12/31/Paper1908"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0640/12/31/Paper1908","headline":"Interpretable deep learning for multimodal super-resolution of medical images","dateModified":"0641-01-04T00:00:00-05:17","datePublished":"0640-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Evaggelia Tsiligianni, Matina Zerva, Iman Marivani, Nikos Deligiannis, Lisimachos Kondi Abstract In medical image acquisition, hardware limitations and scanning time constraints result in degraded images. Super-resolution (SR) is a post-processing approach aiming to reconstruct a high-resolution image from its low-resolution counterpart. Recent advances in medical image SR include the application of deep neural networks, which can improve image quality at a low computational cost. When dealing with medical data, accuracy is important for discovery and diagnosis, therefore, interpretable neural network models are of significant interest as they enable a theoretical study and increase trustworthiness needed in clinical practice. While several interpretable deep learning designs have been proposed to treat unimodal images, to the best of our knowledge, there is no multimodal SR approach applied for medical images. In this paper, we present an interpretable neural network model that exploits information from multiple modalities to super-resolve an image of a target modality. Experiments with simulated and real MRI data show the performance of the proposed approach in terms of numerical and visual results. Link to paper https://doi.org/10.1007/978-3-030-87231-1_41 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This work proposed a multimodel convolutional deep unfolding framework for medical image super-resolution. Specifically, they try to utilize multimodel convolution spare coding model to restore high-resolution T2W image using low-resolution T2W and high-resolution T1W image. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper is clearly written and easy to follow. The proposed framework is technically sound. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Some citations are missing, e.g., [1] and [2] The comparison with other state-of-the-art methods is not sufficient, only one method is compared. There is no detailed analysis and discussion about the results quantitatively and qualitatively [1]. Zhou, Bo, and S. Kevin Zhou. “DuDoRNet: Learning a dual-domain recurrent network for fast MRI reconstruction with deep T1 prior.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. [2]. Xiang, Lei, et al. “Deep-learning-based multi-modal fusion for fast MR reconstruction.” IEEE Transactions on Biomedical Engineering 66.7 (2018): 2105-2114. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance it’s possible to reproduce this work. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Section 2 and 3 should be brief introduction, the authors spends too much context on these two parts. More comparisons with other related work as listed above are needed. There is no visualization comparison between coISTA and proposed method. The figures in Fig. 2 should be rotated by 90 degrees. Ablation study for adding high-resolution T1W or FLAIR images as complementary information to help recover better T2W image should be conducted. The authors claim the proposed approach has fast inference, however there is no experiments to support this. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I think the paper has merit, it is well written and technically sound. Result section is problematic because the comparison between the proposed approach and other state-of-the-art methods is not sufficient. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper An interpretable image Super-Resolution neural network model exploiting information from diverse modalities has been proposed. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Interpretable Deep Learning modeling for multimodal medical image reconstruction is a novel research topic. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Information about computational cost of the proposed method is missing. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I think that reproducibility checklist for this paper has been properly fulfilled except for the average runtime for each result which has not been given. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html A learned multimodal convolutional sparse coding (LMCSC) model has been proposed. It is an interpretable neural network model. The model has been applied to multi-constrast Medical Resonance Imaging obtaining very good results for simulated and real images. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? An interesting approach to a very hard problem has been described. The proposed model seems to have useful clinical application. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The authors proposed an interpretable neural network model, which exploits information from multiple modalities to super-resolve an image of a target modality. Experiments show the good performance in terms of numerical and visual results. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -This paper is well organized. -The authors solved a coupled sparse representation problem using an iterative thresholding algorithm. And then unfolded this algorithm into a neural network form. -The authors incorporated this learned multimodal convolutional sparse coding model into a network to reconstruct high-resolution images. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -The experiment is not sufficient and the details are missing. -The number of article pages needs to be expanded, and the workload is not enough. -Vague discussion about future work. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The details of the paper are not clear enough, and the reproducibility is insufficient. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -The details of experiment are missing, and less experimental comparison results. -The number of article pages needs to be expanded. -A more clear outline of the next steps in research would be appropriate, some discussion but vague. There are already some good work about interpretable deep learning for medical images. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? -An interpretable neural network model was presented to exploit information from multiple modalities to super-resolve an image of a target modality. -The experimental results are few, and the comparative experiments are not convincing, and there is no ablation study. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposed an interpretable deep network by fusing cross-modal medical images for super-resolution. The reviewers mostly feel positive on this work, and scored 6, 9, 6. The reviewers also have some comments on missing references, clarity on details, insufficient comparisons. The authors are suggested to revise the paper according to these comments in the final version. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback We would like to thank the reviewers for the positive evaluation of our work and their constructive comments helping tο improve our paper. Next, we will try to reply to the major concerns raised by the reviewers. We will use the notation R#:Q$ to refer to comments included as a reply to Question $ by Reviewer #. We group the major reviewers’ comments as follows: All reviewers (R1:Q7, R2:Q4, R3:Q4) assert that more reference methods should be included in our experiments for comparison. A major obstacle while designing our experiments is that neither the code of related methods nor the details of the datasets employed in other works are published. Therefore, due to time limitations, we have chosen to implement an interpretable deep learning model and include it as a reference method, since we believe that this work is mostly related with our approach. However, we agree that the comparison is limited and we will do our best to include more reference methods in our final submission. Some comments suggest that an ablation study should also be included (R3:Q9). We agree with the suggestion, and we will provide a study concerning the performance of our model for a varying number of stages in our final submission. However, we think that R1 has misunderstood our experimental setting in R1:Q7, where it is asked to include high-resolution (HR) T1W or HR-FLAIR images as complementary information to super-resolve low-resolution (LR) T2W; this is exactly what we have presented. An alternative experiment would be to test our method with LR-T1W or LR-FLAIR images as complementary information. As long as page limitations allow including such an experiment, it will be added in our final submission. In R1:Q7, the reviewer suggests adding visual results of the reference methods. We will include such results in our final submission exploiting the additional page allowance. In R3:Q6, the reviewer raises concerns regarding the reproducibility of our work. We believe that we have provided all the information concerning the parameter setting and the initialization details of the proposed neural network so as the interested researcher can reproduce the code. Nevertheless, we plan to make our code publicly available after paper publication. Some references suggested in R1:Q4 will be definitely included in our final submission. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Tsiligianni, Evaggelia,Zerva, Matina,Marivani, Iman,Deligiannis, Nikos,Kondi, Lisimachos" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Interpretable deep learning for multimodal super-resolution of medical images</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Image Reconstruction"
        class="post-category">
        Image Reconstruction
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Interpretability / Explainability"
        class="post-category">
        Machine Learning - Interpretability / Explainability
      </a>
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Tsiligianni, Evaggelia"
        class="post-tags">
        Tsiligianni, Evaggelia
      </a> |  
      
      <a href="kittywong/tags#Zerva, Matina"
        class="post-tags">
        Zerva, Matina
      </a> |  
      
      <a href="kittywong/tags#Marivani, Iman"
        class="post-tags">
        Marivani, Iman
      </a> |  
      
      <a href="kittywong/tags#Deligiannis, Nikos"
        class="post-tags">
        Deligiannis, Nikos
      </a> |  
      
      <a href="kittywong/tags#Kondi, Lisimachos"
        class="post-tags">
        Kondi, Lisimachos
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Evaggelia Tsiligianni, Matina Zerva, Iman Marivani, Nikos Deligiannis, Lisimachos Kondi
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>In medical image acquisition, hardware limitations and scanning time constraints result in degraded images. Super-resolution (SR) is a post-processing approach aiming to reconstruct a high-resolution image from its low-resolution counterpart. Recent advances in medical image SR include the application of deep neural networks, which can improve image quality at a low computational cost. When dealing with medical data, accuracy is important for discovery and diagnosis, therefore, interpretable neural network models are of significant interest as they enable a theoretical study and increase trustworthiness needed in clinical practice. While several interpretable deep learning designs have been proposed to treat unimodal images, to the best of our knowledge, there is no multimodal SR approach applied for medical images. In this paper, we present an interpretable neural network model that exploits information from multiple modalities to super-resolve an image of a  target modality. Experiments with simulated and real MRI data show the performance of the proposed approach in terms of numerical and visual results.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87231-1_41">https://doi.org/10.1007/978-3-030-87231-1_41</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This work proposed a multimodel convolutional deep unfolding framework for medical image super-resolution. Specifically, they try to utilize multimodel convolution spare coding model to restore high-resolution T2W image using low-resolution T2W and high-resolution T1W image.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>This paper is clearly written and easy to follow.</li>
        <li>The proposed framework is technically sound.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>Some citations are missing, e.g., [1] and [2]</li>
        <li>The comparison with other state-of-the-art methods is not sufficient, only one method is compared.</li>
        <li>There is no detailed analysis and discussion about the results quantitatively and qualitatively</li>
      </ul>

      <p>[1]. Zhou, Bo, and S. Kevin Zhou. “DuDoRNet: Learning a dual-domain recurrent network for fast MRI reconstruction with deep T1 prior.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
[2]. Xiang, Lei, et al. “Deep-learning-based multi-modal fusion for fast MR reconstruction.” IEEE Transactions on Biomedical Engineering 66.7 (2018): 2105-2114.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>it’s possible to reproduce this work.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ol>
        <li>Section 2 and 3 should be brief introduction, the authors spends too much context on these two parts.</li>
        <li>More comparisons with other related work as listed above are needed.</li>
        <li>There is no visualization comparison between coISTA and proposed method.</li>
        <li>The figures in Fig. 2 should be rotated by 90 degrees.</li>
        <li>Ablation study for adding high-resolution T1W or FLAIR images as complementary information to help recover better T2W image should be conducted.</li>
        <li>The authors claim the proposed approach has fast inference, however there is no experiments to support this.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>I think the paper has merit, it is well written and technically sound. Result section is problematic because the comparison between the proposed approach and other state-of-the-art methods is not sufficient.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>An interpretable image Super-Resolution neural network model exploiting information from diverse modalities has been proposed.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>Interpretable Deep Learning modeling for multimodal medical image reconstruction is a novel research topic.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>Information about computational cost of the proposed method is missing.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>I think that reproducibility checklist for this paper has been properly fulfilled except for the average runtime for each result which has not been given.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>A learned multimodal convolutional sparse coding (LMCSC) model has been proposed. It is an interpretable neural network model.</p>

      <p>The model has been applied to multi-constrast Medical Resonance Imaging obtaining very good results for simulated and real images.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>An interesting approach to a very hard problem has been described. The proposed model seems to have useful clinical application.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors proposed an interpretable neural network model, which exploits information from multiple modalities to super-resolve an image of a target modality. Experiments show the good performance in terms of numerical and visual results.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>-This paper is well organized.
-The authors solved a coupled sparse representation problem using an iterative thresholding algorithm. And then unfolded this algorithm into a neural network form.
-The authors incorporated this learned multimodal convolutional sparse coding model into a network to reconstruct high-resolution images.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>-The experiment is not sufficient and the details are missing.
-The number of article pages needs to be expanded, and the workload is not enough.
-Vague discussion about future work.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The details of the paper are not clear enough, and the reproducibility is insufficient.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>-The details of experiment are missing, and less experimental comparison results.
-The number of article pages needs to be expanded.
-A more clear outline of the next steps in research would be appropriate, some discussion but vague. There are already some good work about interpretable deep learning for medical images.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>-An  interpretable neural network model was presented to exploit information from multiple modalities to super-resolve an image of a target modality.
-The experimental results are few, and the comparative experiments are not convincing, and there is no ablation study.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>This paper proposed an interpretable deep network by fusing cross-modal medical images for super-resolution. The reviewers mostly feel positive on this work, and scored 6, 9, 6. The reviewers also have some comments on missing references, clarity on details, insufficient comparisons. The authors are suggested to revise the paper according to these comments in the final version.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We would like to thank the reviewers for the positive evaluation of our work and their constructive comments helping tο improve our paper. Next, we will try to reply to the major concerns raised by the reviewers. We will use the notation R#:Q$ to refer to comments included as a reply to Question $ by Reviewer #. We group the major reviewers’ comments as follows:</p>
  <ol>
    <li>All reviewers (R1:Q7, R2:Q4, R3:Q4) assert that more reference methods should be included in our experiments for comparison.<br />
A major obstacle while designing our experiments is that neither the code of related methods nor the details of the datasets employed in other works are published. Therefore, due to time limitations, we have chosen to implement an interpretable deep learning model and include it as a reference method, since we believe that this work is mostly related with our approach. However, we agree that the comparison is limited and we will do our best to include more reference methods in our final submission.</li>
    <li>Some comments suggest that an ablation study should also be included (R3:Q9). We agree with the suggestion, and we will provide a study concerning the performance of our model for a varying number of stages in our final submission. However, we think that R1 has misunderstood our experimental setting in R1:Q7, where it is asked to include high-resolution (HR) T1W or HR-FLAIR images as complementary information to super-resolve low-resolution (LR) T2W; this is exactly what we have presented. An alternative experiment would be to test our method with LR-T1W or LR-FLAIR images as complementary information. As long as page limitations allow including such an experiment, it will be added in our final submission.</li>
    <li>In R1:Q7, the reviewer suggests adding visual results of the reference methods. We will include such results in our final submission exploiting the additional page allowance.</li>
    <li>In R3:Q6, the reviewer raises concerns regarding the reproducibility of our work. We believe that we have provided all the information concerning the parameter setting and the initialization details of the proposed neural network so as the interested researcher can reproduce the code. Nevertheless, we plan to make our code publicly available after paper publication.</li>
    <li>Some references suggested in R1:Q4 will be definitely included in our final submission.</li>
  </ol>

</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0640-12-31
      -->
      <!--
      
        ,
        updated at 
        0641-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Image Reconstruction"
        class="post-category">
        Image Reconstruction
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Interpretability / Explainability"
        class="post-category">
        Machine Learning - Interpretability / Explainability
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - MRI"
        class="post-category">
        Modalities - MRI
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Tsiligianni, Evaggelia"
        class="post-category">
        Tsiligianni, Evaggelia
      </a> |  
      
      <a href="kittywong/tags#Zerva, Matina"
        class="post-category">
        Zerva, Matina
      </a> |  
      
      <a href="kittywong/tags#Marivani, Iman"
        class="post-category">
        Marivani, Iman
      </a> |  
      
      <a href="kittywong/tags#Deligiannis, Nikos"
        class="post-category">
        Deligiannis, Nikos
      </a> |  
      
      <a href="kittywong/tags#Kondi, Lisimachos"
        class="post-category">
        Kondi, Lisimachos
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0641/12/31/Paper1934">
          MRI Super-Resolution Through Generative Degradation Learning
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0639/12/31/Paper1815">
          A Data-driven Approach for High Frame Rate Synthetic Transmit Aperture Ultrasound Imaging
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
