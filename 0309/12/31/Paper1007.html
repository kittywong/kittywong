<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Multi-view analysis of unregistered medical images using cross-view transformers | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Multi-view analysis of unregistered medical images using cross-view transformers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Gijs van Tulder, Yao Tong, Elena Marchiori Abstract Multi-view medical image analysis often depends on the combination of information from multiple views. However, differences in perspective or other forms of misalignment can make it difficult to combine views effectively, as registration is not always possible. Without registration, views can only be combined at a global feature level, by joining feature vectors after global pooling. We present a novel cross-view transformer method to transfer information between unregistered views at the level of spatial feature maps. We demonstrate this method on multi-view mammography and chest X-ray datasets. On both datasets, we find that a cross-view transformer that links spatial feature maps can outperform a baseline model that joins feature vectors after global pooling. Link to paper https://doi.org/10.1007/978-3-030-87199-4_10 Link to the code repository https://vantulder.net/code/2021/miccai-transformers/ Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The paper proposes a Transformer model to link different views at the level of spatial feature maps to have a richer feature representation for downstream classification tasks. This model unifies two tasks ( Image registration and Deep Learning classification model) into the same deep learning model. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -Novel application to combine different views of unregistered medical images, it should be useful to combine features from both views without combining both views using image registration techniques. -Detailed ablation study for single view, late join and cross-view models. -Make use of transformer multi-head attention to link relevant areas between views at feature maps levels, instead of using image registration to combine the views at input level. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -The authors should have compared the performance of existing state-of-the-art methods against the proposed method to show the effectiveness of the proposed method. -What classification tasks are considered for the results analysis are not clear. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance -Uses public datasets -It’s indicated that source code will be made public -Supplementary file has detailed model architecture Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -Compare against existing methods for a fixed task to compare the effectiveness of the proposed method. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall, it looks interesting approach to link different views using Transformer model inspired multi-head attention module. It should help extract richer features. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The authors proposed a cross-viewer transformer-based model to process multi-view unregistered images. The multi-head cross attention enables the network to accumulate features from correlated locations from both views. Authors also propose to tokenize features to reduce computational overhead, which in turn improves performance. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Novel architecture: The cross-view transformer is an interesting idea and can be beneficial for other medical image problems as well. Moderately strong evaluation: Authors validate on multiple datasets with an improvement over baseline models. Well written paper: problem statement and the method is well explained. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Comparison with state-of-the-art is missing in both the dataset. Authors only compared with their baseline models. Comparison with SOTA is also important to appreciate the contribution. Computational overhead for the proposed cross-view transformer is missing. Transformers are usually computationally expensive. For a fair comparison, one would think of comparing baseline and proposed methods with similar numbers of learnable parameters. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors used publicly available datasets and the methods are described with sufficient details to be able to reproduce the results. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Why the cross-view transformer is used after block two is not clear. Also, why the transformer is only used once is not clear either. More experiments to support these choices are desired. Why do we need a second transformer for symmetric network formulation? Ideally, two of them should share weights because the task at hand is to compute cross attention between two views, and that should be independent of the choice of source and target selection. Visual inspection of the cross-attention would be of significant clinical interest to see what does the network learns for unregistered views. The argument why positional embedding is not required for cross-attention is not clear. Authors should explain either experimentally or via argument what is the reason behind it. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The technical novelty of this paper is adequate, and the experimental evidence supports it. There are few drawbacks, such as missing comparison with SOTA and computational overhead. However, the contribution outweighs the drawbacks, and hence I recommend probably acceptance. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The paper presents a cross-view transformer method to enhance the feature presentations between views without image registration. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The method achieves better performance than single view based methods and naive late-join based methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The paper only considers semantic relations of cross-view features. However, geometric information [1] [2] of lesions is not considered in the methodology, which implies the method may suffer from lesion mismatch problem. More ablations studies and analysis are needed. For example, why does each proposed component work? [1] Liu, Y. , Zhang, F. , Zhang, Q. , Wang, S. , &amp; Yu, Y. . (2020). Cross-View Correspondence Reasoning Based on Bipartite Graph Convolutional Network for Mammogram Mass Detection. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. [2] Ma, J. , Liang, S. , Li, X. , Li, H. , Menze, B. H. , &amp; Zhang, R. , et al. (2019). Cross-view relation networks for mammogram mass detection. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The requires more specific implementation details. It seems to be hard to reproduce. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The formula part of the paper needs to be improved. The mathematical symbols are confusing. For example, “f is the number of feature maps” in Section 3.2. Does f represent the channel of feature maps? Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The model enhances the cross-view feature representations without image registration. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 2 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors proposes a cross-viewer transformer-based model to process multi-view unregistered images. This model unifies two tasks (Image registration and Deep Learning classification model) into the same deep learning model. The cross-view transformer is an interesting idea, and the novel application to combine different views of unregistered medical images. The authors should have compared the performance of existing state-of-the-art methods against the proposed method to show the effectiveness of the proposed method. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Author Feedback We would like to thank the reviewers for their comments and suggestions. We are happy with the positive reception of our novel cross-view transformer method and experiments on two datasets with unregistered images. The reviewers rightfully highlight that it is important to compare with existing state-of-the-art methods, although they do not mention which. As discussed in our Related Work, other methods either combine features after global pooling, or use an ROI-based approach to match regions. As far as we are aware, there is not much work that considers cross-view analysis of unregistered images, especially in an end-to-end model. For mammography classification, the paper by Wu et al. [15] achieves state-of-the-art results. However, this is based on a large, private dataset, which makes it difficult to compare the results with our method directly. As an imperfect solution, we offer our ablation study with a late-join method that is similar to the architecture used by Wu et al. [15]. Our aim in this paper was to evaluate the cross-view transformer as an architecture for cross-view image analysis. Although the experiments do not allow us to show a performance improvement over the state-of-the-art in mammography classification, we do believe that our ablation study allows a comparison between our cross-view transformer model and the late-join architectures that are commonly used in existing state-of-the-art works. Reviewer #2 mentions the computational overhead of the method. In our experiments the additional computational cost was not prohibitive, but it does depend on where the transformer is applied. We have updated the paper to include some additional information. Reviewer #2 also thinks the argument against positional embedding is not sufficiently clear, and suggests additional experiments. We will keep this in mind for future work. We do believe that relative positional embedding (as implemented in the original transformer) is problematic across views, because relative distance between unregistered views is not clearly defined. However, some other form of position encoding, or the geometric information suggested by Reviewer #4, might be helpful. For future work, we will also remember Reviewer #2’s suggestion for a visual inspection of the cross-attention scores, as well as Reviewer #2 and #4’s suggestions for additional ablation studies considering choices like the position of the transformer and the symmetry of the attention weights. [15] Wu et al. (2019). Deep Neural Networks Improve Radiologists’ Performance in Breast Cancer Screening. IEEE Transactions on Medical Imaging. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Gijs van Tulder, Yao Tong, Elena Marchiori Abstract Multi-view medical image analysis often depends on the combination of information from multiple views. However, differences in perspective or other forms of misalignment can make it difficult to combine views effectively, as registration is not always possible. Without registration, views can only be combined at a global feature level, by joining feature vectors after global pooling. We present a novel cross-view transformer method to transfer information between unregistered views at the level of spatial feature maps. We demonstrate this method on multi-view mammography and chest X-ray datasets. On both datasets, we find that a cross-view transformer that links spatial feature maps can outperform a baseline model that joins feature vectors after global pooling. Link to paper https://doi.org/10.1007/978-3-030-87199-4_10 Link to the code repository https://vantulder.net/code/2021/miccai-transformers/ Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The paper proposes a Transformer model to link different views at the level of spatial feature maps to have a richer feature representation for downstream classification tasks. This model unifies two tasks ( Image registration and Deep Learning classification model) into the same deep learning model. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -Novel application to combine different views of unregistered medical images, it should be useful to combine features from both views without combining both views using image registration techniques. -Detailed ablation study for single view, late join and cross-view models. -Make use of transformer multi-head attention to link relevant areas between views at feature maps levels, instead of using image registration to combine the views at input level. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -The authors should have compared the performance of existing state-of-the-art methods against the proposed method to show the effectiveness of the proposed method. -What classification tasks are considered for the results analysis are not clear. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance -Uses public datasets -It’s indicated that source code will be made public -Supplementary file has detailed model architecture Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -Compare against existing methods for a fixed task to compare the effectiveness of the proposed method. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall, it looks interesting approach to link different views using Transformer model inspired multi-head attention module. It should help extract richer features. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The authors proposed a cross-viewer transformer-based model to process multi-view unregistered images. The multi-head cross attention enables the network to accumulate features from correlated locations from both views. Authors also propose to tokenize features to reduce computational overhead, which in turn improves performance. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Novel architecture: The cross-view transformer is an interesting idea and can be beneficial for other medical image problems as well. Moderately strong evaluation: Authors validate on multiple datasets with an improvement over baseline models. Well written paper: problem statement and the method is well explained. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Comparison with state-of-the-art is missing in both the dataset. Authors only compared with their baseline models. Comparison with SOTA is also important to appreciate the contribution. Computational overhead for the proposed cross-view transformer is missing. Transformers are usually computationally expensive. For a fair comparison, one would think of comparing baseline and proposed methods with similar numbers of learnable parameters. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors used publicly available datasets and the methods are described with sufficient details to be able to reproduce the results. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Why the cross-view transformer is used after block two is not clear. Also, why the transformer is only used once is not clear either. More experiments to support these choices are desired. Why do we need a second transformer for symmetric network formulation? Ideally, two of them should share weights because the task at hand is to compute cross attention between two views, and that should be independent of the choice of source and target selection. Visual inspection of the cross-attention would be of significant clinical interest to see what does the network learns for unregistered views. The argument why positional embedding is not required for cross-attention is not clear. Authors should explain either experimentally or via argument what is the reason behind it. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The technical novelty of this paper is adequate, and the experimental evidence supports it. There are few drawbacks, such as missing comparison with SOTA and computational overhead. However, the contribution outweighs the drawbacks, and hence I recommend probably acceptance. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The paper presents a cross-view transformer method to enhance the feature presentations between views without image registration. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The method achieves better performance than single view based methods and naive late-join based methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The paper only considers semantic relations of cross-view features. However, geometric information [1] [2] of lesions is not considered in the methodology, which implies the method may suffer from lesion mismatch problem. More ablations studies and analysis are needed. For example, why does each proposed component work? [1] Liu, Y. , Zhang, F. , Zhang, Q. , Wang, S. , &amp; Yu, Y. . (2020). Cross-View Correspondence Reasoning Based on Bipartite Graph Convolutional Network for Mammogram Mass Detection. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. [2] Ma, J. , Liang, S. , Li, X. , Li, H. , Menze, B. H. , &amp; Zhang, R. , et al. (2019). Cross-view relation networks for mammogram mass detection. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The requires more specific implementation details. It seems to be hard to reproduce. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The formula part of the paper needs to be improved. The mathematical symbols are confusing. For example, “f is the number of feature maps” in Section 3.2. Does f represent the channel of feature maps? Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The model enhances the cross-view feature representations without image registration. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 2 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors proposes a cross-viewer transformer-based model to process multi-view unregistered images. This model unifies two tasks (Image registration and Deep Learning classification model) into the same deep learning model. The cross-view transformer is an interesting idea, and the novel application to combine different views of unregistered medical images. The authors should have compared the performance of existing state-of-the-art methods against the proposed method to show the effectiveness of the proposed method. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Author Feedback We would like to thank the reviewers for their comments and suggestions. We are happy with the positive reception of our novel cross-view transformer method and experiments on two datasets with unregistered images. The reviewers rightfully highlight that it is important to compare with existing state-of-the-art methods, although they do not mention which. As discussed in our Related Work, other methods either combine features after global pooling, or use an ROI-based approach to match regions. As far as we are aware, there is not much work that considers cross-view analysis of unregistered images, especially in an end-to-end model. For mammography classification, the paper by Wu et al. [15] achieves state-of-the-art results. However, this is based on a large, private dataset, which makes it difficult to compare the results with our method directly. As an imperfect solution, we offer our ablation study with a late-join method that is similar to the architecture used by Wu et al. [15]. Our aim in this paper was to evaluate the cross-view transformer as an architecture for cross-view image analysis. Although the experiments do not allow us to show a performance improvement over the state-of-the-art in mammography classification, we do believe that our ablation study allows a comparison between our cross-view transformer model and the late-join architectures that are commonly used in existing state-of-the-art works. Reviewer #2 mentions the computational overhead of the method. In our experiments the additional computational cost was not prohibitive, but it does depend on where the transformer is applied. We have updated the paper to include some additional information. Reviewer #2 also thinks the argument against positional embedding is not sufficiently clear, and suggests additional experiments. We will keep this in mind for future work. We do believe that relative positional embedding (as implemented in the original transformer) is problematic across views, because relative distance between unregistered views is not clearly defined. However, some other form of position encoding, or the geometric information suggested by Reviewer #4, might be helpful. For future work, we will also remember Reviewer #2’s suggestion for a visual inspection of the cross-attention scores, as well as Reviewer #2 and #4’s suggestions for additional ablation studies considering choices like the position of the transformer and the symmetry of the attention weights. [15] Wu et al. (2019). Deep Neural Networks Improve Radiologists’ Performance in Breast Cancer Screening. IEEE Transactions on Medical Imaging. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0309/12/31/Paper1007" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0309/12/31/Paper1007" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0309-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Multi-view analysis of unregistered medical images using cross-view transformers" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0309/12/31/Paper1007"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0309/12/31/Paper1007","headline":"Multi-view analysis of unregistered medical images using cross-view transformers","dateModified":"0310-01-02T00:00:00-05:17","datePublished":"0309-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Gijs van Tulder, Yao Tong, Elena Marchiori Abstract Multi-view medical image analysis often depends on the combination of information from multiple views. However, differences in perspective or other forms of misalignment can make it difficult to combine views effectively, as registration is not always possible. Without registration, views can only be combined at a global feature level, by joining feature vectors after global pooling. We present a novel cross-view transformer method to transfer information between unregistered views at the level of spatial feature maps. We demonstrate this method on multi-view mammography and chest X-ray datasets. On both datasets, we find that a cross-view transformer that links spatial feature maps can outperform a baseline model that joins feature vectors after global pooling. Link to paper https://doi.org/10.1007/978-3-030-87199-4_10 Link to the code repository https://vantulder.net/code/2021/miccai-transformers/ Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The paper proposes a Transformer model to link different views at the level of spatial feature maps to have a richer feature representation for downstream classification tasks. This model unifies two tasks ( Image registration and Deep Learning classification model) into the same deep learning model. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -Novel application to combine different views of unregistered medical images, it should be useful to combine features from both views without combining both views using image registration techniques. -Detailed ablation study for single view, late join and cross-view models. -Make use of transformer multi-head attention to link relevant areas between views at feature maps levels, instead of using image registration to combine the views at input level. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -The authors should have compared the performance of existing state-of-the-art methods against the proposed method to show the effectiveness of the proposed method. -What classification tasks are considered for the results analysis are not clear. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance -Uses public datasets -It’s indicated that source code will be made public -Supplementary file has detailed model architecture Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -Compare against existing methods for a fixed task to compare the effectiveness of the proposed method. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall, it looks interesting approach to link different views using Transformer model inspired multi-head attention module. It should help extract richer features. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The authors proposed a cross-viewer transformer-based model to process multi-view unregistered images. The multi-head cross attention enables the network to accumulate features from correlated locations from both views. Authors also propose to tokenize features to reduce computational overhead, which in turn improves performance. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Novel architecture: The cross-view transformer is an interesting idea and can be beneficial for other medical image problems as well. Moderately strong evaluation: Authors validate on multiple datasets with an improvement over baseline models. Well written paper: problem statement and the method is well explained. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Comparison with state-of-the-art is missing in both the dataset. Authors only compared with their baseline models. Comparison with SOTA is also important to appreciate the contribution. Computational overhead for the proposed cross-view transformer is missing. Transformers are usually computationally expensive. For a fair comparison, one would think of comparing baseline and proposed methods with similar numbers of learnable parameters. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors used publicly available datasets and the methods are described with sufficient details to be able to reproduce the results. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Why the cross-view transformer is used after block two is not clear. Also, why the transformer is only used once is not clear either. More experiments to support these choices are desired. Why do we need a second transformer for symmetric network formulation? Ideally, two of them should share weights because the task at hand is to compute cross attention between two views, and that should be independent of the choice of source and target selection. Visual inspection of the cross-attention would be of significant clinical interest to see what does the network learns for unregistered views. The argument why positional embedding is not required for cross-attention is not clear. Authors should explain either experimentally or via argument what is the reason behind it. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The technical novelty of this paper is adequate, and the experimental evidence supports it. There are few drawbacks, such as missing comparison with SOTA and computational overhead. However, the contribution outweighs the drawbacks, and hence I recommend probably acceptance. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper The paper presents a cross-view transformer method to enhance the feature presentations between views without image registration. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The method achieves better performance than single view based methods and naive late-join based methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The paper only considers semantic relations of cross-view features. However, geometric information [1] [2] of lesions is not considered in the methodology, which implies the method may suffer from lesion mismatch problem. More ablations studies and analysis are needed. For example, why does each proposed component work? [1] Liu, Y. , Zhang, F. , Zhang, Q. , Wang, S. , &amp; Yu, Y. . (2020). Cross-View Correspondence Reasoning Based on Bipartite Graph Convolutional Network for Mammogram Mass Detection. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. [2] Ma, J. , Liang, S. , Li, X. , Li, H. , Menze, B. H. , &amp; Zhang, R. , et al. (2019). Cross-view relation networks for mammogram mass detection. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The requires more specific implementation details. It seems to be hard to reproduce. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The formula part of the paper needs to be improved. The mathematical symbols are confusing. For example, “f is the number of feature maps” in Section 3.2. Does f represent the channel of feature maps? Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The model enhances the cross-view feature representations without image registration. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 2 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The authors proposes a cross-viewer transformer-based model to process multi-view unregistered images. This model unifies two tasks (Image registration and Deep Learning classification model) into the same deep learning model. The cross-view transformer is an interesting idea, and the novel application to combine different views of unregistered medical images. The authors should have compared the performance of existing state-of-the-art methods against the proposed method to show the effectiveness of the proposed method. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Author Feedback We would like to thank the reviewers for their comments and suggestions. We are happy with the positive reception of our novel cross-view transformer method and experiments on two datasets with unregistered images. The reviewers rightfully highlight that it is important to compare with existing state-of-the-art methods, although they do not mention which. As discussed in our Related Work, other methods either combine features after global pooling, or use an ROI-based approach to match regions. As far as we are aware, there is not much work that considers cross-view analysis of unregistered images, especially in an end-to-end model. For mammography classification, the paper by Wu et al. [15] achieves state-of-the-art results. However, this is based on a large, private dataset, which makes it difficult to compare the results with our method directly. As an imperfect solution, we offer our ablation study with a late-join method that is similar to the architecture used by Wu et al. [15]. Our aim in this paper was to evaluate the cross-view transformer as an architecture for cross-view image analysis. Although the experiments do not allow us to show a performance improvement over the state-of-the-art in mammography classification, we do believe that our ablation study allows a comparison between our cross-view transformer model and the late-join architectures that are commonly used in existing state-of-the-art works. Reviewer #2 mentions the computational overhead of the method. In our experiments the additional computational cost was not prohibitive, but it does depend on where the transformer is applied. We have updated the paper to include some additional information. Reviewer #2 also thinks the argument against positional embedding is not sufficiently clear, and suggests additional experiments. We will keep this in mind for future work. We do believe that relative positional embedding (as implemented in the original transformer) is problematic across views, because relative distance between unregistered views is not clearly defined. However, some other form of position encoding, or the geometric information suggested by Reviewer #4, might be helpful. For future work, we will also remember Reviewer #2’s suggestion for a visual inspection of the cross-attention scores, as well as Reviewer #2 and #4’s suggestions for additional ablation studies considering choices like the position of the transformer and the symmetry of the attention weights. [15] Wu et al. (2019). Deep Neural Networks Improve Radiologists’ Performance in Breast Cancer Screening. IEEE Transactions on Medical Imaging. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="van Tulder, Gijs,Tong, Yao,Marchiori, Elena" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Multi-view analysis of unregistered medical images using cross-view transformers</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Breast"
        class="post-category">
        Clinical applications - Breast
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Lung"
        class="post-category">
        Clinical applications - Lung
      </a>
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a>
      
      <a 
        href="kittywong/categories#Modalities - other"
        class="post-category">
        Modalities - other
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#van Tulder, Gijs"
        class="post-tags">
        van Tulder, Gijs
      </a> |  
      
      <a href="kittywong/tags#Tong, Yao"
        class="post-tags">
        Tong, Yao
      </a> |  
      
      <a href="kittywong/tags#Marchiori, Elena"
        class="post-tags">
        Marchiori, Elena
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Gijs van Tulder, Yao Tong, Elena Marchiori
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Multi-view medical image analysis often depends on the combination of information from multiple views. However, differences in perspective or other forms of misalignment can make it difficult to combine views effectively, as registration is not always possible. Without registration, views can only be combined at a global feature level, by joining feature vectors after global pooling. We present a novel cross-view transformer method to transfer information between unregistered views at the level of spatial feature maps. We demonstrate this method on multi-view mammography and chest X-ray datasets. On both datasets, we find that a cross-view transformer that links spatial feature maps can outperform a baseline model that joins feature vectors after global pooling.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87199-4_10">https://doi.org/10.1007/978-3-030-87199-4_10</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://vantulder.net/code/2021/miccai-transformers/
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper proposes a Transformer model to link different views at the level of spatial feature maps to have a richer feature representation for downstream classification tasks. This model unifies two tasks ( Image registration and Deep Learning classification model) into the same deep learning model.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>-Novel application to combine different views of unregistered medical images, it should be useful to combine features from both views without combining both views using image registration techniques.
-Detailed ablation study for single view, late join and cross-view models.
-Make use of transformer multi-head attention to link relevant areas between views at feature maps levels, instead of using image registration to combine the views at input level.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>-The authors should have compared the performance of existing state-of-the-art methods against the proposed method to show the effectiveness of the proposed method.
-What classification tasks are considered for the results analysis are not clear.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>-Uses public datasets
-It’s indicated that source code will be made public
-Supplementary file has detailed model architecture</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>-Compare against existing methods for a fixed task to compare the effectiveness of the proposed method.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Overall, it looks interesting approach to link different views using Transformer model inspired multi-head attention module. It should help extract richer features.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors proposed a cross-viewer transformer-based model to process multi-view unregistered images. The multi-head cross attention enables the network to accumulate features from correlated locations from both views. Authors also propose to tokenize features to reduce computational overhead, which in turn improves performance.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>Novel architecture: The cross-view transformer is an interesting idea and can be beneficial for other medical image problems as well.</li>
        <li>Moderately strong evaluation: Authors validate on multiple datasets with an improvement over baseline models.</li>
        <li>Well written paper: problem statement and the method is well explained.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>Comparison with state-of-the-art is missing in both the dataset. Authors only compared with their baseline models. Comparison with SOTA is also important to appreciate the contribution.</li>
        <li>Computational overhead for the proposed cross-view transformer is missing. Transformers are usually computationally expensive. For a fair comparison, one would think of comparing baseline and proposed methods with similar numbers of learnable parameters.</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The authors used publicly available datasets and the methods are described with sufficient details to be able to reproduce the results.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ol>
        <li>Why the cross-view transformer is used after block two is not clear. Also, why the transformer is only used once is not clear either. More experiments to support these choices are desired.</li>
        <li>Why do we need a second transformer for symmetric network formulation? Ideally, two of them should share weights because the task at hand is to compute cross attention between two views, and that should be independent of the choice of source and target selection.</li>
        <li>Visual inspection of the cross-attention would be of significant clinical interest to see what does the network learns for unregistered views.</li>
        <li>The argument why positional embedding is not required for cross-attention is not clear. Authors should explain either experimentally or via argument what is the reason behind it.</li>
      </ol>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The technical novelty of this paper is adequate, and the experimental evidence supports it. There are few drawbacks, such as missing comparison with SOTA and computational overhead. However, the contribution outweighs the drawbacks, and hence I recommend probably acceptance.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper presents a cross-view transformer method to enhance the feature presentations between views without image registration.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The method achieves better performance than single view based methods and naive late-join based methods.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>
          <p>The paper only considers semantic relations of cross-view features. However, geometric information [1] [2] of lesions is not considered in the methodology, which implies the method may suffer from lesion mismatch problem.</p>
        </li>
        <li>
          <p>More ablations studies and analysis are needed. For example, why does each proposed component work?</p>
        </li>
      </ol>

      <p>[1] Liu, Y. ,  Zhang, F. ,  Zhang, Q. ,  Wang, S. , &amp;  Yu, Y. . (2020). Cross-View Correspondence Reasoning Based on Bipartite Graph Convolutional Network for Mammogram Mass Detection. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE.</p>

      <p>[2] Ma, J. ,  Liang, S. ,  Li, X. ,  Li, H. ,  Menze, B. H. , &amp;  Zhang, R. , et al. (2019). Cross-view relation networks for mammogram mass detection.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The requires more specific implementation details. It seems to be hard to reproduce.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>The formula part of the paper needs to be improved. The mathematical symbols are confusing. For example, “f is the number of feature maps” in Section 3.2. Does f represent the channel of feature maps?</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The model enhances the cross-view feature representations without image registration.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>The authors proposes a cross-viewer transformer-based model to process multi-view unregistered images. This model unifies two tasks (Image registration and Deep Learning classification model) into the same deep learning model. The cross-view transformer is an interesting idea, and the novel application to combine different views of unregistered medical images. The authors should have compared the performance of existing state-of-the-art methods against the proposed method to show the effectiveness of the proposed method.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We would like to thank the reviewers for their comments and suggestions. We are happy with the positive reception of our novel cross-view transformer method and experiments on two datasets with unregistered images.</p>

  <p>The reviewers rightfully highlight that it is important to compare with existing state-of-the-art methods, although they do not mention which. As discussed in our Related Work, other methods either combine features after global pooling, or use an ROI-based approach to match regions. As far as we are aware, there is not much work that considers cross-view analysis of unregistered images, especially in an end-to-end model.</p>

  <p>For mammography classification, the paper by Wu et al. [15] achieves state-of-the-art results. However, this is based on a large, private dataset, which makes it difficult to compare the results with our method directly. As an imperfect solution, we offer our ablation study with a late-join method that is similar to the architecture used by Wu et al. [15].</p>

  <p>Our aim in this paper was to evaluate the cross-view transformer as an architecture for cross-view image analysis. Although the experiments do not allow us to show a performance improvement over the state-of-the-art in mammography classification, we do believe that our ablation study allows a comparison between our cross-view transformer model and the late-join architectures that are commonly used in existing state-of-the-art works.</p>

  <p>Reviewer #2 mentions the computational overhead of the method. In our experiments the additional computational cost was not prohibitive, but it does depend on where the transformer is applied. We have updated the paper to include some additional information.</p>

  <p>Reviewer #2 also thinks the argument against positional embedding is not sufficiently clear, and suggests additional experiments. We will keep this in mind for future work. We do believe that relative positional embedding (as implemented in the original transformer) is problematic across views, because relative distance between unregistered views is not clearly defined. However, some other form of position encoding, or the geometric information suggested by Reviewer #4, might be helpful.</p>

  <p>For future work, we will also remember Reviewer #2’s suggestion for a visual inspection of the cross-attention scores, as well as Reviewer #2 and #4’s suggestions for additional ablation studies considering choices like the position of the transformer and the symmetry of the attention weights.</p>

  <p>[15] Wu et al. (2019). Deep Neural Networks Improve Radiologists’ Performance in Breast Cancer Screening. IEEE Transactions on Medical Imaging.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0309-12-31
      -->
      <!--
      
        ,
        updated at 
        0310-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Breast"
        class="post-category">
        Clinical applications - Breast
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Lung"
        class="post-category">
        Clinical applications - Lung
      </a> |
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - other"
        class="post-category">
        Modalities - other
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#van Tulder, Gijs"
        class="post-category">
        van Tulder, Gijs
      </a> |  
      
      <a href="kittywong/tags#Tong, Yao"
        class="post-category">
        Tong, Yao
      </a> |  
      
      <a href="kittywong/tags#Marchiori, Elena"
        class="post-category">
        Marchiori, Elena
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0310/12/31/Paper0297">
          Stain Mix-up: Unsupervised Domain Generalization for Histopathology Images
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0308/12/31/Paper0853">
          Spine-Transformers: Vertebra Detection and Localization in Arbitrary Field-of-View Spine CT with Transformers
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
