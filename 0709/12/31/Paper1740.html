<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Domain Generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Domain Generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Zheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue, Dinggang Shen, Jie-Zhi Cheng Abstract Lesion detection is a fundamental problem in the computer-aided diagnosis scheme for mammography. The advance of deep learning techniques have made a remarkable progress for this task, provided that the training data are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, the collection of mammograms from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning model to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor-styles. Afterward, the backbone network is then recalibrated to the downstream task of lesion detection with the specific supervised learning. The proposed method is evaluated with mammograms from four vendors and one unseen public dataset. The experimental results suggest that our approach can effectively improve detection performance on both seen and unseen domains, and outperforms many state-of-the-art (SOTA) generalization methods. Link to paper https://doi.org/10.1007/978-3-030-87234-2_10 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper Due to the difficulties of collecting large-scale labeled training data, unsupervised representation learning followed by supervised learning of the downstream task is becoming general. The authors proposed a domain-specific unsupervised representation learning scheme which exploits contrastive learning of multi-view (CC, MLO) and multi-style (different vendor generated by StyleGAN) images. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The authors properly used well-known previous methods, i.e. StyleGAN and learning with contrastive loss, to resolve domain generalization issues in lesion detection in mammograms. The proposed pretraining method showed better performance compared to 1) basic types of pretraining methods, 2) several domain generalization methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. StyleGAN and Contrastive Learning are well-known techniques. Not only the general settings of those two works but also the details specific to the lesion detection in mammography also need to be added. E.g., training the StyleGAN with medical images tends to be unstable in terms of training &amp; quality, so details of training process needs to be described. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The detailed training recipe of the proposed method is very important especially in StyleGAN part. The quality of the generated images are important in domain generalization. The authors checked ‘NO’ for all questions related to code release. Even though the data used for the experiments is hard to be released, the codes at least need to be released for the reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html MVCL &amp; MSCL showed similar performance improvement in Table 2. Does the quality of the style-transferred image important to guarantee the level of performance improvement (described in Table 2)? If yes, training details of the StyleGAN needs to be added in the literature. The exemplary style-transferred images shown in the supplementary material seem to be normal images. Abnormal images with two types of lesions (especially the micro calc) would be helpful to understand the learned features of the StyleGAN and guestimate why the proposed method works well. MSVCL of the Style B is superior to MSCL &amp; MVCL, while they are similar in other styles. The authors should give some description about the difference of the trend in their experimental results. Mean &amp; std of multiple runs also would be helpful. MSVCL numbers in Table 2 &amp; 3 are different. Is it correct? Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Good adoption of the well-known previous works for the target problem. But, more details of the specific setting and additional explanation of the experimental results are needed to support the superiority of the proposed method in target applications. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper combines multi-style and multi-view data to do contrastive pretraining on multiple domains. The multi-style is achieved using CycleGANs on different domains. The multi-view is the two views (CC and MLO) of the mammograms. The pretrained model, after fine-tuning, performs better on unseen domains than baselines. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This work is a good combination of contrastive learning, multi-style augmentation and multi-view learning. It’s natural that it improves the domain generalization performance. The experimental results support that the proposed method outperformed a few recent methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. In Table 2, by comparing SimCLR (ImageNet → MammoPre) and MVCL, we can see the multi-view brought 0.015 improvement. In Table 3, EISNet was 0.02 worse than MSVCL. Was EISNet (as well as other baselines) trained with multi-view? If they were trained with single view only, then this was not a fair comparison, as the multi-view could largely eliminate this performance gap. Multi-view learning on mammogram has been explored before, but related papers are not discussed or compared. E.g. “Multi-view Multi-task Learning for Improving Autonomous Mammogram Diagnosis”, Machine Learning for Healthcare Conference, 2019. The authors should also compare with adversarial domain adaptation methods, e.g. RevGrad. Although two unseen domains D and E were evaluated, they were actually both acquired using Siemens devices. In Fig 3 we can also see example images from domain D and E are visually similar. Three baseline methods were evaluated in Table 3, which seem not be “many SOTA generalization methods” stated in the abstract. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The method seems not difficult to reproduce by a third-party. Though, it will be a great relief for the community if the authors could release the code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Table 3, was EISNet (as well as other baselines) trained with multi-view? If they were trained with single view only, then this was not a fair comparison, as the multi-view could largely eliminate this performance gap. Why the reported results of MSVCL in Table 2 and 3 are different (both at the bottom row)? Why the “Random (no pretraining” method in Table 2 performed so poorly both on the seen domains and unsee domains? Was it trained with supervision? If so, it shouuld perform well on the seen domains. The authors are suggested to compare with adversarial domain adaptation methods, e.g. RevGrad. Multi-view methods should also be compared with. If possible, the authors should evaluate on another unseen domain (other than Siemens). If it’s infeasble, the authors should clearly indicate that domains D and E were both acquired using Siemens devices. It seems quite time consuming to train M*(M-1)/2 CycleGANs, where M is the number of domains. A few typos/grammar errors: 1) Paragraph 1 in Page 4, “The work [17] unidirectional takes …” 2) Paragraph 1 in Section 3.2, “we compare lesion detection performance with 1) no,” should be “ … 1) no pretraining”. 3) Paragraph 2 in Section 3.2, “The details ablation analysis” =&gt; “The detailed ablation analysis” 4) Paragraph 2 in Page 7, “either seen and unseen domains” =&gt; “either seen or unseen domains”; “our method can outperform the EISNet” =&gt; “our method outperformed the EISNet” Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It makes sense to combine contrastive learning, multi-style augmentation and multi-view learning. It’s natural that it improves the domain generalization performance. Although the experimental results showed that the proposed method outperformed a few recent methods, I have concerns whether the comparison was fair (See weakness 1). What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper These authors present a method for domain generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning. Fist, a CycleGAN is used to generate different styles from one vendor and the generated samples as used for contrastive learning. Second, for the Multiview, the authors treat the CC and MLO view of the same breast from the same patient as positive pair, whereas the other combination of CC and MLO is a negative pair for contrastive learning. Finally, the authors present a unified self-supervised contrastive learning to learn generalizable domain invariant features, which later can be used for the downstream task. The exhaustive experiments clearly show the contribution of their proposed approach for each of the components. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. • The authors address one interesting area of research for domain generalization in medical imaging, more specifically for Multiview and multi-style vendors. • The authors experiment clearly shows the contribution of each component of their proposed approach. • Comparison to classical transfer learning as well as to state-of-the-art domain generalization methods is well described and conducted with consistent improvement, showing the superiority of their proposed approach. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. • The flow of the paper reading is good, but some parts of the paper are unclear please the constructive comments section. • The authors use abbreviations before introducing them. It may be hard for the reader to follow the different methods abbreviation and the views names without defining them first. See the constructive comments section. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors do not include statements regarding the reproducibility or open access to code or data. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html • In the introduction the sentence “Meanwhile, for the multi-view contrastive learning, the CC and MLO views of the same breast are also paired as positive samples” does not make clear what is exactly happening in the multi-view learning. Elaborating better this may be an easier read. • The authors use abbreviations like CC, MLO, MSCL, MVCL, MSVCL. The authors are encourage to introduce and define these abbreviations before using the terms. • I found a big paragraph repeated in the text: “We adopt ResNet-50 as the backbone model for contrastive learning and FCOS detector. For fair comparison, the learning rate and batch size for all contrastive learning schemes are set the same as 0.3 and 256, respectively. Meanwhile, all contrastive learning schemes in all experiments use the same diversifying opera- tions, including random cropping, random rotation in ±10◦, horizontal flipping and random color jittering (strength=0.2).” • Fig 2 and 4 could have better description. Since Figure 2 summarizes the proposed method, more information can be provided in the figure. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This paper addresses an interesting topic in the MICCAI community, domain generalization. The experiments conducted are exhaustive and consistent showing the success of the proposed method for domain generalization. They also show very decent comparison to state-of-the-art methods in domain generalization as well as with transfer learning. Other than minor changes to the paper will result in a really interesting finding for the MICCAI community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This work adopts GAN and contrastive learning for domain generalization. Reviewers recognized the overall contribution in adopting the techniques. However, more details are needed for the specific setting and experiment, especically for the GAN part, which can be unstable and needs to be shown important for the overall performance. Also, experimental comparisons will need some clarification. Please note, the aim of rebuttal is to clarify misunderstandings / rationale behind method and experiment settings. Promise of extra experiments will not be considered. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The overall method setting makes sense to reviewers, though it is a combination of existing well-known methods. Major questions include 1) the details of training CycleGAN, since GAN networks can be unstable, and how it is properly trained is extremely important for representation learning, especially for disease cases, 2) performance gain significance, and 3) evaluation settings and fairness. Rebuttal added more details of the methods, and clarifies the “fairness” question properly. Still, I agree with reviewers on that it will be better if the two testing sets are from two vendors, rather than just population shift. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper brings interesting practical contributions for the community for the important domain generalisation problem. The rebuttal addresses the concerns raised by the AC and reviewers, so I recommend the acceptance of this paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. There are somewhat conflicting reviews given different reasons. The authors addressed most of these concerns. It is generally agreed by the reviewers that the work has value for presentation at MICCAI, which I would support. But, eventually this is a borderline paper and the decision may go either way. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Author Feedback Thank you for your time in review this paper. We hereby clarify several major comments and cordially request your consideration on this paper. Q1: Importance of style transfer quality for detection performance. (R2) A: Our goal is to seek a robust encoder of detector to various manufacture domains with contrastive learning. Good quality of style transfer can be helpful for the contrastive learning. To our experience, reasonable quality of style transfer can be attained with more training epochs of 50 for CycleGAN models. To systematically find suitable models in terms of epoch setting, we devise an indirect selection method based on the detection performance. Specifically, we use the detection performance as metrics to decide which models from the epoch settings can produce helpful style transfer images with quality for the detection tasks (see “Implementation details” in Sec.3). We fix all hyper-parameters of CycleGAN and FCOS, except the epoch of the CycleGAN, in the indirect selection scheme. The reasons of using indirect selection method are twofold. First, it is relatively difficult to quantitatively assess the quality of style transfer. The subjective assessment is impractical and improper. Second, these chosen models are based on the detection performance and may better align with our ultimate goal of detection. In summary, the style transfer quality is important for contrastive learning. Accordingly, we devise an indirect model selection method to find models that may produce good quality of style transfer and detection results. Q2: Training details of the styleGAN. (R2) A: The original CycleGAN was adopted here. The backbone of the generator is ResNet 9 blocks with 20 conv layers, while the discriminator is PatchGAN with 6 conv layers for binary classifications. In training, the sizes of inputs and outputs are 512*512 by random cropping the images. MSE and L1 losses are used for classification and reconstruction. For a style transfer, e.g., A to B, the involved training data from A and B are both 1000 for balanced training. We will add more details in the Supplement. Q3: MSVCL numbers in Table 2 &amp; 3 are different. (R2/R3) A: These are typos. The results of Table 3 are the correct one. We will fix it in the final version. Q4: Support of reproducibility. (R2) A: Although it seems that the concern about reproducibility is different between R2 and R3. We will release the CycleGAN models, which were the major concern of R2, for the support of reproducibility. Q5: Multi-view implementation for the 3 comparing methods. (R3) A: The 3 methods conduct domain generalization along with the downstream task. Therefore, we trained the 3 methods with the multi-view images from various vendors and explored the domain generalization correspondingly. Due to the space limitation, the details of training for 3 methods were not given in submission but will be added in the new Supplement. Q6: No-pretrained performed poorly on seen domains. (R3) A: The no-pretrained detector was trained with supervision. The major reason why no-pretrained detector performed poorly on seen domains may lie in the very limited training data (360*3 annotated data), compared to the pretrained ImageNet from millions of images. Low level features from ImageNet may still be helpful. Q7: Domains D and E may be the same. (R3) A: Although images of domains D and E shown in manuscript may look similar, it is worth noting that data of domain D were collected from Asian populations, whereas images of domain E were from Europe. Therefore, there may still exist domain gap. We’ll try to collect data of non-Siemens devices for further validation in the future study. Q8: Compared with RevGrad. (R3) A: We have tried RevGrad, which majorly requires discriminators for multi-view and -vendor domains. The preliminary results were not very well and may need some modification to our problem. Therefore, we didn’t add it for comparison. We’ll explore it in future work. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Zheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue, Dinggang Shen, Jie-Zhi Cheng Abstract Lesion detection is a fundamental problem in the computer-aided diagnosis scheme for mammography. The advance of deep learning techniques have made a remarkable progress for this task, provided that the training data are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, the collection of mammograms from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning model to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor-styles. Afterward, the backbone network is then recalibrated to the downstream task of lesion detection with the specific supervised learning. The proposed method is evaluated with mammograms from four vendors and one unseen public dataset. The experimental results suggest that our approach can effectively improve detection performance on both seen and unseen domains, and outperforms many state-of-the-art (SOTA) generalization methods. Link to paper https://doi.org/10.1007/978-3-030-87234-2_10 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper Due to the difficulties of collecting large-scale labeled training data, unsupervised representation learning followed by supervised learning of the downstream task is becoming general. The authors proposed a domain-specific unsupervised representation learning scheme which exploits contrastive learning of multi-view (CC, MLO) and multi-style (different vendor generated by StyleGAN) images. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The authors properly used well-known previous methods, i.e. StyleGAN and learning with contrastive loss, to resolve domain generalization issues in lesion detection in mammograms. The proposed pretraining method showed better performance compared to 1) basic types of pretraining methods, 2) several domain generalization methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. StyleGAN and Contrastive Learning are well-known techniques. Not only the general settings of those two works but also the details specific to the lesion detection in mammography also need to be added. E.g., training the StyleGAN with medical images tends to be unstable in terms of training &amp; quality, so details of training process needs to be described. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The detailed training recipe of the proposed method is very important especially in StyleGAN part. The quality of the generated images are important in domain generalization. The authors checked ‘NO’ for all questions related to code release. Even though the data used for the experiments is hard to be released, the codes at least need to be released for the reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html MVCL &amp; MSCL showed similar performance improvement in Table 2. Does the quality of the style-transferred image important to guarantee the level of performance improvement (described in Table 2)? If yes, training details of the StyleGAN needs to be added in the literature. The exemplary style-transferred images shown in the supplementary material seem to be normal images. Abnormal images with two types of lesions (especially the micro calc) would be helpful to understand the learned features of the StyleGAN and guestimate why the proposed method works well. MSVCL of the Style B is superior to MSCL &amp; MVCL, while they are similar in other styles. The authors should give some description about the difference of the trend in their experimental results. Mean &amp; std of multiple runs also would be helpful. MSVCL numbers in Table 2 &amp; 3 are different. Is it correct? Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Good adoption of the well-known previous works for the target problem. But, more details of the specific setting and additional explanation of the experimental results are needed to support the superiority of the proposed method in target applications. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper combines multi-style and multi-view data to do contrastive pretraining on multiple domains. The multi-style is achieved using CycleGANs on different domains. The multi-view is the two views (CC and MLO) of the mammograms. The pretrained model, after fine-tuning, performs better on unseen domains than baselines. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This work is a good combination of contrastive learning, multi-style augmentation and multi-view learning. It’s natural that it improves the domain generalization performance. The experimental results support that the proposed method outperformed a few recent methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. In Table 2, by comparing SimCLR (ImageNet → MammoPre) and MVCL, we can see the multi-view brought 0.015 improvement. In Table 3, EISNet was 0.02 worse than MSVCL. Was EISNet (as well as other baselines) trained with multi-view? If they were trained with single view only, then this was not a fair comparison, as the multi-view could largely eliminate this performance gap. Multi-view learning on mammogram has been explored before, but related papers are not discussed or compared. E.g. “Multi-view Multi-task Learning for Improving Autonomous Mammogram Diagnosis”, Machine Learning for Healthcare Conference, 2019. The authors should also compare with adversarial domain adaptation methods, e.g. RevGrad. Although two unseen domains D and E were evaluated, they were actually both acquired using Siemens devices. In Fig 3 we can also see example images from domain D and E are visually similar. Three baseline methods were evaluated in Table 3, which seem not be “many SOTA generalization methods” stated in the abstract. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The method seems not difficult to reproduce by a third-party. Though, it will be a great relief for the community if the authors could release the code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Table 3, was EISNet (as well as other baselines) trained with multi-view? If they were trained with single view only, then this was not a fair comparison, as the multi-view could largely eliminate this performance gap. Why the reported results of MSVCL in Table 2 and 3 are different (both at the bottom row)? Why the “Random (no pretraining” method in Table 2 performed so poorly both on the seen domains and unsee domains? Was it trained with supervision? If so, it shouuld perform well on the seen domains. The authors are suggested to compare with adversarial domain adaptation methods, e.g. RevGrad. Multi-view methods should also be compared with. If possible, the authors should evaluate on another unseen domain (other than Siemens). If it’s infeasble, the authors should clearly indicate that domains D and E were both acquired using Siemens devices. It seems quite time consuming to train M*(M-1)/2 CycleGANs, where M is the number of domains. A few typos/grammar errors: 1) Paragraph 1 in Page 4, “The work [17] unidirectional takes …” 2) Paragraph 1 in Section 3.2, “we compare lesion detection performance with 1) no,” should be “ … 1) no pretraining”. 3) Paragraph 2 in Section 3.2, “The details ablation analysis” =&gt; “The detailed ablation analysis” 4) Paragraph 2 in Page 7, “either seen and unseen domains” =&gt; “either seen or unseen domains”; “our method can outperform the EISNet” =&gt; “our method outperformed the EISNet” Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It makes sense to combine contrastive learning, multi-style augmentation and multi-view learning. It’s natural that it improves the domain generalization performance. Although the experimental results showed that the proposed method outperformed a few recent methods, I have concerns whether the comparison was fair (See weakness 1). What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper These authors present a method for domain generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning. Fist, a CycleGAN is used to generate different styles from one vendor and the generated samples as used for contrastive learning. Second, for the Multiview, the authors treat the CC and MLO view of the same breast from the same patient as positive pair, whereas the other combination of CC and MLO is a negative pair for contrastive learning. Finally, the authors present a unified self-supervised contrastive learning to learn generalizable domain invariant features, which later can be used for the downstream task. The exhaustive experiments clearly show the contribution of their proposed approach for each of the components. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. • The authors address one interesting area of research for domain generalization in medical imaging, more specifically for Multiview and multi-style vendors. • The authors experiment clearly shows the contribution of each component of their proposed approach. • Comparison to classical transfer learning as well as to state-of-the-art domain generalization methods is well described and conducted with consistent improvement, showing the superiority of their proposed approach. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. • The flow of the paper reading is good, but some parts of the paper are unclear please the constructive comments section. • The authors use abbreviations before introducing them. It may be hard for the reader to follow the different methods abbreviation and the views names without defining them first. See the constructive comments section. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors do not include statements regarding the reproducibility or open access to code or data. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html • In the introduction the sentence “Meanwhile, for the multi-view contrastive learning, the CC and MLO views of the same breast are also paired as positive samples” does not make clear what is exactly happening in the multi-view learning. Elaborating better this may be an easier read. • The authors use abbreviations like CC, MLO, MSCL, MVCL, MSVCL. The authors are encourage to introduce and define these abbreviations before using the terms. • I found a big paragraph repeated in the text: “We adopt ResNet-50 as the backbone model for contrastive learning and FCOS detector. For fair comparison, the learning rate and batch size for all contrastive learning schemes are set the same as 0.3 and 256, respectively. Meanwhile, all contrastive learning schemes in all experiments use the same diversifying opera- tions, including random cropping, random rotation in ±10◦, horizontal flipping and random color jittering (strength=0.2).” • Fig 2 and 4 could have better description. Since Figure 2 summarizes the proposed method, more information can be provided in the figure. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This paper addresses an interesting topic in the MICCAI community, domain generalization. The experiments conducted are exhaustive and consistent showing the success of the proposed method for domain generalization. They also show very decent comparison to state-of-the-art methods in domain generalization as well as with transfer learning. Other than minor changes to the paper will result in a really interesting finding for the MICCAI community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This work adopts GAN and contrastive learning for domain generalization. Reviewers recognized the overall contribution in adopting the techniques. However, more details are needed for the specific setting and experiment, especically for the GAN part, which can be unstable and needs to be shown important for the overall performance. Also, experimental comparisons will need some clarification. Please note, the aim of rebuttal is to clarify misunderstandings / rationale behind method and experiment settings. Promise of extra experiments will not be considered. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The overall method setting makes sense to reviewers, though it is a combination of existing well-known methods. Major questions include 1) the details of training CycleGAN, since GAN networks can be unstable, and how it is properly trained is extremely important for representation learning, especially for disease cases, 2) performance gain significance, and 3) evaluation settings and fairness. Rebuttal added more details of the methods, and clarifies the “fairness” question properly. Still, I agree with reviewers on that it will be better if the two testing sets are from two vendors, rather than just population shift. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper brings interesting practical contributions for the community for the important domain generalisation problem. The rebuttal addresses the concerns raised by the AC and reviewers, so I recommend the acceptance of this paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. There are somewhat conflicting reviews given different reasons. The authors addressed most of these concerns. It is generally agreed by the reviewers that the work has value for presentation at MICCAI, which I would support. But, eventually this is a borderline paper and the decision may go either way. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Author Feedback Thank you for your time in review this paper. We hereby clarify several major comments and cordially request your consideration on this paper. Q1: Importance of style transfer quality for detection performance. (R2) A: Our goal is to seek a robust encoder of detector to various manufacture domains with contrastive learning. Good quality of style transfer can be helpful for the contrastive learning. To our experience, reasonable quality of style transfer can be attained with more training epochs of 50 for CycleGAN models. To systematically find suitable models in terms of epoch setting, we devise an indirect selection method based on the detection performance. Specifically, we use the detection performance as metrics to decide which models from the epoch settings can produce helpful style transfer images with quality for the detection tasks (see “Implementation details” in Sec.3). We fix all hyper-parameters of CycleGAN and FCOS, except the epoch of the CycleGAN, in the indirect selection scheme. The reasons of using indirect selection method are twofold. First, it is relatively difficult to quantitatively assess the quality of style transfer. The subjective assessment is impractical and improper. Second, these chosen models are based on the detection performance and may better align with our ultimate goal of detection. In summary, the style transfer quality is important for contrastive learning. Accordingly, we devise an indirect model selection method to find models that may produce good quality of style transfer and detection results. Q2: Training details of the styleGAN. (R2) A: The original CycleGAN was adopted here. The backbone of the generator is ResNet 9 blocks with 20 conv layers, while the discriminator is PatchGAN with 6 conv layers for binary classifications. In training, the sizes of inputs and outputs are 512*512 by random cropping the images. MSE and L1 losses are used for classification and reconstruction. For a style transfer, e.g., A to B, the involved training data from A and B are both 1000 for balanced training. We will add more details in the Supplement. Q3: MSVCL numbers in Table 2 &amp; 3 are different. (R2/R3) A: These are typos. The results of Table 3 are the correct one. We will fix it in the final version. Q4: Support of reproducibility. (R2) A: Although it seems that the concern about reproducibility is different between R2 and R3. We will release the CycleGAN models, which were the major concern of R2, for the support of reproducibility. Q5: Multi-view implementation for the 3 comparing methods. (R3) A: The 3 methods conduct domain generalization along with the downstream task. Therefore, we trained the 3 methods with the multi-view images from various vendors and explored the domain generalization correspondingly. Due to the space limitation, the details of training for 3 methods were not given in submission but will be added in the new Supplement. Q6: No-pretrained performed poorly on seen domains. (R3) A: The no-pretrained detector was trained with supervision. The major reason why no-pretrained detector performed poorly on seen domains may lie in the very limited training data (360*3 annotated data), compared to the pretrained ImageNet from millions of images. Low level features from ImageNet may still be helpful. Q7: Domains D and E may be the same. (R3) A: Although images of domains D and E shown in manuscript may look similar, it is worth noting that data of domain D were collected from Asian populations, whereas images of domain E were from Europe. Therefore, there may still exist domain gap. We’ll try to collect data of non-Siemens devices for further validation in the future study. Q8: Compared with RevGrad. (R3) A: We have tried RevGrad, which majorly requires discriminators for multi-view and -vendor domains. The preliminary results were not very well and may need some modification to our problem. Therefore, we didn’t add it for comparison. We’ll explore it in future work. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0709/12/31/Paper1740" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0709/12/31/Paper1740" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0709-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Domain Generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0709/12/31/Paper1740"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0709/12/31/Paper1740","headline":"Domain Generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning","dateModified":"0710-01-05T00:00:00-05:17","datePublished":"0709-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Zheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue, Dinggang Shen, Jie-Zhi Cheng Abstract Lesion detection is a fundamental problem in the computer-aided diagnosis scheme for mammography. The advance of deep learning techniques have made a remarkable progress for this task, provided that the training data are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, the collection of mammograms from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning model to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor-styles. Afterward, the backbone network is then recalibrated to the downstream task of lesion detection with the specific supervised learning. The proposed method is evaluated with mammograms from four vendors and one unseen public dataset. The experimental results suggest that our approach can effectively improve detection performance on both seen and unseen domains, and outperforms many state-of-the-art (SOTA) generalization methods. Link to paper https://doi.org/10.1007/978-3-030-87234-2_10 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper Due to the difficulties of collecting large-scale labeled training data, unsupervised representation learning followed by supervised learning of the downstream task is becoming general. The authors proposed a domain-specific unsupervised representation learning scheme which exploits contrastive learning of multi-view (CC, MLO) and multi-style (different vendor generated by StyleGAN) images. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The authors properly used well-known previous methods, i.e. StyleGAN and learning with contrastive loss, to resolve domain generalization issues in lesion detection in mammograms. The proposed pretraining method showed better performance compared to 1) basic types of pretraining methods, 2) several domain generalization methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. StyleGAN and Contrastive Learning are well-known techniques. Not only the general settings of those two works but also the details specific to the lesion detection in mammography also need to be added. E.g., training the StyleGAN with medical images tends to be unstable in terms of training &amp; quality, so details of training process needs to be described. Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The detailed training recipe of the proposed method is very important especially in StyleGAN part. The quality of the generated images are important in domain generalization. The authors checked ‘NO’ for all questions related to code release. Even though the data used for the experiments is hard to be released, the codes at least need to be released for the reproducibility. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html MVCL &amp; MSCL showed similar performance improvement in Table 2. Does the quality of the style-transferred image important to guarantee the level of performance improvement (described in Table 2)? If yes, training details of the StyleGAN needs to be added in the literature. The exemplary style-transferred images shown in the supplementary material seem to be normal images. Abnormal images with two types of lesions (especially the micro calc) would be helpful to understand the learned features of the StyleGAN and guestimate why the proposed method works well. MSVCL of the Style B is superior to MSCL &amp; MVCL, while they are similar in other styles. The authors should give some description about the difference of the trend in their experimental results. Mean &amp; std of multiple runs also would be helpful. MSVCL numbers in Table 2 &amp; 3 are different. Is it correct? Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Good adoption of the well-known previous works for the target problem. But, more details of the specific setting and additional explanation of the experimental results are needed to support the superiority of the proposed method in target applications. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 4 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper combines multi-style and multi-view data to do contrastive pretraining on multiple domains. The multi-style is achieved using CycleGANs on different domains. The multi-view is the two views (CC and MLO) of the mammograms. The pretrained model, after fine-tuning, performs better on unseen domains than baselines. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This work is a good combination of contrastive learning, multi-style augmentation and multi-view learning. It’s natural that it improves the domain generalization performance. The experimental results support that the proposed method outperformed a few recent methods. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. In Table 2, by comparing SimCLR (ImageNet → MammoPre) and MVCL, we can see the multi-view brought 0.015 improvement. In Table 3, EISNet was 0.02 worse than MSVCL. Was EISNet (as well as other baselines) trained with multi-view? If they were trained with single view only, then this was not a fair comparison, as the multi-view could largely eliminate this performance gap. Multi-view learning on mammogram has been explored before, but related papers are not discussed or compared. E.g. “Multi-view Multi-task Learning for Improving Autonomous Mammogram Diagnosis”, Machine Learning for Healthcare Conference, 2019. The authors should also compare with adversarial domain adaptation methods, e.g. RevGrad. Although two unseen domains D and E were evaluated, they were actually both acquired using Siemens devices. In Fig 3 we can also see example images from domain D and E are visually similar. Three baseline methods were evaluated in Table 3, which seem not be “many SOTA generalization methods” stated in the abstract. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The method seems not difficult to reproduce by a third-party. Though, it will be a great relief for the community if the authors could release the code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In Table 3, was EISNet (as well as other baselines) trained with multi-view? If they were trained with single view only, then this was not a fair comparison, as the multi-view could largely eliminate this performance gap. Why the reported results of MSVCL in Table 2 and 3 are different (both at the bottom row)? Why the “Random (no pretraining” method in Table 2 performed so poorly both on the seen domains and unsee domains? Was it trained with supervision? If so, it shouuld perform well on the seen domains. The authors are suggested to compare with adversarial domain adaptation methods, e.g. RevGrad. Multi-view methods should also be compared with. If possible, the authors should evaluate on another unseen domain (other than Siemens). If it’s infeasble, the authors should clearly indicate that domains D and E were both acquired using Siemens devices. It seems quite time consuming to train M*(M-1)/2 CycleGANs, where M is the number of domains. A few typos/grammar errors: 1) Paragraph 1 in Page 4, “The work [17] unidirectional takes …” 2) Paragraph 1 in Section 3.2, “we compare lesion detection performance with 1) no,” should be “ … 1) no pretraining”. 3) Paragraph 2 in Section 3.2, “The details ablation analysis” =&gt; “The detailed ablation analysis” 4) Paragraph 2 in Page 7, “either seen and unseen domains” =&gt; “either seen or unseen domains”; “our method can outperform the EISNet” =&gt; “our method outperformed the EISNet” Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It makes sense to combine contrastive learning, multi-style augmentation and multi-view learning. It’s natural that it improves the domain generalization performance. Although the experimental results showed that the proposed method outperformed a few recent methods, I have concerns whether the comparison was fair (See weakness 1). What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper These authors present a method for domain generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning. Fist, a CycleGAN is used to generate different styles from one vendor and the generated samples as used for contrastive learning. Second, for the Multiview, the authors treat the CC and MLO view of the same breast from the same patient as positive pair, whereas the other combination of CC and MLO is a negative pair for contrastive learning. Finally, the authors present a unified self-supervised contrastive learning to learn generalizable domain invariant features, which later can be used for the downstream task. The exhaustive experiments clearly show the contribution of their proposed approach for each of the components. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. • The authors address one interesting area of research for domain generalization in medical imaging, more specifically for Multiview and multi-style vendors. • The authors experiment clearly shows the contribution of each component of their proposed approach. • Comparison to classical transfer learning as well as to state-of-the-art domain generalization methods is well described and conducted with consistent improvement, showing the superiority of their proposed approach. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. • The flow of the paper reading is good, but some parts of the paper are unclear please the constructive comments section. • The authors use abbreviations before introducing them. It may be hard for the reader to follow the different methods abbreviation and the views names without defining them first. See the constructive comments section. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The authors do not include statements regarding the reproducibility or open access to code or data. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html • In the introduction the sentence “Meanwhile, for the multi-view contrastive learning, the CC and MLO views of the same breast are also paired as positive samples” does not make clear what is exactly happening in the multi-view learning. Elaborating better this may be an easier read. • The authors use abbreviations like CC, MLO, MSCL, MVCL, MSVCL. The authors are encourage to introduce and define these abbreviations before using the terms. • I found a big paragraph repeated in the text: “We adopt ResNet-50 as the backbone model for contrastive learning and FCOS detector. For fair comparison, the learning rate and batch size for all contrastive learning schemes are set the same as 0.3 and 256, respectively. Meanwhile, all contrastive learning schemes in all experiments use the same diversifying opera- tions, including random cropping, random rotation in ±10◦, horizontal flipping and random color jittering (strength=0.2).” • Fig 2 and 4 could have better description. Since Figure 2 summarizes the proposed method, more information can be provided in the figure. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This paper addresses an interesting topic in the MICCAI community, domain generalization. The experiments conducted are exhaustive and consistent showing the success of the proposed method for domain generalization. They also show very decent comparison to state-of-the-art methods in domain generalization as well as with transfer learning. Other than minor changes to the paper will result in a really interesting finding for the MICCAI community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This work adopts GAN and contrastive learning for domain generalization. Reviewers recognized the overall contribution in adopting the techniques. However, more details are needed for the specific setting and experiment, especically for the GAN part, which can be unstable and needs to be shown important for the overall performance. Also, experimental comparisons will need some clarification. Please note, the aim of rebuttal is to clarify misunderstandings / rationale behind method and experiment settings. Promise of extra experiments will not be considered. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The overall method setting makes sense to reviewers, though it is a combination of existing well-known methods. Major questions include 1) the details of training CycleGAN, since GAN networks can be unstable, and how it is properly trained is extremely important for representation learning, especially for disease cases, 2) performance gain significance, and 3) evaluation settings and fairness. Rebuttal added more details of the methods, and clarifies the “fairness” question properly. Still, I agree with reviewers on that it will be better if the two testing sets are from two vendors, rather than just population shift. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This paper brings interesting practical contributions for the community for the important domain generalisation problem. The rebuttal addresses the concerns raised by the AC and reviewers, so I recommend the acceptance of this paper. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. There are somewhat conflicting reviews given different reasons. The authors addressed most of these concerns. It is generally agreed by the reviewers that the work has value for presentation at MICCAI, which I would support. But, eventually this is a borderline paper and the decision may go either way. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Author Feedback Thank you for your time in review this paper. We hereby clarify several major comments and cordially request your consideration on this paper. Q1: Importance of style transfer quality for detection performance. (R2) A: Our goal is to seek a robust encoder of detector to various manufacture domains with contrastive learning. Good quality of style transfer can be helpful for the contrastive learning. To our experience, reasonable quality of style transfer can be attained with more training epochs of 50 for CycleGAN models. To systematically find suitable models in terms of epoch setting, we devise an indirect selection method based on the detection performance. Specifically, we use the detection performance as metrics to decide which models from the epoch settings can produce helpful style transfer images with quality for the detection tasks (see “Implementation details” in Sec.3). We fix all hyper-parameters of CycleGAN and FCOS, except the epoch of the CycleGAN, in the indirect selection scheme. The reasons of using indirect selection method are twofold. First, it is relatively difficult to quantitatively assess the quality of style transfer. The subjective assessment is impractical and improper. Second, these chosen models are based on the detection performance and may better align with our ultimate goal of detection. In summary, the style transfer quality is important for contrastive learning. Accordingly, we devise an indirect model selection method to find models that may produce good quality of style transfer and detection results. Q2: Training details of the styleGAN. (R2) A: The original CycleGAN was adopted here. The backbone of the generator is ResNet 9 blocks with 20 conv layers, while the discriminator is PatchGAN with 6 conv layers for binary classifications. In training, the sizes of inputs and outputs are 512*512 by random cropping the images. MSE and L1 losses are used for classification and reconstruction. For a style transfer, e.g., A to B, the involved training data from A and B are both 1000 for balanced training. We will add more details in the Supplement. Q3: MSVCL numbers in Table 2 &amp; 3 are different. (R2/R3) A: These are typos. The results of Table 3 are the correct one. We will fix it in the final version. Q4: Support of reproducibility. (R2) A: Although it seems that the concern about reproducibility is different between R2 and R3. We will release the CycleGAN models, which were the major concern of R2, for the support of reproducibility. Q5: Multi-view implementation for the 3 comparing methods. (R3) A: The 3 methods conduct domain generalization along with the downstream task. Therefore, we trained the 3 methods with the multi-view images from various vendors and explored the domain generalization correspondingly. Due to the space limitation, the details of training for 3 methods were not given in submission but will be added in the new Supplement. Q6: No-pretrained performed poorly on seen domains. (R3) A: The no-pretrained detector was trained with supervision. The major reason why no-pretrained detector performed poorly on seen domains may lie in the very limited training data (360*3 annotated data), compared to the pretrained ImageNet from millions of images. Low level features from ImageNet may still be helpful. Q7: Domains D and E may be the same. (R3) A: Although images of domains D and E shown in manuscript may look similar, it is worth noting that data of domain D were collected from Asian populations, whereas images of domain E were from Europe. Therefore, there may still exist domain gap. We’ll try to collect data of non-Siemens devices for further validation in the future study. Q8: Compared with RevGrad. (R3) A: We have tried RevGrad, which majorly requires discriminators for multi-view and -vendor domains. The preliminary results were not very well and may need some modification to our problem. Therefore, we didn’t add it for comparison. We’ll explore it in future work. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Li, Zheren,Cui, Zhiming,Wang, Sheng,Qi, Yuji,Ouyang, Xi,Chen, Qitian,Yang, Yuezhi,Xue, Zhong,Shen, Dinggang,Cheng, Jie-Zhi" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Domain Generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Clinical applications - Breast"
        class="post-category">
        Clinical applications - Breast
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Domain adaptation"
        class="post-category">
        Machine Learning - Domain adaptation
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Li, Zheren"
        class="post-tags">
        Li, Zheren
      </a> |  
      
      <a href="kittywong/tags#Cui, Zhiming"
        class="post-tags">
        Cui, Zhiming
      </a> |  
      
      <a href="kittywong/tags#Wang, Sheng"
        class="post-tags">
        Wang, Sheng
      </a> |  
      
      <a href="kittywong/tags#Qi, Yuji"
        class="post-tags">
        Qi, Yuji
      </a> |  
      
      <a href="kittywong/tags#Ouyang, Xi"
        class="post-tags">
        Ouyang, Xi
      </a> |  
      
      <a href="kittywong/tags#Chen, Qitian"
        class="post-tags">
        Chen, Qitian
      </a> |  
      
      <a href="kittywong/tags#Yang, Yuezhi"
        class="post-tags">
        Yang, Yuezhi
      </a> |  
      
      <a href="kittywong/tags#Xue, Zhong"
        class="post-tags">
        Xue, Zhong
      </a> |  
      
      <a href="kittywong/tags#Shen, Dinggang"
        class="post-tags">
        Shen, Dinggang
      </a> |  
      
      <a href="kittywong/tags#Cheng, Jie-Zhi"
        class="post-tags">
        Cheng, Jie-Zhi
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Zheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue, Dinggang Shen, Jie-Zhi Cheng
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Lesion detection is a fundamental problem in the computer-aided diagnosis scheme for mammography. The advance of deep learning techniques have made a remarkable progress for this task, provided that the training data are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, the collection of mammograms from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning model to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor-styles. Afterward, the backbone network is then recalibrated to the downstream task of lesion detection with the specific supervised learning. The proposed method is evaluated with mammograms from four vendors and one unseen public dataset. The experimental results suggest that our approach can effectively improve detection performance on both seen and unseen domains, and outperforms many state-of-the-art (SOTA) generalization methods.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87234-2_10">https://doi.org/10.1007/978-3-030-87234-2_10</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>Due to the difficulties of collecting large-scale labeled training data, unsupervised representation learning followed by supervised learning of the downstream task is becoming general. The authors proposed a domain-specific unsupervised representation learning scheme which exploits contrastive learning of multi-view (CC, MLO) and multi-style (different vendor generated by StyleGAN) images.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The authors properly used well-known previous methods, i.e. StyleGAN and learning with contrastive loss, to resolve domain generalization issues in lesion detection in mammograms. The proposed pretraining method showed better performance compared to 1) basic types of pretraining methods, 2) several domain generalization methods.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>StyleGAN and Contrastive Learning are well-known techniques. Not only the general settings of those two works but also the details specific to the lesion detection in mammography also need to be added. E.g., training the StyleGAN with medical images tends to be unstable in terms of training &amp; quality, so details of training process needs to be described.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Satisfactory</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The detailed training recipe of the proposed method is very important especially in StyleGAN part. The quality of the generated images are important in domain generalization. The authors checked ‘NO’ for all questions related to code release. Even though the data used for the experiments is hard to be released, the codes at least need to be released for the reproducibility.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>MVCL &amp; MSCL showed similar performance improvement in Table 2. Does the quality of the style-transferred image important to guarantee the level of performance improvement (described in Table 2)? If yes, training details of the StyleGAN needs to be added in the literature.</p>

      <p>The exemplary style-transferred images shown in the supplementary material seem to be normal images. Abnormal images with two types of lesions (especially the micro calc) would be helpful to understand the learned features of the StyleGAN and guestimate why the proposed method works well.</p>

      <p>MSVCL of the Style B is superior to MSCL &amp; MVCL, while they are similar in other styles. The authors should give some description about the difference of the trend in their experimental results. Mean &amp; std of multiple runs also would be helpful.</p>

      <p>MSVCL numbers in Table 2 &amp; 3 are different. Is it correct?</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline reject (5)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Good adoption of the well-known previous works for the target problem. But, more details of the specific setting and additional explanation of the experimental results are needed to support the superiority of the proposed method in target applications.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper combines multi-style and multi-view data to do contrastive pretraining on multiple domains. The multi-style is achieved using CycleGANs on different domains. The multi-view is the two views (CC and MLO) of the mammograms. The pretrained model, after fine-tuning, performs better on unseen domains than baselines.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ol>
        <li>
          <p>This work is a good combination of contrastive learning, multi-style augmentation and multi-view learning. It’s natural that it improves the domain generalization performance.</p>
        </li>
        <li>
          <p>The experimental results support that the proposed method outperformed a few recent methods.</p>
        </li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ol>
        <li>
          <p>In Table 2, by comparing SimCLR (ImageNet → MammoPre) and MVCL, we can see the multi-view brought 0.015 improvement. In Table 3, EISNet was 0.02 worse than MSVCL. Was EISNet (as well as other baselines) trained with multi-view? If they were trained with single view only, then this was not a fair comparison, as the multi-view could largely eliminate this performance gap.</p>
        </li>
        <li>
          <p>Multi-view learning on mammogram has been explored before, but related papers are not discussed or compared. E.g. “Multi-view Multi-task Learning for Improving Autonomous Mammogram Diagnosis”, Machine Learning for Healthcare Conference, 2019.</p>
        </li>
        <li>
          <p>The authors should also compare with adversarial domain adaptation methods, e.g. RevGrad.</p>
        </li>
        <li>
          <p>Although two unseen domains D and E were evaluated, they were actually both acquired using Siemens devices. In Fig 3 we can also see example images from domain D and E are visually similar.</p>
        </li>
        <li>
          <p>Three baseline methods were evaluated in Table 3, which seem not be “many SOTA generalization methods” stated in the abstract.</p>
        </li>
      </ol>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The method seems not difficult to reproduce by a third-party. Though, it will be a great relief for the community if the authors could release the code.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <ol>
        <li>
          <p>In Table 3, was EISNet (as well as other baselines) trained with multi-view? If they were trained with single view only, then this was not a fair comparison, as the multi-view could largely eliminate this performance gap.</p>
        </li>
        <li>
          <p>Why the reported results of MSVCL in Table 2 and 3 are different (both at the bottom row)?</p>
        </li>
        <li>
          <p>Why the “Random (no pretraining” method in Table 2 performed so poorly both on the seen domains and unsee domains? Was it trained with supervision? If so, it shouuld perform well on the seen domains.</p>
        </li>
        <li>
          <p>The authors are suggested to compare with adversarial domain adaptation methods, e.g. RevGrad. Multi-view methods should also be compared with.</p>
        </li>
        <li>
          <p>If possible, the authors should evaluate on another unseen domain (other than Siemens). If it’s infeasble, the authors should clearly indicate that domains D and E were both acquired using Siemens devices.</p>
        </li>
        <li>
          <p>It seems quite time consuming to train M*(M-1)/2 CycleGANs, where M is the number of domains.</p>
        </li>
        <li>
          <p>A few typos/grammar errors: 
1) Paragraph 1 in Page 4, “The work [17] unidirectional takes …”
2) Paragraph 1 in Section 3.2, “we compare lesion detection performance with 1) no,” should be “ … 1) no pretraining”.
3) Paragraph 2 in Section 3.2, “The details ablation analysis” =&gt; “The detailed ablation analysis”
4) Paragraph 2 in Page 7, “either seen and unseen domains” =&gt; “either seen or unseen domains”; “our method can outperform the EISNet” =&gt; “our method outperformed the EISNet”</p>
        </li>
      </ol>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <ol>
        <li>
          <p>It makes sense to combine contrastive learning, multi-style augmentation and multi-view learning. It’s natural that it improves the domain generalization performance.</p>
        </li>
        <li>
          <p>Although the experimental results showed that the proposed method outperformed a few recent methods, I have concerns whether the comparison was fair (See weakness 1).</p>
        </li>
      </ol>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>These authors present a method for domain generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning. Fist, a CycleGAN is used to generate different styles from one vendor and the generated samples as used for contrastive learning. Second, for the Multiview, the authors treat the CC and MLO view of the same breast from the same patient as positive pair, whereas the other combination of CC and MLO is a negative pair for contrastive learning. Finally, the authors present a unified self-supervised contrastive learning to learn generalizable domain invariant features, which later can be used for the downstream task. The exhaustive experiments clearly show the contribution of their proposed approach for each of the components.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>•	The authors address one interesting area of research for domain generalization in medical imaging, more specifically for Multiview and multi-style vendors.
•	The authors experiment clearly shows the contribution of each component of their proposed approach.
•	Comparison to classical transfer learning as well as to state-of-the-art domain generalization methods is well described and conducted with consistent improvement, showing the superiority of their proposed approach.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>•	The flow of the paper reading is good, but some parts of the paper are unclear please the constructive comments section.
•	The authors use abbreviations before introducing them. It may be hard for the reader to follow the different methods abbreviation and the views names without defining them first. See the constructive comments section.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The authors do not include statements regarding the reproducibility or open access to code or data.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>•	In the introduction the sentence “Meanwhile, for the multi-view contrastive learning, the CC and MLO views of the same breast are also paired as positive samples” does not make clear what is exactly happening in the multi-view learning. Elaborating better this may be an easier read.
•	The authors use abbreviations like CC, MLO, MSCL, MVCL, MSVCL. The authors are encourage to introduce and define these abbreviations before using the terms.
•	I found a big paragraph repeated in the text: “We adopt ResNet-50 as the backbone model for contrastive learning and FCOS detector. For fair comparison, the learning rate and batch size for all contrastive learning schemes are set the same as 0.3 and 256, respectively. Meanwhile, all contrastive learning schemes in all experiments use the same diversifying opera- tions, including random cropping, random rotation in ±10◦, horizontal flipping and random color jittering (strength=0.2).”
•	Fig 2 and 4 could have better description. Since Figure 2 summarizes the proposed method, more information can be provided in the figure.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>This paper addresses an interesting topic in the MICCAI community, domain generalization. The experiments conducted are exhaustive and consistent showing the success of the proposed method for domain generalization. They also show very decent comparison to state-of-the-art methods in domain generalization as well as with transfer learning. Other than minor changes to the paper will result in a really interesting finding for the MICCAI community.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>This work adopts GAN and contrastive learning for domain generalization. Reviewers recognized the overall contribution in adopting the techniques. However, more details are needed for the specific setting and experiment, especically for the GAN part, which can be unstable and needs to be shown important for the overall performance. Also, experimental comparisons will need some clarification. Please note, the aim of rebuttal is to clarify misunderstandings / rationale behind method and experiment settings. Promise of extra experiments will not be considered.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The overall method setting makes sense to reviewers, though it is a combination of existing well-known methods. Major questions include 1) the details of training CycleGAN, since GAN networks can be unstable, and how it is properly trained is extremely important for representation learning, especially for disease cases, 2) performance gain significance, and 3) evaluation settings and fairness. Rebuttal added more details of the methods, and clarifies the “fairness” question properly. Still, I agree with reviewers on that it will be better if the two testing sets are from two vendors, rather than just population shift.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="meta-review-2">Meta-Review #2</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>This paper brings interesting practical contributions for the community for the important domain generalisation problem.  The rebuttal addresses the concerns raised by the AC and reviewers, so I recommend the acceptance of this paper.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>
<h2 id="meta-review-3">Meta-Review #3</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>There are somewhat conflicting reviews given different reasons.  The authors addressed most of these concerns.  It is generally agreed by the reviewers that the work has value for presentation at MICCAI, which I would support.  But, eventually this is a borderline paper and the decision may go either way.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
</ul>

<h2><br /><br /></h2>
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>Thank you for your time in review this paper. We hereby clarify several major comments and cordially request your consideration on this paper.</p>

  <p>Q1: Importance of style transfer quality for detection performance. (R2)
A: Our goal is to seek a robust encoder of detector to various manufacture domains with contrastive learning. Good quality of style transfer can be helpful for the contrastive learning. To our experience, reasonable quality of style transfer can be attained with more training epochs of 50 for CycleGAN models. To systematically find suitable models in terms of epoch setting, we devise an indirect selection method based on the detection performance. Specifically, we use the detection performance as metrics to decide which models from the epoch settings can produce helpful style transfer images with quality for the detection tasks (see “Implementation details” in Sec.3). We fix all hyper-parameters of CycleGAN and FCOS, except the epoch of the CycleGAN, in the indirect selection scheme.</p>

  <p>The reasons of using indirect selection method are twofold. First, it is relatively difficult to quantitatively assess the quality of style transfer. The subjective assessment is impractical and improper. Second, these chosen models are based on the detection performance and may better align with our ultimate goal of detection.</p>

  <p>In summary, the style transfer quality is important for contrastive learning. Accordingly, we devise an indirect model selection method to find models that may produce good quality of style transfer and detection results.</p>

  <p>Q2: Training details of the styleGAN. (R2)
A: The original CycleGAN was adopted here. The backbone of the generator is ResNet 9 blocks with 20 conv layers, while the discriminator is PatchGAN with 6 conv layers for binary classifications. In training, the sizes of inputs and outputs are 512*512 by random cropping the images. MSE and L1 losses are used for classification and reconstruction. For a style transfer, e.g., A to B, the involved training data from A and B are both 1000 for balanced training. We will add more details in the Supplement.</p>

  <p>Q3: MSVCL numbers in Table 2 &amp; 3 are different. (R2/R3)
A: These are typos. The results of Table 3 are the correct one. We will fix it in the final version.</p>

  <p>Q4: Support of reproducibility. (R2)
A: Although it seems that the concern about reproducibility is different between R2 and R3. We will release the CycleGAN models, which were the major concern of R2, for the support of reproducibility.</p>

  <p>Q5: Multi-view implementation for the 3 comparing methods. (R3)
A: The 3 methods conduct domain generalization along with the downstream task. Therefore, we trained the 3 methods with the multi-view images from various vendors and explored the domain generalization correspondingly. Due to the space limitation, the details of training for 3 methods were not given in submission but will be added in the new Supplement.</p>

  <p>Q6: No-pretrained performed poorly on seen domains. (R3)
A: The no-pretrained detector was trained with supervision. The major reason why no-pretrained detector performed poorly on seen domains may lie in the very limited training data (360*3 annotated data), compared to the pretrained ImageNet from millions of images. Low level features from ImageNet may still be helpful.</p>

  <p>Q7: Domains D and E may be the same. (R3)
A: Although images of domains D and E shown in manuscript may look similar, it is worth noting that data of domain D were collected from Asian populations, whereas images of domain E were from Europe. Therefore, there may still exist domain gap. We’ll try to collect data of non-Siemens devices for further validation in the future study.</p>

  <p>Q8: Compared with RevGrad. (R3)
A: We have tried RevGrad, which majorly requires discriminators for multi-view and -vendor domains. The preliminary results were not very well and may need some modification to our problem. Therefore, we didn’t add it for comparison. We’ll explore it in future work.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0709-12-31
      -->
      <!--
      
        ,
        updated at 
        0710-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Clinical applications - Breast"
        class="post-category">
        Clinical applications - Breast
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Domain adaptation"
        class="post-category">
        Machine Learning - Domain adaptation
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Self-supervised learning"
        class="post-category">
        Machine Learning - Self-supervised learning
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Li, Zheren"
        class="post-category">
        Li, Zheren
      </a> |  
      
      <a href="kittywong/tags#Cui, Zhiming"
        class="post-category">
        Cui, Zhiming
      </a> |  
      
      <a href="kittywong/tags#Wang, Sheng"
        class="post-category">
        Wang, Sheng
      </a> |  
      
      <a href="kittywong/tags#Qi, Yuji"
        class="post-category">
        Qi, Yuji
      </a> |  
      
      <a href="kittywong/tags#Ouyang, Xi"
        class="post-category">
        Ouyang, Xi
      </a> |  
      
      <a href="kittywong/tags#Chen, Qitian"
        class="post-category">
        Chen, Qitian
      </a> |  
      
      <a href="kittywong/tags#Yang, Yuezhi"
        class="post-category">
        Yang, Yuezhi
      </a> |  
      
      <a href="kittywong/tags#Xue, Zhong"
        class="post-category">
        Xue, Zhong
      </a> |  
      
      <a href="kittywong/tags#Shen, Dinggang"
        class="post-category">
        Shen, Dinggang
      </a> |  
      
      <a href="kittywong/tags#Cheng, Jie-Zhi"
        class="post-category">
        Cheng, Jie-Zhi
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0710/12/31/Paper1786">
          Learned super resolution ultrasound for improved breast lesion characterization
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0708/12/31/Paper1469">
          Graph Transformers for Characterization and Interpretation of Surgical Margins
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
