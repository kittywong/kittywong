<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Devavrat Tomar, Lin Zhang, Tiziano Portenier, Orcun Goksel Abstract Interactive simulation of ultrasound imaging greatly facilitates sonography training. Although ray-tracing based methods have shown promising results, obtaining realistic images requires substantial modeling effort and manual parameter tuning. In addition, current techniques still result in a significant appearance gap between simulated images and real clinical scans. Herein we introduce a novel content-preserving image translation framework (ConPres) to bridge this appearance gap, while maintaining the simulated anatomical layout. We achieve this goal by leveraging both simulated images with semantic segmentations and unpaired in-vivo ultrasound scans. Our framework is based on recent contrastive unpaired translation techniques and we propose a regularization approach by learning an auxiliary segmentation-to-real image translation task, which encourages the disentanglement of content and style. In addition, we extend the generator to be class-conditional, which enables the incorporation of additional losses, in particular a cyclic consistency loss, to further improve the translation quality. Qualitative and quantitative comparisons against state-of-the-art unpaired translation methods demonstrate the superiority of our proposed framework. Link to paper https://doi.org/10.1007/978-3-030-87237-3_63 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper describes an image translation framework synthesizing from simulated ultrasound images to realistic ones. The proposed network consists of a single generator and a single discriminator. The generator takes simulated image, its segmentation and an real image as input, computing contrastive loss and noise contrastive estimation loss which was proposed in previous work, sematic-consistent loss with the mask, and finally a cycle consistent loss using the same G twice in contrast to cycleGAN-like models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper leverages recent advances of the domain adaptation work, such as contrastive loss for training, and kernel inception distance for evaluation which I found are interesting. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Unfortunately there are quite some and crucial weakness in this paper. First, lack of novelty. The only differences between this paper and starGAN are contrastive loss and semantic-consistent loss. The contrastive loss comes from Park, T. [16]. The semantic-consistent loss is just replacing X by S in the contrastive loss, since S is just another source image for generating images in Y’s domain. The overall novelty is very limited. Second, the experiment design is poor and even wrong. The data splitting is improper which causes a high risk of ‘training and testing on a same image’. Please see comments part for details. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Reproducing can be hard, mainly because the simulation algorithm details are not listed (either main or supplementary script). Without the details of simulated images, reproducing the work can be very challenging. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Overall this paper is well written and organized. Advantages of many recent proposed works have been incorporated. Besides the lack of novelty issue, my biggest concern is on the data splitting part in Section 3 Experiments. For real in-vivo images, the authors collected 22 ultrasound sequences from 8 patients, each of them is only several seconds long. However, the author puts all frames together and makes an 80%-10%-10% data splitting. First of all, it is not possible to make such splitting over 8 patients, so this splitting is really done over all frames. So there will be overlap in terms patients data between training/validation and testing. In addition, each sequence is only few seconds long. This means the variation in the data is very limited. Overlapping data, limited variation, this significantly weakens the claims of the experiments, especially for GAN related work. I don’t think the segmentation-consistency regularization term is well validated by the experiments. From Fig 3., comparing between CUT and CUTS, on the 4th column, the segmentation corresponding area is lost in CUTS. In Table 1, CUTS is consistently worse than CUT for all metrics. Then what is the advantage of adding this term? Please comment. Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Reject. Lack of novelty. Proposed segmentation regularization is not well justified. Problematic data splitting in numerical experiments. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The authors proposed a novel image translation framework to bridge significant appearance gap between simulated images and real clinical scans, which has two contributions: constrain the generator with the accompanying semantic labels of simulated images by learning an auxiliary segmentation-to-real image translation task and apply a class-conditional generator, which in turn enables the incorporation of a cyclic loss. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -The proposed method is novel. Based on contrastive unpaired translation framework, the authors used the semantic labels of simulated images by learning an auxiliary segmentation-to-real image task to constrain the generator and extended a class-conditional generator to enable a cyclic loss. -The qualitative and quantitative metrics proved that this framework closed the appearance gap between simulated and real images, which had a significant influence on the US images generation. And a user study was performed to evaluate the realism of translated images by US experts. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -A more clear outline of the next steps in research would be appropriate. -There are fewer data sets and all act on fetal images. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The details described in the paper and the reproducibility are good. No code and data set are provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -A more clear outline of the next steps in research would be appropriate, some discussion but vague. There are already some new work based on CUT. -The author should try to experiment on a larger data set, and based on different types of images, which is more convincing, now it is only fetal images. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? -From the experimental results, the framework proposed by the authors can indeed bridge appearance gap between simulated images and real clinical scans to a certain extent. -The algorithm has a certain degree of innovation. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper takes a bag of losses to boost the image-to-image translation. Extensive experiments compared to SOTA methods on realistic datasets evaluated the effectiveness of the proposed method to improve the realism of computationally simulated US images. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper tries to use various losses in the image-to-image translation field to improve the realism of the generated images. According to the experimental results, the simulated images are much more realistic than other methods. The authors use typical evaluation metrics and invite experts to rate the qualities. The semantic-consistent regularization is well designed. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. 1 As claimed in the Abstract, the authors try to improve the quality of simulated US images, which can facilitate sonography training. The authors should evaluate how the generated images by the proposed method help sonography training, compared to the traditional methods. 2 More ablation studies and visualizations should be explored to show the effectiveness of the losses (loss of GAN, CLS, CYC, NCE) Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance As the dataset is private and the code is not going be released, I am not sure this work can be reproduced. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see the “Weakness” part. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I choose the “borderline accept” because of the great performance shown in the experimental results. However, due to the lack of reproducibility, I am not sure if this paper contributes more to society. On the other hand, It needs more experiments to evaluate the clinical effectiveness of those realistic simulated ultrasound images. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 2 Reviewer confidence Somewhat confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers. The rebuttal explains why the employed data splitting is justified: “the goal is not to reproduce the real data, but to abstract and apply the style of the real training data.” This somehow alleviate the issue, however, it is still questionable in my view. For example, a common use of image synthesis is through a subsequent application such as classification, segmentation, etc. In this regard, a strict data splitting is necessary. The rebuttal provides the results based on patient-wise splitting and it seems that similar performance improvements are reported. The rebuttal also provides more ablation studies. Overall, the paper meets the MICCAI acceptance level if the authors can make necessary changes per rebuttal. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors propose a method to simulate realistic ultrasound images, by taking the structure from input simulated images, and the appearance from real images. The method clearly work in translating the appearance to the generated images. Authors have addressed the methodological criticisms, although I am not sure they have correctly addressed the criticisms regarding novelty. Moreover, I am not sure they have addressed at all the issues related to clinical utility (for example raised by R3). I have concerns about the user survey too. Authors said that experts were asked to rank images by “their likelihood for being an image from this machine”. Even if the answer to this is highly positive, it fails to demonstrate that images look realistic, because they don’t. As pointed out by the authors themselves, “the simulated and the real images have substantially different anatomical contents”, i.e. the simulated fetal geometries are highly unrealistic. This has two implications: first, because of the lack of realism, the clinical utility is at best arguable (reason for which authors have not been able to rebut this point). Second, the challenge of obtaining realistic fetal geometries to generate realistic looking images might be greater than that of simulating realistic images. In their rebuttal, authors address one of the main criticisms (lack of novelty) by saying that “ our novelty is to enforce semantic consistency “, however this is precisely (as I pointed out) the main flaw. As a result, I cannot recommend acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 15 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The solution provided by the authors is interesting and the strongest arguments raised by the reviewers (novelty and data splitting) were properly explained and justified in the rebuttal phase. The rebuttal by the authors is clear, well argued and complete, and provides clarifications to the mayor points raised by the reviewers. The weakest point in the rebuttal is the justification and highlighting of the work novelty, yet novelty is detailed in the manuscript. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Author Feedback We thank the reviewers for their valuable feedback. Major comments are addressed below, and will be integrated shortly in the paper: ** Data splitting (R1) Other image analysis tasks such as classification and segmentation require evaluation on unseen patients to avoid bias. However, using train and/or test sets, even together, for computing GAN distribution metrics (FID/KID) is common practice [2,8, StyleGAN-Ada]. This is indeed more justified in an unpaired setting as in our appearance transfer task, where the goal is not to reproduce the real data, but to abstract and apply the style of the real training data. For computing FID/KID, we followed the approach from [8, StyleGAN-Ada] that introduced / used these metrics; i.e., we computed on the train set, since set distribution metrics require large samples for robust estimates. Indeed, adding the test set does not result in any difference (CUTS-C* KID: 0.24/0.24, FID 1.51/1.52 for train/all). The test set alone would be too small to compute these metrics robustly. Moreover, if there were bias from data splitting, this would also affect our baselines, against which our method is still seen to be superior. Indeed, larger models in our baselines might even profit more, if there were any such bias. That said, we have conducted an additional, patient-wise splitting experiment (6 patients for train). This yields a KID of 0.24, the same as earlier random data splitting. ** Limited real data variability (R1), More datasets (R3) For transferring appearance, large variability in real data is not necessary. Indeed, even a single image in the target domain can be sufficient under certain circumstances (cf. [16]). Here we used thousands of real images to capture the appearance of a specific US machine. Our goal here is task-specific, i.e., enhancing US simulation of the common 20-week fetal exams with real-machine like image appearance. Conceptually, there is no limitation in translating other US scenes (although a practical requirement would be the availability of detailed 3D models). ** Semantic-consistent regularization not well justified (R1) We assume a misunderstanding regarding the ablated models: CUTS is CUT with an auxiliary segmentation to real translation, but without the proposed semantic-consistent regularization. As seen in Fig.3 and pointed out by R1, learning this auxiliary task alone does not improve CUT. However, by adding the regularization term (CUTS-C), the translated images are semantically more close to simulated ones, c.f. Fig. 3 (CUT vs. CUTS-C). This is also corroborated with the better SSIM of CUTS-C, clearly demonstrating the benefit of our proposed regularization approach. As the namings may be confusing w.r.t. ablations, and we will rename our method. ** Limited Novelty (R1) For content preservation during appearance transfer, our novelty is to enforce semantic consistency with the proposed regularization during a multi-domain translation. Note that semantic regularization is not “replacing X by S in the contrastive loss”, but enforcing G to generate the same output for X and S. ** Future works (R3&amp;R4), Clinical experiments (R3) Future works include: clinical evaluation of the effects of translated images on US training; improving seg-to-real image translation to bypass any expensive rendering entirely; evaluations with other US/non-US scenes. ** More ablations for other losses (R4) The experiments in the submission demonstrate the benefits of all proposed components. For completeness, we report additional comparisons here (FID/KID/SSIM): proposed model (1.51/0.24/72.13), without NCE (2.22/0.43/64.77), without CYC (2.33/0.45/84.77). The results show that these components are indeed essential. Without CYC, the translated images look similar to the simulated ones. GAN loss is what promotes realism; and without CLS, the discriminator would have no info on domain-dependent classification [5], so these are essential. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Devavrat Tomar, Lin Zhang, Tiziano Portenier, Orcun Goksel Abstract Interactive simulation of ultrasound imaging greatly facilitates sonography training. Although ray-tracing based methods have shown promising results, obtaining realistic images requires substantial modeling effort and manual parameter tuning. In addition, current techniques still result in a significant appearance gap between simulated images and real clinical scans. Herein we introduce a novel content-preserving image translation framework (ConPres) to bridge this appearance gap, while maintaining the simulated anatomical layout. We achieve this goal by leveraging both simulated images with semantic segmentations and unpaired in-vivo ultrasound scans. Our framework is based on recent contrastive unpaired translation techniques and we propose a regularization approach by learning an auxiliary segmentation-to-real image translation task, which encourages the disentanglement of content and style. In addition, we extend the generator to be class-conditional, which enables the incorporation of additional losses, in particular a cyclic consistency loss, to further improve the translation quality. Qualitative and quantitative comparisons against state-of-the-art unpaired translation methods demonstrate the superiority of our proposed framework. Link to paper https://doi.org/10.1007/978-3-030-87237-3_63 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper describes an image translation framework synthesizing from simulated ultrasound images to realistic ones. The proposed network consists of a single generator and a single discriminator. The generator takes simulated image, its segmentation and an real image as input, computing contrastive loss and noise contrastive estimation loss which was proposed in previous work, sematic-consistent loss with the mask, and finally a cycle consistent loss using the same G twice in contrast to cycleGAN-like models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper leverages recent advances of the domain adaptation work, such as contrastive loss for training, and kernel inception distance for evaluation which I found are interesting. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Unfortunately there are quite some and crucial weakness in this paper. First, lack of novelty. The only differences between this paper and starGAN are contrastive loss and semantic-consistent loss. The contrastive loss comes from Park, T. [16]. The semantic-consistent loss is just replacing X by S in the contrastive loss, since S is just another source image for generating images in Y’s domain. The overall novelty is very limited. Second, the experiment design is poor and even wrong. The data splitting is improper which causes a high risk of ‘training and testing on a same image’. Please see comments part for details. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Reproducing can be hard, mainly because the simulation algorithm details are not listed (either main or supplementary script). Without the details of simulated images, reproducing the work can be very challenging. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Overall this paper is well written and organized. Advantages of many recent proposed works have been incorporated. Besides the lack of novelty issue, my biggest concern is on the data splitting part in Section 3 Experiments. For real in-vivo images, the authors collected 22 ultrasound sequences from 8 patients, each of them is only several seconds long. However, the author puts all frames together and makes an 80%-10%-10% data splitting. First of all, it is not possible to make such splitting over 8 patients, so this splitting is really done over all frames. So there will be overlap in terms patients data between training/validation and testing. In addition, each sequence is only few seconds long. This means the variation in the data is very limited. Overlapping data, limited variation, this significantly weakens the claims of the experiments, especially for GAN related work. I don’t think the segmentation-consistency regularization term is well validated by the experiments. From Fig 3., comparing between CUT and CUTS, on the 4th column, the segmentation corresponding area is lost in CUTS. In Table 1, CUTS is consistently worse than CUT for all metrics. Then what is the advantage of adding this term? Please comment. Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Reject. Lack of novelty. Proposed segmentation regularization is not well justified. Problematic data splitting in numerical experiments. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The authors proposed a novel image translation framework to bridge significant appearance gap between simulated images and real clinical scans, which has two contributions: constrain the generator with the accompanying semantic labels of simulated images by learning an auxiliary segmentation-to-real image translation task and apply a class-conditional generator, which in turn enables the incorporation of a cyclic loss. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -The proposed method is novel. Based on contrastive unpaired translation framework, the authors used the semantic labels of simulated images by learning an auxiliary segmentation-to-real image task to constrain the generator and extended a class-conditional generator to enable a cyclic loss. -The qualitative and quantitative metrics proved that this framework closed the appearance gap between simulated and real images, which had a significant influence on the US images generation. And a user study was performed to evaluate the realism of translated images by US experts. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -A more clear outline of the next steps in research would be appropriate. -There are fewer data sets and all act on fetal images. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The details described in the paper and the reproducibility are good. No code and data set are provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -A more clear outline of the next steps in research would be appropriate, some discussion but vague. There are already some new work based on CUT. -The author should try to experiment on a larger data set, and based on different types of images, which is more convincing, now it is only fetal images. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? -From the experimental results, the framework proposed by the authors can indeed bridge appearance gap between simulated images and real clinical scans to a certain extent. -The algorithm has a certain degree of innovation. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper takes a bag of losses to boost the image-to-image translation. Extensive experiments compared to SOTA methods on realistic datasets evaluated the effectiveness of the proposed method to improve the realism of computationally simulated US images. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper tries to use various losses in the image-to-image translation field to improve the realism of the generated images. According to the experimental results, the simulated images are much more realistic than other methods. The authors use typical evaluation metrics and invite experts to rate the qualities. The semantic-consistent regularization is well designed. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. 1 As claimed in the Abstract, the authors try to improve the quality of simulated US images, which can facilitate sonography training. The authors should evaluate how the generated images by the proposed method help sonography training, compared to the traditional methods. 2 More ablation studies and visualizations should be explored to show the effectiveness of the losses (loss of GAN, CLS, CYC, NCE) Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance As the dataset is private and the code is not going be released, I am not sure this work can be reproduced. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see the “Weakness” part. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I choose the “borderline accept” because of the great performance shown in the experimental results. However, due to the lack of reproducibility, I am not sure if this paper contributes more to society. On the other hand, It needs more experiments to evaluate the clinical effectiveness of those realistic simulated ultrasound images. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 2 Reviewer confidence Somewhat confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers. The rebuttal explains why the employed data splitting is justified: “the goal is not to reproduce the real data, but to abstract and apply the style of the real training data.” This somehow alleviate the issue, however, it is still questionable in my view. For example, a common use of image synthesis is through a subsequent application such as classification, segmentation, etc. In this regard, a strict data splitting is necessary. The rebuttal provides the results based on patient-wise splitting and it seems that similar performance improvements are reported. The rebuttal also provides more ablation studies. Overall, the paper meets the MICCAI acceptance level if the authors can make necessary changes per rebuttal. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors propose a method to simulate realistic ultrasound images, by taking the structure from input simulated images, and the appearance from real images. The method clearly work in translating the appearance to the generated images. Authors have addressed the methodological criticisms, although I am not sure they have correctly addressed the criticisms regarding novelty. Moreover, I am not sure they have addressed at all the issues related to clinical utility (for example raised by R3). I have concerns about the user survey too. Authors said that experts were asked to rank images by “their likelihood for being an image from this machine”. Even if the answer to this is highly positive, it fails to demonstrate that images look realistic, because they don’t. As pointed out by the authors themselves, “the simulated and the real images have substantially different anatomical contents”, i.e. the simulated fetal geometries are highly unrealistic. This has two implications: first, because of the lack of realism, the clinical utility is at best arguable (reason for which authors have not been able to rebut this point). Second, the challenge of obtaining realistic fetal geometries to generate realistic looking images might be greater than that of simulating realistic images. In their rebuttal, authors address one of the main criticisms (lack of novelty) by saying that “ our novelty is to enforce semantic consistency “, however this is precisely (as I pointed out) the main flaw. As a result, I cannot recommend acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 15 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The solution provided by the authors is interesting and the strongest arguments raised by the reviewers (novelty and data splitting) were properly explained and justified in the rebuttal phase. The rebuttal by the authors is clear, well argued and complete, and provides clarifications to the mayor points raised by the reviewers. The weakest point in the rebuttal is the justification and highlighting of the work novelty, yet novelty is detailed in the manuscript. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Author Feedback We thank the reviewers for their valuable feedback. Major comments are addressed below, and will be integrated shortly in the paper: ** Data splitting (R1) Other image analysis tasks such as classification and segmentation require evaluation on unseen patients to avoid bias. However, using train and/or test sets, even together, for computing GAN distribution metrics (FID/KID) is common practice [2,8, StyleGAN-Ada]. This is indeed more justified in an unpaired setting as in our appearance transfer task, where the goal is not to reproduce the real data, but to abstract and apply the style of the real training data. For computing FID/KID, we followed the approach from [8, StyleGAN-Ada] that introduced / used these metrics; i.e., we computed on the train set, since set distribution metrics require large samples for robust estimates. Indeed, adding the test set does not result in any difference (CUTS-C* KID: 0.24/0.24, FID 1.51/1.52 for train/all). The test set alone would be too small to compute these metrics robustly. Moreover, if there were bias from data splitting, this would also affect our baselines, against which our method is still seen to be superior. Indeed, larger models in our baselines might even profit more, if there were any such bias. That said, we have conducted an additional, patient-wise splitting experiment (6 patients for train). This yields a KID of 0.24, the same as earlier random data splitting. ** Limited real data variability (R1), More datasets (R3) For transferring appearance, large variability in real data is not necessary. Indeed, even a single image in the target domain can be sufficient under certain circumstances (cf. [16]). Here we used thousands of real images to capture the appearance of a specific US machine. Our goal here is task-specific, i.e., enhancing US simulation of the common 20-week fetal exams with real-machine like image appearance. Conceptually, there is no limitation in translating other US scenes (although a practical requirement would be the availability of detailed 3D models). ** Semantic-consistent regularization not well justified (R1) We assume a misunderstanding regarding the ablated models: CUTS is CUT with an auxiliary segmentation to real translation, but without the proposed semantic-consistent regularization. As seen in Fig.3 and pointed out by R1, learning this auxiliary task alone does not improve CUT. However, by adding the regularization term (CUTS-C), the translated images are semantically more close to simulated ones, c.f. Fig. 3 (CUT vs. CUTS-C). This is also corroborated with the better SSIM of CUTS-C, clearly demonstrating the benefit of our proposed regularization approach. As the namings may be confusing w.r.t. ablations, and we will rename our method. ** Limited Novelty (R1) For content preservation during appearance transfer, our novelty is to enforce semantic consistency with the proposed regularization during a multi-domain translation. Note that semantic regularization is not “replacing X by S in the contrastive loss”, but enforcing G to generate the same output for X and S. ** Future works (R3&amp;R4), Clinical experiments (R3) Future works include: clinical evaluation of the effects of translated images on US training; improving seg-to-real image translation to bypass any expensive rendering entirely; evaluations with other US/non-US scenes. ** More ablations for other losses (R4) The experiments in the submission demonstrate the benefits of all proposed components. For completeness, we report additional comparisons here (FID/KID/SSIM): proposed model (1.51/0.24/72.13), without NCE (2.22/0.43/64.77), without CYC (2.33/0.45/84.77). The results show that these components are indeed essential. Without CYC, the translated images look similar to the simulated ones. GAN loss is what promotes realism; and without CLS, the discriminator would have no info on domain-dependent classification [5], so these are essential. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0862/12/31/Paper1222" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0862/12/31/Paper1222" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0862-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0862/12/31/Paper1222"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0862/12/31/Paper1222","headline":"Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images","dateModified":"0863-01-05T00:00:00-05:17","datePublished":"0862-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Devavrat Tomar, Lin Zhang, Tiziano Portenier, Orcun Goksel Abstract Interactive simulation of ultrasound imaging greatly facilitates sonography training. Although ray-tracing based methods have shown promising results, obtaining realistic images requires substantial modeling effort and manual parameter tuning. In addition, current techniques still result in a significant appearance gap between simulated images and real clinical scans. Herein we introduce a novel content-preserving image translation framework (ConPres) to bridge this appearance gap, while maintaining the simulated anatomical layout. We achieve this goal by leveraging both simulated images with semantic segmentations and unpaired in-vivo ultrasound scans. Our framework is based on recent contrastive unpaired translation techniques and we propose a regularization approach by learning an auxiliary segmentation-to-real image translation task, which encourages the disentanglement of content and style. In addition, we extend the generator to be class-conditional, which enables the incorporation of additional losses, in particular a cyclic consistency loss, to further improve the translation quality. Qualitative and quantitative comparisons against state-of-the-art unpaired translation methods demonstrate the superiority of our proposed framework. Link to paper https://doi.org/10.1007/978-3-030-87237-3_63 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper describes an image translation framework synthesizing from simulated ultrasound images to realistic ones. The proposed network consists of a single generator and a single discriminator. The generator takes simulated image, its segmentation and an real image as input, computing contrastive loss and noise contrastive estimation loss which was proposed in previous work, sematic-consistent loss with the mask, and finally a cycle consistent loss using the same G twice in contrast to cycleGAN-like models. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper leverages recent advances of the domain adaptation work, such as contrastive loss for training, and kernel inception distance for evaluation which I found are interesting. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Unfortunately there are quite some and crucial weakness in this paper. First, lack of novelty. The only differences between this paper and starGAN are contrastive loss and semantic-consistent loss. The contrastive loss comes from Park, T. [16]. The semantic-consistent loss is just replacing X by S in the contrastive loss, since S is just another source image for generating images in Y’s domain. The overall novelty is very limited. Second, the experiment design is poor and even wrong. The data splitting is improper which causes a high risk of ‘training and testing on a same image’. Please see comments part for details. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Reproducing can be hard, mainly because the simulation algorithm details are not listed (either main or supplementary script). Without the details of simulated images, reproducing the work can be very challenging. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Overall this paper is well written and organized. Advantages of many recent proposed works have been incorporated. Besides the lack of novelty issue, my biggest concern is on the data splitting part in Section 3 Experiments. For real in-vivo images, the authors collected 22 ultrasound sequences from 8 patients, each of them is only several seconds long. However, the author puts all frames together and makes an 80%-10%-10% data splitting. First of all, it is not possible to make such splitting over 8 patients, so this splitting is really done over all frames. So there will be overlap in terms patients data between training/validation and testing. In addition, each sequence is only few seconds long. This means the variation in the data is very limited. Overlapping data, limited variation, this significantly weakens the claims of the experiments, especially for GAN related work. I don’t think the segmentation-consistency regularization term is well validated by the experiments. From Fig 3., comparing between CUT and CUTS, on the 4th column, the segmentation corresponding area is lost in CUTS. In Table 1, CUTS is consistently worse than CUT for all metrics. Then what is the advantage of adding this term? Please comment. Please state your overall opinion of the paper reject (3) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Reject. Lack of novelty. Proposed segmentation regularization is not well justified. Problematic data splitting in numerical experiments. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The authors proposed a novel image translation framework to bridge significant appearance gap between simulated images and real clinical scans, which has two contributions: constrain the generator with the accompanying semantic labels of simulated images by learning an auxiliary segmentation-to-real image translation task and apply a class-conditional generator, which in turn enables the incorporation of a cyclic loss. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -The proposed method is novel. Based on contrastive unpaired translation framework, the authors used the semantic labels of simulated images by learning an auxiliary segmentation-to-real image task to constrain the generator and extended a class-conditional generator to enable a cyclic loss. -The qualitative and quantitative metrics proved that this framework closed the appearance gap between simulated and real images, which had a significant influence on the US images generation. And a user study was performed to evaluate the realism of translated images by US experts. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. -A more clear outline of the next steps in research would be appropriate. -There are fewer data sets and all act on fetal images. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The details described in the paper and the reproducibility are good. No code and data set are provided. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html -A more clear outline of the next steps in research would be appropriate, some discussion but vague. There are already some new work based on CUT. -The author should try to experiment on a larger data set, and based on different types of images, which is more convincing, now it is only fetal images. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? -From the experimental results, the framework proposed by the authors can indeed bridge appearance gap between simulated images and real clinical scans to a certain extent. -The algorithm has a certain degree of innovation. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 4 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper This paper takes a bag of losses to boost the image-to-image translation. Extensive experiments compared to SOTA methods on realistic datasets evaluated the effectiveness of the proposed method to improve the realism of computationally simulated US images. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper tries to use various losses in the image-to-image translation field to improve the realism of the generated images. According to the experimental results, the simulated images are much more realistic than other methods. The authors use typical evaluation metrics and invite experts to rate the qualities. The semantic-consistent regularization is well designed. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. 1 As claimed in the Abstract, the authors try to improve the quality of simulated US images, which can facilitate sonography training. The authors should evaluate how the generated images by the proposed method help sonography training, compared to the traditional methods. 2 More ablation studies and visualizations should be explored to show the effectiveness of the losses (loss of GAN, CLS, CYC, NCE) Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance As the dataset is private and the code is not going be released, I am not sure this work can be reproduced. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please see the “Weakness” part. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I choose the “borderline accept” because of the great performance shown in the experimental results. However, due to the lack of reproducibility, I am not sure if this paper contributes more to society. On the other hand, It needs more experiments to evaluate the clinical effectiveness of those realistic simulated ultrasound images. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 2 Reviewer confidence Somewhat confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 6 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers. The rebuttal explains why the employed data splitting is justified: “the goal is not to reproduce the real data, but to abstract and apply the style of the real training data.” This somehow alleviate the issue, however, it is still questionable in my view. For example, a common use of image synthesis is through a subsequent application such as classification, segmentation, etc. In this regard, a strict data splitting is necessary. The rebuttal provides the results based on patient-wise splitting and it seems that similar performance improvements are reported. The rebuttal also provides more ablation studies. Overall, the paper meets the MICCAI acceptance level if the authors can make necessary changes per rebuttal. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 11 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors propose a method to simulate realistic ultrasound images, by taking the structure from input simulated images, and the appearance from real images. The method clearly work in translating the appearance to the generated images. Authors have addressed the methodological criticisms, although I am not sure they have correctly addressed the criticisms regarding novelty. Moreover, I am not sure they have addressed at all the issues related to clinical utility (for example raised by R3). I have concerns about the user survey too. Authors said that experts were asked to rank images by “their likelihood for being an image from this machine”. Even if the answer to this is highly positive, it fails to demonstrate that images look realistic, because they don’t. As pointed out by the authors themselves, “the simulated and the real images have substantially different anatomical contents”, i.e. the simulated fetal geometries are highly unrealistic. This has two implications: first, because of the lack of realism, the clinical utility is at best arguable (reason for which authors have not been able to rebut this point). Second, the challenge of obtaining realistic fetal geometries to generate realistic looking images might be greater than that of simulating realistic images. In their rebuttal, authors address one of the main criticisms (lack of novelty) by saying that “ our novelty is to enforce semantic consistency “, however this is precisely (as I pointed out) the main flaw. As a result, I cannot recommend acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Reject What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 15 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The solution provided by the authors is interesting and the strongest arguments raised by the reviewers (novelty and data splitting) were properly explained and justified in the rebuttal phase. The rebuttal by the authors is clear, well argued and complete, and provides clarifications to the mayor points raised by the reviewers. The weakest point in the rebuttal is the justification and highlighting of the work novelty, yet novelty is detailed in the manuscript. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 10 Author Feedback We thank the reviewers for their valuable feedback. Major comments are addressed below, and will be integrated shortly in the paper: ** Data splitting (R1) Other image analysis tasks such as classification and segmentation require evaluation on unseen patients to avoid bias. However, using train and/or test sets, even together, for computing GAN distribution metrics (FID/KID) is common practice [2,8, StyleGAN-Ada]. This is indeed more justified in an unpaired setting as in our appearance transfer task, where the goal is not to reproduce the real data, but to abstract and apply the style of the real training data. For computing FID/KID, we followed the approach from [8, StyleGAN-Ada] that introduced / used these metrics; i.e., we computed on the train set, since set distribution metrics require large samples for robust estimates. Indeed, adding the test set does not result in any difference (CUTS-C* KID: 0.24/0.24, FID 1.51/1.52 for train/all). The test set alone would be too small to compute these metrics robustly. Moreover, if there were bias from data splitting, this would also affect our baselines, against which our method is still seen to be superior. Indeed, larger models in our baselines might even profit more, if there were any such bias. That said, we have conducted an additional, patient-wise splitting experiment (6 patients for train). This yields a KID of 0.24, the same as earlier random data splitting. ** Limited real data variability (R1), More datasets (R3) For transferring appearance, large variability in real data is not necessary. Indeed, even a single image in the target domain can be sufficient under certain circumstances (cf. [16]). Here we used thousands of real images to capture the appearance of a specific US machine. Our goal here is task-specific, i.e., enhancing US simulation of the common 20-week fetal exams with real-machine like image appearance. Conceptually, there is no limitation in translating other US scenes (although a practical requirement would be the availability of detailed 3D models). ** Semantic-consistent regularization not well justified (R1) We assume a misunderstanding regarding the ablated models: CUTS is CUT with an auxiliary segmentation to real translation, but without the proposed semantic-consistent regularization. As seen in Fig.3 and pointed out by R1, learning this auxiliary task alone does not improve CUT. However, by adding the regularization term (CUTS-C), the translated images are semantically more close to simulated ones, c.f. Fig. 3 (CUT vs. CUTS-C). This is also corroborated with the better SSIM of CUTS-C, clearly demonstrating the benefit of our proposed regularization approach. As the namings may be confusing w.r.t. ablations, and we will rename our method. ** Limited Novelty (R1) For content preservation during appearance transfer, our novelty is to enforce semantic consistency with the proposed regularization during a multi-domain translation. Note that semantic regularization is not “replacing X by S in the contrastive loss”, but enforcing G to generate the same output for X and S. ** Future works (R3&amp;R4), Clinical experiments (R3) Future works include: clinical evaluation of the effects of translated images on US training; improving seg-to-real image translation to bypass any expensive rendering entirely; evaluations with other US/non-US scenes. ** More ablations for other losses (R4) The experiments in the submission demonstrate the benefits of all proposed components. For completeness, we report additional comparisons here (FID/KID/SSIM): proposed model (1.51/0.24/72.13), without NCE (2.22/0.43/64.77), without CYC (2.33/0.45/84.77). The results show that these components are indeed essential. Without CYC, the translated images look similar to the simulated ones. GAN loss is what promotes realism; and without CLS, the discriminator would have no info on domain-dependent classification [5], so these are essential. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Tomar, Devavrat,Zhang, Lin,Portenier, Tiziano,Goksel, Orcun" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Modalities - Ultrasound"
        class="post-category">
        Modalities - Ultrasound
      </a>
      
      <a 
        href="kittywong/categories#Surgical Planning and Simulation"
        class="post-category">
        Surgical Planning and Simulation
      </a>
      
      <a 
        href="kittywong/categories#Surgical Visualization and Mixed, Augmented and Virtual Reality"
        class="post-category">
        Surgical Visualization and Mixed, Augmented and Virtual Reality
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Tomar, Devavrat"
        class="post-tags">
        Tomar, Devavrat
      </a> |  
      
      <a href="kittywong/tags#Zhang, Lin"
        class="post-tags">
        Zhang, Lin
      </a> |  
      
      <a href="kittywong/tags#Portenier, Tiziano"
        class="post-tags">
        Portenier, Tiziano
      </a> |  
      
      <a href="kittywong/tags#Goksel, Orcun"
        class="post-tags">
        Goksel, Orcun
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Devavrat Tomar, Lin Zhang, Tiziano Portenier, Orcun Goksel
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Interactive simulation of ultrasound imaging greatly facilitates sonography training. Although ray-tracing based methods have shown promising results, obtaining realistic images requires substantial modeling effort and manual parameter tuning. In addition, current techniques still result in a significant appearance gap between simulated images and real clinical scans. Herein we introduce a novel content-preserving image translation framework (ConPres) to bridge this appearance gap, while maintaining the simulated anatomical layout. We achieve this goal by leveraging both simulated images with semantic segmentations and unpaired in-vivo ultrasound scans. Our framework is based on recent contrastive unpaired translation techniques and we propose a regularization approach by learning an auxiliary segmentation-to-real image translation task, which encourages the disentanglement of content and style. In addition, we extend the generator to be class-conditional, which enables the incorporation of additional losses, in particular a cyclic consistency loss, to further improve the translation quality. Qualitative and quantitative comparisons against state-of-the-art unpaired translation methods demonstrate the superiority of our proposed framework.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87237-3_63">https://doi.org/10.1007/978-3-030-87237-3_63</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper describes an image translation framework synthesizing from simulated ultrasound images to realistic ones. The proposed network consists of a single generator and a single discriminator. The generator takes simulated image, its segmentation and an real image as input, computing contrastive loss and noise contrastive estimation loss which was proposed in previous work, sematic-consistent loss with the mask, and finally a cycle consistent loss using the same G twice in contrast to cycleGAN-like models.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The paper leverages recent advances of the domain adaptation work, such as contrastive loss for training, and kernel inception distance for evaluation which I found are interesting.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>Unfortunately there are quite some and crucial weakness in this paper.
First,  lack of novelty. The only differences between this paper and starGAN are contrastive loss and semantic-consistent loss. The contrastive loss comes from Park, T. [16]. The semantic-consistent loss is just replacing X by S in the contrastive loss, since S is just another source image for generating images in Y’s domain. The overall novelty is very limited.
Second, the experiment design is poor and even wrong. The data splitting is improper which causes a high risk of ‘training and testing on a same image’. Please see comments part for details.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Reproducing can be hard, mainly because the simulation algorithm details are not listed (either main or supplementary script). Without the details of simulated images, reproducing the work can be very challenging.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Overall this paper is well written and organized. Advantages of many recent proposed works have been incorporated. Besides the lack of novelty issue, my biggest concern is on the data splitting part in Section 3 Experiments.</p>

      <ol>
        <li>
          <p>For real in-vivo images, the authors collected 22 ultrasound sequences from 8 patients, each of them is only several seconds long. However, the author puts all frames together and makes an 80%-10%-10% data splitting. First of all, it is not possible to make such splitting over 8 patients, so this splitting is really done over all frames.  So there will be overlap in terms patients data between training/validation and testing. In addition, each sequence is only few seconds long. This means the variation in the data is very limited. Overlapping data, limited variation, this significantly weakens the claims of the experiments, especially for GAN related work.</p>
        </li>
        <li>
          <p>I don’t think the segmentation-consistency regularization term is well validated by the experiments. From Fig 3., comparing between CUT and CUTS, on the 4th column, the segmentation corresponding area is lost in CUTS. In Table 1, CUTS is consistently worse than CUT for all metrics. Then what is the advantage of adding this term? Please comment.</p>
        </li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>reject (3)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Reject. Lack of novelty. Proposed segmentation regularization is not well justified. Problematic data splitting in numerical experiments.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors proposed a novel image translation framework to bridge significant appearance gap between simulated images and real clinical scans, which has two contributions: constrain the generator with the accompanying semantic labels of simulated images by learning an auxiliary segmentation-to-real image translation task and apply a class-conditional generator, which in turn enables the incorporation of a cyclic loss.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>-The proposed method is novel. Based on contrastive unpaired translation framework, the authors used the semantic labels of simulated images by learning an auxiliary segmentation-to-real image task to constrain the generator and extended a class-conditional generator to enable a cyclic loss.
-The qualitative and quantitative metrics proved that this framework closed the appearance gap between simulated and real images, which had a significant influence on the US images generation. And a user study was performed to evaluate the realism of translated images by US experts.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>-A more clear outline of the next steps in research would be appropriate.
-There are fewer data sets and all act on fetal images.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The details described in the paper and the reproducibility are good. No code and data set are provided.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>-A more clear outline of the next steps in research would be appropriate, some discussion but vague. There are already some new work based on CUT.
-The author should try to experiment on a larger data set, and based on different types of images, which is more convincing, now it is only fetal images.</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>-From the experimental results, the framework proposed by the authors can indeed bridge appearance gap between simulated images and real clinical scans to a certain extent.
-The algorithm has a certain degree of innovation.</p>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper takes a bag of losses to boost the image-to-image translation. Extensive experiments compared to SOTA methods on realistic datasets evaluated the effectiveness of the proposed method to improve the realism of computationally simulated US images.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>This paper tries to use various losses in the image-to-image translation field to improve the realism of the generated images. According to the experimental results, the simulated images are much more realistic than other methods. The authors use typical evaluation metrics and invite experts to rate the qualities. The semantic-consistent regularization is well designed.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>1 As claimed in the Abstract, the authors try to improve the quality of simulated US images, which can facilitate sonography training. The authors should evaluate how the generated images by the proposed method help sonography training, compared to the traditional methods.</p>

      <p>2 More ablation studies and visualizations should be explored to show the effectiveness of the losses (loss of GAN, CLS, CYC, NCE)</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>As the dataset is private and the code is not going be released, I am not sure this work can be reproduced.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Please see the “Weakness” part.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>I choose the “borderline accept” because of the great performance shown in the experimental results. However, due to the lack of reproducibility, I am not sure if this paper contributes more to society. On the other hand, It needs more experiments to evaluate the clinical effectiveness of those realistic simulated ultrasound images.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Somewhat confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The paper recevies mixed reviews from three experts. R1 has a legitimate concern regarding data splitting. Please provide a response to this as well as other issues raised by all reviewers.</p>

      <p>The rebuttal explains why the employed  data splitting is justified: “the goal is not to reproduce the real data, but to abstract and apply the style of the real training data.” This somehow alleviate the issue, however, it is still questionable in my view. For example, a common use of image synthesis is through a subsequent application such as classification, segmentation, etc. In this regard, a strict data splitting is necessary. The rebuttal provides the results based on patient-wise splitting and it seems that similar performance improvements are reported. The rebuttal also provides more ablation studies.</p>

      <p>Overall, the paper meets the MICCAI acceptance level if the authors can make necessary changes per rebuttal.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>11</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="meta-review-2">Meta-Review #2</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The authors propose a method to simulate realistic ultrasound images, by taking the structure from input simulated images, and the appearance from real images.  The method clearly work in translating the appearance to the generated images.
Authors have addressed the methodological criticisms, although I am not sure they have correctly addressed the criticisms regarding novelty.</p>

      <p>Moreover, I am not sure they have addressed at all the issues related to clinical utility (for example raised by R3).</p>

      <p>I have concerns about the user survey too. Authors said that experts were asked to rank images by “their likelihood for being an image from this machine”.  Even if the answer to this is highly positive, it fails to demonstrate that images look realistic, because they don’t. As pointed out by the authors themselves, “the simulated and the real images have substantially different anatomical contents”, i.e. the simulated fetal geometries are highly unrealistic. This has two implications: first, because of the lack of realism, the clinical utility is at best arguable (reason for which authors have not been able to rebut this point). Second, the challenge of obtaining realistic fetal geometries to generate realistic looking images might be greater than that of simulating realistic images. In their rebuttal, authors address one of the main criticisms (lack of novelty) by saying that “ our novelty is to enforce semantic consistency “, however this is precisely (as I pointed out) the main flaw. As a result, I cannot recommend acceptance.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Reject</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>15</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>
<h2 id="meta-review-3">Meta-Review #3</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The solution provided by the authors is interesting and the strongest arguments raised by the reviewers (novelty and data splitting) were properly explained and justified in the rebuttal phase. The rebuttal by the authors is clear, well argued and complete, and provides clarifications to the mayor points raised by the reviewers. The weakest point in the rebuttal is the justification and highlighting of the work novelty, yet novelty is detailed in the manuscript.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>10</p>
    </blockquote>
  </li>
</ul>

<h2><br /><br /></h2>
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We thank the reviewers for their valuable feedback. Major comments are addressed below, and will be integrated shortly in the paper:</p>

  <p>** Data splitting (R1)
Other image analysis tasks such as classification and segmentation require evaluation on unseen patients to avoid bias. However, using train and/or test sets, even together, for computing GAN distribution metrics (FID/KID) is common practice [2,8, StyleGAN-Ada]. This is indeed more justified in an <em>unpaired</em> setting as in our appearance transfer task, where the goal is not to reproduce the real data, but to abstract and apply the style of the real training data.</p>

  <p>For computing FID/KID, we followed the approach from [8, StyleGAN-Ada] that introduced / used these metrics; i.e., we computed on the train set, since set distribution metrics require large samples for robust estimates. Indeed, adding the test set does not result in any difference (CUTS-C* KID: 0.24/0.24, FID 1.51/1.52 for train/all). The test set alone would be too small to compute these metrics robustly.</p>

  <p>Moreover, if there were bias from data splitting, this would also affect our baselines, against which our method is still seen to be superior. Indeed, larger models in our baselines might even profit more, if there were any such bias.</p>

  <p>That said, we have conducted an additional, patient-wise splitting experiment (6 patients for train). This yields a KID of 0.24, the same as earlier random data splitting.</p>

  <p>** Limited real data variability (R1), More datasets (R3)
For transferring appearance, large variability in real data is not necessary. Indeed, even a single image in the target domain can be sufficient under certain circumstances (cf. [16]). Here we used thousands of real images to capture the appearance of a specific US machine.</p>

  <p>Our goal here is task-specific, i.e., enhancing US simulation of the common 20-week fetal exams with real-machine like image appearance. Conceptually, there is no limitation in translating other US scenes (although a practical requirement would be the availability of detailed 3D models).</p>

  <p>** Semantic-consistent regularization not well justified (R1)
We assume a misunderstanding regarding the ablated models: CUTS is CUT with an auxiliary segmentation to real translation, but without the proposed semantic-consistent regularization. As seen in Fig.3 and pointed out by R1, learning this auxiliary task alone does not improve CUT. However, by adding the regularization term (CUTS-C), the translated images are semantically more close to simulated ones, c.f. Fig. 3 (CUT vs. CUTS-C). This is also corroborated with the better SSIM of CUTS-C, clearly demonstrating the benefit of our proposed regularization approach. As the namings may be confusing w.r.t. ablations, and we will rename our method.</p>

  <p>** Limited Novelty (R1)
For content preservation during appearance transfer, our novelty is to enforce semantic consistency with the proposed regularization during a multi-domain translation. Note that semantic regularization is not “replacing X by S in the contrastive loss”, but enforcing G to generate the same output for X and S.</p>

  <p>** Future works (R3&amp;R4), Clinical experiments (R3)
Future works include: clinical evaluation of the effects of translated images on US training; improving seg-to-real image translation to bypass any expensive rendering entirely; evaluations with other US/non-US scenes.</p>

  <p>** More ablations for other losses (R4)
The experiments in the submission demonstrate the benefits of all proposed components. For completeness, we report additional comparisons here (FID/KID/SSIM): proposed model (1.51/0.24/72.13), without NCE (2.22/0.43/64.77), without CYC (2.33/0.45/84.77). The results show that these components are indeed essential. Without CYC, the translated images look similar to the simulated ones. GAN loss is what promotes realism; and without CLS, the discriminator would have no info on domain-dependent classification [5], so these are essential.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0862-12-31
      -->
      <!--
      
        ,
        updated at 
        0863-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Modalities - Ultrasound"
        class="post-category">
        Modalities - Ultrasound
      </a> |
      
      <a 
        href="kittywong/categories#Surgical Planning and Simulation"
        class="post-category">
        Surgical Planning and Simulation
      </a> |
      
      <a 
        href="kittywong/categories#Surgical Visualization and Mixed, Augmented and Virtual Reality"
        class="post-category">
        Surgical Visualization and Mixed, Augmented and Virtual Reality
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Tomar, Devavrat"
        class="post-category">
        Tomar, Devavrat
      </a> |  
      
      <a href="kittywong/tags#Zhang, Lin"
        class="post-category">
        Zhang, Lin
      </a> |  
      
      <a href="kittywong/tags#Portenier, Tiziano"
        class="post-category">
        Portenier, Tiziano
      </a> |  
      
      <a href="kittywong/tags#Goksel, Orcun"
        class="post-category">
        Goksel, Orcun
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0863/12/31/Paper1291">
          Visual-Assisted Probe Movement Guidance for Obstetric Ultrasound Scanning using Landmark Retrieval
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0861/12/31/Paper0403">
          Weakly-Supervised Ultrasound Video Segmentation with Minimal Annotations
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
