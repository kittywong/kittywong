<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Retina-Match: Ipsilateral Mammography Lesion Matching in a Single Shot Detection Pipeline | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Retina-Match: Ipsilateral Mammography Lesion Matching in a Single Shot Detection Pipeline" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Yinhao Ren, Jiafeng Lu, Zisheng Liang, Lars J. Grimm, Connie Kim, Michael Taylor-Cho, Sora Yoon, Jeffrey R. Marks, Joseph Y. Lo Abstract In mammography and tomosynthesis, radiologists use the geometric relationship of the four standard screening views to detect breast abnormalities. To date, computer aided detection methods focus on formulations based only on a single view. Recent multi-view methods are either black box approaches using methods such as relation blocks, or perform extensive, case-level feature aggregation requiring large data redundancy. In this study, we propose Retina-Match, an end-to-end trainable pipeline for detection, matching, and refinement that can effectively perform ipsilateral lesion matching in paired screening mammography images. We demonstrate effectiveness on a private, digital mammography data set with 1,016 biopsied lesions and 2,000 negative cases. Link to paper https://doi.org/10.1007/978-3-030-87240-3_33 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper proposes breast cancer detection in mammography including ipsilateral information. The detection is done in three steps, in the first RetinaTrack is used in each view to detect suspicious regions. Subsequently, candidates from both views are correlated by a Distance metric that has been learnt from ground-truth by a greedy-based approach. The third step aims to fuse the information of the candidates. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The proposal for abnormality detection includes ipsilateral information (CC and MLO views). This is an important aspect which actually mimicks the radiologist procedure. Using only one view, lesions can be ocluded or false positive may arise due to overlapping tissues. The proposal is evaluated using a large dataset of images (almost 3000 images in the training set). Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The use of the Distance metric is controversial. This step is replacing the typical CC-MLO registration using a learning-from-examples stratregy instead of using the own physics of the mammographic acquisition. A second weakness of the method is that it not used bilateral information, which is the first comparison commonly done by radiologists, even before the CC/MLO. Although the dataset used to evaluate the approach is large, the ratio of malignant cases over total cases is 1/10, which is far from the one in screening (around 1/1000). Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I think the reproducibility of the method is difficult. There are many ad-hoc parameters that are not fully explained probably due to the lack of space. Besides, they use a in-house database Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html This paper presents another CNN-based approach for breast cancer detection. It is based on the use of RetinaTrack to detect suspicious regions in MLO and CC views and a subsequent step joining the information. This step is divided into the candidate matching and the information fusion. I’m not sure section 4.2 &amp; 4.3 should be in Experiments, perhaps would be better placed at the end of Section 3 (i.e. section 3.2). Regarding the method itself, authors used the Distance metric as a way to avoid the physical correspondence in CC/MLO registration. This step limits interpretability of the approach and depends on having enough samples in the training set to interpret all the possible cases of mass locations. As already commented, my main concern is about the evaluation performed. Authors should add much more normal mammograms to be sure the number of FP doesn’t increment largely. Section 4.4 is comented without numbers. Seems strange. I don’t understand why authors show two FROCs, one for benign &amp; malignant and the second one for malignant only. Should be fairer to show benign only and malignant only, even the first one doesn’t favour the proposed approach. It would interesting to show the values of alpha and beta optimised by the net. Are similar to the one expected by the authors? The paper misses statistical analysis of the results (including the lack of the confidence interval of the FROCs). There is no comparison with the state of the art. How significant is the proposal of the authors with respect to other proposals? Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I tend to accept the paper due to the single end-to-end training that avoids intermediate steps. However, the experimental section should be improved to gauge the significance of the approach. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper introduced an ipsilateral lesion match step in single-stage detector pipeline to improve lesion detection performance in Mammography images. The method was inspired and adapted from Retina-Match, allowing for an end-to-end training. Experiments using a large, private, curated dataset was conducted to demonstrate the effectiveness of the proposed approach. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is well organized &amp; fairly well written. Clinical context (the use of all views in Mammography lesion detection by clinicians) was well introduced. The analysis of state-of-the-art was extensive. Ablation study &amp; comparison with state-of-the-art methods was thorough. Dataset used to demonstrate the performance is relative large &amp; of good quality compared to public datasets (DDSM, INBREAST …) Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. This paper lacks details in description of the architecture, in particular how the distance metric network connects with other components. Mathematical notations needs further clarification, especially explaining the indication of super/sub-script letters. Performance graphs lacks error-bars, making the statistical significance of the comparison unclear. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Dataset is private, with minimal description. No descriptions on patient demographics &amp; distribution per types of lesions. Training &amp; evaluation codes will be make public. Training details were mentioned in paper (hardware, software framework, key hyper-parameters). Training time &amp; methods to finetune the hyper-parameters were not detailed. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Chapter Introduction, 3rd paragraph “Nevertheless, this design assumes the latent features of each lesion candidate can affect all other candidates, breaking the clinical assumption of one-to-one matching.” Could the author add some references (clinical papers) to back this up? Section Lesion R-identification: please clarify some notions. y_match^k, f_CC^i, f_MLO^j, what do k, i and j refer to? Section Lesion Matching Logic FP-FP is also assigned positive label 1? Please clarify. What does p^i_match refer to? “The proposed lesion re nement loss is than computed…”: typo, than -&gt; then Equation (3): clarify what N and M refer to. Section 4.1 Dataset In biopsied soft-tissue lesion cases: only biopsied lesions were annotated. Also, BIRADS 2 patients may have non-biopsied benign lesions that are not annotated &amp; treated in this work. This was clarified later in section 4.6 Comparison with the Relation Block approach, it would be more clear for readers to understand if the author clarify here. Section 4.2 Network Architecture The description of architecture lacks details: specifically, how does the distance metric network connect to the 4 regressors and the greedy matching? This should be more clearly stated and illustrated in Figure 1. Section 4.5 Ablation Study, Figure 2 There is no error bar in this graph, making it difficult to compare the relative performance of different training set-ups. Also the sensitivity values in these two graphs do not seem to plateau yet, and seem to converge for different set-ups when the false positive rate becomes greater. It would be more convincing to show more sensitivity values with larger false positive rates, to compare the max sensitivity of different training set-ups. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Ipsilateral matching in Mammography is a clinically relevant topic. The proposed method was well-described in general. The experimental results, minus some minor details remain convincing. Seeing this type of research for 2D and 3D Mammography lesion detection is important for the research &amp; clinical community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper proposes an end-to-end network for lesion detection from paired CC-MLO mammography views. The network is capable of simultaneously performing three-tasks: lesion detection on each individual view, CC-MLO matching (via a Siamese network), and classification refinement by integrating information from the two views. Specifically, the scores of the matched bounding boxes are linearly combined, with weights that are not fixed, but predicted by the network and are specific for each anchor. Compared to existing architectures, such as MommiNet where all features embeddings from all detections are input to a Relation Network, the architecture assumes one-to-one matching between regions on the CC and MLO view, which is more similar to the typical radiologists’ workflow. Extensive experimental evaluation shows performance gain with respect to the traditional single-stream RetinaNet as well as other recently proposed dual-stream architectures based on Relation Networks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper proposes an architecture which is novel and original, and allows to effectively perform lesion detection on dual-view mammography. The output of the network is easily interpretable by the radiologists, as it closely resembles the workflow employed a human reader. The method does not require the two views to be registered Strong experimental validation is provided both against single-view detection, as well as against other competing methods, showing clear advantages in terms of performance. The paper is clearly written and easy to understand Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. RetinaMatch tackles two tasks at once: CC-MLO matching, and lesion detection. However, only the latter is experimentally evaluated. The effectiveness of matching, i.e., how often a lesion is actually matched to its corresponding ipsilateral view, is not evaluated. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper is not technically reproducible as the authors do not provide the code nor the dataset. The latter however, is very common in mammography since public datasets are rather small and/or based on screen-film mammography. The description of the method is quite clear, and should be relatively easy to reproduce. The following parameters are missing: Number and type of anchors Input image size Formulation of the regression loss Weights of the individual components of the loss Transformations used for data augmentation The authors should also clarify what they mean by lesion-level sensitivity. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The paper proposes a novel architecture for ipsilateral matching and lesion detection in mammography. Compared to previous architectures, such as MommiNet, the proposed architecture enables explicit one-to-one matching between lesion view, which directly mimics the radiologist’s workflow and provides an interpretable output to the radiologist. The other advantage of the proposed architecture is that it focuses the attention on local features, whereas other architectures (e.g., relational block) are prone to focus on global features. I agree with the authors that local features are essentially in discriminating masses from false positives due to tissue superposition. I do not fully agree that global features do not capture useful information; global aspects, e.g., related to breast density, or to the presence of diffuse calcifications, may impact the final classification. The paper is generally well written, and the experimental section well curated. Some points could be expanded or clarified, perhaps relying also on supplementary material given the space constraints: In the related work, the sentence ” GCNet [2] showed that general non-local blocks often degrade into a global feature extractor.” Should be further clarified and put in the specific context of mammography In Section 3, does the distance function D takes into account the position, as well as the embedding? Why are the embeddings explicitly regressed, instead of being extracted from the classification network or the backbone? What is the advantage of this additional head? In Section 4.1, the authors mention the application of a pectoral muscle segmentation model for the MLO view, and that a 2D breast depth encoding model is constructed. However, it is not clear how this information is used in in the architecture. I suggest adding it to Figure 1. I understand the advantage of regressing parameter alpha and beta, instead of fixing them manually. However, I wonder what is the specific advantage of having anchor-specific parameters, with respect to constant values optimized over the entire dataset. It is not clear in Table 1, what is the difference between case, view and annotation - especially view and annotation. What are considered as lesions in calculating the sensitivity? Please clarify and use a consistent notation across the paper All models should be trained for an equal number of epochs. From the description of the methodology, it appears as the single-view RetinaNet is trained from 25 epochs, and then RetinaMatch is further fine-tuned for 50 epochs. The single-view model should correspondingly be further fine-tuned for 25 epochs. In future works, it would be interesting to compare the convergence properties of different models models, i.e., whether RetinaMatch converges faster or slower than RetinaNet. A few examples that illustrate and compare the detections by different models would be nice. The probability of each candidate lesion on each view is modified based on the probability of the corresponding matched lesion candidate on the opposite view. How is the final lesion probability calculated to calculate the per-lesion probability? The differences between the models are subtle, an analysis of their statistical significance should be included Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This is an interesting paper with well-conducted experiments, with proper baseline and comparisons with other methods. The proposed method is not a trivial extension of existing models and has practical advantages (in terms of simplicity and interpretability) compared with existing methods. The paper is well written and easy to understand. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposes a lesion detector and classifier by analysing CC and MLO views. All reviewers acknowledged the innovation of the method and the solid experimental results, so I am recommending the provisional acceptance of the paper. There are a few points that need to be addressed:  1) can the authors assess the CC-MLO matching, by checking how often a lesion is actually matched to its corresponding ipsilateral view?; 2) the reviewers requested a few methodological clarifications; and 3) can the paper add statistical significance tests to the results? What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We really appreciate the reviewers for your constructive feedback. We have corrected the indicated language errors and made a few cleanups for the mathematical notations and model architecture description for the camera-ready version. Here we would like to respond to a few other points been raised: Regarding the lesion matching performance, we derived our final formulation from a patch-based matching model, where this evaluation is available. The patch level exhaustive pair vs not-pair AUC is around 0.96 when we conduct the matching operation at 3 FP per image. We will include some more detailed matching evaluations in the final camera-ready version. Regarding Lesion Matching Logic, we treat TP-TP pairs as positive and TP-FP pairs as negative. We believe lesion level matching performs better than full mammogram registration (can be supervised and less overhead), and we will conduct more comparisons in the future to prove this point. For position encoding, we used the pectoral muscle segmentation to construct the nipple distance of each lesion. Regarding the statistical significance for this study, we made the assumption that our dataset is relatively large thus decided to not include a cross-validation study that could yield a confidence interval for the FROC. We will pay more attention in our future studies to add statistical significance tests. Thanks again for all the above throughout reviews. We will definitely incorporate those comments into the design of our future studies. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Yinhao Ren, Jiafeng Lu, Zisheng Liang, Lars J. Grimm, Connie Kim, Michael Taylor-Cho, Sora Yoon, Jeffrey R. Marks, Joseph Y. Lo Abstract In mammography and tomosynthesis, radiologists use the geometric relationship of the four standard screening views to detect breast abnormalities. To date, computer aided detection methods focus on formulations based only on a single view. Recent multi-view methods are either black box approaches using methods such as relation blocks, or perform extensive, case-level feature aggregation requiring large data redundancy. In this study, we propose Retina-Match, an end-to-end trainable pipeline for detection, matching, and refinement that can effectively perform ipsilateral lesion matching in paired screening mammography images. We demonstrate effectiveness on a private, digital mammography data set with 1,016 biopsied lesions and 2,000 negative cases. Link to paper https://doi.org/10.1007/978-3-030-87240-3_33 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper proposes breast cancer detection in mammography including ipsilateral information. The detection is done in three steps, in the first RetinaTrack is used in each view to detect suspicious regions. Subsequently, candidates from both views are correlated by a Distance metric that has been learnt from ground-truth by a greedy-based approach. The third step aims to fuse the information of the candidates. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The proposal for abnormality detection includes ipsilateral information (CC and MLO views). This is an important aspect which actually mimicks the radiologist procedure. Using only one view, lesions can be ocluded or false positive may arise due to overlapping tissues. The proposal is evaluated using a large dataset of images (almost 3000 images in the training set). Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The use of the Distance metric is controversial. This step is replacing the typical CC-MLO registration using a learning-from-examples stratregy instead of using the own physics of the mammographic acquisition. A second weakness of the method is that it not used bilateral information, which is the first comparison commonly done by radiologists, even before the CC/MLO. Although the dataset used to evaluate the approach is large, the ratio of malignant cases over total cases is 1/10, which is far from the one in screening (around 1/1000). Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I think the reproducibility of the method is difficult. There are many ad-hoc parameters that are not fully explained probably due to the lack of space. Besides, they use a in-house database Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html This paper presents another CNN-based approach for breast cancer detection. It is based on the use of RetinaTrack to detect suspicious regions in MLO and CC views and a subsequent step joining the information. This step is divided into the candidate matching and the information fusion. I’m not sure section 4.2 &amp; 4.3 should be in Experiments, perhaps would be better placed at the end of Section 3 (i.e. section 3.2). Regarding the method itself, authors used the Distance metric as a way to avoid the physical correspondence in CC/MLO registration. This step limits interpretability of the approach and depends on having enough samples in the training set to interpret all the possible cases of mass locations. As already commented, my main concern is about the evaluation performed. Authors should add much more normal mammograms to be sure the number of FP doesn’t increment largely. Section 4.4 is comented without numbers. Seems strange. I don’t understand why authors show two FROCs, one for benign &amp; malignant and the second one for malignant only. Should be fairer to show benign only and malignant only, even the first one doesn’t favour the proposed approach. It would interesting to show the values of alpha and beta optimised by the net. Are similar to the one expected by the authors? The paper misses statistical analysis of the results (including the lack of the confidence interval of the FROCs). There is no comparison with the state of the art. How significant is the proposal of the authors with respect to other proposals? Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I tend to accept the paper due to the single end-to-end training that avoids intermediate steps. However, the experimental section should be improved to gauge the significance of the approach. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper introduced an ipsilateral lesion match step in single-stage detector pipeline to improve lesion detection performance in Mammography images. The method was inspired and adapted from Retina-Match, allowing for an end-to-end training. Experiments using a large, private, curated dataset was conducted to demonstrate the effectiveness of the proposed approach. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is well organized &amp; fairly well written. Clinical context (the use of all views in Mammography lesion detection by clinicians) was well introduced. The analysis of state-of-the-art was extensive. Ablation study &amp; comparison with state-of-the-art methods was thorough. Dataset used to demonstrate the performance is relative large &amp; of good quality compared to public datasets (DDSM, INBREAST …) Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. This paper lacks details in description of the architecture, in particular how the distance metric network connects with other components. Mathematical notations needs further clarification, especially explaining the indication of super/sub-script letters. Performance graphs lacks error-bars, making the statistical significance of the comparison unclear. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Dataset is private, with minimal description. No descriptions on patient demographics &amp; distribution per types of lesions. Training &amp; evaluation codes will be make public. Training details were mentioned in paper (hardware, software framework, key hyper-parameters). Training time &amp; methods to finetune the hyper-parameters were not detailed. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Chapter Introduction, 3rd paragraph “Nevertheless, this design assumes the latent features of each lesion candidate can affect all other candidates, breaking the clinical assumption of one-to-one matching.” Could the author add some references (clinical papers) to back this up? Section Lesion R-identification: please clarify some notions. y_match^k, f_CC^i, f_MLO^j, what do k, i and j refer to? Section Lesion Matching Logic FP-FP is also assigned positive label 1? Please clarify. What does p^i_match refer to? “The proposed lesion re nement loss is than computed…”: typo, than -&gt; then Equation (3): clarify what N and M refer to. Section 4.1 Dataset In biopsied soft-tissue lesion cases: only biopsied lesions were annotated. Also, BIRADS 2 patients may have non-biopsied benign lesions that are not annotated &amp; treated in this work. This was clarified later in section 4.6 Comparison with the Relation Block approach, it would be more clear for readers to understand if the author clarify here. Section 4.2 Network Architecture The description of architecture lacks details: specifically, how does the distance metric network connect to the 4 regressors and the greedy matching? This should be more clearly stated and illustrated in Figure 1. Section 4.5 Ablation Study, Figure 2 There is no error bar in this graph, making it difficult to compare the relative performance of different training set-ups. Also the sensitivity values in these two graphs do not seem to plateau yet, and seem to converge for different set-ups when the false positive rate becomes greater. It would be more convincing to show more sensitivity values with larger false positive rates, to compare the max sensitivity of different training set-ups. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Ipsilateral matching in Mammography is a clinically relevant topic. The proposed method was well-described in general. The experimental results, minus some minor details remain convincing. Seeing this type of research for 2D and 3D Mammography lesion detection is important for the research &amp; clinical community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper proposes an end-to-end network for lesion detection from paired CC-MLO mammography views. The network is capable of simultaneously performing three-tasks: lesion detection on each individual view, CC-MLO matching (via a Siamese network), and classification refinement by integrating information from the two views. Specifically, the scores of the matched bounding boxes are linearly combined, with weights that are not fixed, but predicted by the network and are specific for each anchor. Compared to existing architectures, such as MommiNet where all features embeddings from all detections are input to a Relation Network, the architecture assumes one-to-one matching between regions on the CC and MLO view, which is more similar to the typical radiologists’ workflow. Extensive experimental evaluation shows performance gain with respect to the traditional single-stream RetinaNet as well as other recently proposed dual-stream architectures based on Relation Networks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper proposes an architecture which is novel and original, and allows to effectively perform lesion detection on dual-view mammography. The output of the network is easily interpretable by the radiologists, as it closely resembles the workflow employed a human reader. The method does not require the two views to be registered Strong experimental validation is provided both against single-view detection, as well as against other competing methods, showing clear advantages in terms of performance. The paper is clearly written and easy to understand Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. RetinaMatch tackles two tasks at once: CC-MLO matching, and lesion detection. However, only the latter is experimentally evaluated. The effectiveness of matching, i.e., how often a lesion is actually matched to its corresponding ipsilateral view, is not evaluated. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper is not technically reproducible as the authors do not provide the code nor the dataset. The latter however, is very common in mammography since public datasets are rather small and/or based on screen-film mammography. The description of the method is quite clear, and should be relatively easy to reproduce. The following parameters are missing: Number and type of anchors Input image size Formulation of the regression loss Weights of the individual components of the loss Transformations used for data augmentation The authors should also clarify what they mean by lesion-level sensitivity. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The paper proposes a novel architecture for ipsilateral matching and lesion detection in mammography. Compared to previous architectures, such as MommiNet, the proposed architecture enables explicit one-to-one matching between lesion view, which directly mimics the radiologist’s workflow and provides an interpretable output to the radiologist. The other advantage of the proposed architecture is that it focuses the attention on local features, whereas other architectures (e.g., relational block) are prone to focus on global features. I agree with the authors that local features are essentially in discriminating masses from false positives due to tissue superposition. I do not fully agree that global features do not capture useful information; global aspects, e.g., related to breast density, or to the presence of diffuse calcifications, may impact the final classification. The paper is generally well written, and the experimental section well curated. Some points could be expanded or clarified, perhaps relying also on supplementary material given the space constraints: In the related work, the sentence ” GCNet [2] showed that general non-local blocks often degrade into a global feature extractor.” Should be further clarified and put in the specific context of mammography In Section 3, does the distance function D takes into account the position, as well as the embedding? Why are the embeddings explicitly regressed, instead of being extracted from the classification network or the backbone? What is the advantage of this additional head? In Section 4.1, the authors mention the application of a pectoral muscle segmentation model for the MLO view, and that a 2D breast depth encoding model is constructed. However, it is not clear how this information is used in in the architecture. I suggest adding it to Figure 1. I understand the advantage of regressing parameter alpha and beta, instead of fixing them manually. However, I wonder what is the specific advantage of having anchor-specific parameters, with respect to constant values optimized over the entire dataset. It is not clear in Table 1, what is the difference between case, view and annotation - especially view and annotation. What are considered as lesions in calculating the sensitivity? Please clarify and use a consistent notation across the paper All models should be trained for an equal number of epochs. From the description of the methodology, it appears as the single-view RetinaNet is trained from 25 epochs, and then RetinaMatch is further fine-tuned for 50 epochs. The single-view model should correspondingly be further fine-tuned for 25 epochs. In future works, it would be interesting to compare the convergence properties of different models models, i.e., whether RetinaMatch converges faster or slower than RetinaNet. A few examples that illustrate and compare the detections by different models would be nice. The probability of each candidate lesion on each view is modified based on the probability of the corresponding matched lesion candidate on the opposite view. How is the final lesion probability calculated to calculate the per-lesion probability? The differences between the models are subtle, an analysis of their statistical significance should be included Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This is an interesting paper with well-conducted experiments, with proper baseline and comparisons with other methods. The proposed method is not a trivial extension of existing models and has practical advantages (in terms of simplicity and interpretability) compared with existing methods. The paper is well written and easy to understand. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposes a lesion detector and classifier by analysing CC and MLO views. All reviewers acknowledged the innovation of the method and the solid experimental results, so I am recommending the provisional acceptance of the paper. There are a few points that need to be addressed:  1) can the authors assess the CC-MLO matching, by checking how often a lesion is actually matched to its corresponding ipsilateral view?; 2) the reviewers requested a few methodological clarifications; and 3) can the paper add statistical significance tests to the results? What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We really appreciate the reviewers for your constructive feedback. We have corrected the indicated language errors and made a few cleanups for the mathematical notations and model architecture description for the camera-ready version. Here we would like to respond to a few other points been raised: Regarding the lesion matching performance, we derived our final formulation from a patch-based matching model, where this evaluation is available. The patch level exhaustive pair vs not-pair AUC is around 0.96 when we conduct the matching operation at 3 FP per image. We will include some more detailed matching evaluations in the final camera-ready version. Regarding Lesion Matching Logic, we treat TP-TP pairs as positive and TP-FP pairs as negative. We believe lesion level matching performs better than full mammogram registration (can be supervised and less overhead), and we will conduct more comparisons in the future to prove this point. For position encoding, we used the pectoral muscle segmentation to construct the nipple distance of each lesion. Regarding the statistical significance for this study, we made the assumption that our dataset is relatively large thus decided to not include a cross-validation study that could yield a confidence interval for the FROC. We will pay more attention in our future studies to add statistical significance tests. Thanks again for all the above throughout reviews. We will definitely incorporate those comments into the design of our future studies. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0532/12/31/Paper1023" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0532/12/31/Paper1023" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0532-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Retina-Match: Ipsilateral Mammography Lesion Matching in a Single Shot Detection Pipeline" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0532/12/31/Paper1023"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0532/12/31/Paper1023","headline":"Retina-Match: Ipsilateral Mammography Lesion Matching in a Single Shot Detection Pipeline","dateModified":"0533-01-03T00:00:00-05:17","datePublished":"0532-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Yinhao Ren, Jiafeng Lu, Zisheng Liang, Lars J. Grimm, Connie Kim, Michael Taylor-Cho, Sora Yoon, Jeffrey R. Marks, Joseph Y. Lo Abstract In mammography and tomosynthesis, radiologists use the geometric relationship of the four standard screening views to detect breast abnormalities. To date, computer aided detection methods focus on formulations based only on a single view. Recent multi-view methods are either black box approaches using methods such as relation blocks, or perform extensive, case-level feature aggregation requiring large data redundancy. In this study, we propose Retina-Match, an end-to-end trainable pipeline for detection, matching, and refinement that can effectively perform ipsilateral lesion matching in paired screening mammography images. We demonstrate effectiveness on a private, digital mammography data set with 1,016 biopsied lesions and 2,000 negative cases. Link to paper https://doi.org/10.1007/978-3-030-87240-3_33 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper This paper proposes breast cancer detection in mammography including ipsilateral information. The detection is done in three steps, in the first RetinaTrack is used in each view to detect suspicious regions. Subsequently, candidates from both views are correlated by a Distance metric that has been learnt from ground-truth by a greedy-based approach. The third step aims to fuse the information of the candidates. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The proposal for abnormality detection includes ipsilateral information (CC and MLO views). This is an important aspect which actually mimicks the radiologist procedure. Using only one view, lesions can be ocluded or false positive may arise due to overlapping tissues. The proposal is evaluated using a large dataset of images (almost 3000 images in the training set). Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The use of the Distance metric is controversial. This step is replacing the typical CC-MLO registration using a learning-from-examples stratregy instead of using the own physics of the mammographic acquisition. A second weakness of the method is that it not used bilateral information, which is the first comparison commonly done by radiologists, even before the CC/MLO. Although the dataset used to evaluate the approach is large, the ratio of malignant cases over total cases is 1/10, which is far from the one in screening (around 1/1000). Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I think the reproducibility of the method is difficult. There are many ad-hoc parameters that are not fully explained probably due to the lack of space. Besides, they use a in-house database Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html This paper presents another CNN-based approach for breast cancer detection. It is based on the use of RetinaTrack to detect suspicious regions in MLO and CC views and a subsequent step joining the information. This step is divided into the candidate matching and the information fusion. I’m not sure section 4.2 &amp; 4.3 should be in Experiments, perhaps would be better placed at the end of Section 3 (i.e. section 3.2). Regarding the method itself, authors used the Distance metric as a way to avoid the physical correspondence in CC/MLO registration. This step limits interpretability of the approach and depends on having enough samples in the training set to interpret all the possible cases of mass locations. As already commented, my main concern is about the evaluation performed. Authors should add much more normal mammograms to be sure the number of FP doesn’t increment largely. Section 4.4 is comented without numbers. Seems strange. I don’t understand why authors show two FROCs, one for benign &amp; malignant and the second one for malignant only. Should be fairer to show benign only and malignant only, even the first one doesn’t favour the proposed approach. It would interesting to show the values of alpha and beta optimised by the net. Are similar to the one expected by the authors? The paper misses statistical analysis of the results (including the lack of the confidence interval of the FROCs). There is no comparison with the state of the art. How significant is the proposal of the authors with respect to other proposals? Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? I tend to accept the paper due to the single end-to-end training that avoids intermediate steps. However, the experimental section should be improved to gauge the significance of the approach. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper introduced an ipsilateral lesion match step in single-stage detector pipeline to improve lesion detection performance in Mammography images. The method was inspired and adapted from Retina-Match, allowing for an end-to-end training. Experiments using a large, private, curated dataset was conducted to demonstrate the effectiveness of the proposed approach. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper is well organized &amp; fairly well written. Clinical context (the use of all views in Mammography lesion detection by clinicians) was well introduced. The analysis of state-of-the-art was extensive. Ablation study &amp; comparison with state-of-the-art methods was thorough. Dataset used to demonstrate the performance is relative large &amp; of good quality compared to public datasets (DDSM, INBREAST …) Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. This paper lacks details in description of the architecture, in particular how the distance metric network connects with other components. Mathematical notations needs further clarification, especially explaining the indication of super/sub-script letters. Performance graphs lacks error-bars, making the statistical significance of the comparison unclear. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Dataset is private, with minimal description. No descriptions on patient demographics &amp; distribution per types of lesions. Training &amp; evaluation codes will be make public. Training details were mentioned in paper (hardware, software framework, key hyper-parameters). Training time &amp; methods to finetune the hyper-parameters were not detailed. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Chapter Introduction, 3rd paragraph “Nevertheless, this design assumes the latent features of each lesion candidate can affect all other candidates, breaking the clinical assumption of one-to-one matching.” Could the author add some references (clinical papers) to back this up? Section Lesion R-identification: please clarify some notions. y_match^k, f_CC^i, f_MLO^j, what do k, i and j refer to? Section Lesion Matching Logic FP-FP is also assigned positive label 1? Please clarify. What does p^i_match refer to? “The proposed lesion re nement loss is than computed…”: typo, than -&gt; then Equation (3): clarify what N and M refer to. Section 4.1 Dataset In biopsied soft-tissue lesion cases: only biopsied lesions were annotated. Also, BIRADS 2 patients may have non-biopsied benign lesions that are not annotated &amp; treated in this work. This was clarified later in section 4.6 Comparison with the Relation Block approach, it would be more clear for readers to understand if the author clarify here. Section 4.2 Network Architecture The description of architecture lacks details: specifically, how does the distance metric network connect to the 4 regressors and the greedy matching? This should be more clearly stated and illustrated in Figure 1. Section 4.5 Ablation Study, Figure 2 There is no error bar in this graph, making it difficult to compare the relative performance of different training set-ups. Also the sensitivity values in these two graphs do not seem to plateau yet, and seem to converge for different set-ups when the false positive rate becomes greater. It would be more convincing to show more sensitivity values with larger false positive rates, to compare the max sensitivity of different training set-ups. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Ipsilateral matching in Mammography is a clinically relevant topic. The proposed method was well-described in general. The experimental results, minus some minor details remain convincing. Seeing this type of research for 2D and 3D Mammography lesion detection is important for the research &amp; clinical community. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper proposes an end-to-end network for lesion detection from paired CC-MLO mammography views. The network is capable of simultaneously performing three-tasks: lesion detection on each individual view, CC-MLO matching (via a Siamese network), and classification refinement by integrating information from the two views. Specifically, the scores of the matched bounding boxes are linearly combined, with weights that are not fixed, but predicted by the network and are specific for each anchor. Compared to existing architectures, such as MommiNet where all features embeddings from all detections are input to a Relation Network, the architecture assumes one-to-one matching between regions on the CC and MLO view, which is more similar to the typical radiologists’ workflow. Extensive experimental evaluation shows performance gain with respect to the traditional single-stream RetinaNet as well as other recently proposed dual-stream architectures based on Relation Networks. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper proposes an architecture which is novel and original, and allows to effectively perform lesion detection on dual-view mammography. The output of the network is easily interpretable by the radiologists, as it closely resembles the workflow employed a human reader. The method does not require the two views to be registered Strong experimental validation is provided both against single-view detection, as well as against other competing methods, showing clear advantages in terms of performance. The paper is clearly written and easy to understand Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. RetinaMatch tackles two tasks at once: CC-MLO matching, and lesion detection. However, only the latter is experimentally evaluated. The effectiveness of matching, i.e., how often a lesion is actually matched to its corresponding ipsilateral view, is not evaluated. Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The paper is not technically reproducible as the authors do not provide the code nor the dataset. The latter however, is very common in mammography since public datasets are rather small and/or based on screen-film mammography. The description of the method is quite clear, and should be relatively easy to reproduce. The following parameters are missing: Number and type of anchors Input image size Formulation of the regression loss Weights of the individual components of the loss Transformations used for data augmentation The authors should also clarify what they mean by lesion-level sensitivity. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The paper proposes a novel architecture for ipsilateral matching and lesion detection in mammography. Compared to previous architectures, such as MommiNet, the proposed architecture enables explicit one-to-one matching between lesion view, which directly mimics the radiologist’s workflow and provides an interpretable output to the radiologist. The other advantage of the proposed architecture is that it focuses the attention on local features, whereas other architectures (e.g., relational block) are prone to focus on global features. I agree with the authors that local features are essentially in discriminating masses from false positives due to tissue superposition. I do not fully agree that global features do not capture useful information; global aspects, e.g., related to breast density, or to the presence of diffuse calcifications, may impact the final classification. The paper is generally well written, and the experimental section well curated. Some points could be expanded or clarified, perhaps relying also on supplementary material given the space constraints: In the related work, the sentence ” GCNet [2] showed that general non-local blocks often degrade into a global feature extractor.” Should be further clarified and put in the specific context of mammography In Section 3, does the distance function D takes into account the position, as well as the embedding? Why are the embeddings explicitly regressed, instead of being extracted from the classification network or the backbone? What is the advantage of this additional head? In Section 4.1, the authors mention the application of a pectoral muscle segmentation model for the MLO view, and that a 2D breast depth encoding model is constructed. However, it is not clear how this information is used in in the architecture. I suggest adding it to Figure 1. I understand the advantage of regressing parameter alpha and beta, instead of fixing them manually. However, I wonder what is the specific advantage of having anchor-specific parameters, with respect to constant values optimized over the entire dataset. It is not clear in Table 1, what is the difference between case, view and annotation - especially view and annotation. What are considered as lesions in calculating the sensitivity? Please clarify and use a consistent notation across the paper All models should be trained for an equal number of epochs. From the description of the methodology, it appears as the single-view RetinaNet is trained from 25 epochs, and then RetinaMatch is further fine-tuned for 50 epochs. The single-view model should correspondingly be further fine-tuned for 25 epochs. In future works, it would be interesting to compare the convergence properties of different models models, i.e., whether RetinaMatch converges faster or slower than RetinaNet. A few examples that illustrate and compare the detections by different models would be nice. The probability of each candidate lesion on each view is modified based on the probability of the corresponding matched lesion candidate on the opposite view. How is the final lesion probability calculated to calculate the per-lesion probability? The differences between the models are subtle, an analysis of their statistical significance should be included Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This is an interesting paper with well-conducted experiments, with proper baseline and comparisons with other methods. The proposed method is not a trivial extension of existing models and has practical advantages (in terms of simplicity and interpretability) compared with existing methods. The paper is well written and easy to understand. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 6 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper proposes a lesion detector and classifier by analysing CC and MLO views. All reviewers acknowledged the innovation of the method and the solid experimental results, so I am recommending the provisional acceptance of the paper. There are a few points that need to be addressed:  1) can the authors assess the CC-MLO matching, by checking how often a lesion is actually matched to its corresponding ipsilateral view?; 2) the reviewers requested a few methodological clarifications; and 3) can the paper add statistical significance tests to the results? What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We really appreciate the reviewers for your constructive feedback. We have corrected the indicated language errors and made a few cleanups for the mathematical notations and model architecture description for the camera-ready version. Here we would like to respond to a few other points been raised: Regarding the lesion matching performance, we derived our final formulation from a patch-based matching model, where this evaluation is available. The patch level exhaustive pair vs not-pair AUC is around 0.96 when we conduct the matching operation at 3 FP per image. We will include some more detailed matching evaluations in the final camera-ready version. Regarding Lesion Matching Logic, we treat TP-TP pairs as positive and TP-FP pairs as negative. We believe lesion level matching performs better than full mammogram registration (can be supervised and less overhead), and we will conduct more comparisons in the future to prove this point. For position encoding, we used the pectoral muscle segmentation to construct the nipple distance of each lesion. Regarding the statistical significance for this study, we made the assumption that our dataset is relatively large thus decided to not include a cross-validation study that could yield a confidence interval for the FROC. We will pay more attention in our future studies to add statistical significance tests. Thanks again for all the above throughout reviews. We will definitely incorporate those comments into the design of our future studies. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Ren, Yinhao,Lu, Jiafeng,Liang, Zisheng,Grimm, Lars J.,Kim, Connie,Taylor-Cho, Michael,Yoon, Sora,Marks, Jeffrey R.,Lo, Joseph Y." />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Retina-Match: Ipsilateral Mammography Lesion Matching in a Single Shot Detection Pipeline</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Breast"
        class="post-category">
        Clinical applications - Breast
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Ren, Yinhao"
        class="post-tags">
        Ren, Yinhao
      </a> |  
      
      <a href="kittywong/tags#Lu, Jiafeng"
        class="post-tags">
        Lu, Jiafeng
      </a> |  
      
      <a href="kittywong/tags#Liang, Zisheng"
        class="post-tags">
        Liang, Zisheng
      </a> |  
      
      <a href="kittywong/tags#Grimm, Lars J."
        class="post-tags">
        Grimm, Lars J.
      </a> |  
      
      <a href="kittywong/tags#Kim, Connie"
        class="post-tags">
        Kim, Connie
      </a> |  
      
      <a href="kittywong/tags#Taylor-Cho, Michael"
        class="post-tags">
        Taylor-Cho, Michael
      </a> |  
      
      <a href="kittywong/tags#Yoon, Sora"
        class="post-tags">
        Yoon, Sora
      </a> |  
      
      <a href="kittywong/tags#Marks, Jeffrey R."
        class="post-tags">
        Marks, Jeffrey R.
      </a> |  
      
      <a href="kittywong/tags#Lo, Joseph Y."
        class="post-tags">
        Lo, Joseph Y.
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Yinhao Ren, Jiafeng Lu, Zisheng Liang, Lars J. Grimm, Connie Kim, Michael Taylor-Cho, Sora Yoon, Jeffrey R. Marks, Joseph Y. Lo
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>In mammography and tomosynthesis, radiologists use the geometric relationship of the four standard screening views to detect breast abnormalities. To date, computer aided detection methods focus on formulations based only on a single view. Recent multi-view methods are either black box approaches using methods such as relation blocks, or perform extensive, case-level feature aggregation requiring large data redundancy. In this study, we propose Retina-Match, an end-to-end trainable pipeline for detection, matching, and refinement that can effectively perform ipsilateral lesion matching in paired screening mammography images. We demonstrate effectiveness on a private, digital mammography data set with 1,016 biopsied lesions and 2,000 negative cases.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87240-3_33">https://doi.org/10.1007/978-3-030-87240-3_33</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper proposes breast cancer detection in mammography including ipsilateral information. The detection is done in three steps, in the first RetinaTrack is used in each view to detect suspicious regions. Subsequently, candidates from both views are correlated by a Distance metric that has been learnt from ground-truth by a greedy-based approach. The third step aims to fuse the information of the candidates.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>
          <p>The proposal for abnormality detection includes ipsilateral information (CC and MLO views). This is an important aspect which actually mimicks the radiologist procedure. Using only one view, lesions can be ocluded or false positive may arise due to overlapping tissues.</p>
        </li>
        <li>
          <p>The proposal is evaluated using a large dataset of images (almost 3000 images in the training set).</p>
        </li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>
          <p>The use of the Distance metric is controversial. This step is replacing the typical CC-MLO registration using a learning-from-examples stratregy instead of using the own physics of the mammographic acquisition.</p>
        </li>
        <li>
          <p>A second weakness of the method is that it not used bilateral information, which is the first comparison commonly done by radiologists, even before the CC/MLO.</p>
        </li>
        <li>
          <p>Although the dataset used to evaluate the approach is large, the ratio of malignant cases over total cases is 1/10, which is far from the one in screening (around 1/1000).</p>
        </li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>I think the reproducibility of the method is difficult. There are many ad-hoc parameters that are not fully explained probably due to the lack of space. Besides, they use a in-house database</p>

    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>This paper presents another CNN-based approach for breast cancer detection. It is based on the use of RetinaTrack to detect suspicious regions in MLO and CC views and a subsequent step joining the information. This step is divided into the candidate matching and the information fusion.</p>

      <p>I’m not sure section 4.2 &amp; 4.3 should be in Experiments, perhaps would be better placed at the end of Section 3 (i.e. section 3.2).</p>

      <p>Regarding the method itself, authors used the Distance metric as a way to avoid the physical correspondence in CC/MLO registration. This step limits interpretability of the approach and depends on having enough samples in the training set to interpret all the possible cases of mass locations.</p>

      <p>As already commented, my main concern is about the evaluation performed. Authors should add much more normal mammograms to be sure the number of FP doesn’t increment largely.</p>

      <p>Section 4.4 is comented without numbers. Seems strange.</p>

      <p>I don’t understand why authors show two FROCs, one for benign &amp; malignant and the second one for malignant only. Should be fairer to show benign only and malignant only, even the first one doesn’t favour the proposed approach.</p>

      <p>It would interesting to show the values of alpha and beta optimised by the net. Are similar to the one expected by the authors?</p>

      <p>The paper misses statistical analysis of the results (including the lack of the confidence interval of the FROCs).</p>

      <p>There is no comparison with the state of the art. How significant is the proposal of the authors with respect to other proposals?</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>I tend to accept the paper due to the single end-to-end training that avoids intermediate steps. However, the experimental section should be improved to gauge the significance of the approach.</p>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper introduced an ipsilateral lesion match step in single-stage detector pipeline to improve lesion detection performance in Mammography images. The method was inspired and adapted from Retina-Match, allowing for an end-to-end training. Experiments using a large, private, curated dataset was conducted to demonstrate the effectiveness of the proposed approach.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>The paper is well organized &amp; fairly well written. Clinical context (the use of all views in Mammography lesion detection by clinicians) was well introduced.</li>
        <li>The analysis of state-of-the-art was extensive. Ablation study &amp; comparison with state-of-the-art methods was thorough.</li>
        <li>Dataset used to demonstrate the performance is relative large &amp; of good quality compared to public datasets (DDSM, INBREAST …)</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>This paper lacks details in description of the architecture, in particular how the distance metric network connects with other components.</li>
        <li>Mathematical notations needs further clarification, especially explaining the indication of super/sub-script letters.</li>
        <li>Performance graphs lacks error-bars, making the statistical significance of the comparison unclear.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <ul>
        <li>Dataset is private, with minimal description. No descriptions on patient demographics &amp; distribution per types of lesions.</li>
        <li>Training &amp; evaluation codes will be make public. Training details were mentioned in paper (hardware, software framework, key hyper-parameters). Training time &amp; methods to finetune the hyper-parameters were not detailed.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Chapter Introduction, 3rd paragraph</p>
      <ul>
        <li>“Nevertheless, this design assumes the latent features of each lesion candidate can affect all other candidates, breaking the clinical assumption of one-to-one matching.” Could the author add some references (clinical papers) to back this up?</li>
        <li>Section Lesion R-identification: please clarify some notions. y_match^k, f_CC^i, f_MLO^j, what do k, i and j refer to?</li>
      </ul>

      <p>Section Lesion Matching Logic</p>
      <ul>
        <li>FP-FP is also assigned positive label 1? Please clarify.</li>
        <li>What does p^i_match refer to?</li>
        <li>“The proposed lesion renement loss is than computed…”: typo, than -&gt; then</li>
        <li>Equation (3): clarify what N and M refer to.</li>
      </ul>

      <p>Section 4.1 Dataset</p>
      <ul>
        <li>In biopsied soft-tissue lesion cases: only biopsied lesions were annotated. Also, BIRADS 2 patients may have non-biopsied benign lesions that are not annotated &amp; treated in this work. This was clarified later in section 4.6 Comparison with the Relation Block approach, it would be more clear for readers to understand if the author clarify here.</li>
      </ul>

      <p>Section 4.2 Network Architecture</p>
      <ul>
        <li>The description of architecture lacks details: specifically, how does the distance metric network connect to the 4 regressors and the greedy matching? This should be more clearly stated and illustrated in Figure 1.</li>
      </ul>

      <p>Section 4.5 Ablation Study, Figure 2</p>
      <ul>
        <li>There is no error bar in this graph, making it difficult to compare the relative performance of different training set-ups. Also the sensitivity values in these two graphs do not seem to plateau yet, and seem to converge for different set-ups when the false positive rate becomes greater. It would be more convincing to show more sensitivity values with larger false positive rates, to compare the max sensitivity of different training set-ups.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Ipsilateral matching in Mammography is a clinically relevant topic. The proposed method was well-described in general. The experimental results, minus some minor details remain convincing. Seeing this type of research for 2D and 3D Mammography lesion detection is important for the research &amp; clinical community.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper proposes an end-to-end network for lesion detection from paired CC-MLO mammography views. The network is capable of simultaneously performing three-tasks: lesion detection on each individual view, CC-MLO matching (via a Siamese network), and classification refinement by integrating information from the two views. Specifically, the scores of the matched bounding boxes are linearly combined, with weights that are not fixed, but predicted by the network and are specific for each anchor. Compared to existing architectures, such as MommiNet where all features embeddings from all detections are input to a Relation Network, the architecture assumes one-to-one matching between regions on the CC and MLO view, which is more similar to the typical radiologists’ workflow.  Extensive experimental evaluation shows performance gain with respect to the traditional single-stream RetinaNet as well as other recently proposed dual-stream architectures based on Relation Networks.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <ul>
        <li>The paper proposes an architecture which is novel and original, and allows to effectively perform lesion detection on dual-view mammography.</li>
        <li>The output of the network is easily interpretable by the radiologists, as it closely resembles the workflow employed a human reader.</li>
        <li>The method does not require the two views to be registered</li>
        <li>Strong experimental validation is provided both against single-view detection, as well as against other competing methods, showing clear advantages in terms of performance.</li>
        <li>The paper is clearly written and easy to understand</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <ul>
        <li>RetinaMatch tackles two tasks at once: CC-MLO matching, and lesion detection. However, only the latter is experimentally evaluated. The effectiveness of matching, i.e., how often a lesion is actually matched to its corresponding ipsilateral view, is not evaluated.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The paper is not technically reproducible as the authors do not provide the code nor the dataset. The latter however, is very common in mammography since public datasets are rather small and/or based on screen-film mammography. 
The description of the method is quite clear, and should be relatively easy to reproduce.  The following parameters are missing:</p>
      <ul>
        <li>Number and type of anchors</li>
        <li>Input image size</li>
        <li>Formulation of the regression loss</li>
        <li>Weights of the individual components of the loss</li>
        <li>Transformations used for data augmentation
The authors should also clarify what they mean by lesion-level sensitivity.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>The paper proposes a novel architecture for ipsilateral matching and lesion detection in mammography. Compared to previous architectures, such as MommiNet, the proposed architecture enables explicit one-to-one matching between lesion view, which directly mimics the radiologist’s workflow and provides an interpretable output to the radiologist. The other advantage of the proposed architecture is that it focuses the attention on local features, whereas other architectures (e.g., relational block) are prone to focus on global features. I agree with the authors that local features are essentially in discriminating masses from false positives due to tissue superposition. I do not fully agree that global features do not capture useful information; global aspects, e.g., related to breast density, or to the presence of diffuse calcifications, may impact the final classification.</p>

      <p>The paper is generally well written, and the experimental section well curated. Some points could be expanded or clarified, perhaps relying also on supplementary material given the space constraints:</p>
      <ul>
        <li>In the related work, the sentence ” GCNet [2] showed that general non-local blocks often degrade into a global feature extractor.” Should be further clarified and put in the specific context of mammography</li>
        <li>In Section 3, does the distance function D takes into account the position, as well as the embedding?</li>
        <li>Why are the embeddings explicitly regressed, instead of being extracted from the classification network or the backbone? What is the advantage of this additional head?</li>
        <li>In Section 4.1, the authors mention the application of a pectoral muscle segmentation model for the MLO view, and that a 2D breast depth encoding model is constructed. However, it is not clear how this information is used in in the architecture. I suggest adding it to Figure 1.</li>
        <li>I understand the advantage of regressing parameter alpha and beta, instead of fixing them manually. However, I wonder what is the specific advantage of having anchor-specific parameters, with respect to constant values optimized over the entire dataset.</li>
        <li>It is not clear in Table 1, what is the difference between case, view and annotation - especially view and annotation. What are considered as lesions in calculating the sensitivity? Please clarify and use a consistent notation across the paper</li>
        <li>All models should be trained for an equal number of epochs. From the description of the methodology, it appears as the single-view RetinaNet is trained from 25 epochs, and then RetinaMatch is further fine-tuned for 50 epochs. The single-view model should correspondingly be further fine-tuned for 25 epochs. In future works, it would be interesting to compare the convergence properties of different models models, i.e., whether RetinaMatch converges faster or slower than RetinaNet.</li>
        <li>A few examples that illustrate and compare the detections by different models would be nice.</li>
        <li>The probability of each candidate lesion on each view is modified based on the probability of the corresponding matched lesion candidate on the opposite view. How is the final lesion probability calculated to calculate the per-lesion probability?</li>
        <li>The differences between the models are subtle, an analysis of their statistical significance should be included</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>This is an interesting paper with well-conducted experiments, with proper baseline and comparisons with other methods. The proposed method is not a trivial extension of existing models and has practical advantages (in terms of simplicity and interpretability) compared with existing methods. The paper is well written and easy to understand.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>This paper proposes a lesion detector and classifier by analysing CC and MLO views.  All reviewers acknowledged the innovation of the method and the solid experimental results, so I am recommending the provisional acceptance of the paper.  There are a few points that need to be addressed:  1) can the authors assess the CC-MLO matching, by checking how often a lesion is actually matched to its corresponding ipsilateral view?; 2) the reviewers requested a few methodological clarifications; and 3) can the paper add statistical significance tests to the results?</p>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We really appreciate the reviewers for your constructive feedback. We have corrected the indicated language errors and made a few cleanups for the mathematical notations and model architecture description for the camera-ready version.</p>

  <p>Here we would like to respond to a few other points been raised:</p>

  <p>Regarding the lesion matching performance, we derived our final formulation from a patch-based matching model, where this evaluation is available. The patch level exhaustive pair vs not-pair AUC is around 0.96 when we conduct the matching operation at 3 FP per image. We will include some more detailed matching evaluations in the final camera-ready version.</p>

  <p>Regarding Lesion Matching Logic, we treat TP-TP pairs as positive and TP-FP pairs as negative. We believe lesion level matching performs better than full mammogram registration (can be supervised and less overhead), and we will conduct more comparisons in the future to prove this point. For position encoding, we used the pectoral muscle segmentation to construct the nipple distance of each lesion.</p>

  <p>Regarding the statistical significance for this study, we made the assumption that our dataset is relatively large thus decided to not include a cross-validation study that could yield a confidence interval for the FROC. We will pay more attention in our future studies to add statistical significance tests.</p>

  <p>Thanks again for all the above throughout reviews. We will definitely incorporate those comments into the design of our future studies.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0532-12-31
      -->
      <!--
      
        ,
        updated at 
        0533-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Breast"
        class="post-category">
        Clinical applications - Breast
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Ren, Yinhao"
        class="post-category">
        Ren, Yinhao
      </a> |  
      
      <a href="kittywong/tags#Lu, Jiafeng"
        class="post-category">
        Lu, Jiafeng
      </a> |  
      
      <a href="kittywong/tags#Liang, Zisheng"
        class="post-category">
        Liang, Zisheng
      </a> |  
      
      <a href="kittywong/tags#Grimm, Lars J."
        class="post-category">
        Grimm, Lars J.
      </a> |  
      
      <a href="kittywong/tags#Kim, Connie"
        class="post-category">
        Kim, Connie
      </a> |  
      
      <a href="kittywong/tags#Taylor-Cho, Michael"
        class="post-category">
        Taylor-Cho, Michael
      </a> |  
      
      <a href="kittywong/tags#Yoon, Sora"
        class="post-category">
        Yoon, Sora
      </a> |  
      
      <a href="kittywong/tags#Marks, Jeffrey R."
        class="post-category">
        Marks, Jeffrey R.
      </a> |  
      
      <a href="kittywong/tags#Lo, Joseph Y."
        class="post-category">
        Lo, Joseph Y.
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0533/12/31/Paper1058">
          Towards Robust Dual-view Transformation via Densifying Sparse Supervision for Mammography Lesion Matching
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0531/12/31/Paper0995">
          Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
