<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>VertNet: Accurate Vertebra Localization and Identification Network from CT Images | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="VertNet: Accurate Vertebra Localization and Identification Network from CT Images" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Zhiming Cui, Changjian Li, Lei Yang, Chunfeng Lian, Feng Shi, Wenping Wang, Dijia Wu, Dinggang Shen Abstract Accurate localization and identification of vertebrae from CT images is a fundamental step in clinical spine diagnosis and treatment. Previous methods have made various attempts in this task; however, they fail to robustly localize the vertebrae with challenging appearance or identify vertebra labels from CT images with a limited field of view. In this paper, we propose a novel two-stage framework, VertNet, for accurate and robust vertebra localization and identification from CT images. Our method first detects all vertebra centers by a weighted voting-based localization network. Then, an identification network is designed to identify the label of each detected vertebra in leveraging the synergy of global and local information. Specifically, a bidirectional relation module is designed to learn the global correlation among vertebrae along the upward and downward directions, and a continuous label map with dense annotation is employed to enhance the feature learning in local vertebra patches. Extensive experiments on a large dataset collected from real-world clinics show that our framework can accurately localize and identify vertebrae in various challenging cases and outperforms the state-of-the-art methods. Link to paper https://doi.org/10.1007/978-3-030-87240-3_27 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors develop a two-stage algorithm, which they call VertNet, for localization and identification (ie, labeling) of vertebra. Numerous works have previously addressed this problem; however, there are multiple known failure modes, such as pathological cases, imaging artifacts (from metal implants), and limited field of view. The system proposed in this paper is designed to leverage both local and global information, and incorporates voting in multiple stages, to address these remaining challenges. The authors evaluate their method against the SOTA algorithms on a large proprietary real-world dataset, and show excellent performance of their method on both normal and challenging cases. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Method The methodological approach is very carefully reasoned, and clearly justified. The role of each step/model in the overall two-stage pipeline is very clear. Furthermore, the authors conduct a thorough ablation analysis, evaluating the performance gain from each of the steps in their algorithmic pipeline. Comparison The authors implement three SOTA methods and compare the performance of their model to each. In each case, their model demonstrates improved performance. The authors show specific example cases where the SOTA methods fail, and their model produces the correct vertebra localization and identification. Maturity: This method and approach is at a level of maturity that would have potential applicability in real-world clinical scenarios. In addition, the presentation of the paper is excellent. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Comparison to SOTA: In addition to the comparison that the authors provide on their in-house dataset, they should also include a comparison on one of the open-source spine datasets, such as CSI 2014. This would help to substantiate the author’s claim of establishing a new SOTA method for the important task of vertebra localization and identification. Algorithm runtime requirements: The authors mention potential clinical applicability of their algorithm, based on the excellent performance. However, for clinical applicability, the runtime requirements will also be an important consideration. The algorithm involves multiple stages: how long does it take to run on CPU? What is the memory consumption? Etc… Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance There are a number of “affirmative” answers in the reproducibility checklist that don’t seem to jibe with the paper: (1) The run time requirements were not presented in the paper (2) Memory footprint of the algorithm was not discussed in the paper (3) Statistical significance of the results was not presented in the paper (4) Failure modes of the proposed algorithm were not presented/discussed. In addition, the authors state that the training and evaluation code, as well as the new dataset will all be made publicly available. This will be extremely beneficial for others to reproduce the results presented in this paper. However, I don’t see links within the paper to indicate that the code and data will be shared. These issues should be addressed in the final manuscript. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In addition to the comments in Field 4 (above), please see the following: Evaluation: The results presented in Tables 1, 2, 3 should be explicitly evaluated for statistical significance. Parameter sensitivity Equation 1 introduces two parameters: delta (=2) and lambda (=5). How sensitive is the overall performance of the pipeline to the choice of these parameters? Minor edits: Last sentence in the Intro: “…, giving the high usability in real-world clinical practices.” Seems to be an incomplete statement. Figure 4: “mental artifacts” should be “metal artifacts” Last sentence in Results: “mental artifacts” should be “metal artifacts”. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is well reasoned, and the presentation is excellent. The proposed method is evaluated, both against SOTA methods, and through a careful ablation analysis. The final results of the method are excellent. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper provides a new two-stage framework for vertebral localization and identification of CT images. The framework first uses a weighted voting location network to detect the center of the vertebrae and then designs an identification network to identify each vertebrae category in leveraging the synergy of global and local information. Specifically, a bidirectional relation module is designed to learn the global correlation among vertebrae along with the upward and downward directions, and a continuous label map with dense annotation is employed to enhance the feature learning in local vertebra patches. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This work makes good use of the top-down sequence relationship of the spine when identifying vertebrae categories. This provides an interesting method for classifying objects based on their label relations. This work provides extensive experiments to prove the effectiveness of the detection framework by a large data set containing 1000 chest CT images. The inter and intra comparison experiments are well done. The English writing of this paper is good. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Firstly, the authors do not clearly explain why the existing literature is not capable of detecting vertebrae, which may not convince the readers about the novelty of this paper. Secondly, some important implementation details are missing in the methodology part, which harms the logical flow of the paper. Also thirdly, the necessity and principles of some methods are not clearly explained. These issues make the paper very hard to follow. Please see question 7 for details for the above-mentioned issues. Lastly, the reference indices are wrong. The first reference should be [1], not [5]. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility of this article is ordinary. The paper provides the setting value of each hyperparameter, and the form of the loss function is also given in the paper. As shown in questions 4 and 7, some implementations are not explained in detail, however, this may be compensated if the authors make their codes available as stated in their Reproducibility Response. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Following question 4, I would like to give some suggestions on how to improve the significance, readability, and reproducibility of the paper, which I hope could benefit the authors for future research: (1) Why the existing literature is not capable of detecting vertebrae from CT images. For example, the authors mentioned that “such a model may not fully capture the global dependency among all vertebrae and usually limit to local regions” (P2), however, it seems that reference [8] can capture global dependency using RNN’s. If the authors explain this more clearly, the significance of this paper would be more clearly explained. (2) Some important implementation details are missing. For example, in Section II, how to convert heatmap H and offset map O to weighted vote map M? It seems that O provides a location where heatmap value should be mapped to, however, the readers are still confused about how this is conducted because there are no in-depth discussions. What is the shape of O? What does the value in each element of O tensor mean? Also, what if the O value points to a location that is out of the range ? In this case, how to guarantee the reliability of the predictions? (for example, if the image height and width are normalized to 1, what if the network gives an O value of 1.2? Do you just clip it to 1? Although I have not tried by myself, but this is intuitively prone to yield wrong localization results) Similar issues happen in the other modules, e.g., what are the inputs and outputs to the fast peak search clustering method? What are the shapes of p_i and how are corresponding f_i extracted? How does the Post-label Voting algorithm function? … All these are not clearly mentioned, which is not conducive to the reproducibility of the work. (3) The necessity and principles of some methods. For example, in the Vertebra Proposal Generation part: a) Why not simply use methods such as RPN for generating vertebra proposals? b) How can a Gaussian filter be used to generate image patches? To the best of my knowledge, Gaussian filters are used for low-pass filtering or high-pass filtering in the image processing community. So, how can they be used to generate patches? Similar issues happen in the other modules, e.g., why can the Bidirectional Relation Module capture global dependency which can’t be captured by RNN modules (as mentioned in the paper)? How is the floating label obtained? How do they interact with the predicted label generated by the FC layers? What if the predicted floating labels are wrong? If the authors explain this more clearly, the readability of this paper would be more clearly explained. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? In general, the authors have made some efforts in collecting data, programming and writing this paper. Also, I know that the acceptance of this paper may be important to the authors, thus, I would like to rate “borderline accept (6)” despite the novelty, readability, and reproducibility issues listed in questions 4 and 7. However, to be frank, this paper still needs a lot of modification before it can be published. Also, hope that the authors could upload the codes and data as they promised in their Reproducibility Response if the paper is accepted. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper presents a method for localization and identification of vertebrae from CT using a two stage framework according to these two tasks. For each of the tasks, the paper presents several dedicated adaptations such as a bi-directional search module and a continuous label map to support labelling. Results are presented on a comparable large (1000 cases) but not publicly available cohort which limits comparison of the method to others. Still, encouraging results are presented on quite challenging cases. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Overall, the paper presents an interesting approach to a still relevant but intensively addressed application. The method is sound and contains many interesting extensions. At the same time, given the comprehensive framework with its several extensions carefully designed to fit the purpose, some components are hard to get (probably due to the limited space). The ablation study is nice to really see the benefit of the different modules. Nice to have evaluation per spine region, but surprising to see performance is best in the cervical region which is usually most challenging. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The clinical problem is still highly relevant, although several work has been presented very recently. For that reason, I find it extremely important to know performance of the method compared to others. Unfortunately, authors have decided to not use a public cohort. Personally, I am wondering why e.g. VERSE 20 data set has not been considered. The approach has been compared to other state of the art method. However, it remains unclear how the methods have been obtained/implemented. I suspect authors are contributors to benchmark approaches as otherwise a fair comparison is probably not possible. Have methods been used out-of the box or any re-training, parameter adaptation etc been applied? It would be great to get a feeling why performance of current framework is better than previous ones - is it better design or just more training data/better parameter adjustment. It would have been great to have even a more detailed evaluation or more description about the data. This is important to judge how challenging the data set is. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Although the method has been applied to a quite sizeable number of cases (1000), reproducibility is limited as the a non-public cohort has been used. Comparison to other methods is done, although it is unclear how implementations were obtained. In general, the level of detail provided is not sufficient for re-implementation of the method. Code will not be made available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I missed information about the data and the achieved results in the abstract. At the same time, the abstract already contained many information about the framework which were hard to grasp from the abstract. In the intro you write “be pathological or with metal artifacts” - just minor, but I found it very closely related as for me metal artifacts are also caused by pathologies. It would be really good to have more infos about the data? What pathologies with what incidences, FOV etc… Would be good to see associated error in Fig. 4 Please clarify the post-label voting. Maybe implementation details could help here. I found it pretty high level. What is the runtime during inference? Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall, I found the paper interesting to read with interesting components. The paper is evaluated nicely (although still there are also some limitations). One of the major limitations for me is the use of a separate not public cohort. As a result, it really remains challenging to judge performance of the method, especially for a problem that has been addressed intensively in the recent years. Still, I find the presented results encouraging on at least selected very challenging cases. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 6 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Dear Authors, I am happy to inform you that the reviewers agreed that your work meets the quality needed to be accepted at the MICCAI conference. Localization and identification of vertebrae is a challenging task mainly due to the similar appearance of the vertebrae in the image. On a large in-house dataset of 1000 CT images, the authors have shown that by leveraging both local and global information the number of failure cases can be reduced. Despite the positive evaluation, the reviewers also expressed several concerns that authors should address in their camera-ready version of the manuscript. Although I agree with the reviewers that evaluation of the method on the VerSe20 challenge dataset would better justify the authors’ claim in outperforming SOTA methods, we cannot expect the missing experiment to be performed in the rebuttal phase. Newertheless, authors could discuss how their method goes beyond the top-ranked methods at the VerSe19 and VerSe20 challenges. They should put special attention to those methods of the challenge that also explore local and global information for vertebral localization and identification. Moreover, this comparison will also strengthen the motivation section and clarify the limitations of existing methods. As much as possible due to page limitations, the authors should provide more information on the representativeness of the dataset and clarify the method section as requested by reviewers. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback N/A back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Zhiming Cui, Changjian Li, Lei Yang, Chunfeng Lian, Feng Shi, Wenping Wang, Dijia Wu, Dinggang Shen Abstract Accurate localization and identification of vertebrae from CT images is a fundamental step in clinical spine diagnosis and treatment. Previous methods have made various attempts in this task; however, they fail to robustly localize the vertebrae with challenging appearance or identify vertebra labels from CT images with a limited field of view. In this paper, we propose a novel two-stage framework, VertNet, for accurate and robust vertebra localization and identification from CT images. Our method first detects all vertebra centers by a weighted voting-based localization network. Then, an identification network is designed to identify the label of each detected vertebra in leveraging the synergy of global and local information. Specifically, a bidirectional relation module is designed to learn the global correlation among vertebrae along the upward and downward directions, and a continuous label map with dense annotation is employed to enhance the feature learning in local vertebra patches. Extensive experiments on a large dataset collected from real-world clinics show that our framework can accurately localize and identify vertebrae in various challenging cases and outperforms the state-of-the-art methods. Link to paper https://doi.org/10.1007/978-3-030-87240-3_27 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors develop a two-stage algorithm, which they call VertNet, for localization and identification (ie, labeling) of vertebra. Numerous works have previously addressed this problem; however, there are multiple known failure modes, such as pathological cases, imaging artifacts (from metal implants), and limited field of view. The system proposed in this paper is designed to leverage both local and global information, and incorporates voting in multiple stages, to address these remaining challenges. The authors evaluate their method against the SOTA algorithms on a large proprietary real-world dataset, and show excellent performance of their method on both normal and challenging cases. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Method The methodological approach is very carefully reasoned, and clearly justified. The role of each step/model in the overall two-stage pipeline is very clear. Furthermore, the authors conduct a thorough ablation analysis, evaluating the performance gain from each of the steps in their algorithmic pipeline. Comparison The authors implement three SOTA methods and compare the performance of their model to each. In each case, their model demonstrates improved performance. The authors show specific example cases where the SOTA methods fail, and their model produces the correct vertebra localization and identification. Maturity: This method and approach is at a level of maturity that would have potential applicability in real-world clinical scenarios. In addition, the presentation of the paper is excellent. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Comparison to SOTA: In addition to the comparison that the authors provide on their in-house dataset, they should also include a comparison on one of the open-source spine datasets, such as CSI 2014. This would help to substantiate the author’s claim of establishing a new SOTA method for the important task of vertebra localization and identification. Algorithm runtime requirements: The authors mention potential clinical applicability of their algorithm, based on the excellent performance. However, for clinical applicability, the runtime requirements will also be an important consideration. The algorithm involves multiple stages: how long does it take to run on CPU? What is the memory consumption? Etc… Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance There are a number of “affirmative” answers in the reproducibility checklist that don’t seem to jibe with the paper: (1) The run time requirements were not presented in the paper (2) Memory footprint of the algorithm was not discussed in the paper (3) Statistical significance of the results was not presented in the paper (4) Failure modes of the proposed algorithm were not presented/discussed. In addition, the authors state that the training and evaluation code, as well as the new dataset will all be made publicly available. This will be extremely beneficial for others to reproduce the results presented in this paper. However, I don’t see links within the paper to indicate that the code and data will be shared. These issues should be addressed in the final manuscript. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In addition to the comments in Field 4 (above), please see the following: Evaluation: The results presented in Tables 1, 2, 3 should be explicitly evaluated for statistical significance. Parameter sensitivity Equation 1 introduces two parameters: delta (=2) and lambda (=5). How sensitive is the overall performance of the pipeline to the choice of these parameters? Minor edits: Last sentence in the Intro: “…, giving the high usability in real-world clinical practices.” Seems to be an incomplete statement. Figure 4: “mental artifacts” should be “metal artifacts” Last sentence in Results: “mental artifacts” should be “metal artifacts”. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is well reasoned, and the presentation is excellent. The proposed method is evaluated, both against SOTA methods, and through a careful ablation analysis. The final results of the method are excellent. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper provides a new two-stage framework for vertebral localization and identification of CT images. The framework first uses a weighted voting location network to detect the center of the vertebrae and then designs an identification network to identify each vertebrae category in leveraging the synergy of global and local information. Specifically, a bidirectional relation module is designed to learn the global correlation among vertebrae along with the upward and downward directions, and a continuous label map with dense annotation is employed to enhance the feature learning in local vertebra patches. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This work makes good use of the top-down sequence relationship of the spine when identifying vertebrae categories. This provides an interesting method for classifying objects based on their label relations. This work provides extensive experiments to prove the effectiveness of the detection framework by a large data set containing 1000 chest CT images. The inter and intra comparison experiments are well done. The English writing of this paper is good. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Firstly, the authors do not clearly explain why the existing literature is not capable of detecting vertebrae, which may not convince the readers about the novelty of this paper. Secondly, some important implementation details are missing in the methodology part, which harms the logical flow of the paper. Also thirdly, the necessity and principles of some methods are not clearly explained. These issues make the paper very hard to follow. Please see question 7 for details for the above-mentioned issues. Lastly, the reference indices are wrong. The first reference should be [1], not [5]. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility of this article is ordinary. The paper provides the setting value of each hyperparameter, and the form of the loss function is also given in the paper. As shown in questions 4 and 7, some implementations are not explained in detail, however, this may be compensated if the authors make their codes available as stated in their Reproducibility Response. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Following question 4, I would like to give some suggestions on how to improve the significance, readability, and reproducibility of the paper, which I hope could benefit the authors for future research: (1) Why the existing literature is not capable of detecting vertebrae from CT images. For example, the authors mentioned that “such a model may not fully capture the global dependency among all vertebrae and usually limit to local regions” (P2), however, it seems that reference [8] can capture global dependency using RNN’s. If the authors explain this more clearly, the significance of this paper would be more clearly explained. (2) Some important implementation details are missing. For example, in Section II, how to convert heatmap H and offset map O to weighted vote map M? It seems that O provides a location where heatmap value should be mapped to, however, the readers are still confused about how this is conducted because there are no in-depth discussions. What is the shape of O? What does the value in each element of O tensor mean? Also, what if the O value points to a location that is out of the range ? In this case, how to guarantee the reliability of the predictions? (for example, if the image height and width are normalized to 1, what if the network gives an O value of 1.2? Do you just clip it to 1? Although I have not tried by myself, but this is intuitively prone to yield wrong localization results) Similar issues happen in the other modules, e.g., what are the inputs and outputs to the fast peak search clustering method? What are the shapes of p_i and how are corresponding f_i extracted? How does the Post-label Voting algorithm function? … All these are not clearly mentioned, which is not conducive to the reproducibility of the work. (3) The necessity and principles of some methods. For example, in the Vertebra Proposal Generation part: a) Why not simply use methods such as RPN for generating vertebra proposals? b) How can a Gaussian filter be used to generate image patches? To the best of my knowledge, Gaussian filters are used for low-pass filtering or high-pass filtering in the image processing community. So, how can they be used to generate patches? Similar issues happen in the other modules, e.g., why can the Bidirectional Relation Module capture global dependency which can’t be captured by RNN modules (as mentioned in the paper)? How is the floating label obtained? How do they interact with the predicted label generated by the FC layers? What if the predicted floating labels are wrong? If the authors explain this more clearly, the readability of this paper would be more clearly explained. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? In general, the authors have made some efforts in collecting data, programming and writing this paper. Also, I know that the acceptance of this paper may be important to the authors, thus, I would like to rate “borderline accept (6)” despite the novelty, readability, and reproducibility issues listed in questions 4 and 7. However, to be frank, this paper still needs a lot of modification before it can be published. Also, hope that the authors could upload the codes and data as they promised in their Reproducibility Response if the paper is accepted. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper presents a method for localization and identification of vertebrae from CT using a two stage framework according to these two tasks. For each of the tasks, the paper presents several dedicated adaptations such as a bi-directional search module and a continuous label map to support labelling. Results are presented on a comparable large (1000 cases) but not publicly available cohort which limits comparison of the method to others. Still, encouraging results are presented on quite challenging cases. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Overall, the paper presents an interesting approach to a still relevant but intensively addressed application. The method is sound and contains many interesting extensions. At the same time, given the comprehensive framework with its several extensions carefully designed to fit the purpose, some components are hard to get (probably due to the limited space). The ablation study is nice to really see the benefit of the different modules. Nice to have evaluation per spine region, but surprising to see performance is best in the cervical region which is usually most challenging. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The clinical problem is still highly relevant, although several work has been presented very recently. For that reason, I find it extremely important to know performance of the method compared to others. Unfortunately, authors have decided to not use a public cohort. Personally, I am wondering why e.g. VERSE 20 data set has not been considered. The approach has been compared to other state of the art method. However, it remains unclear how the methods have been obtained/implemented. I suspect authors are contributors to benchmark approaches as otherwise a fair comparison is probably not possible. Have methods been used out-of the box or any re-training, parameter adaptation etc been applied? It would be great to get a feeling why performance of current framework is better than previous ones - is it better design or just more training data/better parameter adjustment. It would have been great to have even a more detailed evaluation or more description about the data. This is important to judge how challenging the data set is. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Although the method has been applied to a quite sizeable number of cases (1000), reproducibility is limited as the a non-public cohort has been used. Comparison to other methods is done, although it is unclear how implementations were obtained. In general, the level of detail provided is not sufficient for re-implementation of the method. Code will not be made available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I missed information about the data and the achieved results in the abstract. At the same time, the abstract already contained many information about the framework which were hard to grasp from the abstract. In the intro you write “be pathological or with metal artifacts” - just minor, but I found it very closely related as for me metal artifacts are also caused by pathologies. It would be really good to have more infos about the data? What pathologies with what incidences, FOV etc… Would be good to see associated error in Fig. 4 Please clarify the post-label voting. Maybe implementation details could help here. I found it pretty high level. What is the runtime during inference? Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall, I found the paper interesting to read with interesting components. The paper is evaluated nicely (although still there are also some limitations). One of the major limitations for me is the use of a separate not public cohort. As a result, it really remains challenging to judge performance of the method, especially for a problem that has been addressed intensively in the recent years. Still, I find the presented results encouraging on at least selected very challenging cases. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 6 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Dear Authors, I am happy to inform you that the reviewers agreed that your work meets the quality needed to be accepted at the MICCAI conference. Localization and identification of vertebrae is a challenging task mainly due to the similar appearance of the vertebrae in the image. On a large in-house dataset of 1000 CT images, the authors have shown that by leveraging both local and global information the number of failure cases can be reduced. Despite the positive evaluation, the reviewers also expressed several concerns that authors should address in their camera-ready version of the manuscript. Although I agree with the reviewers that evaluation of the method on the VerSe20 challenge dataset would better justify the authors’ claim in outperforming SOTA methods, we cannot expect the missing experiment to be performed in the rebuttal phase. Newertheless, authors could discuss how their method goes beyond the top-ranked methods at the VerSe19 and VerSe20 challenges. They should put special attention to those methods of the challenge that also explore local and global information for vertebral localization and identification. Moreover, this comparison will also strengthen the motivation section and clarify the limitations of existing methods. As much as possible due to page limitations, the authors should provide more information on the representativeness of the dataset and clarify the method section as requested by reviewers. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback N/A back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0526/12/31/Paper0795" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0526/12/31/Paper0795" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0526-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="VertNet: Accurate Vertebra Localization and Identification Network from CT Images" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0526/12/31/Paper0795"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0526/12/31/Paper0795","headline":"VertNet: Accurate Vertebra Localization and Identification Network from CT Images","dateModified":"0527-01-03T00:00:00-05:17","datePublished":"0526-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Zhiming Cui, Changjian Li, Lei Yang, Chunfeng Lian, Feng Shi, Wenping Wang, Dijia Wu, Dinggang Shen Abstract Accurate localization and identification of vertebrae from CT images is a fundamental step in clinical spine diagnosis and treatment. Previous methods have made various attempts in this task; however, they fail to robustly localize the vertebrae with challenging appearance or identify vertebra labels from CT images with a limited field of view. In this paper, we propose a novel two-stage framework, VertNet, for accurate and robust vertebra localization and identification from CT images. Our method first detects all vertebra centers by a weighted voting-based localization network. Then, an identification network is designed to identify the label of each detected vertebra in leveraging the synergy of global and local information. Specifically, a bidirectional relation module is designed to learn the global correlation among vertebrae along the upward and downward directions, and a continuous label map with dense annotation is employed to enhance the feature learning in local vertebra patches. Extensive experiments on a large dataset collected from real-world clinics show that our framework can accurately localize and identify vertebrae in various challenging cases and outperforms the state-of-the-art methods. Link to paper https://doi.org/10.1007/978-3-030-87240-3_27 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper The authors develop a two-stage algorithm, which they call VertNet, for localization and identification (ie, labeling) of vertebra. Numerous works have previously addressed this problem; however, there are multiple known failure modes, such as pathological cases, imaging artifacts (from metal implants), and limited field of view. The system proposed in this paper is designed to leverage both local and global information, and incorporates voting in multiple stages, to address these remaining challenges. The authors evaluate their method against the SOTA algorithms on a large proprietary real-world dataset, and show excellent performance of their method on both normal and challenging cases. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Method The methodological approach is very carefully reasoned, and clearly justified. The role of each step/model in the overall two-stage pipeline is very clear. Furthermore, the authors conduct a thorough ablation analysis, evaluating the performance gain from each of the steps in their algorithmic pipeline. Comparison The authors implement three SOTA methods and compare the performance of their model to each. In each case, their model demonstrates improved performance. The authors show specific example cases where the SOTA methods fail, and their model produces the correct vertebra localization and identification. Maturity: This method and approach is at a level of maturity that would have potential applicability in real-world clinical scenarios. In addition, the presentation of the paper is excellent. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Comparison to SOTA: In addition to the comparison that the authors provide on their in-house dataset, they should also include a comparison on one of the open-source spine datasets, such as CSI 2014. This would help to substantiate the author’s claim of establishing a new SOTA method for the important task of vertebra localization and identification. Algorithm runtime requirements: The authors mention potential clinical applicability of their algorithm, based on the excellent performance. However, for clinical applicability, the runtime requirements will also be an important consideration. The algorithm involves multiple stages: how long does it take to run on CPU? What is the memory consumption? Etc… Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance There are a number of “affirmative” answers in the reproducibility checklist that don’t seem to jibe with the paper: (1) The run time requirements were not presented in the paper (2) Memory footprint of the algorithm was not discussed in the paper (3) Statistical significance of the results was not presented in the paper (4) Failure modes of the proposed algorithm were not presented/discussed. In addition, the authors state that the training and evaluation code, as well as the new dataset will all be made publicly available. This will be extremely beneficial for others to reproduce the results presented in this paper. However, I don’t see links within the paper to indicate that the code and data will be shared. These issues should be addressed in the final manuscript. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html In addition to the comments in Field 4 (above), please see the following: Evaluation: The results presented in Tables 1, 2, 3 should be explicitly evaluated for statistical significance. Parameter sensitivity Equation 1 introduces two parameters: delta (=2) and lambda (=5). How sensitive is the overall performance of the pipeline to the choice of these parameters? Minor edits: Last sentence in the Intro: “…, giving the high usability in real-world clinical practices.” Seems to be an incomplete statement. Figure 4: “mental artifacts” should be “metal artifacts” Last sentence in Results: “mental artifacts” should be “metal artifacts”. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is well reasoned, and the presentation is excellent. The proposed method is evaluated, both against SOTA methods, and through a careful ablation analysis. The final results of the method are excellent. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review #2 Please describe the contribution of the paper This paper provides a new two-stage framework for vertebral localization and identification of CT images. The framework first uses a weighted voting location network to detect the center of the vertebrae and then designs an identification network to identify each vertebrae category in leveraging the synergy of global and local information. Specifically, a bidirectional relation module is designed to learn the global correlation among vertebrae along with the upward and downward directions, and a continuous label map with dense annotation is employed to enhance the feature learning in local vertebra patches. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This work makes good use of the top-down sequence relationship of the spine when identifying vertebrae categories. This provides an interesting method for classifying objects based on their label relations. This work provides extensive experiments to prove the effectiveness of the detection framework by a large data set containing 1000 chest CT images. The inter and intra comparison experiments are well done. The English writing of this paper is good. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Firstly, the authors do not clearly explain why the existing literature is not capable of detecting vertebrae, which may not convince the readers about the novelty of this paper. Secondly, some important implementation details are missing in the methodology part, which harms the logical flow of the paper. Also thirdly, the necessity and principles of some methods are not clearly explained. These issues make the paper very hard to follow. Please see question 7 for details for the above-mentioned issues. Lastly, the reference indices are wrong. The first reference should be [1], not [5]. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility of this article is ordinary. The paper provides the setting value of each hyperparameter, and the form of the loss function is also given in the paper. As shown in questions 4 and 7, some implementations are not explained in detail, however, this may be compensated if the authors make their codes available as stated in their Reproducibility Response. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Following question 4, I would like to give some suggestions on how to improve the significance, readability, and reproducibility of the paper, which I hope could benefit the authors for future research: (1) Why the existing literature is not capable of detecting vertebrae from CT images. For example, the authors mentioned that “such a model may not fully capture the global dependency among all vertebrae and usually limit to local regions” (P2), however, it seems that reference [8] can capture global dependency using RNN’s. If the authors explain this more clearly, the significance of this paper would be more clearly explained. (2) Some important implementation details are missing. For example, in Section II, how to convert heatmap H and offset map O to weighted vote map M? It seems that O provides a location where heatmap value should be mapped to, however, the readers are still confused about how this is conducted because there are no in-depth discussions. What is the shape of O? What does the value in each element of O tensor mean? Also, what if the O value points to a location that is out of the range ? In this case, how to guarantee the reliability of the predictions? (for example, if the image height and width are normalized to 1, what if the network gives an O value of 1.2? Do you just clip it to 1? Although I have not tried by myself, but this is intuitively prone to yield wrong localization results) Similar issues happen in the other modules, e.g., what are the inputs and outputs to the fast peak search clustering method? What are the shapes of p_i and how are corresponding f_i extracted? How does the Post-label Voting algorithm function? … All these are not clearly mentioned, which is not conducive to the reproducibility of the work. (3) The necessity and principles of some methods. For example, in the Vertebra Proposal Generation part: a) Why not simply use methods such as RPN for generating vertebra proposals? b) How can a Gaussian filter be used to generate image patches? To the best of my knowledge, Gaussian filters are used for low-pass filtering or high-pass filtering in the image processing community. So, how can they be used to generate patches? Similar issues happen in the other modules, e.g., why can the Bidirectional Relation Module capture global dependency which can’t be captured by RNN modules (as mentioned in the paper)? How is the floating label obtained? How do they interact with the predicted label generated by the FC layers? What if the predicted floating labels are wrong? If the authors explain this more clearly, the readability of this paper would be more clearly explained. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? In general, the authors have made some efforts in collecting data, programming and writing this paper. Also, I know that the acceptance of this paper may be important to the authors, thus, I would like to rate “borderline accept (6)” despite the novelty, readability, and reproducibility issues listed in questions 4 and 7. However, to be frank, this paper still needs a lot of modification before it can be published. Also, hope that the authors could upload the codes and data as they promised in their Reproducibility Response if the paper is accepted. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 3 Reviewer confidence Confident but not absolutely certain Review #3 Please describe the contribution of the paper This paper presents a method for localization and identification of vertebrae from CT using a two stage framework according to these two tasks. For each of the tasks, the paper presents several dedicated adaptations such as a bi-directional search module and a continuous label map to support labelling. Results are presented on a comparable large (1000 cases) but not publicly available cohort which limits comparison of the method to others. Still, encouraging results are presented on quite challenging cases. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Overall, the paper presents an interesting approach to a still relevant but intensively addressed application. The method is sound and contains many interesting extensions. At the same time, given the comprehensive framework with its several extensions carefully designed to fit the purpose, some components are hard to get (probably due to the limited space). The ablation study is nice to really see the benefit of the different modules. Nice to have evaluation per spine region, but surprising to see performance is best in the cervical region which is usually most challenging. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The clinical problem is still highly relevant, although several work has been presented very recently. For that reason, I find it extremely important to know performance of the method compared to others. Unfortunately, authors have decided to not use a public cohort. Personally, I am wondering why e.g. VERSE 20 data set has not been considered. The approach has been compared to other state of the art method. However, it remains unclear how the methods have been obtained/implemented. I suspect authors are contributors to benchmark approaches as otherwise a fair comparison is probably not possible. Have methods been used out-of the box or any re-training, parameter adaptation etc been applied? It would be great to get a feeling why performance of current framework is better than previous ones - is it better design or just more training data/better parameter adjustment. It would have been great to have even a more detailed evaluation or more description about the data. This is important to judge how challenging the data set is. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Although the method has been applied to a quite sizeable number of cases (1000), reproducibility is limited as the a non-public cohort has been used. Comparison to other methods is done, although it is unclear how implementations were obtained. In general, the level of detail provided is not sufficient for re-implementation of the method. Code will not be made available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I missed information about the data and the achieved results in the abstract. At the same time, the abstract already contained many information about the framework which were hard to grasp from the abstract. In the intro you write “be pathological or with metal artifacts” - just minor, but I found it very closely related as for me metal artifacts are also caused by pathologies. It would be really good to have more infos about the data? What pathologies with what incidences, FOV etc… Would be good to see associated error in Fig. 4 Please clarify the post-label voting. Maybe implementation details could help here. I found it pretty high level. What is the runtime during inference? Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Overall, I found the paper interesting to read with interesting components. The paper is evaluated nicely (although still there are also some limitations). One of the major limitations for me is the use of a separate not public cohort. As a result, it really remains challenging to judge performance of the method, especially for a problem that has been addressed intensively in the recent years. Still, I find the presented results encouraging on at least selected very challenging cases. What is the ranking of this paper in your review stack? 2 Number of papers in your stack 6 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Dear Authors, I am happy to inform you that the reviewers agreed that your work meets the quality needed to be accepted at the MICCAI conference. Localization and identification of vertebrae is a challenging task mainly due to the similar appearance of the vertebrae in the image. On a large in-house dataset of 1000 CT images, the authors have shown that by leveraging both local and global information the number of failure cases can be reduced. Despite the positive evaluation, the reviewers also expressed several concerns that authors should address in their camera-ready version of the manuscript. Although I agree with the reviewers that evaluation of the method on the VerSe20 challenge dataset would better justify the authors’ claim in outperforming SOTA methods, we cannot expect the missing experiment to be performed in the rebuttal phase. Newertheless, authors could discuss how their method goes beyond the top-ranked methods at the VerSe19 and VerSe20 challenges. They should put special attention to those methods of the challenge that also explore local and global information for vertebral localization and identification. Moreover, this comparison will also strengthen the motivation section and clarify the limitations of existing methods. As much as possible due to page limitations, the authors should provide more information on the representativeness of the dataset and clarify the method section as requested by reviewers. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 2 Author Feedback N/A back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Cui, Zhiming,Li, Changjian,Yang, Lei,Lian, Chunfeng,Shi, Feng,Wang, Wenping,Wu, Dijia,Shen, Dinggang" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>VertNet: Accurate Vertebra Localization and Identification Network from CT Images</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a>
      
      <a 
        href="kittywong/categories#Clinical applications - Breast"
        class="post-category">
        Clinical applications - Breast
      </a>
      
      <a 
        href="kittywong/categories#Modalities - CT"
        class="post-category">
        Modalities - CT
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Cui, Zhiming"
        class="post-tags">
        Cui, Zhiming
      </a> |  
      
      <a href="kittywong/tags#Li, Changjian"
        class="post-tags">
        Li, Changjian
      </a> |  
      
      <a href="kittywong/tags#Yang, Lei"
        class="post-tags">
        Yang, Lei
      </a> |  
      
      <a href="kittywong/tags#Lian, Chunfeng"
        class="post-tags">
        Lian, Chunfeng
      </a> |  
      
      <a href="kittywong/tags#Shi, Feng"
        class="post-tags">
        Shi, Feng
      </a> |  
      
      <a href="kittywong/tags#Wang, Wenping"
        class="post-tags">
        Wang, Wenping
      </a> |  
      
      <a href="kittywong/tags#Wu, Dijia"
        class="post-tags">
        Wu, Dijia
      </a> |  
      
      <a href="kittywong/tags#Shen, Dinggang"
        class="post-tags">
        Shen, Dinggang
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Zhiming Cui, Changjian Li, Lei Yang, Chunfeng Lian, Feng Shi, Wenping Wang, Dijia Wu, Dinggang Shen
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Accurate localization and identification of vertebrae from CT images is a fundamental step in clinical spine diagnosis and treatment. Previous methods have made various attempts in this task; however, they fail to robustly localize the vertebrae with challenging appearance or identify vertebra labels from CT images with a limited field of view. In this paper, we propose a novel two-stage framework, VertNet, for accurate and robust vertebra localization and identification from CT images. Our method first detects all vertebra centers by a weighted voting-based localization network. Then, an identification network is designed to identify the label of each detected vertebra in leveraging the synergy of global and local information. Specifically, a bidirectional relation module is designed to learn the global correlation among vertebrae along the upward and downward directions, and a continuous label map with dense annotation is employed to enhance the feature learning in local vertebra patches. Extensive experiments on a large dataset collected from real-world clinics show that our framework can accurately localize and identify vertebrae in various challenging cases and outperforms the state-of-the-art methods.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87240-3_27">https://doi.org/10.1007/978-3-030-87240-3_27</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors develop a two-stage algorithm, which they call VertNet, for localization and identification (ie, labeling) of vertebra.
Numerous works have previously addressed this problem; however, there are multiple known failure modes, such as pathological cases, imaging artifacts (from metal implants), and limited field of view.
The system proposed in this paper is designed to leverage both local and global information, and incorporates voting in multiple stages, to address these remaining challenges.
The authors evaluate their method against the SOTA algorithms on a large proprietary real-world dataset, and show excellent performance of their method on both normal and challenging cases.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>Method</p>
      <ul>
        <li>The methodological approach is very carefully reasoned, and clearly justified.  The role of each step/model in the overall two-stage pipeline is very clear.</li>
        <li>Furthermore, the authors conduct a thorough ablation analysis, evaluating the performance gain from each of the steps in their algorithmic pipeline.</li>
      </ul>

      <p>Comparison</p>
      <ul>
        <li>The authors implement three SOTA methods and compare the performance of their model to each.  In each case, their model demonstrates improved performance.</li>
        <li>The authors show specific example cases where the SOTA methods fail, and their model produces the correct vertebra localization and identification.</li>
      </ul>

      <p>Maturity:</p>
      <ul>
        <li>This method and approach is at a level of maturity that would have potential applicability in real-world clinical scenarios.</li>
      </ul>

      <p>In addition, the presentation of the paper is excellent.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>Comparison to SOTA:</p>
      <ul>
        <li>In addition to the comparison that the authors provide on their in-house dataset, they should also include a comparison on one of the open-source spine datasets, such as CSI 2014.  This would help to substantiate the author’s claim of establishing a new SOTA method for the important task of vertebra localization and identification.</li>
      </ul>

      <p>Algorithm runtime requirements:</p>
      <ul>
        <li>The authors mention potential clinical applicability of their algorithm, based on the excellent performance.  However, for clinical applicability, the runtime requirements will also be an important consideration.  The algorithm involves multiple stages: how long does it take to run on CPU?  What is the memory consumption?  Etc…</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>There are a number of “affirmative” answers in the reproducibility checklist that don’t seem to jibe with the paper:
(1) The run time requirements were not presented in the paper
(2) Memory footprint of the algorithm was not discussed in the paper
(3) Statistical significance of the results was not presented in the paper
(4) Failure modes of the proposed algorithm were not presented/discussed.</p>

      <p>In addition, the authors state that the training and evaluation code, as well as the new dataset will all be made publicly available.  This will be extremely beneficial for others to reproduce the results presented in this paper.  However, I don’t see links within the paper to indicate that the code and data will be shared.</p>

      <p>These issues should be addressed in the final manuscript.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>In addition to the comments in Field 4 (above), please see the following:</p>

      <p>Evaluation:</p>
      <ul>
        <li>The results presented in Tables 1, 2, 3 should be explicitly evaluated for statistical significance.</li>
      </ul>

      <p>Parameter sensitivity</p>
      <ul>
        <li>Equation 1 introduces two parameters: delta (=2) and lambda (=5).  How sensitive is the overall performance of the pipeline to the choice of these parameters?</li>
      </ul>

      <p>Minor edits:</p>
      <ul>
        <li>Last sentence in the Intro: “…, giving the high usability in real-world clinical practices.”  Seems to be an incomplete statement.</li>
        <li>Figure 4: “mental artifacts” should be “metal artifacts”</li>
        <li>Last sentence in Results: “mental artifacts” should be “metal artifacts”.</li>
      </ul>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper is well reasoned, and the presentation is excellent.
The proposed method is evaluated, both against SOTA methods, and through a careful ablation analysis.
The final results of the method are excellent.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper provides a new two-stage framework for vertebral localization and identification of CT images. The framework first uses a weighted voting location network to detect the center of the vertebrae and then designs an identification network to identify each vertebrae category in leveraging the synergy of global and local information. Specifically, a bidirectional relation module is designed to learn the global correlation among vertebrae along with the upward and downward directions, and a continuous label map with dense annotation is employed to enhance the feature learning in local vertebra patches.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>This work makes good use of the top-down sequence relationship of the spine when identifying vertebrae categories. This provides an interesting method for classifying objects based on their label relations.
This work provides extensive experiments to prove the effectiveness of the detection framework by a large data set containing 1000 chest CT images. The inter and intra comparison experiments are well done.
The English writing of this paper is good.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>Firstly, the authors do not clearly explain why the existing literature is not capable of detecting vertebrae, which may not convince the readers about the novelty of this paper. 
Secondly, some important implementation details are missing in the methodology part, which harms the logical flow of the paper. Also thirdly, the necessity and principles of some methods are not clearly explained. These issues make the paper very hard to follow.
Please see question 7 for details for the above-mentioned issues.
Lastly, the reference indices are wrong. The first reference should be [1], not [5].</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The reproducibility of this article is ordinary. The paper provides the setting value of each hyperparameter, and the form of the loss function is also given in the paper. As shown in questions 4 and 7, some implementations are not explained in detail, however, this may be compensated if the authors make their codes available as stated in their Reproducibility Response.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Following question 4, I would like to give some suggestions on how to improve the significance, readability, and reproducibility of the paper, which I hope could benefit the authors for future research: 
(1) Why the existing literature is not capable of detecting vertebrae from CT images.
For example, the authors mentioned that “such a model may not fully capture the global dependency among all vertebrae and usually limit to local regions” (P2), however, it seems that reference [8] can capture global dependency using RNN’s. If the authors explain this more clearly, the significance of this paper would be more clearly explained.
(2) Some important implementation details are missing. 
For example, in Section II, how to convert heatmap H and offset map O to weighted vote map M? It seems that O provides a location where heatmap value should be mapped to, however, the readers are still confused about how this is conducted because there are no in-depth discussions. What is the shape of O? What does the value in each element of O tensor mean? Also, what if the O value points to a location that is out of the range ? In this case, how to guarantee the reliability of the predictions? (for example, if the image height and width are normalized to 1, what if the network gives an O value of 1.2? Do you just clip it to 1? Although I have not tried by myself, but this is intuitively prone to yield wrong localization results)
Similar issues happen in the other modules, e.g., what are the inputs and outputs to the fast peak search clustering method? What are the shapes of p_i and how are corresponding f_i extracted? How does the Post-label Voting algorithm function? … All these are not clearly mentioned, which is not conducive to the reproducibility of the work.
(3) The necessity and principles of some methods. 
For example, in the Vertebra Proposal Generation part: a) Why not simply use methods such as RPN for generating vertebra proposals? b) How can a Gaussian filter be used to generate image patches? To the best of my knowledge, Gaussian filters are used for low-pass filtering or high-pass filtering in the image processing community. So, how can they be used to generate patches?
Similar issues happen in the other modules, e.g., why can the Bidirectional Relation Module capture global dependency which can’t be captured by RNN modules (as mentioned in the paper)? How is the floating label obtained? How do they interact with the predicted label generated by the FC layers? What if the predicted floating labels are wrong? If the authors explain this more clearly, the readability of this paper would be more clearly explained.</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>In general, the authors have made some efforts in collecting data, programming and writing this paper. Also, I know that the acceptance of this paper may be important to the authors, thus, I would like to rate “borderline accept (6)” despite the novelty, readability, and reproducibility issues listed in questions 4 and 7. However, to be frank, this paper still needs a lot of modification before it can be published. Also, hope that the authors could upload the codes and data as they promised in their Reproducibility Response if the paper is accepted.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper presents a method for localization and identification of vertebrae from CT using a two stage framework according to these two tasks. For each of the tasks, the paper presents several dedicated adaptations such as a bi-directional search module and a continuous label map to support labelling. Results are presented on a comparable large (1000 cases) but not publicly available cohort which limits comparison of the method to others. Still, encouraging results are presented on quite challenging cases.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>Overall, the paper presents an interesting approach to a still relevant but intensively addressed application. The method is sound and contains many interesting extensions. At the same time, given the comprehensive framework with its several extensions carefully designed to fit the purpose, some components are hard to get (probably due to the limited space).</p>

      <p>The ablation study is nice to really see the benefit of the different modules.</p>

      <p>Nice to have evaluation per spine region, but surprising to see performance is best in the cervical region which is usually most challenging.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The clinical problem is still highly relevant, although several work has been presented very recently. For that reason, I find it extremely important to know performance of the method compared to others. Unfortunately, authors have decided to not use a public cohort. Personally, I am wondering why e.g. VERSE 20 data set has not been considered.</p>

      <p>The approach has been compared to other state of the art method. However, it remains unclear how the methods have been obtained/implemented. I suspect authors are contributors to benchmark approaches as otherwise a fair comparison is probably not possible. Have methods been used out-of the box or any re-training, parameter adaptation etc been applied? It would be great to get a feeling why performance of current framework is better than previous ones - is it better design or just more training data/better parameter adjustment.</p>

      <p>It would have been great to have even a more detailed evaluation or more description about the data. This is important to judge how challenging the data set is.</p>

    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Although the method has been applied to a quite sizeable number of cases (1000), reproducibility is limited as the a non-public cohort has been used. Comparison to other methods is done, although it is unclear how implementations were obtained. In general, the level of detail provided is not sufficient for re-implementation of the method. Code will not be made available.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>I missed information about the data and the achieved results in the abstract. At the same time, the abstract already contained many information about the framework which were hard to grasp from the abstract.</p>

      <p>In the intro you write “be pathological or with metal artifacts” - just minor, but I found it very closely related as for me metal artifacts are also caused by pathologies.</p>

      <p>It would be really good to have more infos about the data? What pathologies with what incidences, FOV etc…</p>

      <p>Would be good to see associated error in Fig. 4</p>

      <p>Please clarify the post-label voting. Maybe implementation details could help here. I found it pretty high level.</p>

      <p>What is the runtime during inference?</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline accept (6)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Overall, I found the paper interesting to read with interesting components. The paper is evaluated nicely (although still there are also some limitations). One of the major limitations for me is the use of a separate not public cohort. As a result, it really remains challenging to judge performance of the method, especially for a problem that has been addressed intensively in the recent years. Still, I find the presented results encouraging on at least selected very challenging cases.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>Dear Authors, I am happy to inform you that the reviewers agreed that your work meets the quality needed to be accepted at the MICCAI conference.</p>

      <p>Localization and identification of vertebrae is a challenging task mainly due to the similar appearance of the vertebrae in the image. On a large in-house dataset of 1000 CT images, the authors have shown that by leveraging both local and global information the number of failure cases can be reduced.</p>

      <p>Despite the positive evaluation, the reviewers also expressed several concerns that authors should address in their camera-ready version of the manuscript. Although I agree with the reviewers that evaluation of the method on the VerSe20 challenge dataset would better justify the authors’ claim in outperforming SOTA methods, we cannot expect the missing experiment to be performed in the rebuttal phase. Newertheless, authors could discuss how their method goes beyond the top-ranked methods at the VerSe19 and VerSe20 challenges. They should put special attention to those methods of the challenge that also explore local and global information for vertebral localization and identification. Moreover, this comparison will also strengthen the motivation section and clarify the limitations of existing methods. As much as possible due to page limitations, the authors should provide more information on the representativeness of the dataset and clarify the method section as requested by reviewers.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>2</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>N/A</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0526-12-31
      -->
      <!--
      
        ,
        updated at 
        0527-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a> |
      
      <a 
        href="kittywong/categories#Clinical applications - Breast"
        class="post-category">
        Clinical applications - Breast
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - CT"
        class="post-category">
        Modalities - CT
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Cui, Zhiming"
        class="post-category">
        Cui, Zhiming
      </a> |  
      
      <a href="kittywong/tags#Li, Changjian"
        class="post-category">
        Li, Changjian
      </a> |  
      
      <a href="kittywong/tags#Yang, Lei"
        class="post-category">
        Yang, Lei
      </a> |  
      
      <a href="kittywong/tags#Lian, Chunfeng"
        class="post-category">
        Lian, Chunfeng
      </a> |  
      
      <a href="kittywong/tags#Shi, Feng"
        class="post-category">
        Shi, Feng
      </a> |  
      
      <a href="kittywong/tags#Wang, Wenping"
        class="post-category">
        Wang, Wenping
      </a> |  
      
      <a href="kittywong/tags#Wu, Dijia"
        class="post-category">
        Wu, Dijia
      </a> |  
      
      <a href="kittywong/tags#Shen, Dinggang"
        class="post-category">
        Shen, Dinggang
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0527/12/31/Paper0837">
          VinDr-SpineXR: A deep learning framework for spinal lesions detection and classification from radiographs
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0525/12/31/Paper0754">
          Learning from Subjective Ratings Using Auto-Decoded Deep Latent Embeddings
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
