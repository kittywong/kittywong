<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Developmental Stage Classification of Embryos Using Two-Stream Neural Network with Linear-Chain Conditional Random Field | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Developmental Stage Classification of Embryos Using Two-Stream Neural Network with Linear-Chain Conditional Random Field" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Stanislav Lukyanenko, Won-Dong Jang, Donglai Wei, Robbert Struyven, Yoon Kim, Brian Leahy, Helen Yang, Alexander Rush, Dalit Ben-Yosef, Daniel Needleman, Hanspeter Pfister Abstract The developmental process of embryos follows a monotonic order. An embryo can progressively cleave from one cell to multiple cells and finally transform to morula and blastocyst. For time-lapse videos of embryos, most existing developmental stage classification methods conduct per-frame predictions using an image frame at each time step. However, classification using only images suffers from overlapping between cells and imbalance between stages. Temporal information can be valuable in addressing this problem by capturing movements between neighboring frames. In this work, we propose a two-stream model for developmental stage classification. Unlike previous methods, our two-stream model accepts both temporal and image information. We develop a linear-chain conditional random field (CRF) on top of neural network features extracted from the temporal and image streams to make use of both modalities. The linear-chain CRF formulation enables tractable training of global sequential models over multiple frames while also making it possible to inject monotonic development order constraints into the learning process explicitly. We demonstrate our algorithm on two time-lapse embryo video datasets: i) mouse and ii) human embryo datasets. Our method achieves 98.1% and 80.6% for mouse and human embryo stage classification, respectively. Our approach will enable more profound clinical and biological studies and suggests a new direction for developmental stage classification by utilizing temporal information. Link to paper https://doi.org/10.1007/978-3-030-87237-3_35 Link to the code repository https://github.com/stlukyanenko/lc-crf-embryo-classification Link to the dataset(s) http://celltracking.bio.nyu.edu/ Reviews Review #1 Please describe the contribution of the paper The authors present a Machine Learning model for embryonic staging from time-lapse videos. The method consists of a two-stream model to use image information for stage classification but to also incorporate temporal information to detect changes. This information is then combined within a CRF model that enables the incorporation of additional constraints (such as monotonicity). The methods performs very well in the validation on mouse and human videos presented in the manuscript. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper presents a strong method to classify stages from videos of development. Such a tool can be of great interest to many different applications in developmental biology (apart from the use-case given in the paper) and even beyond. The presentation of the paper is excellent. The story is well written and well motivated. The authors give all necessary technical details. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. I do not see a major weakness of this paper. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I would rate the reprocubility of the paper as very high. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I just have two minor questions: Footnote 1 says “Since the potentials in a CRF do not need to be log probabilities, normalization via the softmax function is not strictly necessary.” Did you really mean log probabilities here or just probabilities? Maybe I’ve been missing something here. On p. 6 when explaining the training, the authors mention: “To construct a batch, we randomly sample 50 frames from each video in a consecutive order.” Just to be clear: Does it mean you sample a starting point k and then take the frames in [k, k+50] or do you really sample 50 frames randomly (as long as they remain ordered in the end)? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It is one of the best papers in my stack. I have no doubt that the presented method will be of interest to the community. I see a lot of applications in developmental biology in general, where stage classification is a common problem. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The paper describes a method for automatic staging of microscopy images of early embryos (here: mouse and human embryos) that is of importance for in vitro fertilization. The method reasonably combines deep learning and a classical method to come up with the stage prediction. The obtained results are promising and outperform the state-of-the-art in most scenarios. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The work presents a combination of CNNs and a linear-chain CRF that elegantly exploits the strengths of both approaches. Exploiting both the spatial and the temporal domain for the stage prediction is clearly beneficial and the linear-chain CRF allows to constrain the model predictions with reasonable assumptions about the acquisition process of the image data. While the obtained results are not significantly different, the ablation studies indicate that all of the design choices are well-justified and yield the best-performing method. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors state that “most” other methods don’t use the temporal information many times in the paper. I think this should not be stressed that boldly, as already two of the state-of-the-art methods that are compared to their method indeed use the time domain as well (in a different fashion, however). It is not fully clear how the ResNet50 is used. Is this model retrained on this particular data set or do you use a network pretrained on ImageNet or the like? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Methods are clearly described, used data sets are from public repositories and the evaluation scores are also clearly described. Thus, I think the findings should defintely be reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Except of the unclear usage of the ResNet50 as stated above, I think this is a solid conference contribution without much to add/change. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? While the individual methods are not new per se, the authors present a valuable combination of a state-or-the-art deep learning approach with established CRF-based method. The combination seems to be well-suited for the particular application of stage classification and outperforms the existing methods (at least for the most part). Overall, a solid conference contribution. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Somewhat confident Review #3 Please describe the contribution of the paper This paper combine an image model, transition detector and a CRF model together to tackle the problem of Embryo classification in videos. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The proposed model take in both temporal and image as input. By capturing the temporal movements between two frames, the performance is better than single-image model. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. In transition detector, why feeding two consecutive frames but not more frames? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Yes Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html My only concern is the two consecutive frames in transition detector. To my understanding, it is possible that the transition from one frame to another is not obvious. So maybe adding more frames will help. The author could do some experiments in the future. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is well-written and has enough experiments to support the work. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper uses two streams of inputs (image and temporal motion) for neural networks to extract features which are then input to a CRF to classify the stages of embryos. The reviewers support the proposed method as a good tool application in developmental biology. The paper is well written. Though individual components of the proposed method are not new, the combination convinced the reviewers. The reviewers also brought up a few comments/questions to improve the paper: more ablation studies on the temporal motion input; fair comparison with other method (e.g., fine-tune ResNet50 on the datasets); incorrect claims (e.g., most other methods do not use temporal information); and some details on the training. In addition to the reviewers’ comments, the AC has a few more: Multi-stream bidirectional LSTM has been widely used in the computer vision community for action/event detection. What is the advantage of the proposed “network-based feature extraction + CRF” method compared to those multi-stream (bidirectional) LSTM? There is no ablation study in the paper to show the effectiveness of the two streams compared with single stream (i.e., image vs. motion vs. image + motion). There is no ablation study to show the effectiveness of the CRF. For example, if replacing the CRF by a (bidirectional) LSTM using the same multi-stream inputs, what will its performance be? What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We thank all of the reviewers for their constructive comments. [Reviewer1-Q1] Clarification of Footnote 1 (probabilities or log-probabilities) A: Thank you for pointing this out. We meant probabilities. We will correct the footnote accordingly. [Reviewer1-Q2] Details of sampling strategy A: We have randomly sampled 50 frames from the entire video but have sorted them sequentially when training. [Reviewer2-Q1] Clarification of ResNet50 used in experiments A: We have used ResNet50 pretrained on ImageNet. [Reviewer3-Q1] Different number of input frames for the transition detector A: When sampling frames for training, rare stages (e.g., 3 cells or 5-7 cells) may be present only on 1-2 frames. Accepting more frames in our transition detector might make the detection of these rare transitions harder. Nonetheless, we appreciate your suggestion and will experiment with more input frames for the transition detector in the future. [MetaReviewer-Q1] What is the advantage of the proposed method compared to LSTM? A: Our method allows imposing additional temporal constraints on monotonicity, which is not available with LSTM. In addition to that, our CRF-based method is easily interpretable, as it provides both unary and pairwise potentials. We will make it clear in the camera-ready version. [MetaReviewer-Q2] Effectiveness of the two streams compared with single stream A: As shown in many video classification works, motion information is quite valuable. We agree that using motion information could improve the performance of stage classification methods. However, we think that is out of scope in this work, as we aim to show the effectiveness of the linear chain CRF rather than compare different input modalities. [MetaReviewer-Q3] What would the performance be if LSTM replaces CRF? A: We can replace CRF with LSTM by accepting the concatenation of features from the classifier and the transition detector as its input. While CRF explicitly injects the monotonicity into the model using pairwise potentials, it is not available to force LSTM to use the transition information for monotonic predictions. As such, CRF may perform better than LSTM when consecutive frames have mixed predictions. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Stanislav Lukyanenko, Won-Dong Jang, Donglai Wei, Robbert Struyven, Yoon Kim, Brian Leahy, Helen Yang, Alexander Rush, Dalit Ben-Yosef, Daniel Needleman, Hanspeter Pfister Abstract The developmental process of embryos follows a monotonic order. An embryo can progressively cleave from one cell to multiple cells and finally transform to morula and blastocyst. For time-lapse videos of embryos, most existing developmental stage classification methods conduct per-frame predictions using an image frame at each time step. However, classification using only images suffers from overlapping between cells and imbalance between stages. Temporal information can be valuable in addressing this problem by capturing movements between neighboring frames. In this work, we propose a two-stream model for developmental stage classification. Unlike previous methods, our two-stream model accepts both temporal and image information. We develop a linear-chain conditional random field (CRF) on top of neural network features extracted from the temporal and image streams to make use of both modalities. The linear-chain CRF formulation enables tractable training of global sequential models over multiple frames while also making it possible to inject monotonic development order constraints into the learning process explicitly. We demonstrate our algorithm on two time-lapse embryo video datasets: i) mouse and ii) human embryo datasets. Our method achieves 98.1% and 80.6% for mouse and human embryo stage classification, respectively. Our approach will enable more profound clinical and biological studies and suggests a new direction for developmental stage classification by utilizing temporal information. Link to paper https://doi.org/10.1007/978-3-030-87237-3_35 Link to the code repository https://github.com/stlukyanenko/lc-crf-embryo-classification Link to the dataset(s) http://celltracking.bio.nyu.edu/ Reviews Review #1 Please describe the contribution of the paper The authors present a Machine Learning model for embryonic staging from time-lapse videos. The method consists of a two-stream model to use image information for stage classification but to also incorporate temporal information to detect changes. This information is then combined within a CRF model that enables the incorporation of additional constraints (such as monotonicity). The methods performs very well in the validation on mouse and human videos presented in the manuscript. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper presents a strong method to classify stages from videos of development. Such a tool can be of great interest to many different applications in developmental biology (apart from the use-case given in the paper) and even beyond. The presentation of the paper is excellent. The story is well written and well motivated. The authors give all necessary technical details. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. I do not see a major weakness of this paper. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I would rate the reprocubility of the paper as very high. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I just have two minor questions: Footnote 1 says “Since the potentials in a CRF do not need to be log probabilities, normalization via the softmax function is not strictly necessary.” Did you really mean log probabilities here or just probabilities? Maybe I’ve been missing something here. On p. 6 when explaining the training, the authors mention: “To construct a batch, we randomly sample 50 frames from each video in a consecutive order.” Just to be clear: Does it mean you sample a starting point k and then take the frames in [k, k+50] or do you really sample 50 frames randomly (as long as they remain ordered in the end)? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It is one of the best papers in my stack. I have no doubt that the presented method will be of interest to the community. I see a lot of applications in developmental biology in general, where stage classification is a common problem. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The paper describes a method for automatic staging of microscopy images of early embryos (here: mouse and human embryos) that is of importance for in vitro fertilization. The method reasonably combines deep learning and a classical method to come up with the stage prediction. The obtained results are promising and outperform the state-of-the-art in most scenarios. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The work presents a combination of CNNs and a linear-chain CRF that elegantly exploits the strengths of both approaches. Exploiting both the spatial and the temporal domain for the stage prediction is clearly beneficial and the linear-chain CRF allows to constrain the model predictions with reasonable assumptions about the acquisition process of the image data. While the obtained results are not significantly different, the ablation studies indicate that all of the design choices are well-justified and yield the best-performing method. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors state that “most” other methods don’t use the temporal information many times in the paper. I think this should not be stressed that boldly, as already two of the state-of-the-art methods that are compared to their method indeed use the time domain as well (in a different fashion, however). It is not fully clear how the ResNet50 is used. Is this model retrained on this particular data set or do you use a network pretrained on ImageNet or the like? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Methods are clearly described, used data sets are from public repositories and the evaluation scores are also clearly described. Thus, I think the findings should defintely be reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Except of the unclear usage of the ResNet50 as stated above, I think this is a solid conference contribution without much to add/change. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? While the individual methods are not new per se, the authors present a valuable combination of a state-or-the-art deep learning approach with established CRF-based method. The combination seems to be well-suited for the particular application of stage classification and outperforms the existing methods (at least for the most part). Overall, a solid conference contribution. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Somewhat confident Review #3 Please describe the contribution of the paper This paper combine an image model, transition detector and a CRF model together to tackle the problem of Embryo classification in videos. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The proposed model take in both temporal and image as input. By capturing the temporal movements between two frames, the performance is better than single-image model. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. In transition detector, why feeding two consecutive frames but not more frames? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Yes Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html My only concern is the two consecutive frames in transition detector. To my understanding, it is possible that the transition from one frame to another is not obvious. So maybe adding more frames will help. The author could do some experiments in the future. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is well-written and has enough experiments to support the work. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper uses two streams of inputs (image and temporal motion) for neural networks to extract features which are then input to a CRF to classify the stages of embryos. The reviewers support the proposed method as a good tool application in developmental biology. The paper is well written. Though individual components of the proposed method are not new, the combination convinced the reviewers. The reviewers also brought up a few comments/questions to improve the paper: more ablation studies on the temporal motion input; fair comparison with other method (e.g., fine-tune ResNet50 on the datasets); incorrect claims (e.g., most other methods do not use temporal information); and some details on the training. In addition to the reviewers’ comments, the AC has a few more: Multi-stream bidirectional LSTM has been widely used in the computer vision community for action/event detection. What is the advantage of the proposed “network-based feature extraction + CRF” method compared to those multi-stream (bidirectional) LSTM? There is no ablation study in the paper to show the effectiveness of the two streams compared with single stream (i.e., image vs. motion vs. image + motion). There is no ablation study to show the effectiveness of the CRF. For example, if replacing the CRF by a (bidirectional) LSTM using the same multi-stream inputs, what will its performance be? What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We thank all of the reviewers for their constructive comments. [Reviewer1-Q1] Clarification of Footnote 1 (probabilities or log-probabilities) A: Thank you for pointing this out. We meant probabilities. We will correct the footnote accordingly. [Reviewer1-Q2] Details of sampling strategy A: We have randomly sampled 50 frames from the entire video but have sorted them sequentially when training. [Reviewer2-Q1] Clarification of ResNet50 used in experiments A: We have used ResNet50 pretrained on ImageNet. [Reviewer3-Q1] Different number of input frames for the transition detector A: When sampling frames for training, rare stages (e.g., 3 cells or 5-7 cells) may be present only on 1-2 frames. Accepting more frames in our transition detector might make the detection of these rare transitions harder. Nonetheless, we appreciate your suggestion and will experiment with more input frames for the transition detector in the future. [MetaReviewer-Q1] What is the advantage of the proposed method compared to LSTM? A: Our method allows imposing additional temporal constraints on monotonicity, which is not available with LSTM. In addition to that, our CRF-based method is easily interpretable, as it provides both unary and pairwise potentials. We will make it clear in the camera-ready version. [MetaReviewer-Q2] Effectiveness of the two streams compared with single stream A: As shown in many video classification works, motion information is quite valuable. We agree that using motion information could improve the performance of stage classification methods. However, we think that is out of scope in this work, as we aim to show the effectiveness of the linear chain CRF rather than compare different input modalities. [MetaReviewer-Q3] What would the performance be if LSTM replaces CRF? A: We can replace CRF with LSTM by accepting the concatenation of features from the classifier and the transition detector as its input. While CRF explicitly injects the monotonicity into the model using pairwise potentials, it is not available to force LSTM to use the transition information for monotonic predictions. As such, CRF may perform better than LSTM when consecutive frames have mixed predictions. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0834/12/31/Paper0437" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0834/12/31/Paper0437" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0834-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Developmental Stage Classification of Embryos Using Two-Stream Neural Network with Linear-Chain Conditional Random Field" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0834/12/31/Paper0437"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0834/12/31/Paper0437","headline":"Developmental Stage Classification of Embryos Using Two-Stream Neural Network with Linear-Chain Conditional Random Field","dateModified":"0835-01-05T00:00:00-05:17","datePublished":"0834-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Stanislav Lukyanenko, Won-Dong Jang, Donglai Wei, Robbert Struyven, Yoon Kim, Brian Leahy, Helen Yang, Alexander Rush, Dalit Ben-Yosef, Daniel Needleman, Hanspeter Pfister Abstract The developmental process of embryos follows a monotonic order. An embryo can progressively cleave from one cell to multiple cells and finally transform to morula and blastocyst. For time-lapse videos of embryos, most existing developmental stage classification methods conduct per-frame predictions using an image frame at each time step. However, classification using only images suffers from overlapping between cells and imbalance between stages. Temporal information can be valuable in addressing this problem by capturing movements between neighboring frames. In this work, we propose a two-stream model for developmental stage classification. Unlike previous methods, our two-stream model accepts both temporal and image information. We develop a linear-chain conditional random field (CRF) on top of neural network features extracted from the temporal and image streams to make use of both modalities. The linear-chain CRF formulation enables tractable training of global sequential models over multiple frames while also making it possible to inject monotonic development order constraints into the learning process explicitly. We demonstrate our algorithm on two time-lapse embryo video datasets: i) mouse and ii) human embryo datasets. Our method achieves 98.1% and 80.6% for mouse and human embryo stage classification, respectively. Our approach will enable more profound clinical and biological studies and suggests a new direction for developmental stage classification by utilizing temporal information. Link to paper https://doi.org/10.1007/978-3-030-87237-3_35 Link to the code repository https://github.com/stlukyanenko/lc-crf-embryo-classification Link to the dataset(s) http://celltracking.bio.nyu.edu/ Reviews Review #1 Please describe the contribution of the paper The authors present a Machine Learning model for embryonic staging from time-lapse videos. The method consists of a two-stream model to use image information for stage classification but to also incorporate temporal information to detect changes. This information is then combined within a CRF model that enables the incorporation of additional constraints (such as monotonicity). The methods performs very well in the validation on mouse and human videos presented in the manuscript. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper presents a strong method to classify stages from videos of development. Such a tool can be of great interest to many different applications in developmental biology (apart from the use-case given in the paper) and even beyond. The presentation of the paper is excellent. The story is well written and well motivated. The authors give all necessary technical details. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. I do not see a major weakness of this paper. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I would rate the reprocubility of the paper as very high. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I just have two minor questions: Footnote 1 says “Since the potentials in a CRF do not need to be log probabilities, normalization via the softmax function is not strictly necessary.” Did you really mean log probabilities here or just probabilities? Maybe I’ve been missing something here. On p. 6 when explaining the training, the authors mention: “To construct a batch, we randomly sample 50 frames from each video in a consecutive order.” Just to be clear: Does it mean you sample a starting point k and then take the frames in [k, k+50] or do you really sample 50 frames randomly (as long as they remain ordered in the end)? Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? It is one of the best papers in my stack. I have no doubt that the presented method will be of interest to the community. I see a lot of applications in developmental biology in general, where stage classification is a common problem. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper The paper describes a method for automatic staging of microscopy images of early embryos (here: mouse and human embryos) that is of importance for in vitro fertilization. The method reasonably combines deep learning and a classical method to come up with the stage prediction. The obtained results are promising and outperform the state-of-the-art in most scenarios. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The work presents a combination of CNNs and a linear-chain CRF that elegantly exploits the strengths of both approaches. Exploiting both the spatial and the temporal domain for the stage prediction is clearly beneficial and the linear-chain CRF allows to constrain the model predictions with reasonable assumptions about the acquisition process of the image data. While the obtained results are not significantly different, the ablation studies indicate that all of the design choices are well-justified and yield the best-performing method. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors state that “most” other methods don’t use the temporal information many times in the paper. I think this should not be stressed that boldly, as already two of the state-of-the-art methods that are compared to their method indeed use the time domain as well (in a different fashion, however). It is not fully clear how the ResNet50 is used. Is this model retrained on this particular data set or do you use a network pretrained on ImageNet or the like? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Methods are clearly described, used data sets are from public repositories and the evaluation scores are also clearly described. Thus, I think the findings should defintely be reproducible. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Except of the unclear usage of the ResNet50 as stated above, I think this is a solid conference contribution without much to add/change. Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? While the individual methods are not new per se, the authors present a valuable combination of a state-or-the-art deep learning approach with established CRF-based method. The combination seems to be well-suited for the particular application of stage classification and outperforms the existing methods (at least for the most part). Overall, a solid conference contribution. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 3 Reviewer confidence Somewhat confident Review #3 Please describe the contribution of the paper This paper combine an image model, transition detector and a CRF model together to tackle the problem of Embryo classification in videos. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The proposed model take in both temporal and image as input. By capturing the temporal movements between two frames, the performance is better than single-image model. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. In transition detector, why feeding two consecutive frames but not more frames? Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Yes Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html My only concern is the two consecutive frames in transition detector. To my understanding, it is possible that the transition from one frame to another is not obvious. So maybe adding more frames will help. The author could do some experiments in the future. Please state your overall opinion of the paper strong accept (9) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The paper is well-written and has enough experiments to support the work. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Very confident Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. This paper uses two streams of inputs (image and temporal motion) for neural networks to extract features which are then input to a CRF to classify the stages of embryos. The reviewers support the proposed method as a good tool application in developmental biology. The paper is well written. Though individual components of the proposed method are not new, the combination convinced the reviewers. The reviewers also brought up a few comments/questions to improve the paper: more ablation studies on the temporal motion input; fair comparison with other method (e.g., fine-tune ResNet50 on the datasets); incorrect claims (e.g., most other methods do not use temporal information); and some details on the training. In addition to the reviewers’ comments, the AC has a few more: Multi-stream bidirectional LSTM has been widely used in the computer vision community for action/event detection. What is the advantage of the proposed “network-based feature extraction + CRF” method compared to those multi-stream (bidirectional) LSTM? There is no ablation study in the paper to show the effectiveness of the two streams compared with single stream (i.e., image vs. motion vs. image + motion). There is no ablation study to show the effectiveness of the CRF. For example, if replacing the CRF by a (bidirectional) LSTM using the same multi-stream inputs, what will its performance be? What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We thank all of the reviewers for their constructive comments. [Reviewer1-Q1] Clarification of Footnote 1 (probabilities or log-probabilities) A: Thank you for pointing this out. We meant probabilities. We will correct the footnote accordingly. [Reviewer1-Q2] Details of sampling strategy A: We have randomly sampled 50 frames from the entire video but have sorted them sequentially when training. [Reviewer2-Q1] Clarification of ResNet50 used in experiments A: We have used ResNet50 pretrained on ImageNet. [Reviewer3-Q1] Different number of input frames for the transition detector A: When sampling frames for training, rare stages (e.g., 3 cells or 5-7 cells) may be present only on 1-2 frames. Accepting more frames in our transition detector might make the detection of these rare transitions harder. Nonetheless, we appreciate your suggestion and will experiment with more input frames for the transition detector in the future. [MetaReviewer-Q1] What is the advantage of the proposed method compared to LSTM? A: Our method allows imposing additional temporal constraints on monotonicity, which is not available with LSTM. In addition to that, our CRF-based method is easily interpretable, as it provides both unary and pairwise potentials. We will make it clear in the camera-ready version. [MetaReviewer-Q2] Effectiveness of the two streams compared with single stream A: As shown in many video classification works, motion information is quite valuable. We agree that using motion information could improve the performance of stage classification methods. However, we think that is out of scope in this work, as we aim to show the effectiveness of the linear chain CRF rather than compare different input modalities. [MetaReviewer-Q3] What would the performance be if LSTM replaces CRF? A: We can replace CRF with LSTM by accepting the concatenation of features from the classifier and the transition detector as its input. While CRF explicitly injects the monotonicity into the model using pairwise potentials, it is not available to force LSTM to use the transition information for monotonic predictions. As such, CRF may perform better than LSTM when consecutive frames have mixed predictions. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Lukyanenko, Stanislav,Jang, Won-Dong,Wei, Donglai,Struyven, Robbert,Kim, Yoon,Leahy, Brian,Yang, Helen,Rush, Alexander,Ben-Yosef, Dalit,Needleman, Daniel,Pfister, Hanspeter" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Developmental Stage Classification of Embryos Using Two-Stream Neural Network with Linear-Chain Conditional Random Field</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Modalities - Microscopy"
        class="post-category">
        Modalities - Microscopy
      </a>
      
      <a 
        href="kittywong/categories#Modalities - Video"
        class="post-category">
        Modalities - Video
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Lukyanenko, Stanislav"
        class="post-tags">
        Lukyanenko, Stanislav
      </a> |  
      
      <a href="kittywong/tags#Jang, Won-Dong"
        class="post-tags">
        Jang, Won-Dong
      </a> |  
      
      <a href="kittywong/tags#Wei, Donglai"
        class="post-tags">
        Wei, Donglai
      </a> |  
      
      <a href="kittywong/tags#Struyven, Robbert"
        class="post-tags">
        Struyven, Robbert
      </a> |  
      
      <a href="kittywong/tags#Kim, Yoon"
        class="post-tags">
        Kim, Yoon
      </a> |  
      
      <a href="kittywong/tags#Leahy, Brian"
        class="post-tags">
        Leahy, Brian
      </a> |  
      
      <a href="kittywong/tags#Yang, Helen"
        class="post-tags">
        Yang, Helen
      </a> |  
      
      <a href="kittywong/tags#Rush, Alexander"
        class="post-tags">
        Rush, Alexander
      </a> |  
      
      <a href="kittywong/tags#Ben-Yosef, Dalit"
        class="post-tags">
        Ben-Yosef, Dalit
      </a> |  
      
      <a href="kittywong/tags#Needleman, Daniel"
        class="post-tags">
        Needleman, Daniel
      </a> |  
      
      <a href="kittywong/tags#Pfister, Hanspeter"
        class="post-tags">
        Pfister, Hanspeter
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Stanislav Lukyanenko, Won-Dong Jang, Donglai Wei, Robbert Struyven, Yoon Kim, Brian Leahy, Helen Yang, Alexander Rush, Dalit Ben-Yosef, Daniel Needleman, Hanspeter Pfister
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>The developmental process of embryos follows a monotonic order. An embryo can progressively cleave from one cell to multiple cells and finally transform to morula and blastocyst. For time-lapse videos of embryos, most existing developmental stage classification methods conduct per-frame predictions using an image frame at each time step. However, classification using only images suffers from overlapping between cells and imbalance between stages. Temporal information can be valuable in addressing this problem by capturing movements between neighboring frames. In this work, we propose a two-stream model for developmental stage classification. Unlike previous methods, our two-stream model accepts both temporal and image information. We develop a linear-chain conditional random field (CRF) on top of neural network features extracted from the temporal and image streams to make use of both modalities. The linear-chain CRF formulation enables tractable training of global sequential models over multiple frames while also making it possible to inject monotonic development order constraints into the learning process explicitly. We demonstrate our algorithm on two time-lapse embryo video datasets: i) mouse and ii) human embryo datasets. Our method achieves 98.1% and 80.6% for mouse and human embryo stage classification, respectively. Our approach will enable more profound clinical and biological studies and suggests a new direction for developmental stage classification by utilizing temporal information.
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87237-3_35">https://doi.org/10.1007/978-3-030-87237-3_35</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>https://github.com/stlukyanenko/lc-crf-embryo-classification
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>http://celltracking.bio.nyu.edu/
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The authors present a Machine Learning model for embryonic staging from time-lapse videos. The method consists of a two-stream model to use image information for stage classification but to also incorporate temporal information to detect changes. This information is then combined within a CRF model that enables the incorporation of additional constraints (such as monotonicity). The methods performs very well in the validation on mouse and human videos presented in the manuscript.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The paper presents a strong method to classify stages from videos of development. Such a tool can be of great interest to many different applications in developmental biology (apart from the use-case given in the paper) and even beyond.</p>

      <p>The presentation of the paper is excellent. The story is well written and well motivated. The authors give all necessary technical details.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>I do not see a major weakness of this paper.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>I would rate the reprocubility of the paper as very high.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>I just have two minor questions:</p>

      <ol>
        <li>
          <p>Footnote 1 says “Since the potentials in a CRF do not need to be log probabilities, normalization via the softmax function is not strictly necessary.” Did you really mean log probabilities here or just probabilities? Maybe I’ve been missing something here.</p>
        </li>
        <li>
          <p>On p. 6 when explaining the training, the authors mention: “To construct a batch, we randomly sample 50 frames from each video in a consecutive order.” Just to be clear: Does it mean you sample a starting point k and then take the frames in [k, k+50] or do you really sample 50 frames randomly (as long as they remain ordered in the end)?</p>
        </li>
      </ol>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>It is one of the best papers in my stack. I have no doubt that the presented method will be of interest to the community. I see a lot of applications in developmental biology in general, where stage classification is a common problem.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper describes a method for automatic staging of microscopy images of early embryos (here: mouse and human embryos) that is of importance for in vitro fertilization. The method reasonably combines deep learning and a classical method to come up with the stage prediction. The obtained results are promising and outperform the state-of-the-art in most scenarios.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The work presents a combination of CNNs and a linear-chain CRF that elegantly exploits the strengths of both approaches. Exploiting both the spatial and the temporal domain for the stage prediction is clearly beneficial and the linear-chain CRF allows to constrain the model predictions with reasonable assumptions about the acquisition process of the image data. While the obtained results are not significantly different, the ablation studies indicate that all of the design choices are well-justified and yield the best-performing method.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The authors state that “most” other methods don’t use the temporal information many times in the paper. I think this should not be stressed that boldly, as already two of the state-of-the-art methods that are compared to their method indeed use the time domain as well (in a different fashion, however).</p>

      <p>It is not fully clear how the ResNet50 is used. Is this model retrained on this particular data set or do you use a network pretrained on ImageNet or the like?</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Methods are clearly described, used data sets are from public repositories and the evaluation scores are also clearly described. Thus, I think the findings should defintely be reproducible.</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Except of the unclear usage of the ResNet50 as stated above, I think this is a solid conference contribution without much to add/change.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>Probably accept (7)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>While the individual methods are not new per se, the authors present a valuable combination of a state-or-the-art deep learning approach with established CRF-based method. The combination seems to be well-suited for the particular application of stage classification and outperforms the existing methods (at least for the most part). Overall, a solid conference contribution.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Somewhat confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper combine an image model, transition detector and a CRF model together to tackle the problem of Embryo classification in videos.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>The proposed model take in both temporal and image as input. By capturing the temporal movements between two frames, the performance is better than single-image model.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>In transition detector, why feeding two consecutive frames but not more frames?</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Very Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Yes</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>My only concern is the two consecutive frames in transition detector. To my understanding, it is possible that the transition from one frame to another is not obvious. So maybe adding more frames will help. The author could do some experiments in the future.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>strong accept (9)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The paper is well-written and has enough experiments to support the work.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>This paper uses two streams of inputs (image and temporal motion) for neural networks to extract features which are then input to a CRF to classify the stages of embryos.</p>

      <p>The reviewers support the proposed method as a good tool application in developmental biology.  The paper is well written. Though individual components of the proposed method are not new, the combination convinced the reviewers.</p>

      <p>The reviewers also brought up a few comments/questions to improve the paper: more ablation studies on the temporal motion input; fair comparison with other method (e.g., fine-tune ResNet50 on the datasets); incorrect claims (e.g., most other methods do not use temporal information); and some details on the training.</p>

      <p>In addition to the reviewers’ comments, the AC has a few more:</p>
      <ol>
        <li>Multi-stream bidirectional LSTM has been widely used in the computer vision community for action/event detection. What is the advantage of the proposed “network-based feature extraction + CRF” method compared to those multi-stream (bidirectional) LSTM?</li>
        <li>There is no ablation study in the paper to show the effectiveness of the two streams compared with single stream (i.e., image vs. motion vs. image + motion).</li>
        <li>There is no ablation study to show the effectiveness of the CRF. For example, if replacing the CRF by a (bidirectional) LSTM using the same multi-stream inputs, what will its performance be?</li>
      </ol>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We thank all of the reviewers for their constructive comments.</p>

  <p>[Reviewer1-Q1] Clarification of Footnote 1 (probabilities or log-probabilities)</p>

  <p>A: Thank you for pointing this out. We meant probabilities. We will correct the footnote accordingly.</p>

  <p>[Reviewer1-Q2] Details of sampling strategy</p>

  <p>A: We have randomly sampled 50 frames from the entire video but have sorted them sequentially when training.</p>

  <p>[Reviewer2-Q1] Clarification of ResNet50 used in experiments</p>

  <p>A: We have used ResNet50 pretrained on ImageNet.</p>

  <p>[Reviewer3-Q1] Different number of input frames for the transition detector</p>

  <p>A: When sampling frames for training, rare stages (e.g., 3 cells or 5-7 cells) may be present only on 1-2 frames. Accepting more frames in our transition detector might make the detection of these rare transitions harder. Nonetheless, we appreciate your suggestion and will experiment with more input frames for the transition detector in the future.</p>

  <p>[MetaReviewer-Q1] What is the advantage of the proposed method compared to LSTM?</p>

  <p>A: Our method allows imposing additional temporal constraints on monotonicity, which is not available with LSTM. In addition to that, our CRF-based method is easily interpretable, as it provides both unary and pairwise potentials. We will make it clear in the camera-ready version.</p>

  <p>[MetaReviewer-Q2] Effectiveness of the two streams compared with single stream</p>

  <p>A: As shown in many video classification works, motion information is quite valuable. We agree that using motion information could improve the performance of stage classification methods. However, we think that is out of scope in this work, as we aim to show the effectiveness of the linear chain CRF rather than compare different input modalities.</p>

  <p>[MetaReviewer-Q3] What would the performance be if LSTM replaces CRF?</p>

  <p>A: We can replace CRF with LSTM by accepting the concatenation of features from the classifier and the transition detector as its input. While CRF explicitly injects the monotonicity into the model using pairwise potentials, it is not available to force LSTM to use the transition information for monotonic predictions. As such, CRF may perform better than LSTM when consecutive frames have mixed predictions.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0834-12-31
      -->
      <!--
      
        ,
        updated at 
        0835-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Modalities - Microscopy"
        class="post-category">
        Modalities - Microscopy
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - Video"
        class="post-category">
        Modalities - Video
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Lukyanenko, Stanislav"
        class="post-category">
        Lukyanenko, Stanislav
      </a> |  
      
      <a href="kittywong/tags#Jang, Won-Dong"
        class="post-category">
        Jang, Won-Dong
      </a> |  
      
      <a href="kittywong/tags#Wei, Donglai"
        class="post-category">
        Wei, Donglai
      </a> |  
      
      <a href="kittywong/tags#Struyven, Robbert"
        class="post-category">
        Struyven, Robbert
      </a> |  
      
      <a href="kittywong/tags#Kim, Yoon"
        class="post-category">
        Kim, Yoon
      </a> |  
      
      <a href="kittywong/tags#Leahy, Brian"
        class="post-category">
        Leahy, Brian
      </a> |  
      
      <a href="kittywong/tags#Yang, Helen"
        class="post-category">
        Yang, Helen
      </a> |  
      
      <a href="kittywong/tags#Rush, Alexander"
        class="post-category">
        Rush, Alexander
      </a> |  
      
      <a href="kittywong/tags#Ben-Yosef, Dalit"
        class="post-category">
        Ben-Yosef, Dalit
      </a> |  
      
      <a href="kittywong/tags#Needleman, Daniel"
        class="post-category">
        Needleman, Daniel
      </a> |  
      
      <a href="kittywong/tags#Pfister, Hanspeter"
        class="post-category">
        Pfister, Hanspeter
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0835/12/31/Paper0502">
          Semi-supervised Cell Detection in Time-lapse Images Using Temporal Consistency
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0833/12/31/Paper2416">
          Pay Attention with Focus: A Novel Learning Scheme for Classification of Whole Slide Images
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
