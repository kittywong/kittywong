<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Accounting for Dependencies in Deep Learning based Multiple Instance Learning for Whole Slide Imaging | MICCAI 2021 - Accepted Papers and Reviews</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Accounting for Dependencies in Deep Learning based Multiple Instance Learning for Whole Slide Imaging" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Andriy Myronenko, Ziyue Xu, Dong Yang, Holger R. Roth, Daguang Xu Abstract Multiple instance learning (MIL) is a key algorithm for classification of whole slide images (WSI). Histology WSIs can have billions of pixels, which create enormous computational and annotation challenges. Typically, such images are divided into a set of patches (a bag of instances), where only bag-level class labels are provided. Deep learning based MIL methods calculate instance features using convolutional neural network (CNN). Our proposed approach is also deep learning based, with the following two contributions: Firstly, we propose to explicitly account for dependencies between instances during training by embedding self-attention Transformer blocks to capture dependencies between instances. For example, a tumor grade may depend on the presence of several particular patterns at different locations in WSI, which requires to account for dependencies between patches. Secondly, we propose an instance-wise loss function based on instance pseudo-labels. We compare the proposed algorithm to multiple baseline methods, evaluate it on the PANDA challenge dataset, the largest publicly available WSI dataset with over 11K images, and demonstrate state-of-the-art results. Link to paper https://doi.org/10.1007/978-3-030-87237-3_32 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper A self-attention Transformer blocks is proposed to be embedded in model to capture dependencies between instances. An instance-wise loss function based on instance pseudo-labels is proposed to increase instances’ labels. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -This paper proposed to explicitly account for dependencies between instances during training by embedding self-attention Transformer blocks. -It generated instance-level pseudo label to add instance level loss supervision. -The weight attention selection strategy improved the quality of pseudo labels. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. 1. The use of transformer in the article is not new, many similar methods have been proposed both in natural image and medical image[1,2,3,4]. The difference exists in the variant of a deeper integration of the transformer with the backbone CNN, but there is still only common operation. 2. The experimental part contains experiment compared with other methods and ablation study but no analytical corroboration experiment. For example, replace transformer with non-local, the model can also learn the relationship between instances, but there is no experimental comparison. 3. This paper proposed a method to improve the performance of model, but lacks the mechanistic explanation of why the performance of the model is improved. [1]Medical Transformer: Gated Axial-Attention for Medical Image Segmentation [2]An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale [3]End-to-End Object Detection with Transformers [4]Pre-Trained Image Processing Transformer Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility is good because the description of the method details is very clear Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please further explain the difference between adding transformer and classic non-local. And further elaborate on the novelty of the article, and add analytical experiments to support the author’s point of view. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Although this article proposed a model that did not exist before the WSI task, and proved the effectiveness of its method through experiments, the analysis of the experiment was not sufficient. At the same time, the novelty of this paper is limited. The proposed method is similarity to other methods already exists, for example it is similar to the transformer usage in natural images and pseudo-label usage in semi-supervised images. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 6 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper used embedding self-attention Transformer blocks to capture dependencies between instances for explicitly account for dependencies between instances, which is able to make better use of the spatial information between patches. And Instance-wise loss based on instance pseudo-labels has also been developed for conquering gradient vanishing. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper is a novel applicatio introduced transformer into MIL to capture dependencies between instances for explicitly. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The problem of how transformer accounting for dependency among instances not thoroughly analyzed. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Since the code will not be available in the reproducibility checklist. However, the details of model have been provided in the paper, it may reproduce the results of the paper Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The paper evaluated the transformer module can help improve the performance of deep learning-based MIL for WSI classification. Whie it should give more discussion on how transformer capture dependencies between instances and more ablation experiments should perfomed to evaluate the instance-wise loss. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Experiments results gives the best performance compared with other MIL methods and have been top three winning teams of the PANDA Kaggle challenge. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper highlights a common weakness in the multiple instance learning framework which is that instances are normally assumed to be independent. To address this, the authors propose using transformer encoding blocks to achieve cross instance communication. The authors also propose using pseudo-labels during training to improve signal for training the neural network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This method of obtaining cross instance communication is novel and works well as is demonstrated by the experiments section. The paper is well written and easy to understand. The use of transformers is well motived by the literature review. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Pseudo-labeling appears to only seems to add marginal improvements to the results, an analysis deeper than just the QWK would be interesting. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The algorithm is relatively complex given the additional step of constructing pseudo labels but the majority of hyper-parameters are given Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html More analysis on why pseudo-labeling is useful during training would be interesting. Additionally, confusion matrices for the different models would also add more clarity then just the QWK. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The contribution is interesting to the community and well presented, the results show the model works better than other approaches. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper integrates visual transformer within a multiple instance learning framework to model the interdependence of the instance. Additionally, it also proposes a pseudo instance labelling scheme or conquering gradient vanishing. The paper is generally well written, with a clear explanation of the methodology motivation, network architecture, experimental results and ablation studies. The evaluation dataset is relatively large and comprehensive and the comparison is also fair (particularly to other top-ranked teams are convincing). The paper integrates visual transformer within a multiple instance learning framework to model the interdependence of the instance. Additionally, it also proposes a pseudo instance labelling scheme or conquering gradient vanishing. The paper is generally well written, with a clear explanation of the methodology motivation, network architecture, experimental results and ablation studies. The evaluation dataset is relatively large and comprehensive and the comparison is also fair (particularly to other top-ranked teams are convincing). Yet R1 and R2 has some concerns about how transformer account for dependency among instances (see R1 &amp;R2’s main weakness) and this needs to be addressed in the rebuttal. Additionally, given the complexity of the method, I think release code will greatly improve its reproducibility. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors have nicely addressed the major concerns of the reviewers and I thereby recommend the paper acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This work proposes to use transformers and attention to model dependencies between instances for multi-instance learning. The transformer is integrated with a backbone CNN to model inter-instance dependencies. Evaluation is on a large whole slide image (WSI) dataset for prostate cancer. Good performance of the proposed method is reported. There were some concerns that were raised during the review, in particular, related to the performance of non-local approaches and what the transformer approach captured / how it worked. These were, in my opinion, well addressed in the rebuttal. While there was some criticism regarding novelty in the reviews, the approach has novel aspects and shows compelling results which is appreciated. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper introduces Transformer blocks into a Multiple Instance Learning framework for Whole Slide Imaging analysis. It presents strong experimental results on the PANDA Kaggle challenge. The rebuttal adequately addresses the main concerns expressed by the reviewers. Experimental details and ablations were included, and an extended motivation to use Transformers to model instance dependencies. The only missing reference pointed out by R1 with medical images is an unpublished preprint. The other three references should be discussed in the related work review. In summary, sufficient technical novelty and competitive results on a public benchmark are adequate for acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We’d like to thank all reviewers for their valuable comments. We are grateful for their acknowledgement of the strengths and contributions of our paper. As pointed out by R3 “This method is novel and works well as is demonstrated by the experiments section”, R2 “This paper is a novel application to capture dependencies between instances”. We’d like to address how the “transformer account for dependency among instances”: Transformers were initially introduced to capture long range dependencies between words in sentences [1] and later applied to vision [2]. Whereas traditional convolutions are local operation, the self-attention block of Transformers computes attention between all combinations of tokens at a larger range directly. R2 asked to analyze ”how transformer accounting for dependency among instances”, R1 asked to explain “why the performance of the model is improved”: We have inspected the self-attention matrices (inner product of Q and K, normalized). We found that for many tested WSI pathology images, the self-attention matrices have distinct off-diagonal high value elements, indicating the higher weight for certain combinations of instances. In particular, instances with WSI tumor cells of different Gleason scores have higher off-diagonal values, indicating that such a combination is valuable for the final classification, which was captured by the transformer self-attention. We’ll add the visualization of the self-attention matrices to the supplementary materials. R1 asked to “replace transformer with non-local blocks to learn the relationship between instances”: We have re-implemented these blocks based on the official paper [3] implementation, and re-ran the experiments for various Non-Local (NL) block configurations. We considered 4 NL block types from the reference paper (Embedded Gaussian, Gaussian, Dot product and Concatenation). We have replaced the Transformer encoder blocks in our network with the NL blocks (at the end of encoder) and computed the QWK scores: NL EmbedGauss (0.947±0.035) NL Gauss (0.952±0.065) NL DotProd (0.951±0.043) NL Concat (0.943±0.023) Attention MIL(0.948±0.036) Transformer MIL [ours] (0.960±0.034) In all tests the NL blocks performance was similar only to the baseline Attention MIL method, whereas our method demonstrated noticeable improvements.We’ve also tried stacking NL blocks (4x) and embedding them in a feature pyramid, but it didn’t lead to any improvements.Even though, as pointed in non-local NN paper [3], Self-attention can be considered as one of the forms of Non-local operation (Embedded Gaussian), there are differences of its implementation in Transformers ( it’s multiheaded, and it’s followed by MLP and layernorm). We’ll add these (more detailed) additional ablation experiments to the supplementary materials. While Reviewer 1 is criticizing the limited novelty of our paper, Reviewers 2 and 3 are acknowledging our novel contributions, e.g. “this method is novel and works well as is demonstrated by the experiments section”. To the best of our knowledge, our work is the first work to introduce Transformers to obtain cross instance communication in Multiple Instance Learning, as well as the first work of its kind for pathology Whole Slide Imaging task and medical images in general. Our work also includes a novel integration of the transformer block at various levels of the feature pyramid of the backbone network. Finally, our paper considers dependencies between large image regions (224x224px); in comparison, the vision transformer [2] uses tiny 16x16px patches, which mostly represent low level features. In our work each instance includes a complete picture, a different cancer cell group. The dependency we want to capture is thus between large complete regions of several tumor subtypes. [1] Attention Is All You Need: Vaswani et al. [2] An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale: Dosovitskiy et al. [3] Non-local Neural Networks: Wang et al. back to top" />
<meta property="og:description" content="Paper Info Reviews Meta-review(s) Author Feedback Authors Andriy Myronenko, Ziyue Xu, Dong Yang, Holger R. Roth, Daguang Xu Abstract Multiple instance learning (MIL) is a key algorithm for classification of whole slide images (WSI). Histology WSIs can have billions of pixels, which create enormous computational and annotation challenges. Typically, such images are divided into a set of patches (a bag of instances), where only bag-level class labels are provided. Deep learning based MIL methods calculate instance features using convolutional neural network (CNN). Our proposed approach is also deep learning based, with the following two contributions: Firstly, we propose to explicitly account for dependencies between instances during training by embedding self-attention Transformer blocks to capture dependencies between instances. For example, a tumor grade may depend on the presence of several particular patterns at different locations in WSI, which requires to account for dependencies between patches. Secondly, we propose an instance-wise loss function based on instance pseudo-labels. We compare the proposed algorithm to multiple baseline methods, evaluate it on the PANDA challenge dataset, the largest publicly available WSI dataset with over 11K images, and demonstrate state-of-the-art results. Link to paper https://doi.org/10.1007/978-3-030-87237-3_32 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper A self-attention Transformer blocks is proposed to be embedded in model to capture dependencies between instances. An instance-wise loss function based on instance pseudo-labels is proposed to increase instances’ labels. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -This paper proposed to explicitly account for dependencies between instances during training by embedding self-attention Transformer blocks. -It generated instance-level pseudo label to add instance level loss supervision. -The weight attention selection strategy improved the quality of pseudo labels. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. 1. The use of transformer in the article is not new, many similar methods have been proposed both in natural image and medical image[1,2,3,4]. The difference exists in the variant of a deeper integration of the transformer with the backbone CNN, but there is still only common operation. 2. The experimental part contains experiment compared with other methods and ablation study but no analytical corroboration experiment. For example, replace transformer with non-local, the model can also learn the relationship between instances, but there is no experimental comparison. 3. This paper proposed a method to improve the performance of model, but lacks the mechanistic explanation of why the performance of the model is improved. [1]Medical Transformer: Gated Axial-Attention for Medical Image Segmentation [2]An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale [3]End-to-End Object Detection with Transformers [4]Pre-Trained Image Processing Transformer Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility is good because the description of the method details is very clear Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please further explain the difference between adding transformer and classic non-local. And further elaborate on the novelty of the article, and add analytical experiments to support the author’s point of view. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Although this article proposed a model that did not exist before the WSI task, and proved the effectiveness of its method through experiments, the analysis of the experiment was not sufficient. At the same time, the novelty of this paper is limited. The proposed method is similarity to other methods already exists, for example it is similar to the transformer usage in natural images and pseudo-label usage in semi-supervised images. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 6 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper used embedding self-attention Transformer blocks to capture dependencies between instances for explicitly account for dependencies between instances, which is able to make better use of the spatial information between patches. And Instance-wise loss based on instance pseudo-labels has also been developed for conquering gradient vanishing. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper is a novel applicatio introduced transformer into MIL to capture dependencies between instances for explicitly. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The problem of how transformer accounting for dependency among instances not thoroughly analyzed. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Since the code will not be available in the reproducibility checklist. However, the details of model have been provided in the paper, it may reproduce the results of the paper Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The paper evaluated the transformer module can help improve the performance of deep learning-based MIL for WSI classification. Whie it should give more discussion on how transformer capture dependencies between instances and more ablation experiments should perfomed to evaluate the instance-wise loss. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Experiments results gives the best performance compared with other MIL methods and have been top three winning teams of the PANDA Kaggle challenge. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper highlights a common weakness in the multiple instance learning framework which is that instances are normally assumed to be independent. To address this, the authors propose using transformer encoding blocks to achieve cross instance communication. The authors also propose using pseudo-labels during training to improve signal for training the neural network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This method of obtaining cross instance communication is novel and works well as is demonstrated by the experiments section. The paper is well written and easy to understand. The use of transformers is well motived by the literature review. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Pseudo-labeling appears to only seems to add marginal improvements to the results, an analysis deeper than just the QWK would be interesting. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The algorithm is relatively complex given the additional step of constructing pseudo labels but the majority of hyper-parameters are given Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html More analysis on why pseudo-labeling is useful during training would be interesting. Additionally, confusion matrices for the different models would also add more clarity then just the QWK. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The contribution is interesting to the community and well presented, the results show the model works better than other approaches. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper integrates visual transformer within a multiple instance learning framework to model the interdependence of the instance. Additionally, it also proposes a pseudo instance labelling scheme or conquering gradient vanishing. The paper is generally well written, with a clear explanation of the methodology motivation, network architecture, experimental results and ablation studies. The evaluation dataset is relatively large and comprehensive and the comparison is also fair (particularly to other top-ranked teams are convincing). The paper integrates visual transformer within a multiple instance learning framework to model the interdependence of the instance. Additionally, it also proposes a pseudo instance labelling scheme or conquering gradient vanishing. The paper is generally well written, with a clear explanation of the methodology motivation, network architecture, experimental results and ablation studies. The evaluation dataset is relatively large and comprehensive and the comparison is also fair (particularly to other top-ranked teams are convincing). Yet R1 and R2 has some concerns about how transformer account for dependency among instances (see R1 &amp;R2’s main weakness) and this needs to be addressed in the rebuttal. Additionally, given the complexity of the method, I think release code will greatly improve its reproducibility. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors have nicely addressed the major concerns of the reviewers and I thereby recommend the paper acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This work proposes to use transformers and attention to model dependencies between instances for multi-instance learning. The transformer is integrated with a backbone CNN to model inter-instance dependencies. Evaluation is on a large whole slide image (WSI) dataset for prostate cancer. Good performance of the proposed method is reported. There were some concerns that were raised during the review, in particular, related to the performance of non-local approaches and what the transformer approach captured / how it worked. These were, in my opinion, well addressed in the rebuttal. While there was some criticism regarding novelty in the reviews, the approach has novel aspects and shows compelling results which is appreciated. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper introduces Transformer blocks into a Multiple Instance Learning framework for Whole Slide Imaging analysis. It presents strong experimental results on the PANDA Kaggle challenge. The rebuttal adequately addresses the main concerns expressed by the reviewers. Experimental details and ablations were included, and an extended motivation to use Transformers to model instance dependencies. The only missing reference pointed out by R1 with medical images is an unpublished preprint. The other three references should be discussed in the related work review. In summary, sufficient technical novelty and competitive results on a public benchmark are adequate for acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We’d like to thank all reviewers for their valuable comments. We are grateful for their acknowledgement of the strengths and contributions of our paper. As pointed out by R3 “This method is novel and works well as is demonstrated by the experiments section”, R2 “This paper is a novel application to capture dependencies between instances”. We’d like to address how the “transformer account for dependency among instances”: Transformers were initially introduced to capture long range dependencies between words in sentences [1] and later applied to vision [2]. Whereas traditional convolutions are local operation, the self-attention block of Transformers computes attention between all combinations of tokens at a larger range directly. R2 asked to analyze ”how transformer accounting for dependency among instances”, R1 asked to explain “why the performance of the model is improved”: We have inspected the self-attention matrices (inner product of Q and K, normalized). We found that for many tested WSI pathology images, the self-attention matrices have distinct off-diagonal high value elements, indicating the higher weight for certain combinations of instances. In particular, instances with WSI tumor cells of different Gleason scores have higher off-diagonal values, indicating that such a combination is valuable for the final classification, which was captured by the transformer self-attention. We’ll add the visualization of the self-attention matrices to the supplementary materials. R1 asked to “replace transformer with non-local blocks to learn the relationship between instances”: We have re-implemented these blocks based on the official paper [3] implementation, and re-ran the experiments for various Non-Local (NL) block configurations. We considered 4 NL block types from the reference paper (Embedded Gaussian, Gaussian, Dot product and Concatenation). We have replaced the Transformer encoder blocks in our network with the NL blocks (at the end of encoder) and computed the QWK scores: NL EmbedGauss (0.947±0.035) NL Gauss (0.952±0.065) NL DotProd (0.951±0.043) NL Concat (0.943±0.023) Attention MIL(0.948±0.036) Transformer MIL [ours] (0.960±0.034) In all tests the NL blocks performance was similar only to the baseline Attention MIL method, whereas our method demonstrated noticeable improvements.We’ve also tried stacking NL blocks (4x) and embedding them in a feature pyramid, but it didn’t lead to any improvements.Even though, as pointed in non-local NN paper [3], Self-attention can be considered as one of the forms of Non-local operation (Embedded Gaussian), there are differences of its implementation in Transformers ( it’s multiheaded, and it’s followed by MLP and layernorm). We’ll add these (more detailed) additional ablation experiments to the supplementary materials. While Reviewer 1 is criticizing the limited novelty of our paper, Reviewers 2 and 3 are acknowledging our novel contributions, e.g. “this method is novel and works well as is demonstrated by the experiments section”. To the best of our knowledge, our work is the first work to introduce Transformers to obtain cross instance communication in Multiple Instance Learning, as well as the first work of its kind for pathology Whole Slide Imaging task and medical images in general. Our work also includes a novel integration of the transformer block at various levels of the feature pyramid of the backbone network. Finally, our paper considers dependencies between large image regions (224x224px); in comparison, the vision transformer [2] uses tiny 16x16px patches, which mostly represent low level features. In our work each instance includes a complete picture, a different cancer cell group. The dependency we want to capture is thus between large complete regions of several tumor subtypes. [1] Attention Is All You Need: Vaswani et al. [2] An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale: Dosovitskiy et al. [3] Non-local Neural Networks: Wang et al. back to top" />
<link rel="canonical" href="https://kittywong.github.io/kittywong/0831/12/31/Paper2323" />
<meta property="og:url" content="https://kittywong.github.io/kittywong/0831/12/31/Paper2323" />
<meta property="og:site_name" content="MICCAI 2021 - Accepted Papers and Reviews" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="0831-12-31T23:58:56-05:17" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Accounting for Dependencies in Deep Learning based Multiple Instance Learning for Whole Slide Imaging" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://kittywong.github.io/kittywong/0831/12/31/Paper2323"},"@type":"BlogPosting","url":"https://kittywong.github.io/kittywong/0831/12/31/Paper2323","headline":"Accounting for Dependencies in Deep Learning based Multiple Instance Learning for Whole Slide Imaging","dateModified":"0832-01-05T00:00:00-05:17","datePublished":"0831-12-31T23:58:56-05:17","description":"Paper Info Reviews Meta-review(s) Author Feedback Authors Andriy Myronenko, Ziyue Xu, Dong Yang, Holger R. Roth, Daguang Xu Abstract Multiple instance learning (MIL) is a key algorithm for classification of whole slide images (WSI). Histology WSIs can have billions of pixels, which create enormous computational and annotation challenges. Typically, such images are divided into a set of patches (a bag of instances), where only bag-level class labels are provided. Deep learning based MIL methods calculate instance features using convolutional neural network (CNN). Our proposed approach is also deep learning based, with the following two contributions: Firstly, we propose to explicitly account for dependencies between instances during training by embedding self-attention Transformer blocks to capture dependencies between instances. For example, a tumor grade may depend on the presence of several particular patterns at different locations in WSI, which requires to account for dependencies between patches. Secondly, we propose an instance-wise loss function based on instance pseudo-labels. We compare the proposed algorithm to multiple baseline methods, evaluate it on the PANDA challenge dataset, the largest publicly available WSI dataset with over 11K images, and demonstrate state-of-the-art results. Link to paper https://doi.org/10.1007/978-3-030-87237-3_32 Link to the code repository N/A Link to the dataset(s) N/A Reviews Review #1 Please describe the contribution of the paper A self-attention Transformer blocks is proposed to be embedded in model to capture dependencies between instances. An instance-wise loss function based on instance pseudo-labels is proposed to increase instances’ labels. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. -This paper proposed to explicitly account for dependencies between instances during training by embedding self-attention Transformer blocks. -It generated instance-level pseudo label to add instance level loss supervision. -The weight attention selection strategy improved the quality of pseudo labels. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. 1. The use of transformer in the article is not new, many similar methods have been proposed both in natural image and medical image[1,2,3,4]. The difference exists in the variant of a deeper integration of the transformer with the backbone CNN, but there is still only common operation. 2. The experimental part contains experiment compared with other methods and ablation study but no analytical corroboration experiment. For example, replace transformer with non-local, the model can also learn the relationship between instances, but there is no experimental comparison. 3. This paper proposed a method to improve the performance of model, but lacks the mechanistic explanation of why the performance of the model is improved. [1]Medical Transformer: Gated Axial-Attention for Medical Image Segmentation [2]An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale [3]End-to-End Object Detection with Transformers [4]Pre-Trained Image Processing Transformer Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The reproducibility is good because the description of the method details is very clear Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html Please further explain the difference between adding transformer and classic non-local. And further elaborate on the novelty of the article, and add analytical experiments to support the author’s point of view. Please state your overall opinion of the paper borderline reject (5) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Although this article proposed a model that did not exist before the WSI task, and proved the effectiveness of its method through experiments, the analysis of the experiment was not sufficient. At the same time, the novelty of this paper is limited. The proposed method is similarity to other methods already exists, for example it is similar to the transformer usage in natural images and pseudo-label usage in semi-supervised images. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 6 Reviewer confidence Very confident Review #2 Please describe the contribution of the paper This paper used embedding self-attention Transformer blocks to capture dependencies between instances for explicitly account for dependencies between instances, which is able to make better use of the spatial information between patches. And Instance-wise loss based on instance pseudo-labels has also been developed for conquering gradient vanishing. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This paper is a novel applicatio introduced transformer into MIL to capture dependencies between instances for explicitly. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The problem of how transformer accounting for dependency among instances not thoroughly analyzed. Please rate the clarity and organization of this paper Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance Since the code will not be available in the reproducibility checklist. However, the details of model have been provided in the paper, it may reproduce the results of the paper Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The paper evaluated the transformer module can help improve the performance of deep learning-based MIL for WSI classification. Whie it should give more discussion on how transformer capture dependencies between instances and more ablation experiments should perfomed to evaluate the instance-wise loss. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Experiments results gives the best performance compared with other MIL methods and have been top three winning teams of the PANDA Kaggle challenge. What is the ranking of this paper in your review stack? 3 Number of papers in your stack 5 Reviewer confidence Very confident Review #3 Please describe the contribution of the paper The paper highlights a common weakness in the multiple instance learning framework which is that instances are normally assumed to be independent. To address this, the authors propose using transformer encoding blocks to achieve cross instance communication. The authors also propose using pseudo-labels during training to improve signal for training the neural network. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. This method of obtaining cross instance communication is novel and works well as is demonstrated by the experiments section. The paper is well written and easy to understand. The use of transformers is well motived by the literature review. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Pseudo-labeling appears to only seems to add marginal improvements to the results, an analysis deeper than just the QWK would be interesting. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The algorithm is relatively complex given the additional step of constructing pseudo labels but the majority of hyper-parameters are given Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html More analysis on why pseudo-labeling is useful during training would be interesting. Additionally, confusion matrices for the different models would also add more clarity then just the QWK. Please state your overall opinion of the paper accept (8) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? The contribution is interesting to the community and well presented, the results show the model works better than other approaches. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Meta-Review(s) Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. The paper integrates visual transformer within a multiple instance learning framework to model the interdependence of the instance. Additionally, it also proposes a pseudo instance labelling scheme or conquering gradient vanishing. The paper is generally well written, with a clear explanation of the methodology motivation, network architecture, experimental results and ablation studies. The evaluation dataset is relatively large and comprehensive and the comparison is also fair (particularly to other top-ranked teams are convincing). The paper integrates visual transformer within a multiple instance learning framework to model the interdependence of the instance. Additionally, it also proposes a pseudo instance labelling scheme or conquering gradient vanishing. The paper is generally well written, with a clear explanation of the methodology motivation, network architecture, experimental results and ablation studies. The evaluation dataset is relatively large and comprehensive and the comparison is also fair (particularly to other top-ranked teams are convincing). Yet R1 and R2 has some concerns about how transformer account for dependency among instances (see R1 &amp;R2’s main weakness) and this needs to be addressed in the rebuttal. Additionally, given the complexity of the method, I think release code will greatly improve its reproducibility. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The authors have nicely addressed the major concerns of the reviewers and I thereby recommend the paper acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 4 Meta-Review #2 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. This work proposes to use transformers and attention to model dependencies between instances for multi-instance learning. The transformer is integrated with a backbone CNN to model inter-instance dependencies. Evaluation is on a large whole slide image (WSI) dataset for prostate cancer. Good performance of the proposed method is reported. There were some concerns that were raised during the review, in particular, related to the performance of non-local approaches and what the transformer approach captured / how it worked. These were, in my opinion, well addressed in the rebuttal. While there was some criticism regarding novelty in the reviews, the approach has novel aspects and shows compelling results which is appreciated. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 5 Meta-Review #3 Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores, indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision. The paper introduces Transformer blocks into a Multiple Instance Learning framework for Whole Slide Imaging analysis. It presents strong experimental results on the PANDA Kaggle challenge. The rebuttal adequately addresses the main concerns expressed by the reviewers. Experimental details and ablations were included, and an extended motivation to use Transformers to model instance dependencies. The only missing reference pointed out by R1 with medical images is an unpublished preprint. The other three references should be discussed in the related work review. In summary, sufficient technical novelty and competitive results on a public benchmark are adequate for acceptance. After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal. Accept What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 1 Author Feedback We’d like to thank all reviewers for their valuable comments. We are grateful for their acknowledgement of the strengths and contributions of our paper. As pointed out by R3 “This method is novel and works well as is demonstrated by the experiments section”, R2 “This paper is a novel application to capture dependencies between instances”. We’d like to address how the “transformer account for dependency among instances”: Transformers were initially introduced to capture long range dependencies between words in sentences [1] and later applied to vision [2]. Whereas traditional convolutions are local operation, the self-attention block of Transformers computes attention between all combinations of tokens at a larger range directly. R2 asked to analyze ”how transformer accounting for dependency among instances”, R1 asked to explain “why the performance of the model is improved”: We have inspected the self-attention matrices (inner product of Q and K, normalized). We found that for many tested WSI pathology images, the self-attention matrices have distinct off-diagonal high value elements, indicating the higher weight for certain combinations of instances. In particular, instances with WSI tumor cells of different Gleason scores have higher off-diagonal values, indicating that such a combination is valuable for the final classification, which was captured by the transformer self-attention. We’ll add the visualization of the self-attention matrices to the supplementary materials. R1 asked to “replace transformer with non-local blocks to learn the relationship between instances”: We have re-implemented these blocks based on the official paper [3] implementation, and re-ran the experiments for various Non-Local (NL) block configurations. We considered 4 NL block types from the reference paper (Embedded Gaussian, Gaussian, Dot product and Concatenation). We have replaced the Transformer encoder blocks in our network with the NL blocks (at the end of encoder) and computed the QWK scores: NL EmbedGauss (0.947±0.035) NL Gauss (0.952±0.065) NL DotProd (0.951±0.043) NL Concat (0.943±0.023) Attention MIL(0.948±0.036) Transformer MIL [ours] (0.960±0.034) In all tests the NL blocks performance was similar only to the baseline Attention MIL method, whereas our method demonstrated noticeable improvements.We’ve also tried stacking NL blocks (4x) and embedding them in a feature pyramid, but it didn’t lead to any improvements.Even though, as pointed in non-local NN paper [3], Self-attention can be considered as one of the forms of Non-local operation (Embedded Gaussian), there are differences of its implementation in Transformers ( it’s multiheaded, and it’s followed by MLP and layernorm). We’ll add these (more detailed) additional ablation experiments to the supplementary materials. While Reviewer 1 is criticizing the limited novelty of our paper, Reviewers 2 and 3 are acknowledging our novel contributions, e.g. “this method is novel and works well as is demonstrated by the experiments section”. To the best of our knowledge, our work is the first work to introduce Transformers to obtain cross instance communication in Multiple Instance Learning, as well as the first work of its kind for pathology Whole Slide Imaging task and medical images in general. Our work also includes a novel integration of the transformer block at various levels of the feature pyramid of the backbone network. Finally, our paper considers dependencies between large image regions (224x224px); in comparison, the vision transformer [2] uses tiny 16x16px patches, which mostly represent low level features. In our work each instance includes a complete picture, a different cancer cell group. The dependency we want to capture is thus between large complete regions of several tumor subtypes. [1] Attention Is All You Need: Vaswani et al. [2] An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale: Dosovitskiy et al. [3] Non-local Neural Networks: Wang et al. back to top","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<meta
  name="keywords"
  content="Myronenko, Andriy,Xu, Ziyue,Yang, Dong,Roth, Holger R.,Xu, Daguang" />

<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" href="/favicon.ico" />
<link
  rel="alternate"
  type="application/rss+xml"
  title="MICCAI 2021 - Accepted Papers and Reviews - "
  href="kittywong/feed.xml" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/base.css" />
<link
  rel="stylesheet"
  type="text/css"
  href="kittywong/assets/css/highlight.css" />

<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->


<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  async>
</script>


<script src="/assets/scripts/jekyllpaper.js" async></script>
<script
  src="https://unpkg.com/mermaid@8.5.1/dist/mermaid.min.js"
  onload="javascript:loadMermaid();"
  async>
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  </head>
  <body>
    <div class="container-wrapper">
      <header class="container-header">
        <div class="header-info">
  <!-- Added by Elvis Chen -->
  <img src="https://kittywong.github.io/assets/images/miccai2021header.jpg" alt="MICCAI banner">
  <!-- END Added by Elvis Chen -->
  <span class="header-info-name">MICCAI 2021 - Accepted Papers and Reviews</span>
  <span class="header-info-desc"></span>
</div>
<nav class="header-nav">
  <ul class="header-main-nav">
    <h2>
    
    <li class="header-main-nav-item">
      <a href="kittywong/">
        
          <b>List of Papers</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/categories">
        
          <b>By topics</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/tags">
        
          <b>Author List</b>
        
      </a>
    </li>
      
    <li class="header-main-nav-item">
      <a href="kittywong/about">
        
          <b>About</b>
        
      </a>
    </li>
      
    </h2>
  </ul>
</nav>
      </header>
      <main class="container-main">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<article class="container-post">
  <div class="post-title">
    <h1><span style="color:#1040a7"><b>Accounting for Dependencies in Deep Learning based Multiple Instance Learning for Whole Slide Imaging</b></span></h1>
  </div>
  
  <div class="post-author print-post-author">
    <span>Kitty K. Wong</span>
  </div>
  <hr>
    
   
  <div class="post-content">
    <!--
    <div class="post-categories">
      <h2><span style="color:#1040a7">Paper Topic(s):</span></h2> 
      
      
      <a 
        href="kittywong/categories#Computational (Integrative) Pathology"
        class="post-category">
        Computational (Integrative) Pathology
      </a>
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Interpretability / Explainability"
        class="post-category">
        Machine Learning - Interpretability / Explainability
      </a>
      
      <a 
        href="kittywong/categories#Machine Learning - Weakly supervised learning"
        class="post-category">
        Machine Learning - Weakly supervised learning
      </a>
      
      <a 
        href="kittywong/categories#Modalities - Histopathology"
        class="post-category">
        Modalities - Histopathology
      </a>
      
    
    </div>
    <div class="post-tags">
      
      <h2><span style="color:#1040a7">Author(s):</span></h2> 
      
      <a href="kittywong/tags#Myronenko, Andriy"
        class="post-tags">
        Myronenko, Andriy
      </a> |  
      
      <a href="kittywong/tags#Xu, Ziyue"
        class="post-tags">
        Xu, Ziyue
      </a> |  
      
      <a href="kittywong/tags#Yang, Dong"
        class="post-tags">
        Yang, Dong
      </a> |  
      
      <a href="kittywong/tags#Roth, Holger R."
        class="post-tags">
        Roth, Holger R.
      </a> |  
      
      <a href="kittywong/tags#Xu, Daguang"
        class="post-tags">
        Xu, Daguang
      </a> |  
      
    </div>
  -->
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    .sidenav {
      width: 200px;
      position: fixed;
      z-index: 1;
      top: 150px;
      left: 50px;
      background: #eee;
      overflow-x: hidden;
      padding: 0px 0;
    }
  
    .sidenav a {
      padding: 6px 8px 6px 16px;
      text-decoration: none;
      font-size: 16px;
      color: #2196F3;
      color: #FF4500;
      display: block;
    }
  
    .sidenav a:hover {
      color: #064579;
    }
    </style>
  </head>

  <div class="sidenav">
    <a href="#author-id">Paper Info</a>
    <a href="#review-id">Reviews</a>
    <a href="#metareview-id">Meta-Review(s)</a>
    <a href="#authorFeedback-id">Author Feedback</a>
    <a href="">Back to top</a>
    <a href="https://kittywong.github.io">Back to List of papers</a>
  </div>

    <table>
  <tbody>
    <tr>
      <td><a href="#author-id"><span style="color:#ff9900"><b>Paper Info</b></span></a></td>
      <td><a href="#review-id"><span style="color:#ff9900"><b>Reviews</b></span></a></td>
      <td><a href="#metareview-id"><span style="color:#ff9900"><b>Meta-review(s)</b></span></a></td>
      <td><a href="#authorFeedback-id"><span style="color:#ff9900"><b>Author Feedback</b></span></a></td>
    </tr>
  </tbody>
</table>

<h1 id="author-id">Authors</h1>
<p>Andriy Myronenko, Ziyue Xu, Dong Yang, Holger R. Roth, Daguang Xu
<br /><br /></p>

<h1 id="abstract-id">Abstract</h1>
<p>Multiple instance learning (MIL) is a key algorithm for classification of whole slide images (WSI). Histology WSIs can have billions of pixels, which create enormous computational and annotation challenges. Typically, such images are divided into a set of patches (a bag of instances), where only bag-level class labels are provided. Deep learning based MIL methods calculate instance features using convolutional neural network (CNN). Our proposed approach is also deep learning based, with the following two contributions: Firstly, we propose to explicitly account for dependencies between instances during training by embedding self-attention Transformer blocks to capture dependencies between instances. For example, a tumor grade may depend on the presence of several particular patterns at different locations in WSI, which requires to account for dependencies between patches. Secondly, we propose an instance-wise loss function based on instance pseudo-labels. We compare the proposed algorithm to multiple baseline methods, evaluate it on the PANDA challenge dataset, the largest publicly available WSI dataset with over 11K images, and demonstrate state-of-the-art results. 
<br /><br /></p>

<h1 id="link-id">Link to paper</h1>
<p><a href="https://doi.org/10.1007/978-3-030-87237-3_32">https://doi.org/10.1007/978-3-030-87237-3_32</a>
<br /><br /></p>

<h1 id="code-id">Link to the code repository</h1>
<p>N/A
<br /><br /></p>

<h1 id="dataset-id">Link to the dataset(s)</h1>
<p>N/A
<br /><br /></p>

<hr />
<h1 id="review-id">Reviews</h1>

<h3 id="review-1">Review #1</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>A self-attention Transformer blocks is proposed to be embedded in model to capture dependencies between instances.</p>

      <p>An instance-wise loss function based on instance pseudo-labels is proposed to increase instances’ labels.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>-This paper proposed to explicitly account for dependencies between instances during training by embedding self-attention Transformer blocks.</p>

      <p>-It generated instance-level pseudo label to add instance level loss supervision.
-The weight attention selection strategy improved the quality of pseudo labels.</p>

    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>1.
The use of transformer in the article is not new, many similar methods have been proposed both in natural image and medical image[1,2,3,4].
The difference exists in the variant of a deeper integration of the transformer with the 
backbone CNN, but there is still only common operation.
2.
The experimental part contains experiment compared with other methods and ablation study but no analytical corroboration experiment.
For example, replace transformer with non-local, the model can also learn the relationship between instances, but there is no experimental comparison.
3.
This paper proposed a method to improve the performance of model, but lacks the mechanistic explanation of why the performance of the model is improved.</p>

      <p>[1]Medical Transformer: Gated Axial-Attention for Medical Image Segmentation
[2]An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale
[3]End-to-End Object Detection with Transformers
[4]Pre-Trained Image Processing Transformer</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The reproducibility is good because the description of the method details is very clear</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>Please further explain the difference between adding transformer and classic non-local.
And further elaborate on the novelty of the article, and add analytical experiments to support the author’s point of view.</p>

    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>borderline reject (5)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <ul>
        <li>Although this article proposed a model that did not exist before the WSI task, and proved the effectiveness of its method through experiments, the analysis of the experiment was not sufficient.</li>
        <li>At the same time, the novelty of this paper is limited.</li>
        <li>The proposed method is similarity to other methods already exists, for example it is similar to the transformer usage in natural images and pseudo-label usage in semi-supervised images.</li>
      </ul>

    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>6</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-2">Review #2</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>This paper used embedding self-attention Transformer blocks to capture dependencies between instances for explicitly account for dependencies between instances, which is able to make better use of the spatial information between patches. And Instance-wise loss based on instance pseudo-labels has also been developed for conquering gradient vanishing.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>This paper is a novel applicatio introduced transformer into MIL to capture dependencies between instances for explicitly.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>The problem of how transformer accounting for dependency among instances not thoroughly analyzed.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Good</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>Since the code will not be available in the  reproducibility checklist. However, the details of model have been provided in the paper, it may reproduce the results of the paper</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>The paper evaluated the transformer module can help improve the performance of deep learning-based MIL for WSI classification. Whie it should give more discussion on how transformer capture dependencies between instances and more ablation experiments should perfomed to evaluate the instance-wise loss.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>Experiments results gives the best performance compared with other MIL methods and have been top three winning teams of the PANDA Kaggle challenge.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>3</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Very confident</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h3 id="review-3">Review #3</h3>

<ul>
  <li><strong>Please describe the contribution of the paper</strong>
    <blockquote>
      <p>The paper highlights a common weakness in the multiple instance learning framework which is that instances are normally assumed to be independent. To address this, the authors propose using transformer encoding blocks to achieve cross instance communication. The authors also propose using pseudo-labels during training to improve signal for training the neural network.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting.</strong>
    <blockquote>
      <p>This method of obtaining cross instance communication is novel and works well as is demonstrated by the experiments section. The paper is well written and easy to understand. The use of transformers is well motived by the literature review.</p>
    </blockquote>
  </li>
  <li><strong>Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work.</strong>
    <blockquote>
      <p>Pseudo-labeling appears to only seems to add marginal improvements to the results, an analysis deeper than just the QWK would be interesting.</p>
    </blockquote>
  </li>
  <li><strong>Please rate the clarity and organization of this paper</strong>
    <blockquote>
      <p>Excellent</p>
    </blockquote>
  </li>
  <li><strong>Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance</strong>
    <blockquote>
      <p>The algorithm is relatively complex given the additional step of constructing pseudo labels but the majority of hyper-parameters are given</p>
    </blockquote>
  </li>
  <li><strong>Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer’s guide on what makes a good review: <a href="https://miccai2021.org/en/REVIEWER-GUIDELINES.html">https://miccai2021.org/en/REVIEWER-GUIDELINES.html</a></strong>
    <blockquote>
      <p>More analysis on why pseudo-labeling is useful during training would be interesting. Additionally, confusion matrices for the different models would also add more clarity then just the QWK.</p>
    </blockquote>
  </li>
  <li><strong>Please state your overall opinion of the paper</strong>
    <blockquote>
      <p>accept (8)</p>
    </blockquote>
  </li>
  <li><strong>Please justify your recommendation. What were the major factors that led you to your overall score for this paper?</strong>
    <blockquote>
      <p>The contribution is interesting to the community and well presented, the results show the model works better than other approaches.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your review stack?</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
  <li><strong>Number of papers in your stack</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Reviewer confidence</strong>
    <blockquote>
      <p>Confident but not absolutely certain</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<hr />
<h1 id="metareview-id">Meta-Review(s)</h1>

<h2 id="primary-meta-review">Primary Meta-Review</h2>
<ul>
  <li><strong>Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers’ recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal.</strong>
    <blockquote>
      <p>The paper integrates visual transformer within a multiple instance learning framework to model the interdependence of the instance. Additionally, it also proposes a pseudo instance labelling scheme or conquering gradient vanishing. The paper is generally well written, with a clear explanation of the methodology motivation, network architecture, experimental results and ablation studies. The evaluation dataset is relatively large and comprehensive and the comparison is also fair (particularly to other top-ranked 
teams are convincing). The paper integrates visual transformer within a multiple instance learning framework to model the interdependence of the instance. 
Additionally, it also proposes a pseudo instance labelling scheme or conquering gradient vanishing. The paper is generally well written, 
with a clear explanation of the methodology motivation, network architecture, experimental results and ablation studies. The evaluation 
dataset is relatively large and comprehensive and the comparison is also fair (particularly to other top-ranked 
teams are convincing). Yet R1 and R2 has some concerns about how transformer account for dependency among instances (see R1 
&amp;R2’s main weakness) and this needs to be addressed in the rebuttal. Additionally, 
given the complexity of the method, I think release code will greatly improve its reproducibility.</p>
    </blockquote>
  </li>
  <li><strong>What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The authors have nicely addressed the major concerns of the reviewers and I thereby recommend the paper acceptance.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>4</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="meta-review-2">Meta-Review #2</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>This work proposes to use transformers and attention to model dependencies between instances for multi-instance learning. The transformer is integrated with a backbone CNN to model inter-instance dependencies. Evaluation is on a large whole slide image (WSI) dataset for prostate cancer. Good performance of the proposed method is reported. There were some concerns that were raised during the review, in particular, related to the performance of non-local approaches and what the transformer approach captured / how it worked. These were, in my opinion, well addressed in the rebuttal. While there was some criticism regarding novelty in the reviews, the approach has novel aspects and shows compelling results which is appreciated.</p>
    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>5</p>
    </blockquote>
  </li>
</ul>

<p><br /><br /></p>
<h2 id="meta-review-3">Meta-Review #3</h2>
<ul>
  <li><strong>Please provide your assessment of the paper taking all information into account, including rebuttal. Highlight the key strengths and weaknesses of the paper, clarify how you reconciled contrasting review comments and scores,  indicate if concerns were successfully addressed in the rebuttal, and provide a clear justification of your decision. If you disagree with some of the (meta)reviewer statements, you can indicate so in your meta-review. Please make sure that the authors, program chairs, and the public can understand the reason for your decision.</strong>
    <blockquote>
      <p>The paper introduces Transformer blocks into a Multiple Instance Learning framework for Whole Slide Imaging analysis. It presents strong experimental results on the PANDA Kaggle challenge. The rebuttal adequately addresses the main concerns expressed by the reviewers. Experimental details and ablations were included, and an extended motivation to use Transformers to model instance dependencies. The only missing reference pointed out by R1 with medical images is an unpublished preprint. The other three references should be discussed in the related work review. In summary,  sufficient technical novelty and competitive results on a public benchmark are adequate for acceptance.</p>

    </blockquote>
  </li>
  <li><strong>After you have reviewed the rebuttal, please provide your final rating based on all reviews and the authors’ rebuttal.</strong>
    <blockquote>
      <p>Accept</p>
    </blockquote>
  </li>
  <li><strong>What is the rank of this paper among all your rebuttal papers? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers).</strong>
    <blockquote>
      <p>1</p>
    </blockquote>
  </li>
</ul>

<h2><br /><br /></h2>
<h1 id="authorFeedback-id">Author Feedback</h1>
<blockquote>
  <p>We’d like to thank all reviewers for their valuable comments. We are grateful for their acknowledgement of the strengths and contributions of our paper. As pointed out by R3 “This method is novel and works well as is demonstrated by the experiments section”, R2 “This paper is a novel application to capture dependencies between instances”.</p>

  <p>We’d like to address how the “transformer account for dependency among instances”: Transformers were initially introduced to capture long range dependencies between words in sentences [1] and later applied to vision [2]. Whereas traditional convolutions are local operation, the self-attention block of Transformers computes attention between all combinations of tokens at a larger range directly.</p>

  <p>R2 asked to analyze ”how transformer accounting for dependency among instances”, R1 asked to explain “why the performance of the model is improved”:
We have inspected the self-attention matrices (inner product of Q and K, normalized). We found that for many tested WSI pathology images, the self-attention matrices have distinct off-diagonal high value elements, indicating the higher weight for certain combinations of instances. In particular, instances with WSI tumor cells of different Gleason scores have higher off-diagonal values, indicating that such a combination is valuable for the final classification, which was captured by the transformer self-attention. We’ll add the visualization of the self-attention matrices to the supplementary materials.</p>

  <p>R1 asked to “replace transformer with non-local blocks to learn the relationship between instances”: We have re-implemented these blocks based on the official paper [3] implementation, and re-ran the experiments for various Non-Local (NL) block configurations.
We considered 4 NL block types from the reference paper (Embedded Gaussian, Gaussian, Dot product and Concatenation). We have replaced the Transformer encoder blocks in our network with the NL blocks (at the end of encoder) and computed the QWK scores:</p>

  <p>NL EmbedGauss (0.947±0.035)
NL Gauss (0.952±0.065)
NL DotProd (0.951±0.043)
NL Concat (0.943±0.023)
Attention MIL(0.948±0.036)
Transformer MIL [ours] (0.960±0.034)</p>

  <p>In all tests the NL blocks performance was similar only to the baseline Attention MIL method, whereas our method demonstrated noticeable improvements.We’ve also tried stacking NL blocks (4x) and embedding them in a feature pyramid, but it didn’t lead to any improvements.Even though, as pointed in non-local NN paper [3], Self-attention can be considered as one of the forms of Non-local operation (Embedded Gaussian), there are differences of its implementation in Transformers ( it’s multiheaded, and it’s followed by MLP and layernorm). We’ll add these (more detailed) additional ablation experiments to the supplementary materials.</p>

  <p>While Reviewer 1 is criticizing the limited novelty of our paper, Reviewers 2 and 3 are acknowledging our novel contributions, e.g. “this method is novel and works well as is demonstrated by the experiments section”. To the best of our knowledge, our work is the first work to introduce Transformers to obtain cross instance communication in Multiple Instance Learning, as well as the first work of its kind for pathology Whole Slide Imaging task and medical images in general. Our work also includes a novel integration of the transformer block at various levels of the feature pyramid of the backbone network. Finally, our paper considers dependencies between large image regions (224x224px); in comparison, the vision transformer [2] uses tiny 16x16px patches, which mostly represent low level features. In our work each instance includes a complete picture, a different cancer cell group. The dependency we want to capture is thus between large complete regions of several tumor subtypes.
[1] Attention Is All You Need: Vaswani et al.
[2] An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale: Dosovitskiy et al.
[3] Non-local Neural Networks: Wang et al.</p>
</blockquote>

<p><br /><br />
<a href=""><span style="color:#ff9900"><b>back to top</b></span></a></p>

<hr />


  </div>

  <div class="post-info">
    <!--
    <div class="post-date">
      Poster presentation date:  
      0831-12-31
      -->
      <!--
      
        ,
        updated at 
        0832-01-01
      
      .
      
    </div>
    -->
    <!--
    
    <div class="post-author">
      Author: Kitty K. Wong
    </div>
    
    -->
    <div class="post-categories">
      <span><b>Topic(s): </b></span>
      
      <a 
        href="kittywong/categories#Computational (Integrative) Pathology"
        class="post-category">
        Computational (Integrative) Pathology
      </a> |
      
      <a 
        href="kittywong/categories#Computer Aided Diagnosis"
        class="post-category">
        Computer Aided Diagnosis
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Attention models"
        class="post-category">
        Machine Learning - Attention models
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Interpretability / Explainability"
        class="post-category">
        Machine Learning - Interpretability / Explainability
      </a> |
      
      <a 
        href="kittywong/categories#Machine Learning - Weakly supervised learning"
        class="post-category">
        Machine Learning - Weakly supervised learning
      </a> |
      
      <a 
        href="kittywong/categories#Modalities - Histopathology"
        class="post-category">
        Modalities - Histopathology
      </a> |
      
    </div>
    <div class="post-tags">
      <!--<span>Author List</span>-->
      <span><b> Author(s): </b></span>
      
      <a href="kittywong/tags#Myronenko, Andriy"
        class="post-category">
        Myronenko, Andriy
      </a> |  
      
      <a href="kittywong/tags#Xu, Ziyue"
        class="post-category">
        Xu, Ziyue
      </a> |  
      
      <a href="kittywong/tags#Yang, Dong"
        class="post-category">
        Yang, Dong
      </a> |  
      
      <a href="kittywong/tags#Roth, Holger R."
        class="post-category">
        Roth, Holger R.
      </a> |  
      
      <a href="kittywong/tags#Xu, Daguang"
        class="post-category">
        Xu, Daguang
      </a> |  
      
    </div>
    <div class="post-other">
      
      <div>
        <span><b>
          Next paper: 
        </b>
        </span>
        <a href="/0832/12/31/Paper2410">
          Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction using Patch-based Graph Convolutional Networks
        </a>
      </div>
      
      
      <div>
        <span><b>
          Previous paper: 
        </b>
        </span>
        <a href="/0830/12/31/Paper2299">
          MorphSet: Improving Renal Histopathology Case Assessment Through Learned Prognostic Vectors
        </a>
      </div>
      
    </div>
    
  </div>
</article>

      </main>
      <footer class="container-footer">
        <div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. Kitty K. Wong, The MICCAI Society
  </span>
</div>
<!--
<div class="footer-copyright">
  <span class="footer-copyright-text float-left">
    Copyright &copy; 2021. example.com.
  </span>
  <span class="footer-copyright-text float-right">
    Powered by <a href="https://jekyllrb.com/">Jekyll</a>, themed by <a href="https://github.com/ghosind/Jekyll-Paper-Github">Jekyll-Paper-Github</a>.
  </span>
</div>
-->

      </footer>
    </div>
  </body>
</html>
